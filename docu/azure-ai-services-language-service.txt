Tell us about your PDF experience.

Azure Language in Foundry Tools
documentation
Learn how to integrate AI into your applications that can extract information, classify text,
understand conversational language, answer questions and more.

OVERVIEW

W H AT ' S N E W

Extract information

Summarize text-based
content

What is Azure
Language in
Foundry Tools?

Use Natural Language
Understanding (NLU) to extract
information from unstructured
text.

ï½… Extract key phrases
ï½… Find linked entities

CONCEPT

What's new?

Summarize lengthy documents
and conversation transcripts.

ï½… Document summarization
ï½… Conversation
summarization

ï½… Named Entity Recognition

Responsible
use of AI

Classify Text
Use Natural Language
Understanding (NLU) to detect
the language or classify the
sentiment of text you have.â€¦

ï½… Analyze sentiment and
mine text for opinions

ï½… Detect Language
ï½… Custom text classification

(NER)

ï½… Personally Identifiable
Information (PII) detection

ï½… Custom Named Entity

Recognition (custom NER)

ï½… Text analytics for health

Answer questions
Provide answers to questions
being asked in unstructured
texts using our prebuilt
capabilities, or customize youâ€¦

ï½… Question answering

Understand
conversations
Create your own models to
classify conversational
utterances and extract detailed
information from them to fulfâ€¦

ï½… Conversational language
understanding

Translate text
Use cloud-based neural
machine translation to build
intelligent, multi-language
solutions for your applications.

ï½… Use machine translation on
text.

ï½… Orchestration workflow

Resources

Support

Pricing

Support and help

Learn

Announced for
Deprecation
Language Understanding
QnA Maker

What is Azure Language in Foundry Tools?
Azure Language is a cloud-based service that provides Natural Language Processing (NLP)
features for understanding and analyzing text. Use this service to help build intelligent
applications using the web-based Language Studio, REST APIs, and client libraries. For AI agent
development, the service capabilities are also available as tools in Azure Language MCP server,
which is available both as a remote server in the Microsoft Foundry Tool Catalog and as a local
server for self-hosted environments.

Available tools
Azure Language provides specialized tools that enable seamless integration between AI agents
and language processing services through standardized protocols.

Azure Language MCP server ðŸ†•
The MCP (Model Context Protocol) server creates a standardized bridge that connects AI
agents directly to Azure Language services through industry-standard protocols. This
integration enables developers to build sophisticated conversational applications with reliable
natural language processing capabilities while ensuring enterprise-grade compliance, data
protection, and processing accuracy throughout their AI workflows.
Azure Language provides both remote and local MCP server options:
Remote server: Available through Foundry Tool Catalog for cloud-hosted deployments.
Local server: Available for developers who prefer to host the server in their own
environment.
For more information, see Azure Language MCP server.

Available agents
Azure Language offers prebuilt agents that handle specific conversational AI scenarios with
built-in governance, routing logic, and quality control mechanisms.

Azure Language Intent Routing agent ðŸ†•
The Intent Routing agent intelligently manages conversation flows by understanding user
intentions and delivering accurate responses in conversational AI applications. This agent uses
predictable decision-making processes combined with controlled response generation to
ensure consistent, reliable interactions that organizations can trust and monitor.

For more information, see Azure Language Intent Routing agent.

Azure Language Exact Question Answering agent ðŸ†•
The Exact Question Answering agent provides reliable, word-for-word responses to your most
important business questions. This agent automates frequently asked questions while
maintaining human oversight and quality control to ensure accuracy and compliance.
For more information, see Azure Language Exact Question Answering agent.

Available features
This Language service unifies the following previously available Foundry Tools: Text Analytics,
QnA Maker, and LUIS. If you need to migrate from these services, see the migration section.
The Language also provides several new features as well, which can either be:
Preconfigured, which means the AI models that the feature uses aren't customizable. You
just send your data, and use the feature's output in your applications.
Customizable, which means you train an AI model using our tools to fit your data
specifically.
îª€ Tip
Unsure which feature to use? See Which Language feature should I use to help you
decide.
Foundry

enables you to use most of the following service features without needing to write

code.

Named Entity Recognition (NER)

ï Š

Named entity recognition identifies different entries in text and categorizes them into
predefined types.

Personal and health data information detection
ï¼‰ Important
The Azure Language in Foundry Tools Text Personally Identifiable Information (PII)
detection anonymization feature (synthetic replacement) is currently available in preview
and licensed to you as part of your Azure subscription. Your use of this feature is subject
to the terms applicable to Previews as described in the Supplemental Terms of Use for
Microsoft Azure Previews
Addendum (DPA)

.

and the Microsoft Products and Services Data Protection

ï Š

Personally Identifiable Information (PII) detection identifies entities in text and conversations
(chat or transcripts) that are associated with individuals.

Language detection

ï Š

Language detection evaluates text and detects a wide range of languages and variant dialects.

Sentiment Analysis and opinion mining

ï Š

Sentiment analysis and opinion mining preconfigured features that help you understand public
perception of your brand or topic. These features analyze text to identify positive or negative
sentiments and can link them to specific elements within the text.

Summarization

ï Š

Summarization condenses information for text and conversations (chat and transcripts). Text
summarization generates a summary, supporting two approaches: Extractive summarization
creates a summary by selecting key sentences from the document and preserving their original
positions. In contrast, abstractive summarization generates a summary by producing new,
concise, and coherent sentences or phrases that aren't directly copied from the original
document. Conversation summarization recaps and segments long meetings into timestamped
chapters. Call center summarization summarizes customer issues and resolution.

Key phrase extraction

ï Š

Key phrase extraction is a preconfigured feature that evaluates and returns the main concepts
in unstructured text, and returns them as a list.

Entity linking
ï¼‰ Important
Entity Linking is retiring from Azure Language in Foundry Tools effective September 1,
2028. After this date, the Entity Linking feature is no longer supported. During the support
window, we recommend that users migrate existing workloads and direct all new projects
to Azure Language Named Entity Recognition or consider other alternative solutions.

ï Š

Entity linking is a preconfigured feature that disambiguates the identity of entities (words or
phrases) found in unstructured text and returns links to Wikipedia.

Text analytics for health

ï Š

Text analytics for health Extracts and labels relevant health information from unstructured text.

Custom text classification

ï Š

Custom text classification enables you to build custom AI models to classify unstructured text
documents into custom classes you define.

Custom Named Entity Recognition (Custom NER)

ï Š

Custom NER enables you to build custom AI models to extract custom entity categories (labels
for words or phrases), using unstructured text that you provide.

Conversational language understanding

ï Š

Conversational language understanding (CLU) enables users to build custom natural language
understanding models to predict the overall intention of an incoming utterance and extract
important information from it.

Orchestration workflow

ï Š

Orchestration workflow is a custom feature that enables you to connect Conversational
Language Understanding (CLU), question answering, and LUIS applications.

Question answering

ï Š

Question answering is a custom feature that identifies the most suitable answer for user inputs.
This feature is typically utilized to develop conversational client applications, including social
media platforms, chat bots, and speech-enabled desktop applications.

Which Language feature should I use?
This section helps you decide which Language feature you should use for your application:
ï¾‰

What do you want to do?

Document
format

Your best solution

Detect and/or redact sensitive
information such as PII and PHI .

Unstructured
text,

PII detection

transcribed
conversations

Expand table

Is this solution
customizable?*

What do you want to do?

Document

Your best solution

format
Extract categories of information

Unstructured text

without creating a custom model.
Extract categories of information

Is this solution
customizable?*

The preconfigured
NER feature

Unstructured text

Custom NER

Unstructured text

Key phrase extraction

Determine the sentiment and
opinions expressed in text.

Unstructured text

Sentiment analysis
and opinion mining

Summarize long chunks of text or
conversations.

Unstructured
text,

Summarization

âœ“

using a model specific to your data.
Extract main topics and important
phrases.

transcribed
conversations.
Disambiguate entities and get links

Unstructured text

Entity linking

Unstructured text

Custom text

to Wikipedia.
Classify documents into one or more
categories.
Extract medical information from

âœ“

classification
Unstructured text

clinical/medical documents, without
building a model.

Text analytics for
health

Build a conversational application
that responds to user inputs.

Unstructured
user inputs

Question answering

Detect the language that a text was

Unstructured text

Language detection

Predict the intention of user inputs

Unstructured

Conversational

and extract information from them.

user inputs

language
understanding

Connect apps from conversational
language understanding, LUIS, and

Unstructured
user inputs

Orchestration
workflow

âœ“

written in.
âœ“

âœ“

question answering.

* If a feature is customizable, you can train an AI model using our tools to fit your data
specifically. Otherwise a feature is preconfigured, meaning the AI models it uses can't be
changed. You just send your data, and use the feature's output in your applications.

Migrate from Text Analytics, QnA Maker, or
Language Understanding (LUIS)
Azure Language unifies three individual Languages in Foundry Tools - Text Analytics, QnA
Maker, and Language Understanding (LUIS). If you have been using these three services, you
can easily migrate to the new Azure Language. For instructions see Migrating to Azure
Language.

Tutorials
After you get started with Azure Language quickstarts, try our tutorials that show you how to
solve various scenarios.
Extract key phrases from text stored in Power BI
Use Power Automate to sort information in Microsoft Excel
Use Flask to translate text, analyze sentiment, and synthesize speech
Use Foundry Tools in canvas apps
Create an FAQ Bot

Code samples
You can find more code samples on GitHub for the following languages:
C#
Java
JavaScript
Python

Deploy on premises using Docker containers
Use Language containers to deploy API features on-premises. These Docker containers enable
you to bring the service closer to your data for compliance, security, or other operational
reasons. The Language offers the following containers:
Sentiment analysis
Language detection
Key phrase extraction
Custom Named Entity Recognition
Text Analytics for health
Summarization

Responsible AI
An AI system includes not only the technology, but also the people who use it, the people
affected by it, and the deployment environment. Read the following articles to learn about
responsible AI use and deployment in your systems:
Transparency note for Azure Language
Integration and responsible use
Data, privacy, and security

Last updated on 11/18/2025

What's new in Azure Language in Foundry
Tools?
Azure Language in Foundry Tools is updated on an ongoing basis. Bookmark this page to stay
up to date with release notes, feature enhancements, and our newest documentation.

December 2025
Azure Language .NET SDK preview release. New .NET SDK packages with support for the
2025-11-15-preview API are now available:
Azure.AI.Language.Text 1.0.0-beta.4
Azure.AI.Language.Conversation.Authoring 2.0.0-beta.5
Language Studio deprecation. Azure Language Studio is scheduled for deprecation in the
near future. All existing features, along with upcoming enhancements, will be accessible
through Microsoft Foundry. If you need guidance on exporting your projects from
Language Studio, see Export project.

November 2025
Azure Language integrates with Foundry Tools. Azure Language now provides specialized
tools and agents for building conversational AI applications in Foundry:
Azure Language MCP server. Connects AI agents to Azure Language services through the
Model Context Protocol.
Azure Language Intent Routing agent. Manages conversation flows by combining intent
classification with answer delivery.
Azure Language Exact Question Answering agent. Delivers consistent responses to
frequently asked business questions.
Azure Language capabilities now available in Foundry. Several Azure Language capabilities
are now available with the Foundry:
Conversational Language Understanding multi-turn conversations. Enable natural,
context-aware dialogues through entity slot filling â†’ Microsoft Foundry (new).
Language detection. Automatically detect the language of user utterances in
conversational applications â†’ Microsoft Foundry (new).
PII detection for text. Detect and redact personally identifiable information in text
documents â†’ Microsoft Foundry (new).

Custom Named Entity Recognition. Test, train, and deploy custom NER models directly in
the Foundry playground â†’ Microsoft Foundry (classic).
PII detection for conversations. Identify and redact personally identifiable information in
conversations â†’ Microsoft Foundry (classic).
Text PII detection enhancements (2025-11-15-preview API). The preview API introduces
several new feature parameters for PII detection:
Anonymization. The syntheticReplacement redaction policy enables masking detected PII
entities with synthetic replacement values. For example, "John Doe received a call from
424-878-9193" can be transformed into "Sam Johnson received a call from 401-2556901."
Disable type-validation enforcement. Disable entity type validation to bypass strict
validation when operational efficiency is prioritized over data integrity checks.
Confidence threshold score. Set a minimum confidence score threshold to control which
entities appear in the output based on detection confidence.
Entity Tags generally available. Entity Tags are now generally available, providing enhanced
metadata and categorization for named entities.
New preview model for PII detection. The updated preview model introduces support for the
following new entity types:
Airport
City
Geopolitical Entity
South Korea Drivers License Number
South Korea Passport Number
South Korea Social Security Number
Location
State
ZipCode
Model improvements. Significant quality improvements for the following entity types:
Date Of Birth
License Plate
Sort Code
VIN

October 2025

Summarization model 2025-06-10 generally available. The Summarization model version
2025-06-10 is now generally available. This model is fine-tuned using the Phi open model
family , delivering enhanced performance for Issue and Resolution summary generation.
Expanded Azure Language in Foundry Tools MCP server capabilities. The Model Context
Protocol (MCP) server for Azure Language now provides eight additional NLP tools:
Named Entity Recognition, Text Analytics for health, Conversational Language
Understanding, Custom Question Answering, Language Detection, Sentiment Analysis,
Summarization, and Key Phrase Extraction. These tools complement the existing PII
Detection capability.

September 2025
Introducing CQA deploy-to-agent. Custom Question Answering (CQA) projects can now be
deployed as intelligent agents directly within the Foundry playground through a streamlined
deployment experience.
This feature enables users to transform fine-tuned CQA knowledge bases into
production-ready agents with minimal configuration steps.
The deployment process provides parity with CLU workflows and accelerates the agent
development timeline within the unified Foundry environment.
Custom Named Entity Recognition (NER) capabilities integrated into Language Playground.
Users can now access a testing playground for custom Named Entity Recognition (NER) within
Foundry.
This interactive interface allows training, deployment, testing, and fine-tuning for custom
models while experimenting with custom NER capabilities in real-time.
The playground accelerates the onboarding process and provides enhanced debugging
capabilities for custom NER implementations. For more information, see Quickstart:
Custom named entity recognition.
New Python SDKs. The new Python SDKs azure-ai-textanalytics 6.0.0b1
textanalytics-authoring 1.0.0b1

and azure-ai-

are now available:

azure-ai-textanalytics 6.0.0b1 offers runtime APIs that enable users to utilize various
prebuilt features within Azure Language, such as sentiment analysis, named entity
recognition (NER), language detection, key phrase extraction, text PII detection, Text
Analytics for health, and document summarization.
Additionally, the SDK can be used to access inference APIs for custom NER and text

classification models. This release supports the latest 2025-05-15-preview API, and
previous versions. The 2025-05-15-preview API introduces several new capabilities:
Added support for new entity types in Named Entity Recognition (NER) and Text PII
detection: DateOfBirth, BankAccountNumber, PassportNumber, and
DriversLicenseNumber.
Enhanced functionality allows users to define values to be excluded from the results
produced by Text PII detection.

August 2025
Release of new Text PII and NER model (2025-08-01-preview). This new preview model
version introduces broader functionality and expanded capabilities for Text personal
information identification (PII) and named entity recognition (NER) services:
Expanded language support for DateOfBirth entity. The DateOfBirth entity, which
initially supported English only, now includes Tier 1 language coverage. This expansion
supports French, German, Italian, Spanish, Portuguese, Brazilian Portuguese, and Dutch,
ensuring broader international applicability.
Two new entity types added:
SortCode: A financial identifier used in the UK and Ireland to specify the bank and
branch associated with an account.
LicensePlateNumber: Support is now available for standard alphanumeric vehicle
identification codes. At this time, license plates that consist exclusively of letters aren't
supported.
Improved AI accuracy in financial entity recognition. The 2025-08-01-preview model is
further optimized to minimize both false positives and false negatives in financial entity
recognition, resulting in greater accuracy and reliability.
New Python SDK release: azure-ai-language-conversations 2.0.0b1. The latest Python SDK,
azure-ai-language-conversations 2.0.0b1, is now available and supports the 2025-15-05preview REST API for conversation runtime.
Conversational Language Understanding (CLU) inference now allows for seamless
integration with advanced large-scale language models, providing real-time recognition
of user intent without the need for extra model training.
Enhanced intent prediction capabilities enable support for complex, multi-turn
conversations. These advancements contribute to greater sophistication in conversational
AI systems and, as a result, workflow automation processes are improved.

July 2025
Expanded .NET SDK support for text and conversation authoring APIs.
Azure.AI.Language.Text.Authoring 1.0.0-beta.2

now supports project import with raw

JSON string for custom NER and custom text classification.
Azure.AI.Language.Conversation.Authoring 1.0.0-beta.2

introduces new authoring

capabilities in the 2025-15-05-preview API, including LLM-based CLU intent authoring, a
quick-deploy feature, and multi-turn CLU model training with autogenerated synthetic
data.
Azure.AI.Language.Text.Authoring 1.0.0-beta.2

and

Azure.AI.Language.Conversation.Authoring 1.0.0-beta.2

.NET SDK versions support

the following REST APIs:
2025-15-05-preview (latest/default)
2023-04-01
2023-04-15-preview
2024-11-15-preview

June 2025
New version of the Conversational Language Understanding (CLU) training configuration.
This new version is aimed at minimizing over-predictions of the None intentâ€”particularly in
multilingual contextsâ€”is now available via the REST API using trainingConfigVersion 2025-0701-preview. For more information, see Train your model: request body data.
Updated Build your conversational agent

accelerator project. The update includes a new

routing strategyâ€”TRIAGE_AGENT. This strategy uses an agent hosted on Foundry Agent
Service. It utilizes Conversational Language Understanding (CLU) and Custom Question
Answering (CQA) as tools to triage user intent for downstream agent routing. Additionally,
these tools help deliver precise answers to specific questions. For more information, see
TechCommunity Blog Post
.NET SDKs support. The following .NET SDK s are now available, and support the latest REST
API version 2025-15-05-preview:
Azure.AI.Language.Text 1.0.0-beta.3

provides inference capabilities for a wide range of

language processing tasks. These tasks include language detection, sentiment analysis,
key phrase extraction, and named entity recognition (NER). The capabilities also include
recognizing and linking personally identifiable information (PII) entities. Additionally, they

offer text analytics for healthcare, custom named entity recognition (NER), and custom
text classification. Both extractive and abstractive text summarization are also supported.
Azure.AI.Language.Conversation 2.0.0-beta.3

provides inference capabilities for

conversational PII, conversational language understanding (CLU), and conversation
summarization.
Text PII GPU container is now available for integration. You can access this container on the
Microsoft Artifact Registry

using the tag gpu .

May 2025
2025-05-15-preview release. The latest API preview version includes updates for named entity
recognition (NER) and PII detection:
New entity type support for DateOfBirth , BankAccountNumber , PassportNumber , and
DriversLicenseNumber .

Improved AI quality for PhoneNumber entity type.
New agent templates. Azure Language now supports the following agent templates:
Intent routing: Detects user intent and provides precise answers, ideal for deterministic
intent routing, and exact question answering with human oversight.
Exact question answering: Delivers consistent, accurate responses to high-value
predefined questions through deterministic methods.
PII detection enhancements. Azure Language introduces new customization and entity
subtype features for PII detection:
Customize PII detection using your own regex (Text PII container only).
Specify values to exclude from PII output.
Use entity synonyms for tailored PII detection.
Enhanced CLU and CQA Capabilities in Foundry. Foundry now offers enhanced capabilities for
fine-tuning with custom conversational language understanding (CLU) and conversational
question-and-answer (CQA) AI features:
CLU and CQA authoring tools are now available in Foundry.
CLU offers a quick deploy option powered by large language models (LLMs) for rapid
deployment.
CQA incorporates the QnA Maker scoring algorithm for more accurate responses.
CQA enables exact match answering for precise query resolutions.
For more updates, see our latest TechCommunity Blog Post

.

April 2025
Updated and improved model GA release for NER
Expanded context window for PII redaction â€“ This service update expands the window of
detection the PII redaction service considers, enhancing quality and accuracy.
/python/api/azure-cognitiveservices-language-luis/index* Added prediction capability for
custom models, including conversational language Understanding (CLU), custom named
entity recognition (NER), and custom text classification, are now available in three new
regions: Jio India Central, UK West, and Canada East.
Scanned PDF PII. Document input for PII redaction now supports scanned PDFs, enabling
PII detection and redaction in both digital and nondigital documents using OCR .

March 2025
Azure Language resource now can be deployed to three new regions, Jio India Central,
UK West, and Canada East, for the following capabilities:
Language detection
Sentiment analysis
Key phrase extraction
Named entity recognition (NER)
Personally identifiable information (PII) entity recognition
Entity linking
Text analytics for health
Extractive text summarization
Back-end infrastructure for the Named entity recognition (NER) and Text Personally
identifiable information (PII) entity recognition models is now updated with extended
context window limits.
Our Conversational PII redaction service is now powered by an upgraded GA model. This
revised version enhances the quality and accuracy of Credit Card Number entities and
Numeric Identification entities. These entities include Social Security numbers, Driver's
license numbers, Policy numbers, Medicare Beneficiary Identifiers, and Financial account
numbers.

February 2025
Document and text abstractive summarization is now powered by fine-tuned Phi-3.5mini! Check out the Announcing Blog

for more information.

More skills are available in Foundry

: Extract key phrase, Extract named entities, Analyze

sentiment, and Detect language. More skills are yet to come.

January 2025
.NET SDK for Azure Language text analytics, Azure.AI.Language.Text 1.0.0-beta.2

, is now

available. This client library supports the latest REST API version, 2024-11-01 , and 202411-15-preview , for the following features:

Language detection
Sentiment analysis
Key phrase extraction
Named entity recognition (NER)
Personally identifiable information (PII) entity recognition
Entity linking
Text analytics for health
Custom named entity recognition (Custom NER)
Custom text classification
Extractive text summarization
Abstractive text summarization
Custom sentiment analysis (preview), custom text analytics for health (preview) and
custom summarization (preview) were retired on January 10, 2025, as Azure AI features
are constantly evaluated based on customer demand and feedback. Based on the
customers' feedback of these preview features, Microsoft is retiring this feature and
prioritize new custom model features using the power of generative AI to better serve
customers' needs.

November 2024
Azure Language is moving to Foundry

. These skills are now available in Foundry

playground: Extract health information, Extract PII from conversation, Extract PII from text,
Summarize text, Summarize conversation, Summarize for call center. More skills follow.
Runtime Container for Conversational Language Understanding (CLU) is available for onpremises connections.
Both our Text PII redaction service and our Conversational PII service preview API (version
2024-11-15-preview) now support the option to mask detected sensitive entities with a
label beyond just redaction characters. Customers can specify if personal data content
such as names and phone numbers, that is, "John Doe received a call from 424-878-9192"
are masked with a redaction character, that is, "******** received a call from ************"
or masked with an entity label, that is, " PERSON_1 received a call from PHONENUMBER_1 ."

More on how to specify the redaction policy style for your outputs can be found in our
how-to guides.
Native document support gating is removed with the latest API version, 2024-11-15preview, allowing customers to access native document support for PII redaction and
summarization. Key updates in this version include:
Increased Maximum File Size Limits (from 1 MB to 10 MB).
Enhanced PII Redaction Customization: Customers can now specify whether they want
only the redacted document or both the redacted document and a JSON file
containing the detected entities.
Language detection is a built-in feature designed to identify the language in which a
document is written. It provides a language code that corresponds to a wide array of
languages. This feature includes not only standard languages but also their variants,
dialects, and certain regional or cultural languages. Today the general availability of script
detection capability, and 16 more languages support, which adds up to 139 total
supported languages is announced.
Named Entity Recognition service, Entity Resolution was upgraded to the Entity Metadata
starting in API version 2023-04-15-preview. If you're calling the preview version of the API
equal or newer than 2023-04-15-preview, check out the Entity Metadata article to use the
resolution feature. The service now supports the ability to specify a list of entity tags to be
included into the response or excluded from the response. If a piece of text is classified as
more than one entity type, the overlapPolicy parameter allows customers to specify how
the service handles the overlap. The inferenceOptions parameter enables users to modify
the inference process, such as preventing detected entity values from being normalized
and added to the metadata. Along with these optional input parameters, we support an
updated output structure (with new fields tags, type, and metadata) to ensure enhanced
user customization and deeper analysis Learn more on our documentation.
Text Analytics for Health (TA4H) is a specialized tool designed to extract and categorize
key medical details from unstructured sources. These sources include doctor's notes,
discharge summaries, clinical documentation, and electronic health records. Today, we
released support for Fast Healthcare Interoperability Resources (FHIR) structuring and
temporal assertion detection in the Generally Available API.

October 2024
Custom Language features enable you to deploy your project to multiple resources within
a single region via the API.

September 2024

PII detection now has container support. See more details in the Azure Update post:
Announcing Text PII Redaction Container Release

.

Custom sentiment analysis (preview) will be retired January 10, 2025. You can transition to
other custom model training services, such as custom text classification in Azure
Language. See more details in the Azure Update post: Retirement: Announcing upcoming
retirement of custom sentiment analysis (preview) in Azure Language (microsoft.com)

.

Custom text analytics for health (preview) will be retired on January 10, 2025. Transition to
other custom model training services, such as custom named entity recognition in Azure
Language, by that date. See more details in the Azure Update post: Retirement:
Announcing upcoming retirement of custom text analytics for health (preview) in Azure
Language (microsoft.com)

.

August 2024
CLU utterance limit in a project increased from 25,000 to 50,000.
CLU new version of training configuration, version 2024-08-01-preview, is available now,
which improves the quality of intent identification for out of domain utterances.

July 2024
Conversational PII redaction

service in English-language contexts is now Generally

Available (GA).
Conversation Summarization now supports 12 added languages in preview as listed here.
Summarization Meeting or Conversation Chapter titles features support reduced length
to focus on the key topics.
Enable support for data augmentation for diacritics to generate variations of training data
for diacritic variations used in some natural languages, which are especially useful for
Germanic and Slavic languages.

February 2024
Expanded language detection support for added scripts according to the ISO 15924
standard

is now available starting in API version 2023-11-15-preview .

January 2024
Native document support is now available in 2023-11-15-preview public preview.

December 2023
Text Analytics for health new model 2023-12-01 is now available.
New Relation Type: BodySiteOfExamination
Quality enhancements to support radiology documents
Significant latency improvements
Various bug fixes: Improvements across NER, Entity Linking, Relations, and Assertion
Detection

November 2023
Named Entity Recognition Container is now Generally Available (GA).

July 2023
Custom sentiment analysis is now available in preview.

May 2023
Custom Named Entity Recognition (NER) Docker containers are now available for onpremises deployment.

April 2023
Custom Text analytics for health is available in public preview, which enables you to build
custom AI models to extract healthcare specific entities from unstructured text
You can now use Azure OpenAI to automatically label or generate data during authoring.
Learn more with the following links:
Autolabel your documents in Custom text classification or Custom named entity
recognition.
Generate suggested utterances in Conversational language understanding.
The latest model version ( 2022-10-01 ) for Language Detection now supports 6 more
International languages and 12 Romanized Indic languages.

March 2023
New model version ('2023-01-01-preview') for Personally Identifiable Information (PII)
detection with quality updates and new language support

New versions of the text analysis client library are available in preview:
C#

Package (NuGet)
Changelog/Release History
ReadMe
Samples

February 2023
Conversational language understanding and orchestration workflow now available in the
following regions in the sovereign cloud for China:
China East 2 (Authoring and Prediction)
China North 2 (Prediction)
New model evaluation updates for Conversational language understanding and
Orchestration workflow.
New model version ('2023-01-01-preview') for Text Analytics for health featuring new
entity categories for social determinants of health.
New model version ('2023-02-01-preview') for named entity recognition features
improved accuracy and more language support with up to 79 languages.

December 2022
New version (v5.2.0-beta.1) of the text analysis client library is available in preview for
C#/.NET:
Package (NuGet)
Changelog/Release History
ReadMe
Samples
New model version ( 2022-10-01 ) released for Language Detection. The new model
version comes with improvements in language detection quality on short texts.

November 2022
Expanded language support for:
Opinion mining

Conversational PII now supports up to 40,000 characters as document size.
New versions of the text analysis client library are available in preview:
Java
Package (Maven)
Changelog/Release History
ReadMe
Samples
JavaScript
Package (npm)
Changelog/Release History
ReadMe
Samples
Python
Package (PyPi)
Changelog/Release History
ReadMe
Samples

October 2022
The summarization feature now has the following capabilities:
Document summarization:
Abstractive summarization, which generates a summary of a document that can't
use the same words as presented in the document, but captures the main idea.
Conversation summarization
Chapter title summarization, which returns suggested chapter titles of input
conversations.
Narrative summarization, which returns call notes, meeting notes or chat summaries
of input conversations.
Expanded language support for:
Sentiment analysis
Key phrase extraction
Named entity recognition
Text Analytics for health
Multi-region deployment and project asset versioning for:
Conversational language understanding
Orchestration workflow
Custom text classification

Custom named entity recognition
Regular expressions in conversational language understanding and required components,
offering an added ability to influence entity predictions.
Entity resolution in named entity recognition
New region support for:
Conversational language understanding
Orchestration workflow
Custom text classification
Custom named entity recognition
Document type as an input supported for Text Analytics for health FHIR requests

September 2022
Conversational language understanding is available in the following regions:
Central India
Switzerland North
West US 2
Text Analytics for Health now supports more languages in preview: Spanish, French,
German Italian, Portuguese and Hebrew. These languages are available when using a
docker container to deploy the API service.
The Azure.AI.TextAnalytics client library v5.2.0 are generally available and ready for use in
production applications. For more information on Language client libraries, see the
Developer overview.
Java
Package (Maven)
Changelog/Release History
ReadMe
Samples
Python
Package (PyPi)
Changelog/Release History
ReadMe
Samples
C#/.NET
Package (NuGet)
Changelog/Release History
ReadMe
Samples

August 2022
Role-based access control for Azure Language.

July 2022
New AI models for sentiment analysis and key phrase extraction based on z-code
models

, providing:

Performance and quality improvements for the following 11 languages supported by
sentiment analysis: ar , da , el , fi , hi , nl , no , pl , ru , sv , tr
Performance and quality improvements for the following 20 languages supported by
key phrase extraction: af , bg , ca , hr , da , nl , et , fi , el , hu , id , lv , no , pl , ro , ru ,
sk , sl , sv , tr

Conversational PII is now available in all Azure regions supported by Azure Language.
A new version of Azure Language API ( 2022-07-01-preview ) is available. It provides:
Automatic language detection for asynchronous tasks.
Text Analytics for health confidence scores are now returned in relations.
To use this version in your REST API calls, use the following URL:
HTTP
<your-language-resource-endpoint>/language/:analyze-text?api-version=2022-0701-preview

June 2022
v1.0 client libraries for conversational language understanding and orchestration
workflow are Generally Available for the following languages:
C#
Python
v1.1.0b1 client library for conversation summarization is available as a preview for:
Python
There's a new endpoint URL and request format for making REST API calls to prebuilt
Language features. See the following quickstart guides and reference documentation for
information on structuring your API calls. All text analytics 3.2-preview.2 API users can
begin migrating their workloads to this new endpoint.
Entity linking
Language detection

Key phrase extraction
Named entity recognition
PII detection
Sentiment analysis and opinion mining
Text analytics for health

May 2022
PII detection for conversations.
Rebranded Text Summarization to Document summarization.
Conversation summarization is now available in public preview.
The following features are now Generally Available (GA):
Custom text classification
Custom Named Entity Recognition (NER)
Conversational language understanding
Orchestration workflow
The following updates for custom text classification, custom Named Entity Recognition
(NER), conversational language understanding, and orchestration workflow:
Data splitting controls.
Ability to cancel training jobs.
Custom deployments can be named. You can have up to 10 deployments.
Ability to swap deployments.
Auto labeling (preview) for custom named entity recognition
Enterprise readiness support
Training modes for conversational language understanding
Updated service limits
Ability to use free (F0) tier for Language resources
Expanded regional availability
Updated model life cycle to add training configuration versions

April 2022
Fast Healthcare Interoperability Resources (FHIR) support is available in the Language
REST API preview for Text Analytics for health.

March 2022

Expanded language support for:
Custom text classification
Custom Named Entity Recognition (NER)
Conversational language understanding

February 2022
Model improvements for latest model-version for text summarization
Model 2021-10-01 is Generally Available (GA) for Sentiment Analysis and Opinion Mining,
featuring enhanced modeling for emojis and better accuracy across all supported
languages.
Question Answering: Active learning v2 incorporates a better clustering logic providing
improved accuracy of suggestions. It considers user actions when suggestions are
accepted or rejected to avoid duplicate suggestions, and improve query suggestions.

December 2021
The version 3.1-preview.x REST endpoints and 5.1.0-beta.x client library are retired.
Upgrade to the General Available version of the API(v3.1). If you're using the client
libraries, use package version 5.1.0 or higher. See the migration guide for details.

November 2021
Based on ongoing customer feedback, we increased the character limit per document for
Text Analytics for health from 5,120 to 30,720.
Azure Language release, with support for:
Question Answering (now Generally Available)
Sentiment Analysis and opinion mining
Key Phrase Extraction
Named Entity Recognition (NER), Personally Identifying Information (PII)
Language Detection
Text Analytics for health
Text summarization preview
Custom Named Entity Recognition (Custom NER) preview
Custom Text Classification preview
Conversational Language Understanding preview

Preview model version 2021-10-01-preview for Sentiment Analysis and Opinion mining,
which provides:
Improved prediction quality.
Added language support for the opinion mining feature.
For more information, see the project z-code site

.

To use this model version, you must specify it in your API calls, using the model version
parameter.
SDK support for sending requests to custom models:
Custom Named Entity Recognition
Custom text classification
Custom language understanding

Next steps
See the previous updates article for service updates not listed here.

Last updated on 12/08/2025

Language support for Language features
Use this article to learn about the languages currently supported by different features.
ï¾‰

Language

Expand table

Language

Custom text

Custom named

Conversational

Entity

Language

Key

Named entity

Orchestration

Person

code

classification

entity
recognition(NER)

language
understanding

linking

detection

phrase
extraction

recognition(NER)

workflow

Identifia
Informa
(PII)

Afrikaans

af

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Albanian

sq

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Amharic

am

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Arabic

ar

âœ“

âœ“

âœ“

âœ“

âœ“

Armenian

hy

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Assamese

as

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Azerbaijani

az

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Basque

eu

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Belarusian

be

âœ“

âœ“

âœ“

âœ“

âœ“

Bengali

bn

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Bosnian

bs

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Breton

br

âœ“

âœ“

âœ“

âœ“

âœ“

Bulgarian

bg

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Burmese

my

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Catalan

ca

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Central Khmer

km

Chinese

zh-hans

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Chinese
(Traditional)

zh-hant

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Corsican

co

Croatian

hr

âœ“

âœ“

âœ“

âœ“

âœ“

Czech

cs

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Danish

da

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Dari

prs

âœ“

Divehi

dv

âœ“

Dutch

nl

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

English (UK)

en-gb

âœ“

âœ“

âœ“

âœ“

English (US)

en-us

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Esperanto

eo

âœ“

âœ“

âœ“

âœ“

âœ“

Estonian

et

âœ“

âœ“

âœ“

âœ“

âœ“

Fijian

fj

Filipino

tl

âœ“

âœ“

âœ“

âœ“

âœ“

Finnish

fi

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

French

fr

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

(Simplified)

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“
âœ“

âœ“

Language

Language
code

Custom text
classification

Custom named
entity
recognition(NER)

Conversational
language
understanding

Entity
linking

Language
detection

Key
phrase
extraction

Named entity
recognition(NER)

Orchestration
workflow

Person
Identifia
Informa
(PII)

Galician

gl

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Georgian

ka

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

German

de

âœ“

âœ“

âœ“

âœ“

âœ“

Greek

el

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Gujarati

gu

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Haitian

ht

Hausa

ha

âœ“

âœ“

âœ“

âœ“

âœ“

Hebrew

he

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Hindi

hi

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Hmong Daw

mww

Hungarian

hu

âœ“

âœ“

âœ“

Icelandic

is

âœ“

Igbo

ig

âœ“

Indonesian

id

Inuktitut

iu

Irish

ga

âœ“

âœ“

âœ“

Italian

it

âœ“

âœ“

Japanese

ja

âœ“

Javanese

jv

Kannada

âœ“

âœ“

âœ“

âœ“

âœ“
âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

kn

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Kazakh

kk

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Khmer

km

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Kinyarwanda

rw

Korean

ko

âœ“

âœ“

âœ“

âœ“

âœ“

Kurdish
(Kurmanji)

ku

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Kyrgyz

ky

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Lao

lo

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Latin

la

âœ“

âœ“

âœ“

âœ“

âœ“

Latvian

lv

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Lithuanian

lt

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Luxembourgish

lb

Macedonian

mk

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Malagasy

mg

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Malay

ms

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Malayalam

ml

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Maltese

mt

âœ“

Maori

mi

âœ“

âœ“

âœ“

âœ“
âœ“

âœ“
âœ“

âœ“

âœ“

Language

Language

Custom text

Custom named

Conversational

Entity

Language

Key

Named entity

Orchestration

Person

code

classification

entity
recognition(NER)

language
understanding

linking

detection

phrase
extraction

recognition(NER)

workflow

Identifia
Informa
(PII)

Marathi

mr

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Mongolian

mn

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Nepali

ne

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Norwegian
(Bokmal)

nb

âœ“

âœ“

âœ“

âœ“

âœ“

Norwegian

no

âœ“

Norwegian
Nynorsk

nn

âœ“

Odia

or

Oromo

om

Pashto

ps

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Persian

fa

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Polish

pl

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Portuguese
(Brazil)

pt-br

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Portuguese
(Portugal)

pt-pt

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Punjabi

pa

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Queretaro
Otomi

otq

Romanian

ro

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Russian

ru

âœ“

âœ“

âœ“

âœ“

âœ“

Samoan

sm

Sanskrit

sa

âœ“

âœ“

âœ“

âœ“

âœ“

Scottish Gaelic

gd

âœ“

âœ“

âœ“

âœ“

âœ“

Serbian

sr

âœ“

âœ“

âœ“

âœ“

âœ“

Shona

sn

Sindhi

sd

âœ“

âœ“

âœ“

âœ“

âœ“

Sinhala

si

âœ“

âœ“

âœ“

âœ“

âœ“

Slovak

sk

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Slovenian

sl

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Somali

so

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Spanish

es

âœ“

âœ“

âœ“

âœ“

âœ“

Sundanese

su

âœ“

âœ“

âœ“

âœ“

âœ“

Swahili

sw

âœ“

âœ“

âœ“

âœ“

âœ“

Swati

ss

Swedish

sv

Tahitian

ty

âœ“

Tajik

tg

âœ“

Tamil

ta

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“
âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“
âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Language

Language
code

Custom text
classification

Custom named
entity

Conversational
language

recognition(NER)

understanding

Entity
linking

Language
detection

Key
phrase

Named entity
recognition(NER)

extraction

Orchestration
workflow

Person
Identifia
Informa
(PII)

Tatar

tt

âœ“

Telugu

te

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Thai

th

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Tibetan

bo

âœ“

Tigrinya

ti

âœ“

Tongan

to

âœ“

Turkish

tr

Turkmen

tk

Ukrainian

uk

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Urdu

ur

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Uyghur

ug

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Uzbek

uz

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Vietnamese

vi

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Welsh

cy

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Western Frisian

fy

âœ“

âœ“

âœ“

âœ“

âœ“

Xhosa

xh

âœ“

âœ“

âœ“

âœ“

âœ“

Yiddish

yi

âœ“

âœ“

âœ“

âœ“

âœ“

Yoruba

yo

âœ“

Yucatec Maya

yua

âœ“

Zulu

zu

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

See also
See the following service-level language support articles for more information on language support:
Custom text classification
Custom named entity recognition(NER)
Conversational language understanding
Entity linking
Language detection
Key phrase extraction
Named entity recognition(NER)
Orchestration workflow
Personally Identifiable Information (PII)
Conversation PII
Question answering
Sentiment analysis
Opinion mining
Text Analytics for health
Summarization
Conversation summarization

Last updated on 11/18/2025

âœ“

Language supported regions
The Language is available for use in several Azure regions. Use this article to learn about the
regional support and limitations.

Region support overview
Typically you can refer to the region support

for details, and most Language capabilities are

available in all supported regions. Some Language capabilities, however, are only available in
select regions.
ï¼— Note
Language doesn't store or process customer data outside the region you deploy the
service instance in.

Conversational language understanding and
orchestration workflow
Conversational language understanding and orchestration workflow are only available in some
Azure regions. Some regions are available for both authoring and prediction, while other
regions are prediction only. Language resources in authoring regions allow you to create, edit,
train, and deploy your projects. Language resources in prediction regions allow you to get
predictions from a deployment.
ï¾‰

Region

Authoring

Prediction

AustraliaEast

âœ“

âœ“

BrazilSouth
CanadaCentral

âœ“
âœ“

CanadaEast
CentralIndia

âœ“
âœ“

CentralUS
ChinaEast2

âœ“

âœ“
âœ“

âœ“

âœ“

Expand table

Region

Authoring

Prediction

ChinaNorth2

âœ“

EastAsia

âœ“

EastUS

âœ“

âœ“

EastUS2

âœ“

âœ“

FranceCentral

âœ“

GermanyWestCentral

âœ“

ItalyNorth

âœ“

JapanEast

âœ“

JapanWest

âœ“

JioIndiaCentral

âœ“

JioIndiaWest

âœ“

KoreaCentral

âœ“

NorthCentralUS

âœ“

NorthEurope

âœ“

âœ“

NorwayEast

âœ“

QatarCentral

âœ“

SouthAfricaNorth

âœ“

SouthCentralUS

âœ“

âœ“

SoutheastAsia

âœ“

SwedenCentral

âœ“

SwitzerlandNorth

âœ“

UAENorth
UKSouth

âœ“
âœ“

âœ“

âœ“

UKWest

âœ“

WestCentralUS

âœ“

WestEurope

âœ“

âœ“

Region

Authoring

WestUS

Prediction
âœ“

WestUS2

âœ“

âœ“

WestUS3

âœ“

âœ“

Custom named entity recognition
Custom named entity recognition is only available in some Azure regions. Some regions are
available for both authoring and prediction, while other regions are prediction only. Language
resources in authoring regions allow you to create, edit, train, and deploy your projects.
Language resources in prediction regions allow you to get predictions from a deployment.
ï¾‰

Region

Authoring

Prediction

AustraliaEast

âœ“

âœ“

BrazilSouth
CanadaCentral

âœ“
âœ“

CanadaEast
CentralIndia

âœ“
âœ“

âœ“

âœ“

CentralUS

âœ“

EastAsia

âœ“

EastUS

âœ“

âœ“

EastUS2

âœ“

âœ“

FranceCentral

âœ“

GermanyWestCentral

âœ“

JapanEast

âœ“

JapanWest

âœ“

JioIndiaCentral

âœ“

JioIndiaWest

âœ“

Expand table

Region

Authoring

Prediction

KoreaCentral

âœ“

NorthCentralUS

âœ“

NorthEurope

âœ“

âœ“

NorwayEast

âœ“

QatarCentral

âœ“

SouthAfricaNorth

âœ“

SouthCentralUS

âœ“

âœ“

SoutheastAsia

âœ“

SwedenCentral

âœ“

SwitzerlandNorth

âœ“

UAENorth
UKSouth

âœ“
âœ“

âœ“

âœ“

UKWest

âœ“

WestCentralUS

âœ“

WestEurope

âœ“

WestUS

âœ“
âœ“

WestUS2

âœ“

âœ“

WestUS3

âœ“

âœ“

Data augmentation feature in Custom Named Entity
Recognition
The data augmentation capability allows users to use an Azure OpenAI model to generate
supplementary training data during the creation of a custom named entity recognition model.
This functionality is currently limited to select regions.
All entity data utilized with data augmentation is required to include a description field.
Supported Azure OpenAI model types for this feature are gpt-4o and gpt-4 32k.
The language resource must also have the Azure OpenAI Contributor role assigned on
the Azure OpenAI resource.

ï¾‰

Region

CNER data augmentation support

EastUS

âœ“

SwitzerlandNorth

âœ“

AustraliaEast

âœ“

Expand table

Custom text classification
Custom text classification is only available in some Azure regions. Some regions are available
for both authoring and prediction, while other regions are prediction only. Language
resources in authoring regions allow you to create, edit, train, and deploy your projects.
Language resources in prediction regions allow you to get predictions from a deployment.
ï¾‰

Region

Authoring

Prediction

AustraliaEast

âœ“

âœ“

BrazilSouth
CanadaCentral

âœ“
âœ“

CanadaEast
CentralIndia

âœ“
âœ“

âœ“

âœ“

CentralUS

âœ“

EastAsia

âœ“

EastUS

âœ“

âœ“

EastUS2

âœ“

âœ“

FranceCentral

âœ“

GermanyWestCentral

âœ“

JapanEast

âœ“

JapanWest

âœ“

JioIndiaCentral

âœ“

Expand table

Region

Authoring

Prediction

JioIndiaWest

âœ“

KoreaCentral

âœ“

NorthCentralUS

âœ“

NorthEurope

âœ“

âœ“

NorwayEast

âœ“

QatarCentral

âœ“

SouthAfricaNorth

âœ“

SouthCentralUS

âœ“

âœ“

SoutheastAsia

âœ“

SwedenCentral

âœ“

SwitzerlandNorth

âœ“

âœ“

UAENorth

âœ“

UKSouth

âœ“

âœ“

UKWest

âœ“

WestCentralUS

âœ“

WestEurope

âœ“

âœ“

WestUS

âœ“

WestUS2

âœ“

âœ“

WestUS3

âœ“

âœ“

Summarization
ï¾‰

Expand table

Region

Text abstractive summarization

Conversation summarization

AustraliaEast

âœ“

âœ“

CanadaCentral

âœ“

âœ“

CentralUS

âœ“

âœ“

Region

Text abstractive summarization

Conversation summarization

ChinaNorth3

âœ“

âœ“

EastUS

âœ“

âœ“

EastUS2

âœ“

âœ“

FranceCentral

âœ“

âœ“

GermanyWestCentral

âœ“

âœ“

ItalyNorth

âœ“

âœ“

JapanEast

âœ“

âœ“

NorthCentralUS

âœ“

âœ“

NorthEurope

âœ“

âœ“

SouthCentralUS

âœ“

âœ“

SouthUK

âœ“

âœ“

SoutheastAsia

âœ“

âœ“

SwitzerlandNorth

âœ“

âœ“

USGovVirginia

âœ“

âœ“

USGovArizona

âœ“

âœ“

USNatEast

âœ“

âœ“

USNatWest

âœ“

âœ“

USSecEast

âœ“

âœ“

USSecWest

âœ“

âœ“

WestEurope

âœ“

âœ“

WestUS

âœ“

âœ“

WestUS2

âœ“

âœ“

Next steps
Language support
Quotas and limits

Last updated on 11/18/2025

Service limits for Azure Language in
Foundry Tools
ï¼— Note
This article only describes the limits for preconfigured features in Azure Language in
Foundry Tools: To see the service limits for customizable features, see the following
articles:
Custom classification
Custom NER
Conversational language understanding
Question answering
Use this article to find the limits for the size, and rates that you can send data to the following
features of Azure Language.
Named Entity Recognition (NER)
Personally Identifiable Information (PII) detection
Key phrase extraction
Entity linking
Text Analytics for health
Sentiment analysis and opinion mining
Language detection
When using features of Azure Language, keep the following information in mind:
Pricing is independent of data or rate limits. Pricing is based on the number of text
records you send to the API, and is subject to your Language resource's pricing details
A text record is measured as 1000 characters.
Data and rate limits are based on the number of documents you send to the API. If you
need to analyze larger documents than the limit allows, you can break the text into
smaller chunks of text before sending them to the API.
A document is a single string of text characters.

Maximum characters per document
The following limit specifies the maximum number of characters that can be in a single
document.

.

ï¾‰

Expand table

Feature

Value

Text Analytics for health

125,000 characters as measured by StringInfo.LengthInTextElements.

All other preconfigured
features (synchronous)

5,120 as measured by StringInfo.LengthInTextElements. If you need to
submit larger documents, consider using the feature asynchronously.

All other preconfigured

125,000 characters across all submitted documents, as measured by

features (asynchronous)

StringInfo.LengthInTextElements (maximum of 25 documents).

If a document exceeds the character limit, the API behaves differently depending on how
you're sending requests.
If you're sending requests synchronously:
The API doesn't process documents that exceed the maximum size, and returns an invalid
document error for it. If an API request has multiple documents, the API continues
processing them if they are within the character limit.
If you're sending requests asynchronously:
The API rejects the entire request and returns a 400 bad request error if any document
within it exceeds the maximum size.

Maximum request size
The following limit specifies the maximum size of documents contained in the entire request.
ï¾‰

Feature

Value

All preconfigured features

1 MB

Maximum documents per request
Exceeding the following document limits generates an HTTP 400 error code.
ï¼— Note

Expand table

When sending asynchronous API requests, you can send a maximum of 25 documents per
request.

ï¾‰

Expand table

Feature

Max Documents Per Request

Conversation summarization

1

Language Detection

1000

Sentiment Analysis

10

Opinion Mining

10

Key Phrase Extraction

10

Named Entity Recognition (NER)

5

Personally Identifying Information (PII)
detection

5

Document summarization

25

Entity Linking

5

Text Analytics for health

25 for the web-based API, 1000 for the container. (125,000
characters in total)

Rate limits
Your rate limit varies with your pricing tier

. These limits are the same for both versions of the

API. These rate limits don't apply to the Text Analytics for health container, which doesn't have
a set rate limit.
ï¾‰

Tier

Requests per second

Requests per minute

S / Multi-service

1000

1000

S0 / F0

100

300

Expand table

Requests rates are measured for each feature separately. You can send the maximum number
of requests for your pricing tier to each feature, at the same time. For example, if you're in the

S tier and send 1000 requests at once, you wouldn't be able to send another request for 59

seconds.

See also
What is Azure Language in Foundry Tools
Pricing details

Last updated on 11/18/2025

Configure your environment for Azure AI
resources and permissions
In this guide, we walk you through configuring your Azure AI resources and permissions for
Microsoft Foundry tasks. We present two options:
Option 1: Configure a Foundry resource. Foundry offers a unified environment for
building generative AI applications and using Foundry Tools. All essential tools are
together in one environment for all stages of AI app development.
Option 2: Configure Azure Language in Foundry Tools and Azure OpenAI resources.
Azure OpenAI allows users to access OpenAI's language models within the Azure
platform, providing security, regulatory compliance, and integration with other Azure
services.
Completing these setups is essential for fully integrating your environment with Foundry Tools.
You only need to perform this setup onceâ€”afterward, you have seamless access to advanced,
AI-powered conversational language understanding capabilities.
In addition, we show you how to assign the correct roles and permissions within the Azure
portal. These steps help you get started quickly and effectively with Azure Language in Foundry
Tools.

Prerequisites
Before you can set up your resources, you need:
An active Azure subscription. If you don't have one, you can create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
A Foundry resource or an Azure Language resource
An Azure OpenAI resource
ï¼— Note

.

(optional but required for option 2)

We highly recommend that you use a Foundry resource in the Foundry; however, you can
also follow these instructions using a Language resource.

Option 1: Configure a Foundry resource
Foundry offers a unified platform for building, managing, and deploying AI solutions with a
wide array of models and tools. With this integration, you gain access to features like Quick
Deploy for rapid model fine-tuning and suggest utterances to expand your training data with
generative AI. New features are continually added, making Foundry is the recommended
choice for scalable solutions.
1. Navigate to the Azure portal .
2. Go to your Foundry resource (select All resources to locate your resource).
3. Next, select Access Control (IAM) on the left panel, then select Add role assignment.

4. Search and select the Cognitive Services User role. Select Next.

5. Navigate to the Members tab and then select Managed Identity.

6. Choose Select members, then in the right panel, search for and choose your Foundry
resource (the one you're using for this project), and choose Select.
7. Finally, select Review + assign to confirm your selection.
8. Your resources are now set up properly. Proceed with initializing the fine-tuning process
and optimizing your AI models and solutions for advanced customization and
deployment.

Option 2: Configure Azure Language resource and
Azure OpenAI resources
Azure OpenAI is a cloud-based solution that brings the advanced capabilities of OpenAI's
language models to the Azure platform. With this service, you can easily incorporate natural
language processing features into your applications without extensive AI or machine learning
expertise.

Step 1: Assign the correct role to the Azure OpenAI resource
1. Navigate to the Azure portal .
2. Go to your Azure OpenAI resource. (select All resources to locate your resource).
3. Next, select Access Control (IAM) on the left panel, then select Add role assignment.

4. Search and select the Cognitive Services User role, then select Next.

5. Navigate to the Members tab and then select Managed Identity.

6. Select Select members, then in the right panel, search for and choose your Foundry
resource (the one you're using for this project), and choose Select.
7. Finally, select Review + assign to confirm your selection.

Step 2: Configure connections in Foundry
Foundry offers a unified platform where you can easily build, manage, and deploy AI solutions
using a wide range of models and tools. Connections enable authentication and access to both
Microsoft and external resources within your Foundry projects.
1. Sign into Foundry

using your account and required subscription. Then, select the

project containing your desired Foundry resource.
2. Next, navigate to the Management Center in the bottom left corner of the page.
3. Scroll to the Connected resources section of the Management center.

4. Select the + New connection button.

5. In the new window, select Azure Language as the resource type, then find your Azure
Language resource.
6. Select Add connection in the corner of your selected Azure Language resource.
7. Select Azure OpenAI as the resource type, then find your desired Azure OpenAI resource.
8. Ensure Authentication is set to API key.
9. Select Add connection, then select Close.

Import an existing Foundry project
Foundry allows you to connect to your existing Foundry Tools resources. This means you can
establish a connection within your Foundry project to the Azure Language resource where your
custom models are stored.
To import an existing Foundry Tools project with Foundry, you need to create a connection to
the Foundry Tools resource within your Foundry project. For more information, see Connect
Foundry Tools projects to Foundry

Export a project
You can download a project as a config.json file:
1. Navigate to your project home page.
2. At the top of the page, select your project from the right page ribbon area.
3. Select Download config file.

That's it! Your resources are now set up properly. Continue with setting up the fine-tuning task
and customizing your CLU project.

Next Steps

Model lifecycle

Last updated on 11/18/2025

Migrate from Language Studio to Microsoft
Foundry
Azure Language Studio retires on February 16, 2026. The service is no longer available after this
date. All existing capabilities, along with new feature enhancements, are fully available in
Microsoft Foundry. This guide provides step-by-step migration instructions to ensure
uninterrupted access to Azure AI Language features and seamless project continuity within the
Foundry environment.

Why migrate to Microsoft Foundry?
Microsoft Foundry offers a unified platform for building, managing, and deploying AI solutions
with a wide array of models and tools. Migrating to Foundry provides the following benefits:
Unified development experience. Access all Azure AI Language features alongside other
AI services in one environment.
Enhanced capabilities. Use features like Quick Deploy for rapid fine-tuning and Suggest
Utterances to expand training data with generative AI.
Continuous updates. Benefit from new features continually added to Foundry.
Integration with Foundry Tools. Build conversational AI applications using the Azure
Language MCP server, Intent Routing agent, and Exact Question Answering agent.

Migration overview
The migration process consists of the following steps:
1. Export your custom projects from Language Studio.
2. Set up your Foundry environment.
3. Import your projects into Foundry.
4. Validate and test your migrated projects.

Prerequisites
ï¼— Note
If you already have an Azure Language resource, you can continue to use your
existing Language resources within the Microsoft Foundry portal via a Foundry Hub
and Hub project. For more information, see Which type of project do I need?.

If you plan to use a Foundry resource, you can create a new Foundry resource
directly in the Microsoft Foundry portal when creating a new project. For more
information, see Create a Foundry project.
In the Foundry, a fine-tuning task serves as your workspace when customizing your
custom models. Previously, a fine-tuning task was referred to as a project. You might
encounter both terms used interchangeably in some documentation.
Before you begin the migration process, ensure that the following resources and
permissions are in place to complete the steps in this guide:
Azure subscription. If you don't have one, you can create one for free

.

Azure account with a role that allows you to create resources, such as Contributor or
Owner at the subscription level.

Step 1: Export your projects from Language Studio
Before migrating to Microsoft Foundry, export all custom projects you want to transfer. The
export process preserves your project configuration, training data, and model settings for
import into Foundry:

Export a Custom Question Answering project
1. Sign in to Language Studio

.

2. Select the Azure Language resource containing the project you want to export.
3. Navigate to Custom Question Answering.
4. On the Projects page, select the project to export.
5. Choose the export format (Excel or TSV). The file is exported as a .zip file containing
your project contents.

Export a Conversational Language Understanding, Custom
Named Entity Recognition, Custom Text Classification, or
Orchestration Workflow project

1. Sign in to Language Studio

.

2. Select the Azure Language resource containing your CLU project.
3. Navigate to your project.
4. On the project home page, select your project from the right page ribbon area.
5. Select Download config file to download the project as a config.json file.

Step 2: Set up your Foundry environment
Before migrating your Azure Language projects to Microsoft Foundry, you need to complete
several configuration tasks:
Configure Azure resources. Create the required Foundry or Language resources in the
Azure portal.
Establish access control. Assign the appropriate role-based access control (RBAC)
permissions to your resources.
Verify region availability. Confirm that your target capabilities are supported in your
chosen Azure region.
The following sections outline the necessary prerequisites and permissions for both custom
model training (fine-tuning) and pretrained model access.

Custom model training (fine-tuning) in Microsoft Foundry
Microsoft Foundry supports custom model training (fine-tuning) for Azure AI Language
capabilities using two resource configurations:
Foundry resource (recommended). Create and use a Foundry resource directly, which
provides access to both Foundry classic and new Foundry experiences.
Language resource. Use an existing Azure Language resource connected to a Foundry
hub-based project, which is supported in Foundry classic only.
Choose the tab that matches your preferred resource configuration.
Foundry resource (recommended)

ï¼‰ Important
This configuration is supported in both Foundry classic and new Foundry. Microsoft
Foundry can automatically provision and manage Foundry resources. However,
manually configuring your Foundry resource for Language capabilities in the Azure

portal ensures correct role-based access control (RBAC) assignments, managed
identity configurations, and network security settings.

Create a Foundry resource
To use Azure AI Language capabilities with a Foundry resource, you need both the
resource and an associated Foundry project. Set up these resources using either of the
following approaches:
Azure portal. Create a Foundry resource first, then create an associated Foundry
project. This approach provides explicit control over resource configuration settings.
For step-by-step instructions, see Create a Foundry resource in the Azure portal.
Microsoft Foundry portal. Create a Foundry project directly, which automatically
provisions the underlying Foundry resource. This approach streamlines setup by
handling resource creation automatically. For step-by-step instructions, see Create a
Foundry project.
ï¼‰ Important
Custom NER (CNER) requires a storage account to be linked to the Foundry resource
during initial resource creation. To establish this link, you must configure the Foundry
resource in the Azure portal .

ï¾‰

Expand table

Capability

Required prerequisites

Region support

Conversational
Language
Understanding

â€¢ Foundry resource and Foundry
project created in the Azure
portal.

Limited to select Azure regions. Some
regions support both authoring and
prediction; others support prediction only.

(CLU)

Custom Question
Answering (CQA)

For more information, see Region support
for Conversational Language
Understanding.
â€¢ Foundry resource and Foundry
project created in the Azure
portal.
â€¢ Azure AI Search resource
connected to your project via
the Foundry Management
Center. For more information,
see Create a new connection.

Available in supported Azure regions. For
more information, see Product availability
by region .

Capability

Required prerequisites

Region support

Custom Question
Answering agent

â€¢ Foundry resource and Foundry
project created in the Azure

Available in supported Azure regions. For
more information, see Product availability

portal.
â€¢ Azure AI Search resource

by region

.

connected to your project via
the Foundry Management
Center. For more information,
see Create a new connection.
â€¢ Deployed knowledge base.
â€¢ Deployed Azure OpenAI
model in Microsoft Foundry.
â€¢ API key connected to your
project.
Custom Named

â€¢ Language resource with a

Limited to select Azure regions. Some

Entity Recognition
(CNER)

storage account linked during
resource creation in the
Foundry portal.

regions support both authoring and
prediction; others support prediction only.
For more information, see Region support

â€¢ Foundry project created in the
Azure portal.

for CNER.

Orchestration

â€¢ Foundry resource and Foundry

Limited to select Azure regions. Some

Workflow

project, or Language resource
and Foundry hub-based project.
â€¢ A CLU or CQA project created

regions support both authoring and
prediction; others support prediction only.
For more information, see Region support

in the same resource.

for Orchestration workflow.

Pretrained models (prebuilt) supported in Microsoft Foundry
The following table lists the pretrained (prebuilt) capabilities available in Microsoft Foundry,
required prerequisites, and supported regions. Ensure these prerequisites are in place before
proceeding with the migration.
ï¾‰

Expand table

Capability

Input

Region support

Language detection
(Foundry classic)

On the Playground tab, you can choose a text
sample from the drop-down menu, choose the
paperclip icon to upload your own text, or type

Available in supported Azure
regions. For more
information, see Product

your text directly into the sample window. For
more information, see Language detection in
Foundry.

availability by region

.

Capability

Input

Region support

Language detection

On the Playground tab, you can choose a text

Available in supported Azure

(Foundry new)

sample from the drop-down menu, choose the
paperclip icon to upload your own text, or type
your text directly into the sample window. For

regions. For more
information, see Product
availability by region .

more information, see Language detection in
Foundry.
Key phrase

On the Playground tab, you can upload a file or

Available in supported Azure

extraction (Foundry
classic)

type text directly into the sample window. For
more information, see Key phrase extraction.

regions. For more
information, see Product
availability by region .

Named Entity
Recognition

On the Playground tab, you can upload a file or
type text directly into the sample window. For

Available in supported Azure
regions. For more

(Foundry classic)

more information, see Named Entity Recognition
in Foundry.

information, see Product
availability by region .

PII detection for text
or conversation
(Foundry classic)

On the Playground tab, you can choose a text
sample from the drop-down menu, choose the
paperclip icon to upload your own text, or type
your text directly into the sample window. For

Available in supported Azure
regions. For more
information, see Product
availability by region .

more information, see PII detection in Foundry.
PII detection for text
(Foundry new)

On the Playground tab, you can choose a text
sample from the drop-down menu, choose the
paperclip icon to upload your own text, or type
your text directly into the sample window. For
more information, see PII detection in Foundry.

Available in supported Azure
regions. For more
information, see Product
availability by region .

Sentiment analysis

On the Playground tab, you can upload a file or

Available in all supported

(Foundry classic)

type text directly into the sample window. For
more information, see Sentiment analysis in
Foundry.

Azure regions

Summarization
(Foundry classic)
â€¢ Conversation

On the Playground tab, you can choose a text
sample from the drop-down menu, choose the
paperclip icon to upload your own text, or type
your text directly into the sample window. For

Region support is limited to
select Azure regions. For
more information, see
Region support for

â€¢ Call center
â€¢ Text

more information, see Summarization in
Foundry.

summarization

Text Analytics for
health (Foundry
classic)

On the Playground tab, you can upload a file or
type text directly into the sample window. A
storage account isn't required. For more
information, see Text Analytics for health in

Available in all supported
Azure regions .

Foundry.

.

ï¼— Note
The following Azure AI Language features aren't available in the Microsoft Foundry portal.
To use these capabilities, call the Azure Language REST API directly:
Custom Text Classification. For regional availability, see Region support for Custom
Text Classification.
Entity linking. Available in all supported Azure regions

.

Step 3: Import your projects into Foundry
After you connect your Language resource or Foundry resource, your existing projects are
accessible within Foundry. For new projects or to import exported projects:

Import Custom Named Entity Recognition (CNER) project
assets
1. In the Azure portal, grant the Foundry managed identity permissions to the storage
account by assigning the Storage Blob Data Contributor role under Access Control
(IAM).
2. Sign in to the Microsoft Foundry portal

and select your project.

3. In the left pane, select Management Center, and then select Connected Resources.
4. Select + New connection, choose Azure Blob Storage, and provide a name, subscription,
and the specific storage account name.
5. Select Create.
You can now train and deploy your CNER project using the Getting started workflow in
Foundry.

Import a Custom Question Answering (CQA) project
1. In Foundry, navigate to your project.
2. Select Fine-tuning from the left navigation pane.
3. From the main window, select the AI Service fine-tuning tab, and then select + Finetune.
4. In the Create CQA fine-tuning task window, select your connected Azure AI Search
resource.
5. Enter a Name for your project and select the Language.

6. Optionally, update the Default answer when no answer is returned field (default is "No
answer found").
7. Select Create.
8. From the Getting started menu, select Manage sources.
9. Select + Add source, and then select Add Files to upload your exported question-answer
pairs.
After adding your source files, you can train and deploy the CQA project using the Getting
started workflow in Foundry.

Import a Conversational Language Understanding (CLU)
project
1. In Foundry, navigate to your project.
2. Select Fine-tuning from the left navigation pane.
3. From the main window, select the AI Service fine-tuning tab, and then select + Finetune.
4. In the Create service fine-tuning window, select the Conversational language
understanding tab, and then select Next.
5. In the Create CLU fine-tuning task window, select Import an existing fine-tuning task.
6. Enter a name for your imported project.
7. Drag and drop or browse to the config.json file you exported from Language Studio.
8. Select Create to import the project.
After importing, you can train and deploy your CLU project using the Getting started workflow
in Foundry.

Import an Orchestration Workflow project
1. In Foundry, navigate to your project.
2. Select Fine-tuning from the left navigation pane.
3. From the main window, select the AI Service fine-tuning tab, and then select + Finetune.
4. In the Create fine-tuning task window, select Import an existing fine-tuning task.
5. Enter a name for your imported project.
6. Drag and drop or browse to the CLU or CQA config.json file you exported from
Language Studio.
7. Select Create to import the project.
After importing, you can train and deploy the Orchestration project using the Getting started
workflow in Foundry.

Step 4: Validate and test your migrated projects
After importing your projects, validate that the migration is successful:
1. Review project contents. Verify that all intents, entities, question-answer pairs, and
training data are correctly imported.
2. Test your models. Use the Foundry test panel to validate model responses.
3. Deploy and monitor. Deploy your models and monitor performance to ensure they
function as expected.

Troubleshooting
If you encounter issues during the migration process, use the following guidance to diagnose
and resolve common problems. For issues not covered here, consult the Azure AI Language
documentation or contact Azure Support

.

Connection issues
If you encounter issues connecting your Language resource to Foundry:
Verify that you have the correct role assignments (Cognitive Services User or higher).
Ensure the Language resource and Foundry project are in compatible regions.
Check that managed identity is properly configured.

Import failures
If project import fails:
Verify the export file format matches the expected import format.
Check for any data corruption in the exported .zip or config,json file.
Ensure the project name doesn't conflict with existing projects.

Related content
Azure Language role-based access control
Steps to assign an Azure role
Configure your environment for Azure AI resources and permissions
Connect Foundry Tools to a Foundry project

Last updated on 01/23/2026

What is conversational language
understanding?
Conversational language understanding is one of the custom features offered by Azure
Language. It's a cloud-based API service that applies machine-learning intelligence to enable
you to build natural language understanding component to be used in an end-to-end
conversational application.
Conversational language understanding (CLU) enables users to build custom natural language
understanding models to predict the overall intention of an incoming utterance and extract
important information from it. CLU only provides the intelligence to understand the input text
for the client application and doesn't perform any actions. Developers can iteratively label
utterances, train, and evaluate model performance before making it available for consumption
by creating a CLU project. The quality of the labeled data greatly impacts model performance.
To simplify building and customizing your model, the service offers a custom web portal that
can be accessed through the Microsoft Foundry

. You can easily get started with the service

by following the steps in this quickstart.
This documentation contains the following article types:
Quickstarts are getting-started instructions to guide you through making requests to the
service.
Concepts provide explanations of the service functionality and features.
How-to guides contain instructions for using the service in more specific or customized
ways.

Example usage scenarios
CLU can be used in multiple scenarios across various industries. Some examples are:

Multi-turn conversations ðŸ†•
Use CLU with entity slot filling to enable natural, progressive information gathering across
multiple conversation turns. Instead of overwhelming users with complex forms, your
application can collect required details as they emerge naturally in dialogue. This approach is
ideal for scenarios like booking systems, customer service workflows, or any application where
complete information needs to be gathered through conversational exchanges.
For more information, see Multi-turn conversations.
To get started, see Build a multi-turn model.

End-to-end conversational bot
Use CLU to build and train a custom natural language understanding model tailored to a
specific domain and the expected users' utterances. You can then connect this solution with
any end-to-end conversational bot. This process enables the bot to handle and interpret
incoming messages in real time. This integration allows the bot to determine the user's intent
and extract key information from the conversation as it happens. The bot performs the desired
action based on the intention and extracted information. An example would be a customized
retail bot for online shopping or food ordering.
By combining it with a comprehensive conversational bot framework, the system is able to
analyze text instantaneously, accurately identify user intentions, and pull out relevant details for
further processing.

Human assistant bots
A human assistant bot can enhance customer service by sorting customer inquiries and
directing them to the right support engineer. Similarly, in a corporate environment, a human
resources bot enables employees to ask questions in everyday language and receive relevant
guidance based on their requests.

Command and control application
When you integrate a client application with a speech to text component, users can speak a
command in natural language for CLU to process, identify intent, and extract information from
the text for the client application to perform an action. This use case has many applications,
such as to stop, play, forward, and rewind a song or turn lights on or off.

Enterprise chat bot
Within a large corporation, the enterprise chatbot actively handles a wide range of employee
matters. Employees rely on the chatbot to address frequently asked questions, drawing on a
custom question-answering knowledge base. When users interact with their calendars, the
chatbot uses a calendar-specific skill powered by conversational language understanding.
Employees also benefit from an interview feedback skill, which operates through CLU. The
Orchestration workflow seamlessly connects these skills, ensuring each request routes directly
to the appropriate service.

Project development lifecycle

Creating a CLU project typically involves several different steps.

ï Š

ï¼— Note
In the Foundry, you create a fine-tuning task as your workspace for customizing your CLU
model. Previously, a CLU fine-tuning task was referred to as a CLU project. You may see
these terms used interchangeably in legacy CLU documentation.
CLU offers two paths for you to get the most out of your implementation.
Option 1 (LLM-powered quick deploy):
1. Define your schema: Know your data and define the actions and relevant information
that needs to be recognized from user's input utterances. In this step, you create
the intents and provide a detailed description on the meaning of your intents that you
want to assign to user's utterances.
2. Deploy the model: Deploying a model with the LLM-based training config makes it
available for use via the Runtime API.
3. Predict intents and entities: Use your custom model deployment to predict custom
intents and prebuilt entities from user's utterances.
Option 2 (Custom machine learned model)
Follow these steps to get the most out of your trained model:
1. Define your schema: Know your data and define the actions and relevant information
that needs to be recognized from user's input utterances. In this step, you create the
intents that you want to assign to user's utterances, and the relevant entities you want
extracted.

2. Label your data: The quality of data labeling is a key factor in determining model
performance.
3. Train the model: Your model starts learning from your labeled data.
4. View the model's performance: View the evaluation details for your model to determine
how well it performs when introduced to new data.
5. Improve the model: After reviewing the model's performance, you can then learn how
you can improve the model.
6. Deploy the model: Deploying a model makes it available for use via the Runtime API .
7. Predict intents and entities: Use your custom model to predict intents and entities from
user's utterances.

Reference documentation and code samples
As you use CLU, see the following reference documentation and samples for Azure Language:
ï¾‰

Expand table

Development option / language

Reference documentation

Samples

REST APIs (Authoring)

REST API documentation

REST APIs (Runtime)

REST API documentation

C# (Runtime)

C# documentation

C# samples

Python (Runtime)

Python documentation

Python samples

Responsible AI
An AI system includes the technology, the individuals who operate the system, the people who
experience its effects, and the broader environment where the system functions all play a role.
Read the transparency note for CLU to learn about responsible AI use and deployment in your
systems.
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Next steps
Use the quickstart article to start using conversational language understanding.
As you go through the project development lifecycle, review the glossary to learn more
about the terms used throughout the documentation for this feature.
Remember to view the service limits for information such as regional availability.

Last updated on 12/05/2025

Quickstart: Conversational language
understanding
Microsoft Foundry offers a unified platform for building, managing, and deploying AI solutions
with a wide array of models and tools. Foundry playgrounds are interactive environments
within the Foundry portal designed for exploring, testing, and prototyping with various AI
models and tools.
Use this article to get started with Conversational Language understanding using Foundry or
the REST API.
ï¼— Note
If you already have an Azure Language in Foundry Tools or multi-service resourceâ€”
whether used on its own or through Language Studioâ€”you can continue to use
those existing Language resources within the Microsoft Foundry portal.
For more information, see How to use Foundry Tools in the Foundry portal.
We highly recommended that you use a Foundry resource in the Foundry; however,
you can also follow these instructions using a Language resource.

Prerequisites
Azure subscription. If you don't have one, you can create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
Foundry resource. For more information, see Configure a Foundry resource. Alternately,
you can use a Language resource .
A Foundry project created in the Foundry. For more information, see Create a Foundry
project.

Get started with Foundry
To complete this quickstart, you need a Conversational Language Understanding (CLU) finetuning task project that includes a defined schema and labeled utterances.

You can download our sample project file , which comes preconfigured with both a schema
and labeled utterances. This project enables the prediction of user intent for commands such
as reading emails, deleting emails, and attaching documents to emails.
Let's begin:
1. Navigate to the Foundry

.

2. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
3. Once signed in, you can create or access your existing projects within Foundry.
4. If you're not already at your project for this task, select it.
5. On the left side navigation pane, select Playgrounds, navigate to the Language
playground tile, and then choose the Try Azure Language playground button.

Try Azure Language playground
The top section of Azure Language playground is where you can view and select the available
Languages.
1. Select the Conversational language understanding tile.

2. Next, scroll to and select the Fine-tune button.

3. From Create service fine-tuning window, choose the Conversational language
understanding card. Then select Next.

4. In Create CLU fine tuning task window, select Import an existing project, then choose
your Connected service from the drop-down menu and complete the Name field.

5. Next, add the sample project file

that you downloaded earlier to the upload area.

6. Select the Create button. It can take a few minutes for the creating operation to complete.
7. Once your fine-tuning task project is created, the Getting started with fine-tuning page
opens.

ï Š

Train your model
After project creation, the next steps are schema construction and utterance labeling. For this
quickstart, these steps are preconfigured in the sample project. Therefore, you can go ahead
and initiate a training job by selecting Train model from the Getting Started menu to generate
your model.

1. Select the âž•Train model button from the Train your model window.

2. Complete the Select a mode form by completing the Model name field and selecting a
Training mode. For this quickstart, select the free Standard training mode. For more
information, see Training modes.
3. Choose a Training version from the drop-down menu, then select the Next button.

4. Check your selections in the Review window, then select the Create button

Deploy your model
Typically, after training a model, you review its evaluation details. For this quickstart, you can
just deploy your model and make it available to test in Azure Language playground, or by
calling the prediction API

. However, if you wish, you can take a moment to select Evaluate

your model from the left-side menu and explore the in-depth telemetry for your model.
Complete the following steps to deploy your model within Foundry:
1. Select Deploy model from the left-side menu.
2. Next, select âž•Deploy a trained model from the Deploy your model window.

3. Make sure the Create a new deployment button is selected.
4. Complete the Deploy a trained model window fields:
Create a deployment name.
Select your trained model from the Assign a model drop-down menu.
Select a subscription from the Subscription drop-down menu.
Select a region from the Region drop-down menu.
Select a resource from the Resource drop-down menu. The resource must be in the
same deployment region.

5. Finally, select the Create button. It may take a few minutes for your model to deploy.
6. After successful deployment, you can view your model's deployment status on the
Deploy your model page. The expiration date that appears marks the date when your
deployed model becomes unavailable for prediction tasks. This date is usually 18 months
after a training configuration is deployed.

7. From the far-left menu, navigate to Azure Language playground.
Playgrounds â†’ Language playground (Try Azure Language playground).
8. Select the Conversational language understanding card.
9. A Configuration window with your deployed model should appear in the main/center
window.
10. In the text box, enter an utterance to test. For example, if you used our sample project
application for email-related utterances you could enter Check email.
11. After you enter your test text, select the Run button.

12. After you run the test, you should see the response of the model in the result.

13. You can view the results in a text or JSON format view.

That's it, congratulations!
In this quickstart, you deployed a CLU model and tested it in the Foundry Language
playground. Next, learn how to Create your own fine-tuning task project for your applications
and workflows.

Clean up resources
If you no longer need your project, you can delete it from the Foundry.
1. Navigate to the Foundry

home page. Initiate the authentication process by signing in,

unless you already completed this step and your session is active.
2. Select the project that you want to delete from the Keep building with Foundry
3. Select Management center.
4. Select Delete project.

To delete the hub along with all its projects:
1. Navigate to the Overview tab inn the Hub section.

2. On the right, select Delete hub.

3. The link opens the Azure portal for you to delete the hub there.

Next steps
Build a fine-tuning schema
Label utterances
Train a model

Last updated on 12/05/2025

Quickstart: Multi-turn CLU models with
entity slot filling
In this article, get started building a CLU model that uses entity slot filling to enable multi-turn
conversations. This approach allows your model to collect information progressively across
multiple conversation turns, rather than requiring users to provide all details in a single
interaction to complete tasks naturally and efficiently.

Prerequisites
Azure subscription - If you don't have one, you can create one for free

.

Required permissions - Ensure that the person establishing the account and project has
the Azure AI Account Owner role at the subscription level. Alternatively, the Contributor
or Cognitive Services Contributor role at the subscription scope also meets this
requirement. For more information, see Role based access control (RBAC).
Azure Language in Foundry Tools resource - Create a Language resource

in the Azure

portal.
ï¼— Note
You need the owner role assigned on the resource group to create a Language
resource.
Microsoft Foundry project - Create a project in Foundry. For more information, see
Create a Foundry project.
Deployed OpenAI model - Deploy an OpenAI model in Foundry as described in the
Deploy an OpenAI model section.

Configure roles, permissions, and settings
Begin by configuring your Azure resources with the appropriate roles and permissions.

Add roles for your Language resource
1. Navigate to your Language resource page in the Azure portal
Control (IAM) from the left navigation pane.

and select Access

2. Select Add > Add Role Assignments, and assign either the Cognitive Services Language
Owner or Cognitive Services Contributor role for your Language resource.
3. Under Assign access to, select User, group, or service principal.
4. Select Select members.
5. Choose your user name from the list. You can search for user names in the Select field.
Repeat this step for all required roles.
6. Repeat these steps for all user accounts that require access to this resource.

Connect Azure Language resource to Foundry
To enable secure access, create a connection between your Language resource and Foundry.
This connection provides secure identity management, authentication, and controlled access to
your data.
ï¼— Note
The multi-turn capability is currently only available in the Foundry (classic) portal.
1. Navigate to Foundry (classic) .
2. Access your existing Foundry project for this tutorial.
3. Select Management center from the left navigation menu.
4. Select Connected resources from the Hub section of the Management center menu.
5. In the main window, select + New connection.
6. Select Language from the Add a connection to external assets window.
7. Select Add connection, then select Close.

Deploy an OpenAI model in Foundry portal
Deploy an OpenAI model to provide the foundational intelligence and advanced reasoning
capabilities for your CLU model.
1. Select Models + endpoints from the My assets section of the navigation menu.

2. From the main window, select + Deploy model.
3. Select Deploy base model from the dropdown menu.

4. In the Select a model window, choose a model. The gpt-4 base model is recommended
for this project.

5. Select Confirm.
6. In the Deploy gpt-4 window, retain the default values and select Deploy.

7. The model deployment is now complete.

Build a multi-turn model
Now that your Language resource, Foundry project, and OpenAI deployment are configured,
you're ready to build your CLU model.

Create your CLU project
In this section, you create a travel agent model and deploy it using Quick Deploy.
1. Navigate to Foundry (classic) .

2. If you aren't already signed in, the portal prompts you to authenticate with your Azure
credentials.
3. Once signed in, create or access your existing projects within Foundry.
4. If you're not already in your project for this task, select it.
5. In the left navigation pane Overview section, select Fine-tuning.
6. From the main window, select the AI Service fine-tuning tab, then select + Fine-tune.
7. In the Create service fine-tuning window, choose the Conversational language
understanding tab, then select Next.
8. In the Create CLU fine-tuning task window, complete the following fields:
Connected service - The name of your language service resource should appear by
default. If not, select it from the dropdown menu.
Name - Provide a name for your fine-tuning task project.
Language - English is set as the default and should already appear in the field.
Description - Optionally provide a description or leave this field empty.
9. Select Create. The creation operation may take a few minutes to complete.

Add your intents
1. From the Getting Started menu, select Define schema.
2. In the main window, select Add Intent.
3. The Add Intent window contains two required fields:
Intent name(Pascal case)
Intent description (required for Quick Deploy)
4. After completing these fields, select + Add to create your intents.
5. Add the following intents:
ï¾‰

Intent name

Intent description

BookFlight

Make a travel reservation for an airline flight.

FlightTime

The scheduled departure and/or arrival time for an airline flight.

Expand table

Intent name

Intent description

FlightStatus

The current status of a scheduled flight.

6. After defining all intents, select Add Intent.

Add your entities
1. Select the Entities tab, then select Add entity.
2. The Add an entity window contains two required fields:
Entity name (Pascal case)|
Entity description
3. After completing the entity fields, select Add an entity.
4. Add the following entities:
ï¾‰

Entity name

Entity description

FlightNumber

Flight number for scheduled flight.

FlightDepartureTime

Departure time for scheduled flight.

TravelDate

Desired travel date.

Expand table

5. For common data types, add prebuilt entity components:
Select your TravelDate entity:
Under the Prebuilt section, select Add prebuilt.
Select DateTime from the dropdown list, then select Add.
Select your FlightDepartureTime entity:
Under the Prebuilt section, select Add prebuilt.
Select DateTime from the dropdown list, then select Add.

Associate your intents with entities
1. Select the Associations tab.
2. Select each intent and link it to the required entities from the Available entities section.
This step ensures the model knows which slots to fill for each intent.
3. All entities must have an association with at least one intent. After configuring the
associations, select Update associations.
ï¾‰

Intent

Association

BookFlight

TravelDate

FlightTime

FlightDepartureTime, TravelDate

FlightStatus

FlightNumber, FlightDepartureTime, TravelDate

Expand table

Now that all entities have associations, you can proceed with Quick Deploy using a large
language model (LLM).

Use Quick deploy with LLM
1. From the Getting Started menu, select Train model.
2. In the Train your model section, select Quick deploy with LLM.
3. Complete the Quick deploy with LLM window fields:
Deployment name - Provide a name for the deployment.
Select Azure OpenAI Model Deployment - Choose the Azure OpenAI model
deployment you created for this project.
Deployment regions - Select the region associated with your Azure Language
resource.
4. Select Create. Foundry manages the configuration and deployment processes through
backend operations.

Test your model in the playground
1. From the Getting Started menu, select Deploy your model.
2. In the main window, select your model.
3. Select Try in playground.
4. In the playground, select the Conversational language understanding tile.
5. Under the Configuration menu on the left, verify that the following fields are completed
correctly:
Project name - Ensure this matches the project you created for this CLU fine-tuning
task.
Deployment name - Verify this matches the name you assigned to your OpenAI
model.
6. Select the Multi-turn understanding checkbox.
7. Simulate a multi-turn dialog by entering the following conversation:
text

User: Hello, I would like to book a flight.
Agent: Hello! On which day do you intend to travel?
User: Tomorrow.
Agent: What departure time would you prefer for your flight?
User: Anytime after 5:00 PM.

8. Select Run.
9. The model returns a response in both Text and JSON formats.

10. In the Details panel on the right, review the Top Intent and detected Entities.

You successfully created a multi-turn CLU model with entity slot filling capabilities to collect
required information across multiple dialog turns.

Clean up your resources
To clean up and remove an Azure AI resource, delete either the individual resource or the
entire resource group. Deleting the resource group removes all contained resources.

Related content
Learn how CLU handles entity slot-filling across multi-turn conversations.

Last updated on 11/18/2025

Language support for Conversational
Language Understanding (CLU)
Use this article to learn about the languages currently supported by CLU feature.

Multi-lingual option
îª€ Tip
We recommend using English for the LLM-powered features, like Quick Deploy and
Conversation-level understanding, but your project continues to function for all
languages.
With conversational language understanding, you can train a model in one language and use
to predict intents and entities from utterances in another language. This feature is powerful
because it helps save time and effort. Instead of building separate projects for every language,
you can handle multi-lingual dataset in one project. Your dataset doesn't have to be entirely in
the same language but you should enable the multi-lingual option for your project while
creating or later in project settings. If you notice your model performing poorly in certain
languages during the evaluation process, consider adding more data in these languages to
your training set.
You can train your project entirely with English utterances, and query it in: French, German,
Mandarin, Japanese, Korean, and others. Conversational language understanding makes it easy
for you to scale your projects to multiple languages by using multilingual technology to train
your models.
Whenever you identify that a particular language isn't performing as well as other languages,
you can add utterances for that language in your project. In the tag utterances page in
Microsoft Foundry, you can select the language of the utterance you're adding. When you
introduce examples for that language to the model, it introduces the model to more of the
syntax of that language, and it learns to predict it better.
You aren't expected to add the same number of utterances for every language. You should
build most your project in one language, and only add a few utterances in languages you
observe aren't performing well. In that case, consider adding 5% of your original English
examples in German, train a new model and test in German again. You should see better results
for German queries. The more utterances you add, the more likely the results are going to get
better.

When you add data in another language, you shouldn't expect it to negatively affect other
languages.

List and prebuilt components in multiple languages
Projects with multiple languages enabled allow you to specify synonyms per language for
every list key. Depending on the language you query, your project with, you only get matches
for the list component with synonyms of that language. When you query your project, you can
specify the language in the request body:
JSON
"query": "{query}"
"language": "{language code}"

If you don't provide a language, it falls back to the default language of your project.
Prebuilt components are similar, where you should expect to get predictions for prebuilt
components that are available in specific languages. The request's language again determines
which components are attempting to be predicted.

Languages supported by conversational language
understanding
Conversational language understanding supports utterances in the following languages:
ï¾‰

Language

Language code

Afrikaans

af

Amharic

am

Arabic

ar

Assamese

as

Azerbaijani

az

Belarusian

be

Bulgarian

bg

Bengali

bn

Expand table

Language

Language code

Breton

br

Bosnian

bs

Catalan

ca

Czech

cs

Welsh

cy

Danish

da

German

de

Greek

el

English (US)

en-us

English (UK)

en-gb

Esperanto

eo

Spanish

es

Estonian

et

Basque

eu

Persian

fa

Finnish

fi

French

fr

Western Frisian

fy

Irish

ga

Scottish Gaelic

gd

Galician

gl

Gujarati

gu

Hausa

ha

Hebrew

he

Hindi

hi

Croatian

hr

Language

Language code

Hungarian

hu

Armenian

hy

Indonesian

id

Italian

it

Japanese

ja

Javanese

jv

Georgian

ka

Kazakh

kk

Khmer

km

Kannada

kn

Korean

ko

Kurdish (Kurmanji)

ku

Kyrgyz

ky

Latin

la

Lao

lo

Lithuanian

lt

Latvian

lv

Malagasy

mg

Macedonian

mk

Malayalam

ml

Mongolian

mn

Marathi

mr

Malay

ms

Burmese

my

Nepali

ne

Dutch

nl

Language

Language code

Norwegian (Bokmal)

nb

Odia

or

Punjabi

pa

Polish

pl

Pashto

ps

Portuguese (Brazil)

pt-br

Portuguese (Portugal)

pt-pt

Romanian

ro

Russian

ru

Sanskrit

sa

Sindhi

sd

Sinhala

si

Slovak

sk

Slovenian

sl

Somali

so

Albanian

sq

Serbian

sr

Sundanese

su

Swedish

sv

Swahili

sw

Tamil

ta

Telugu

te

Thai

th

Filipino

tl

Turkish

tr

Uyghur

ug

Language

Language code

Ukrainian

uk

Urdu

ur

Uzbek

uz

Vietnamese

vi

Xhosa

xh

Yiddish

yi

Chinese (Simplified)

zh-hans

Chinese (Traditional)

zh-hant

Zulu

zu

Next steps
Conversational language understanding overview
Service limits

Last updated on 12/05/2025

Frequently asked questions for
conversational language understanding
This article provides quick answers to essential FAQs about conversational language
understanding.

How do I create a project?
Use the quickstart to get started creating your first project, or the how-to article for more
details.

Can I use more than one conversational language
understanding project together?
Yes, using orchestration workflow. For more information, see the orchestration workflow
documentation.

What is the difference between language
understanding and conversational language
understanding?
Conversational language understanding is the next generation of LUIS.

Training is taking a long time, is this behavior
expected?
For conversation projects, long training times are expected. Based on the number of examples
you have your training times may vary from 5 minutes to 1 hour or more.

How do I use entity components?
See the entity components article.

Which languages are supported in this feature?
See the language support article.

How do I get more accurate results for my project?
Take a look at the recommended guidelines for information on improving accuracy.

How do I get predictions in different languages?
When you train and deploy a conversation project in any language, you can immediately try
querying it in multiple languages. You may get varied results for different languages. To
improve the accuracy of any language, add utterances to your project in that language to
introduce the trained model to more syntax of that language.

How many intents, entities, utterances can I add to
a project?
See the service limits article.

Can I label the same word as two different entities?
Unlike LUIS, you can't label the same text as two different entities. Learned components across
different entities are mutually exclusive, and only one learned span is predicted for each set of
characters.

Can I import a LUIS JSON file into conversational
language understanding?
Yes, you can import any LUIS application JSON file from the latest version in the service.

Can I import a LUIS .LU file into conversational
language understanding?
No, the service only supports JSON format. You can go to LUIS, import the .LU file, and export
it as a JSON file.

Can I use conversational language understanding
with custom question answering?

Yes, you can use orchestration workflow to orchestrate between different conversational
language understanding and question answering projects. Start by creating orchestration
workflow projects, then connect your conversational language understanding and custom
question answering projects. To perform this action, make sure that your projects are under the
same Language resource.

How do I handle out of scope or domain
utterances that aren't relevant to my intents?
Add any out of scope utterances to the none intent.

How do I control the none intent?
You can control the none intent threshold from UI through the project settings, by changing
the none intent threshold value. The values can be between 0.0 and 1.0. Also, you can change
this threshold from the APIs by changing the confidenceThreshold in settings object. Learn
more about none intent

Is there any SDK support?
Yes, only for predictions, and samples are available for Python

and C#

. There's currently no

authoring support for the SDK.

What are the training modes?
ï¾‰

Expand table

Training
mode

Description

Language
availability

Pricing

Standard

Faster training times for quicker model

Can only train

Included in your

training

iteration.

projects in English.

pricing tier

Advanced
training

Slower training times using fine-tuned
neural network transformer models.

Can train multilingual
projects.

May incur added
charges .

For more information, see training modes.

Are there APIs for this feature?

.

Yes, all the APIs are available.
Authoring APIs
Prediction API

Next steps
Conversational language understanding overview

Last updated on 11/18/2025

Use cases for conversational language
understanding
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a Transparency Note?
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance. Microsoftâ€™s Transparency
Notes are intended to help you understand how our AI technology works, the choices system
owners can make that influence system performance and behavior, and the importance of
thinking about the whole system, including the technology, the people, and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who will use or be affected by your system.
Microsoftâ€™s Transparency Notes are part of a broader effort at Microsoft to put our AI Principles
into practice. To find out more, visit Microsoft AI principles

.

Introduction to conversational language
understanding
Conversational language understanding (CLU) is a cloud-based API feature that applies custom
machine-learning on top of Microsoft Turing technologies to a user's natural language text. It
predicts the overall meaning of an input text and pulls out specific information from it. CLU
needs to be integrated with a client application, which can be any conversational application
that communicates with a user in natural language to complete a task. The most common
client application is a chat bot.
Client applications use the output returned by CLU to make a decision or perform an action to
fulfill the user's requests. For example, a user enters "I want to order a pizza" in a chat bot,
which is sent to CLU for interpretation. CLU analyzes the input text and returns its
interpretation in a form that can be processed by the chat bot. CLU links the input text with a
preconfigured action to order the pizza for the user. CLU only provides the intelligence to

understand the input text for the client application and doesn't perform any actions. CLU is not
only supported in multiple languages, but also it supports multi-lingual projects. Learn more
about multi-linguality.

The basics of conversational language understanding
Conversational language understanding (CLU) is offered as part of the custom features within
Azure AI Language. This feature is a natural language understanding component in an end-toend conversational application that predicts the overall intention of an incoming text and
extracts important information from it. By creating a CLU project, developers can iteratively tag
data, train, evaluate, and improve model performance before they make it available for
consumption.
Users of the service need to provide and label training data relevant to the domain of the client
application being built. The quality of the provided training data is important and needs to be
similar to the expected user input. Users may also connect different custom capabilities
together including other CLU projects, Custom question answering knowledge bases, and LUIS
applications using the Orchestration workflow feature within CLU. The Azure AI Language
provides a web portal, Language Studio

, to simplify the customization experience for domain

experts and nontechnical users. Get started with the feature by following the steps in this
quickstart.
For more information, see:
Create a new CLU project
Build your project's schema to determine user intentions and extract data
Connect multiple features in an Orchestration workflow project

Conversational language understanding
terminology
The following terms are commonly used within CLU.
ï¾‰

Expand table

Term

Definition

Project

A project is a work area for building your custom ML models based on your data. Within a
project, you can tag data, build models, evaluate and improve them where necessary, and
eventually deploy a model to be ready for consumption.

Utterances

Utterances represent the input text from a user that CLU needs to interpret. Developers add
example utterances as training data and tag them with the intents and entities to train the

Term

Definition
model. For example, "I want to order a large cheese pizza" would be an example utterance in
a model that orders pizza.

Intents

Intents are tasks or actions that the user wants to perform. Intent models understand and
classify the overall meaning and intention of an input text. Developers define a set of intents
to trigger an action users want to take in the client application. For example, intents in a
model that orders pizza could be "Make Order," "Edit Order," or "Cancel Order." Learn more

Entities

Entities represent a word or phrase in an utterance that's relevant to the userâ€™s intent. Entity
models extract different types of entities, as defined by developers. In the example utterance
"I want to order a large cheese pizza," developers could define a "size" entity to extract
"large" and a "type" entity to extract "cheese" from the utterance. Developers define entities
to extract key data from user utterances in CLU models. When developers create a CLU
model, they tag a word or multiple words they want extracted within the example utterances
with a specific entity. Learn more

Example use cases
CLU can be used in multiple scenarios across a variety of industries. Some examples are:
End-to-end conversational bot. Use CLU to build and train a custom natural language
understanding model based on a specific domain and the expected users' utterances.
Integrate it with any end-to-end conversational bot so that it can process and analyze
incoming text in real time to identify the intention of the text and extract important
information from it. Have the bot perform the desired action based on the intention and
extracted information. An example would be a customized retail bot for online shopping
or food ordering.
Human assistant bots. One example of a human assistant bot is to help staff improve
customer engagements by triaging customer queries and assigning them to the
appropriate support engineer. Another example would be a human resources bot in an
enterprise that allows employees to communicate in natural language and receive
guidance based on the query.
Command and control application. When you integrate a client application with a
speech to text component, users can speak a command in natural language for CLU to
process, identify intent, and extract information from the text for the client application to
perform an action. This use case has many applications, such as to stop, play, forward,
and rewind a song or turn lights on or off.
Enterprise chat bot. In a large corporation, an enterprise chat bot may handle a variety of
employee affairs. It may be able to handle frequently asked questions served by a custom
question answering knowledge base, a calendar specific skill served by conversational
language understanding, and an interview feedback skill served by LUIS. Use

Orchestration workflow to connect all these skills together and appropriately route the
incoming requests to the correct service.

Consideration when you choose a use case
Avoid using CLU for decisions that might have serious adverse impacts. For example,
suggesting medications or diagnoses, as replacing a doctor's advice may have serious
adverse impacts.
Avoid creating custom entities that extract unnecessary or sensitive information. It's
your responsibility to ensure that the entities being created only extract the necessary
information for your end-to-end scenario. Avoid extracting sensitive user information if it
isn't required for your scenario. For example, if your scenario requires extracting your
user's city and country, create entities that extract only the city and country from a user's
address. Don't create one that extracts their full address. To ensure that your model is
inclusive, make sure you represent a variety of cities, countries, and address formats
within your training data (example utterances).
Legal and regulatory considerations: Organizations need to evaluate potential specific
legal and regulatory obligations when using any AI services and solutions, which may not
be appropriate for use in every industry or scenario. Additionally, AI services or solutions
are not designed for and may not be used in ways prohibited in applicable terms of
service and relevant codes of conduct.

Next steps
Introduction to conversational language understanding
Characteristics and limitations for using Language Understanding
Data, privacy, and security
Microsoft AI principles
Building responsible bots

Last updated on 06/24/2025

Guidance for integration and responsible
use with Azure AI Language
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure AI
Language. We are taking a principled approach to upholding personal agency and dignity by
considering the fairness, reliability & safety, privacy & security, inclusiveness, transparency, and
human accountability of our AI systems. These considerations are in line with our commitment
to developing Responsible AI.
This article discusses Azure AI Language features and the key considerations for making use of
this technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Azure AI Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Azure AI Language

Last updated on 06/24/2025

Characteristics and limitations for using
conversational language understanding
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Performance with conversational language understanding (CLU) varies based on the scenario,
input data, and enabled features. The following sections are designed to help the reader
understand key concepts about performance as they apply to CLU.

Understand and measure performance
The performance of CLU is measured by examining the predicted intents and entities for a
given utterance and how well the system recognizes the custom natural language processing
(NLP) concepts (at a threshold value in comparison with a human judge). In the process of
building a model, training and testing sets are either defined by developers during tagging or
chosen at random during training. The training set is used to train the custom model, while the
testing set is used as a blind set to evaluate the model's performance.
The model evaluation process is triggered after training is completed successfully. The
evaluation process takes place by using the trained model to predict user-defined entities from
the testing set and compare them with the provided data tags (ground truth). The results are
returned to the developer to review the model's performance.
The first step in calculating the model's evaluation is categorizing the extracted entities in one
of the following categories: true positives, false positives, or false negatives. The following table
shows the options by using a "Make call" intent as an example.
ï¾‰

Expand table

Term

Correct/Incorrect

Definition

Example

True
positive

Correct

The system returns the
same results that would be

For the utterance "Make a phone call to
Sarah," the system correctly predicts the

expected from a human
judge.

intent as "Make call."

The system returns an
incorrect result where a

For the utterance "Turn off the lights,"
the system incorrectly predicts the intent

human judge wouldn't.

as "Make call."

False
positive

Incorrect

Term

Correct/Incorrect

Definition

Example

False

Incorrect

The system doesn't return a

For the utterance "I need to call Sarah to

result when a human judge
would return a correct
result.

tell her that I am late," the system
incorrectly predicts this utterance as a
"Schedule meeting" intent.

negative

The preceding categories are then used to calculate precision, recall and F1 score. These metrics
are provided to developers as part of the service's model evaluation. Here are the metric
definitions and how they're calculated:
Precision: The ratio between the true positives and all the positives. Out of predicted
positive classes, precision reveals how many of them are actual positives (belong to the
right entity type as predicted).
Recall: The measure of the model's ability to extract actual positive entities. It's the ratio
between the predicted true positives and the actually tagged positives. Out of actual
tagged entities, recall reveals how many of them are predicted correctly.
F1 score: A function of precision and recall. You need it when you seek a balance between
precision and recall.
Errors that happen with CLU are mostly dependent on the utterances provided as training data.
Any CLU model experiences both false negative and false positive errors. Developers need to
consider how each type of error affects the overall system and carefully think through scenarios
where true events won't be recognized and incorrect events are recognized. Customers should
assess the downstream effects that occur in the implementation and understand the
consequences of both types of errors on their client application. Developers should create ways
to identify, report, and respond to each type of error.
Given that a CLU prediction response triggers the client application to perform a specific
action, if the prediction is incorrect, the client application performs an action different to the
intended one. Depending on the scenario, precision or recall could be a more suitable metric
for evaluating your model's performance.
For example, suppose the user utterance was "I need to make a phone call to Sarah" and the
predicted intent from CLU was a "Schedule meeting" intent. In this case, the client application
won't go through the "making a call" logic and will be unable to perform the expected action
properly. The system should then be more sensitive to false positives, and precision would be a
more relevant metric for evaluation in this scenario. Handling this error from the client
application side is the short-term solution. The system would expect another clear input from
the user similar to the example utterances given to the system before training like "Make call."

System limitations and best practices for
enhancing system performance
Understand service limitations: Be aware of service limitations such as number of intents
per project and number of example utterances per intent. Learn more on service limits
Plan schema: Think about user utterances and the main actions that the client application
will perform based on these utterances. Developers need to plan the CLU project schema
accordingly with the intents, example utterances, and entities to be extracted. This step is
essential, and we advise all CLU users to do it. Learn more
Use good training data: The quality of example utterances provided during training
affects the end results. Carefully choose realistic example utterances for training the
model. Capture a variety of different example utterances expected to be sent from the
users with varied terminologies and contextual differences. Beware of using sensitive
information in example utterances that are added during tagging. For example, don't add
a real credit card number in an example utterance. Example utterances are saved in CLU
storage accounts to train models.
Build models by using real-world data: Avoid using automatically generated data
because then the model learns a fixed grammar, which diminishes the model's ability to
generalize across different ways of speaking. A good practice is to deploy a simple model
and start collecting data that's used in training the final model. This practice helps give an
understanding of how users are shifting and how they might express different things over
time.
Review evaluation and improve model: After the model was trained successfully, check
the model evaluation and confusion matrix. This review helps you understand where your
model went wrong and learn about entities/intents that aren't performing well. It's also
considered as a best practice to review the testing set and view the predicted and tagged
entities side by side. It helps you to gauge the model's performance and decide if any
changes in the schema or the tags are necessary.
Provide secondary paths: CLU might not perform well when used with speech to text in
situations with a lot of background noise or for people with speaking difficulties and
speech impairments. Ensure there's always a secondary path for users to enact commands
when the cloud doesn't perform as expected.

General guidelines to understand and improve
performance
The following guidelines help you understand and improve performance in CLU.

Understand confidence scores

If you're content with the model's performance after training, the model should then be
deployed to be consumed in a production environment. Deploying a model means making it
available for use via the analyze-conversation API

to detect the intent and extract entities

from a given text. The API returns a JSON object containing a list of all intents that were
created along with their confidence scores and the extracted entities each with its start index,
length, and confidence score.
These scores serve as an indicator of how confident the service is with the system's response. A
higher value indicates the service is more confident that the result is accurate. A confidence
score is between zero (0) and one (1), zero being no confidence and 1 being high confidence.
The returned score is directly affected by the data provided during tagging data.
The top two intents can have a very small score difference between them. CLU doesn't indicate
proximity. It only returns the scores for each created intent, which is calculated independently.
If the user's input is similar to utterances in the training data, an intent with a higher score will
be returned, and more accurate entities are extracted. If user input is different from the
utterances in the training data, scores are lower. To obtain an accurate prediction and a high
confidence score, provide multiple variations of an utterance with the same meaning.

Set confidence score thresholds
You might choose to make decisions in the system based on the intent confidence score the
system returns. The confidence score threshold the system uses can be adjusted to meet the
needs in your scenario. If it's more important to identify all potential intents of the text, use a
lower threshold. As a result, you might get more false positives but fewer false negatives. If it's
more important for the system to recognize only true intents of the text being analyzed, use a
higher threshold.
Different intent scenarios call for different approaches. For example, if there's an intent for
greetings or starting the bot, you might accept a lower threshold (0.40, for example) because
you want to accept multiple variations of the incoming text. But if there's an intent for a
specific action like making a phone call, you might want to set a higher threshold (0.95, for
example) to ensure accuracy of the predicted text. Returning and reviewing all intent scores is a
good way to verify that the correct intent is identified. The difference between the confidence
scores of the top returned intent and the next best returned intent should be significant.
If multiple intents have close prediction scores, you might want to review and add more
example utterances for each intent. Define a threshold for the difference between the top two
intents. If the difference is lower than the threshold defined, make programmatic choices about
handling such cases.

It's critical to test the system with any thresholds being added by using real data that the
system will process in production to determine the effects of various threshold values. At any
point, you can always continue to add example utterances with a wider variety of contextual
differences and retrain and redeploy your model.

Different training sessions and changes in evaluation
Retraining the same model without any changes in tagged data results in the same model
output, and as a result, the same evaluation scores. If you add or remove tags, the model
performance changes accordingly. The evaluation scores can then be compared with the
previous version of the model because both have the same files in the training and testing sets,
provided that no new files were added during tagging.
Adding new files or training a different model with random set splits leads to different files in
training and testing sets. Although changes in evaluation scores might be noticed, they can't
be directly compared to other models because performance is calculated on different splits for
testing sets.

Quality of the incoming text to the system affects results
CLU only processes text. The fidelity and formatting of the incoming text affects the
performance of the system. Make sure to consider the following:
Speech transcription quality might affect the quality of the results. If the source data is
voice, use the highest quality combination of automatic and human transcription to
ensure the best performance. Consider using custom speech models to obtain betterquality results.
Lack of standard punctuation or casing might affect the quality of the results. If you're
using a speech system, like Azure AI Speech to Text, select the option to include
punctuation.
Frequent misspellings in the training data might affect the confidence of the response.
Consider using a spell-checking service to correct misspelled words. However, introducing
spell-checking may not be the best solution. In all cases, the use of actual data, including
spelling mistakes, provides the best results.
The training data for CLU models is provided by the developer. Data used in production
that most closely resembles the training data yields the best performance.

Performance varies across features and languages
CLU gives users the option to use data in multiple languages. Developers can have multiple
files in a dataset with different languages. A model that's trained with one language can be

used to query text in other languages. Enable the multilingual option during project creation. If
low scores in a certain language are noticed, consider adding more data in this language to
your training set.
Not all features are at the same language parity. For example, language support in CLU varies
for prebuilt entities. You might find that performance for a particular feature isn't consistent
with another feature. Also, you might find that for a particular feature that performance isn't
consistent across various languages. Understand language support across CLU.

Next steps
Introduction to conversational language understanding
Language Understanding transparency note
Microsoft AI principles
Building responsible bots

Last updated on 06/24/2025

Data and privacy for conversational
language understanding
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides high-level information about how data is processed by conversational
language understanding (CLU). You're responsible for your use and implementation of this
technology, which includes complying with all laws and regulations that apply to you. For
example, it's your responsibility to:
Understand where your data is processed and stored by CLU to meet regulatory
obligations for your application.
Ensure that you have all necessary licenses, proprietary rights, or other permissions
required for the content in your dataset that's used as the basis for building your CLU
models.
It's your responsibility to comply with all applicable laws and regulations in your jurisdiction.

What data does CLU process?
CLU processes the following types of data:
Example utterances: Example utterances are the tagged text utterances provided by the
customer to train the custom CLU model. Providing the example utterances and tagging
them are prerequisites before you train the model. CLU users can provide example
utterances interactively through the Language Studio portal

or programmatically by

using CLU APIs.
Example utterances are split into train and test sets, where the split can either be
predefined by developers during tagging or chosen at random during training. The train
set and the tags file are processed during training to create the CLU model. The test set is
later processed by the trained model to evaluate its performance.
Natural language understanding (NLU) CLU model: Based on the user's request to train
the model, CLU processes the provided example utterances and tags to output a trained
NLU model. The user can choose to train a new model or overwrite an existing one. The
trained model is then stored on the service's side and used for processing the model
evaluation.

After you're content with the model's performance, you can request to deploy the model
for consumption use. The deployed model will also be stored on the service's side, which
is used to process the user's requests for prediction through the analyze conversation API.
Endpoint utterances: Endpoint utterances are the user's text utterances sent from a
customer's client application, for example, a chat bot, and received by the deployed CLU
model. Endpoint utterances are processed in real time in the Microsoft Azure cloud.
Output of the processed data contains predictions for the overall meaning and
extractions of the detailed information of the incoming text based on the customer's
customized model. Output is then returned to the client application to perform an action
to fulfill the user's request.
CLU doesn't collect customer data to improve its machine-learned models or for product
improvement purposes. We use aggregate telemetry, such as which APIs are used and the
number of calls from each subscription and resource, for service monitoring purposes.

How does CLU process data?
The following diagram illustrates how your data is processed.

How is data retained, and what customer controls
are available?
CLU is a data processor for General Data Protection Regulation (GDPR) purposes. In compliance
with GDPR policies, CLU users have full control to view, export, or delete any user content
either through the Language Studio

or programmatically by using CLU APIs. For more

information, see Export and delete your customer data.
Customer controls include:
Example utterances and tags provided by the CLU user as a prerequisite to train the
model are saved until the customer deletes this data. Customers can add example
utterances, edit them, or add and remove tags at any time.
Trained CLU models persist in Azure Storage accounts until the customer deletes the CLU
model or the whole project this model is part of.
Deployed CLU models persist in Azure Storage accounts until the customer deletes the
CLU model or the whole project this model is part of. The model is overridden each time
the user deploys to the same deployment name.

Security for customers' data
Azure services are implemented while maintaining appropriate technical and organizational
measures to protect customer data in the cloud.
To learn more about Microsoft's privacy and security commitments, see the Microsoft Trust
Center

.

Next steps
Introduction to conversational language understanding
Language Understanding transparency note
Microsoft AI principles
Building responsible bots

Last updated on 06/24/2025

Build multi-turn CLU models with entity
slot filling
In this article, learn how to build a CLU model that implements entity slot filling to facilitate
multi-turn conversations. With this approach, your model can incrementally collect the
required information across multiple conversation turns. Users don't need to provide all the
details in a single interaction. As a result, you can complete tasks more naturally and efficiently.

Prerequisites
Azure subscription - If you don't have one, you can create one for free

.

Required permissions - Ensure that the person establishing the account and project has
the Azure AI Account Owner role at the subscription level. Alternatively, the Contributor
or Cognitive Services Contributor role at the subscription scope also meets this
requirement. For more information, see Role based access control (RBAC).
Azure Language in Foundry Tools resource - Create a Language resource

in the Azure

portal.
ï¼— Note
You need the owner role assigned on the resource group to create a Language
resource.
Microsoft Foundry project - Create a project in Foundry. For more information, see
Create a Foundry project.
Deployed OpenAI model - Deploy an OpenAI model in Foundry as described in the
Deploy an OpenAI model section.

Configure required roles, permissions, and settings
Begin by configuring your Azure resources with the appropriate roles and permissions.

Add required roles for your Language resource
1. Navigate to your Language resource page in the Azure portal
Control (IAM) from the left navigation pane.

and select Access

2. Select Add > Add Role Assignments, and assign either the Cognitive Services Language
Owner or Cognitive Services Contributor role for your Language resource.
3. Under Assign access to, select User, group, or service principal.
4. Select Select members.
5. Choose your user name from the list. You can search for user names in the Select field.
Repeat this step for all required roles.
6. Repeat these steps for all user accounts that require access to this resource.

Connect your Azure Language resource to Foundry
To enable secure access, create a connection between your Language resource and Foundry.
This connection provides secure identity management, authentication, and controlled access to
your data.
ï¼— Note
The multi-turn capability is currently only available in the Foundry (classic) portal.
1. Navigate to Foundry (classic) .
2. Access your existing Foundry project for this tutorial.
3. Select Management center from the left navigation menu.
4. Select Connected resources from the Hub section of the Management center menu.
5. In the main window, select + New connection.
6. Select Language from the Add a connection to external assets window.
7. Select Add connection, then select Close.

Deploy an OpenAI model in Foundry
Deploy an OpenAI model to provide the foundational intelligence and advanced reasoning
capabilities for your CLU model.
1. Select Models + endpoints from the My assets section of the navigation menu.

2. From the main window, select + Deploy model.
3. Select Deploy base model from the dropdown menu.

4. In the Select a model window, choose a model. The gpt-4 base model is recommended
for this project.

5. Select Confirm.
6. In the Deploy gpt-4 window, retain the default values and select Deploy.

7. The model deployment is now complete.

Build your multi-turn model
Now that your Language resource, Foundry project, and OpenAI deployment are configured,
you're ready to build your CLU model.

Create a CLU project
In this section, you create a travel agent model and deploy it using Quick Deploy.
1. Navigate to Foundry (classic) .

2. If you aren't already signed in, the portal prompts you to authenticate with your Azure
credentials.
3. Once signed in, create or access your existing projects within Foundry.
4. If you're not already in your project for this task, select it.
5. In the left navigation pane Overview section, select Fine-tuning.
6. From the main window, select the AI Service fine-tuning tab, then select + Fine-tune.
7. In the Create service fine-tuning window, choose the Conversational language
understanding tab, then select Next.
8. In the Create CLU fine-tuning task window, complete the following fields:
Connected service - The name of your language service resource should appear by
default. If not, select it from the dropdown menu.
Name - Provide a name for your fine-tuning task project.
Language - English is set as the default and should already appear in the field.
Description - Optionally provide a description or leave this field empty.
9. Select Create. The creation operation may take a few minutes to complete.

Add intents
1. From the Getting Started menu, select Define schema.
2. In the main window, select Add Intent.
3. The Add Intent window contains two required fields:
Intent name(Pascal case)
Intent description (required for Quick Deploy)
4. After completing these fields, select + Add to create your intents.
5. After defining all intents, select Add Intent.

Add entities
1. Select the Entities tab, then select Add entity.
2. The Add an entity window contains two required fields:
Entity name (Pascal case)|

Entity description
3. After completing the entity fields, select Add an entity.

Associate intents with entities
1. Select the Associations tab.
2. Select each intent and link it to the required entities from the Available entities section.
This step ensures the model knows which slots to fill for each intent.
3. All entities must have an association with at least one intent. After configuring the
associations, select Update associations.
Once all entities have associations, you can proceed with Quick Deploy using a large language
model (LLM).

Quick deploy with LLM
1. From the Getting Started menu, select Train model.
2. In the Train your model section, select Quick deploy with LLM.
3. Complete the Quick deploy with LLM window fields:
Deployment name - Provide a name for the deployment.
Select Azure OpenAI Model Deployment - Choose the Azure OpenAI model
deployment you created for this project.
Deployment regions - Select the region associated with your Azure Language
resource.
4. Select Create. Foundry manages the configuration and deployment processes through
backend operations.

Test in playground
1. From the Getting Started menu, select Deploy your model.
2. In the main window, select your model.
3. Select Try in playground.
4. In the playground, select the Conversational language understanding tile.

5. Under the Configuration menu on the left, verify that the following fields are completed
correctly:
Project name - Ensure this matches the project you created for this CLU fine-tuning
task.
Deployment name - Verify this matches the name you assigned to your OpenAI
model.
6. Select the Multi-turn understanding checkbox.
7. Simulate a multi-turn dialog by entering a conversation.
8. Select Run.
9. The model returns a response in both Text and JSON formats.
10. In the Details panel on the right, review the Top Intent and detected Entities.

That's it! You successfully created a multi-turn CLU model with entity slot filling capabilities to
collect required information across multiple dialog turns.

Clean up resources
To clean up and remove an Azure AI resource, delete either the individual resource or the
entire resource group. Deleting the resource group removes all contained resources.

Related content
Learn how CLU handles entity slot-filling across multi-turn conversations

Last updated on 11/18/2025

Install and run Conversational Language
Understanding (CLU) containers
ï¼— Note
The data limits in a single synchronous API call for the CLU container are 5,120 characters
per document and up to 10 documents per call.
Containers enable you to host the CLU API on your own infrastructure. If you have security or
data governance requirements that can't be fulfilled by calling CLU remotely, then containers
might be a good option.
If you don't have an Azure subscription, create a free account

before you begin.

Prerequisites
You must meet the following prerequisites before using CLU containers.
An active Azure subscription. If you don't have one, create a free account .
Docker

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts

.

A Language resource

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:

Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the available
container. Each CPU core must be at least 2.6 gigahertz (GHz) or faster.
We recommended that you have a CPU with AVX-512 instruction set, for the best experience
(performance and accuracy).
ï¾‰

Processor

Minimum host specs

Recommended host specs

CPU

1-core, 2-GB memory

4-cores, 8-GB memory

Expand table

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Export your Conversational Language
Understanding model
Before you proceed with running the docker image, you need to export your own trained
model to expose it to your container. Use the following command to extract your model and
replace the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Language
resource. You can find it on

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

your resource's Key and
endpoint page, on the Azure
portal.
{ENDPOINT_URI}

The endpoint for accessing
the Conversational Language
Understanding API. You can
find it on your resource's Key
and endpoint page, on the
Azure portal.

https://<your-customsubdomain>.cognitiveservices.azure.com

Placeholder

Value

Format or example

{PROJECT_NAME}

The name of the project
containing the model that

myProject

you want to export. You can
find it on your projects tab in
Azure Language Studio
portal.
{TRAINED_MODEL_NAME}

The name of the trained
model you want to export.
You can find your trained

myTrainedModel

models on your model
evaluation tab under your
project in Azure Language
Studio portal
{EXPORTED_MODEL_NAME}

The name to assign for the
new exported model created.

myExportedModel

Bash
curl --location --request PUT '{ENDPOINT_URI}/language/authoring/analyzeconversations/projects/{PROJECT_NAME}/exported-models/{EXPORTED_MODEL_NAME}?apiversion=2024-11-15-preview' \
--header 'Ocp-Apim-Subscription-Key: {API_KEY}' \
--header 'Content-Type: application/json' \
--data-raw '{
"TrainedModelLabel": "{TRAINED_MODEL_NAME}"
}'

Get the container image with docker pull
The CLU container image can be found on the mcr.microsoft.com container registry syndicate.
It resides within the azure-cognitive-services/language/ repository and is named clu . The
fully qualified container image name is, mcr.microsoft.com/azure-cognitiveservices/language/clu

To use the latest version of the container, you can use the latest tag, which is for English. You
can also find a full list of containers for supported languages using the tags on the MCR

.

The latest CLU container is available in several languages. To download the container for the
English container, use the following command:
Bash

docker pull mcr.microsoft.com/azure-cognitive-services/language/clu:latest

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container in download model mode
After the exported model is created, users have to run the container to download the
deployment package that was created specifically for their exported models.
ï¾‰

Placeholder

Value

Format or example

{API_KEY}

The key for your Language
resource. You can find it on

See Azure portal

Expand table

your resource's Key and
endpoint page, on the
Azure portal.
{ENDPOINT_URI}

The endpoint for accessing
the API. You can find it on
your resource's Key and

https://<your-customsubdomain>.cognitiveservices.azure.com

endpoint page, on the
Azure portal.
{IMAGE_TAG}

The image tag representing
the language of the

latest

container you want to run.
Make sure the tag matches
the docker pull command
you used.
{LOCAL_CLU_PORT}

Port number assigned for
the container in local

5000

Placeholder

Value

Format or example

machine.
{LOCAL_MODEL_DIRECTORY}

Absolute directory in host

C:\usr\local\myDeploymentPackage

machine where exported
models are saved in.
{PROJECT_NAME}

Name of the project that

myProject

the exported model
belongs to
{EXPORTED_MODEL_NAME}

Exported model to be
downloaded

myExportedModel

Bash
docker run --rm -it -p {LOCAL_CLU_PORT}:80 \
mcr.microsoft.com/azure-cognitive-services/language/clu:{IMAGE_TAG} \
-v {LOCAL_MODEL_DIRECTORY}:/DeploymentPackage \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY} \
downloadmodel \
projectName={PROJECT_NAME} \
exportedModelName={EXPORTED_MODEL_NAME}

DO NOT alter the downloaded files. Even altering the name or folder structure can affect the
integrity of the container and might break it.
Repeat those steps to download as many models as you'd like to test. Your models can belong
to different projects and have different (exported) model names.

Run the container with docker run
Once the container is on the host computer, use the docker run

command to run the

containers. The container continues to run until you stop it. Replace the placeholders with your
own values:
ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove the back slash based on your host
operating system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container doesn't start. For more information, see Billing.

To run the CLU container, execute the following docker run command. Replace the
placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your
Language resource. You
can find it on your

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

resource's Key and
endpoint page, on the
Azure portal.
{ENDPOINT_URI}

The endpoint for
accessing the API. You
can find it on your
resource's Key and

https://<your-customsubdomain>.cognitiveservices.azure.com

endpoint page, on the
Azure portal.
{IMAGE_TAG}

The image tag
representing the
language of the container
you want to run. Make

latest

sure the tag matches the
docker pull command
you used.
{LOCAL_CLU_PORT}

Port number assigned for
the container in local
machine.

5000

{LOCAL_NER_PORT}

Port number of the NER
container. See Run NER

5001 (Has to be different than the port
number)

Container section.
{LOCAL_LOGGING_DIRECTORY}

Absolute directory in host

C:\usr\local\mylogs

machine where that logs
are saved in.
{LOCAL_MODEL_DIRECTORY}

Absolute directory in host
machine where exported
models are saved in.

C:\usr\local\myDeploymentPackage

Bash
docker run --rm -it -p 5000:5000 --memory 8g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/language/clu:{IMAGE_TAG} \

Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY}

This command:
Runs a CLU container from the container image
Allocates one CPU core and 8 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Running named entity recognition (NER) Container
CLU relies on NER to handle prebuilt entities. The CLU container works properly without NER if
users decide not to integrate it. NER billing is turned off when accessed via CLU, so there are
no added charges unless you make a direct call to the NER container.
To set up NER in CLU container
Follow the NER container documentation.
When running CLU container, make sure to set the parameter Ner_Url so that
Ner_Url=http://host.docker.internal:{LOCAL_NER_PORT}

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running

There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Expand table

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

For information on how to call CLU see our guide.

Run the container disconnected from the internet
To use this container disconnected from the internet, you must first request access by filling
out an application, and purchasing a commitment plan. See Use Docker containers in
disconnected environments for more information.
If you have been approved to run the container disconnected from the internet, use the
following example shows the formatting of the docker run command you'll use, with
placeholder values. Replace these placeholder values with your own values.
The DownloadLicense=True parameter in your docker run command will download a license file
that will enable your Docker container to run when it isn't connected to the internet. It also
contains an expiration date, after which the license file will be invalid to run the container. You
can only use a license file with the appropriate container that you've been approved for. For
example, you can't use a license file for a speech to text container with a Document
Intelligence container.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image

mcr.microsoft.com/azure-cognitive-

you want to use.

services/form-recognizer/invoice

The path where the
license will be

/host/license:/path/to/license/directory

{LICENSE_MOUNT}

downloaded, and
mounted.
{ENDPOINT_URI}

The endpoint for
authenticating your
service request. You can
find it on your

https://<your-customsubdomain>.cognitiveservices.azure.com

resource's Key and
endpoint page, on the
Azure portal.
{API_KEY}

The key for your Text
Analytics resource. You
can find it on your

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

resource's Key and
endpoint page, on the
Azure portal.
{CONTAINER_LICENSE_DIRECTORY}

Location of the license
folder on the

/path/to/license/directory

Placeholder

Value

Format or example

container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 \
-v {LICENSE_MOUNT} \
{IMAGE} \
eula=accept \
billing={ENDPOINT_URI} \
apikey={API_KEY} \
DownloadLicense=True \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}

Once the license file has been downloaded, you can run the container in a disconnected
environment. The following example shows the formatting of the docker run command you'll
use, with placeholder values. Replace these placeholder values with your own values.
Wherever the container is run, the license file must be mounted to the container and the
location of the license folder on the container's local filesystem must be specified with
Mounts:License= . An output mount must also be specified so that billing usage records can be

written.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitive-

{MEMORY_SIZE}

The appropriate size
of memory to

services/form-recognizer/invoice
4g

allocate for your
container.
{NUMBER_CPUS}

The appropriate
number of CPUs to

4

allocate for your
container.
{LICENSE_MOUNT}

The path where the

/host/license:/path/to/license/directory

license will be
located and
mounted.
{OUTPUT_PATH}

The output path for
logging usage

/host/output:/path/to/output/directory

Placeholder

Value

Format or example

records.
{CONTAINER_LICENSE_DIRECTORY}

Location of the
license folder on the

/path/to/license/directory

container's local
filesystem.
{CONTAINER_OUTPUT_DIRECTORY}

Location of the
output folder on the

/path/to/output/directory

container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 --memory {MEMORY_SIZE} --cpus {NUMBER_CPUS} \
-v {LICENSE_MOUNT} \
-v {OUTPUT_PATH} \
{IMAGE} \
eula=accept \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}
Mounts:Output={CONTAINER_OUTPUT_DIRECTORY}

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with both an output mount and logging enabled, the container
generates log files. The log files can help you troubleshoot any issues that occur during startup
or while the container is running.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing

The CLU containers send billing information to Azure, using a Language resource on your Azure
account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure
The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If
the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

For more information about these options, see Configure containers.

Summary

In this article, you learned concepts and workflow for downloading, installing, and running CLU
containers. In summary:
CLU provides Linux containers for Docker
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You must specify billing information when instantiating a container.
ï¼‰ Important
Azure AI containers aren't licensed to run without being connected to Azure for metering.
Customers must enable containers to always communicate billing information to the
metering service. Azure AI containers don't send customer data (for example, text that is
being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

Create a fine-tuning task project
A Conversational Language Understanding (CLU) fine-tuning task is a workspace project where
you customize a language model to identify user intent and extract key information (entities)
from user input (utterances). In this workspace, you define the intents and entities relevant to
your application, label sample user utterances accordingly, and use this labeled data to finetune the model. This process tailors the model to better understand the specific needs and
nuances of your conversational application. In this guide, we walk you through configuring a
fine-tuning workspace in the Microsoft Foundry or using the REST API.
ï¼— Note
If you already have an Azure Language in Foundry Tools or multi-service resourceâ€”
whether used on its own or through Language Studioâ€”you can continue to use
those existing Language resources within the Foundry portal. For more information,
see How to use Foundry Tools in the Foundry portal.
In Foundry, a fine-tuning task serves as your workspace when customizing your CLU
model. Previously, a fine-tuning task was referred to as a CLU project. You might
encounter both terms used interchangeably in older CLU documentation.
We highly recommend that you use a Foundry resource in the Foundry; however, you
can also follow these instructions using a Language resource.

Prerequisites
An Azure subscription. If you don't have one, you can create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
An Foundry resource. For more information, see Configure a Foundry resource.
Alternately, you can use a Language resource .
A Foundry project created in the Foundry. For more information, see Create a Foundry
project.

Fine-tune a CLU model

To create a CLU fine-tuning model, you first configure your environment and then create a
fine-tuning project, which serves as your workspace for customizing your CLU model.
Foundry

1. Navigate to the Foundry

.

2. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
3. Once signed in, you can create or access your existing projects within Foundry.
4. If you're not already at your project for this task, select it.
5. Select Fine-tuning from the left navigation panel.
Screenshot of fine-tuning selector in the Foundry.
6. From the main window, select the AI Service fine-tuning tab and then the + Finetune button.
Screenshot of fine-tune button in the Foundry.
7. From the Create service fine-tuning window, choose the Conversational language
understanding tab, and then select Next.

8. In the Create CLU fine-tuning task window, complete the Name and Language
fields. If you're planning to fine-tune a model using the free Standard Training mode,
select English for the language field.
9. Select the Create button. It can take a few minutes for the creating operation to
complete.
ï¼— Note
Standard training enables faster training times and quicker iterations;
however it's only available for English.

Advanced training includes longer training durations and is supported for
English, other languages, and multilingual projects.
For more information, see Training modes.
10. Once the task creation is complete, select the task from the Foundry Tool fine-tuning
window to arrive at the Getting started with fine-tuning page.

ï Š

That's it! You can get started on your fine-tuning task project. For more information, see Next
steps.

View and manage project details
You can retrieve up-to-date information about your projects, make any necessary changes, and
oversee project management tasks efficiently through the Foundry or REST API endpoints.
Foundry

Your Foundry project overview page displays information about your fine-tuning task
project, including its name, subscription, resource group, and connected resources. You
can also access the project's resources in the Azure portal by selecting Manage in Azure
portal on the overview page.

On the project Home page, information about the project is found in the Project
details section.
To view project settings, select Management center from the bottom of the left
navigation pane, then select one of the following tabs:
Overview to view project details.
Users to manage users and roles.
Models + endpoints to manage deployments of your models and services.
Connected resources to manage connected resources for the project.
Screenshot of the project details list in the Foundry.

Import an existing Foundry project
Importing the configuration file allows you to bring your existing settings directly into the
platform, making it easier to set up and customize your service based on your predefined
preferences.
Foundry

To import an existing Foundry Tools project with Foundry, you need to create a connection
to the Foundry Tools resource within your Foundry project. For more information, see
Connect Foundry Tools projects to Foundry

Export a fine-tuning project
Exporting your configuration file enables you to save the current state of your project's settings
and structure, making it easy to back up or transfer your project as needed.
Foundry

You can download a Microsoft Foundry fine-tuning task project as a config.json file:
1. Navigate to your project home page.
2. At the top of the page, select your project from the right page ribbon area.
3. Select Download config file.

Delete a project
Deleting a project ensures that it and all of its associated data are permanently removed from
the system.

Foundry

If you no longer need your project, you can delete it from the Foundry.
1. Navigate to the Foundry

home page. Initiate the authentication process by signing

in, unless you already completed this step and your session is active.
2. Select the project that you want to delete from the Keep building with Foundry
3. Select Management center.
4. Select Delete project.

To delete the hub along with all its projects:
1. Navigate to the Overview tab inn the Hub section.

2. On the right, select Delete hub.

3. The link opens the Azure portal for you to delete the hub.

Next steps
After you create your fine-tuning workspace, start your fine-tuning task by defining your
intents and entities and adding them to your schema:
Build your fine-tuning schema
Label utterances

Last updated on 11/18/2025

Build your fine-tuning schema
In conversational language understanding projects, the schema is defined as the combination
of intents and entities within your project. Schema design is a crucial part of your project's
success. When you create a schema, think about which intents and entities should be included
in your project.

Guidelines and recommendations
Consider the following guidelines when you choose intents for your project:
Create distinct, separable intents. An intent is best described as action that the user
wants to perform. Think of the project you're building and identify all the different actions
that your users might take when they interact with your project. Sending, calling, and
canceling are all actions that are best represented as different intents. "Canceling an
order" and "canceling an appointment" are similar, with the distinction being what they're
canceling. Those two actions should be represented under the same intent, cancel.
Create entities to extract relevant pieces of information within your text. The entities
should be used to capture the relevant information that's needed to fulfill your user's
action. For example, order or appointment could be different things that a user is trying to
cancel, and you should create an entity to capture that piece of information.
You can "send a message," "send an email," or "send a package." Creating an intent to capture
each of those requirements won't scale over time, and you should use entities to identify what
the user was sending. The combination of intents and entities should determine your
conversation flow.
For example, consider a company where the bot developers identified the three most common
actions that their users take when they use a calendar:
Set up new meetings.
Respond to meeting requests.
Cancel meetings.
They might create an intent to represent each of these actions. They might also include entities
to help complete these actions, such as:
Meeting attendants
Date
Meeting durations

Add intents

To build a project schema within Foundry

:

1. On the left pane, select Define schema.
2. Select the Intents or Entities tabs.
3. To create an intent, select + Add intent. You're prompted to enter names and
descriptions for as many intents as you want to create. Descriptions are required only for
using the Quick Deploy option to help Azure OpenAI better understand the context of
your intents.
4. Repeat the steps to develop intents that encompass all the actions that the user is likely
to perform while interacting with the project.

ï Š

5. If you want to continue with data labeling and advanced training a custom CLU model, on
the left pane, select Manage data to add examples for intents and label them with
entities, if desired.

Add entities
1. Select the Entities tab.
2. To add an entity, select + Add entity. You're prompted to enter a name to create the
entity.
3. After you create an entity, you can select the entity name to change the Entity
components type. Multiple components like learned, list, regex, or prebuilt are used to
define every entity. A learned component is added to all your entities after you label them
in your utterances.

ï Š

4. You can also add a list, regex, or prebuilt component to each entity.

Add a prebuilt component
To add a prebuilt component, select the prebuilt type from the dropdown menu in the Entity
options section.

Add a list component
To add a list component, select Add list. You can add multiple lists to each entity:
1. Create a new list, and in the List key text box, enter the normalized value that was
returned when any of the synonyms values were extracted.
2. Enter your synonyms and select Enter after each one. We recommend having a synonym
list in multiple languages.

Add a regex component

To add a regex component, select Add expression. Name the regex key, and enter a regular
expression that matches the entity to be extracted.

Define entity options
Select the Entity Options tab on the entity details page. When multiple components are
defined for an entity, their predictions might overlap. When an overlap occurs, each entity's
final prediction is determined based on the entity option that you select in this step. Select the
option that you want to apply to this entity, and then select Save.
After you create your entities, you can come back and edit them. You can edit entity
components or delete them by selecting Edit or Delete.

Related content
Add utterances and label your data

Last updated on 11/18/2025

Label your utterances in Microsoft Foundry
After you build a schema for your fine-tuning task, you add training utterances to your project.
The utterances should be similar to what your users use when they interact with the project.
When you add an utterance, you have to assign which intent it belongs to. After the utterance
is added, label the words within your utterance that you want to extract as entities.
Data labeling is a crucial step in the conversational language understanding (CLU) trained
development lifecycle. This data is used in the next step when you train your model so that
your model can learn from the labeled data. If you already labeled utterances, you can directly
import them into your project, if your data follows the accepted data format. To learn more
about importing labeled data, see Create a CLU fine-tuning task. Labeled data informs the
model about how to interpret text and is used for training and evaluation.
îª€ Tip
Use the Quick Deploy option to implement custom CLU intent routing, which is powered
by your own large language model deployment without adding or labeling any training
data.

Prerequisites
A successfully created project.
For more information, see the CLU development lifecycle.

Data labeling guidelines
After you build your schema and create your project, you need to label your data. Labeling
your data is important so that your model knows which sentences and words are associated
with the intents and entities in your project. Spend time labeling your utterances to introduce
and refine the data that's used in training your models.
As you add utterances and label them, keep in mind:
The machine learning models generalize based on the labeled examples that you provide.
The more examples that you provide, the more data points the model has to make better
generalizations.
The precision, consistency, and completeness of your labeled data are key factors to
determining model performance:

Label precisely: Label each intent and entity to its right type always. Only include what
you want classified and extracted. Avoid unnecessary data in your labels.
Label consistently: The same entity should have the same label across all the
utterances.
Label completely: Provide varied utterances for every intent. Label all the instances of
the entity in all your utterances.

Clearly label utterances
Ensure that the concepts that your entities refer to are well defined and separable. Check
if you can easily determine the differences reliably. If you can't, this lack of distinction
might indicate difficulty for the learned component.
Ensure that some aspect of your data can provide a signal for differences when there's a
similarity between entities.
For example, if you built a model to book flights, a user might use an utterance like "I
want a flight from Boston to Seattle." The origin city and destination city for such
utterances would be expected to be similar. A signal to differentiate origin city might be
that the word from often precedes it.
Ensure that you label all instances of each entity in both your training and testing data.
One approach is to use the search function to find all instances of a word or phrase in
your data to check if they're correctly labeled.
Ensure that you label test data for entities without learned components and also for the
entities with them. This practice helps to ensure that your evaluation metrics are accurate.
For multilingual projects, adding utterances in other languages increases the model's
performance in these languages. Avoid duplicating your data across all the languages
that you want to support. For example, to improve a calender bot's performance with
users, a developer might add examples mostly in English and a few in Spanish or French.
They might add utterances such as:
Set a meeting with **Matt** and **Kevin** **tomorrow** at **12 PM**. (English)
Reply as **tentative** to the **weekly update** meeting. (English)
Cancelar mi **prÃ³xima** reuniÃ³n. (Spanish)

Label your utterances
Use the following steps to label your utterances:
1. Go to your project page in Microsoft Foundry

.

2. On the left pane, select Manage data. On this page, you can add your utterances and
label them. You can also upload your utterances directly by selecting Upload utterance
file from the top menu. Make sure to follow the accepted format.
3. By using the top tabs, you can change the view to Training set or Testing set. Learn more
about training and testing sets and how they're used for model training and evaluation.

ï Š

A screenshot that shows the page for tagging utterances in Foundry.
îª€ Tip
If you plan to use Automatically split the testing set from training data splitting,
add all your utterances to the training set.
4. From the Select intent dropdown menu, select one of the intents, the language of the
utterance (for multilingual projects), and the utterance itself. Press the Enter key in the
utterance's text box and add the utterance.
5. You have two options to label entities in an utterance:
ï¾‰

Expand table

Option

Description

Label by using a

Select the brush icon next to an entity in the pane on the right, and then

brush

highlight the text in the utterance that you want to label.

Label by using
inline menu

Highlight the word that you want to label as an entity, and a menu appears.
Select the entity that you want to label these words with.

6. In the pane on the right, on the Labels tab, you can find all the entity types in your
project and the count of labeled instances per each one.
7. On the Distribution tab, you can view the distribution across training and testing sets.
You have these options for viewing:
Total instances per labeled entity: You can view the count of all labeled instances of
a specific entity.
Unique utterances per labeled entity: Each utterance is counted if it contains at
least one labeled instance of this entity.
Utterances per intent: You can view the count of utterances per intent.

ï Š

ï¼— Note
List, regex, and prebuilt components aren't shown on the data labeling page. All labels
here apply to the learned component only.
To remove a label:
1. From within your utterance, select the entity from which you want to remove a label.
2. Scroll through the menu that appears, and select Remove label.
To delete an entity:
1. Select the garbage bin icon next to the entity that you want to edit in the pane on the
right.
2. Select Delete to confirm.

Suggest utterances with Azure OpenAI
In CLU, use Azure OpenAI to suggest utterances to add to your project by using generative
language models. We recommend that you use a Foundry resource while you use CLU so that
you don't need to connect multiple resources.
To use the Foundry resource, you need to provide your Foundry resource with elevated access.
To do so, access the Azure portal. Within your Azure AI resource, provide access as a Cognitive

Services User to itself. This step ensures that all parts of your resource are communicating
correctly.

Connect with separate Language and Azure OpenAI resources
You first need to get access and create a resource in Azure OpenAI. Next, create a connection
to the Azure OpenAI resource within the same Foundry project in the Management center on
the left pane of the Foundry page. You then need to create a deployment for the Azure OpenAI
models within the connected Azure OpenAI resource. To create a new resource, follow the
steps in Create and deploy an Azure OpenAI in Foundry Models resource.
Before you get started, the suggested utterances feature is available only if your Language
resource is in the following regions:
East US
South Central US
West Europe
On the Data labeling page:
1. Select Suggest utterances. A pane opens and prompts you to select your Azure OpenAI
resource and deployment.
2. After you select an Azure OpenAI resource, select Connect so that your Language
resource has direct access to your Azure OpenAI resource. It assigns your Language
resource the Cognitive Services User role to your Azure OpenAI resource. Now your
current Language resource has access to Azure OpenAI. If the connection fails, follow
these steps to manually add the correct role to your Azure OpenAI resource.
3. After the resource is connected, select the deployment. The model that we recommend
for the Azure OpenAI deployment is gpt-35-turbo-instruct .
4. Select the intent for which you want to get suggestions. Make sure the intent that you
selected has at least five saved utterances to be enabled for utterance suggestions. The
suggestions provided by Azure OpenAI are based on the most recent utterances that you
added for that intent.
5. Select Generate utterances.
The suggested utterances show up with a dotted line around them and the note
Generated by AI. Those suggestions must be accepted or rejected. Accepting a
suggestion adds it to your project, as if you had added it yourself. Rejecting a suggestion
deletes it entirely. Only accepted utterances are part of your project and used for training
or testing.

To accept or reject, select the green check mark or red cancel buttons beside each
utterance. You can also use Accept all and Reject all on the toolbar.

ï Š

Use of this feature entails a charge to your Azure OpenAI resource for a similar number of
tokens to the suggested utterances that are generated. For information on Azure OpenAI
pricing, see Azure OpenAI Service pricing

.

Add required configurations to Azure OpenAI resource
Enable identity management for your Language resource by using the following options.
Azure portal

Your Language resource must have identity management. To enable it by using the Azure
portal

:

1. Go to your Language resource.
2. On the left pane, under the Resource Management section, select Identity.
3. On the System assigned tab, set Status to On.

After you enable managed identity, assign the Cognitive Services User role to your Azure
OpenAI resource by using the managed identity of your Language resource.
1. Sign in to the Azure portal

and go to your Azure OpenAI resource.

2. Select the Access Control (IAM) tab.
3. Select Add > Add role assignment.
4. Select Job function roles and select Next.
5. Select Cognitive Services User from the list of roles, and select Next.
6. Select Assign access to: Managed identity and choose Select members.
7. Under Managed identity, select Language.
8. Search for your resource and select it. Then select Next and complete the process.
9. Review the details and select Review + assign.
Multiple screenshots that show the steps to add the required role to your Azure
OpenAI resource.
After a few minutes, refresh Foundry, and you can successfully connect to Azure OpenAI.

Related content
Train your conversational language understanding model

Last updated on 11/18/2025

ï Š

Train a conversational language
understanding model
After you complete labeling your utterances, you can start training a model. Training is the
process where the model learns from your labeled utterances.
To train a model, start a training job. Only successfully completed jobs create a model. Training
jobs expire after seven days, then you can no longer retrieve the job details. If your training job
completed successfully and a model was created, the job doesn't expire. You can only have one
training job running at a time, and you can't start other jobs in the same fine tuning task.
ï¼— Note
When using the Quick Deploy option, Conversational Language Understanding (CLU)
automatically creates an instant training job to set up your CLU intent router using your
selected LLM deployment.
The training times can be anywhere from a few seconds for simple projects, up to several hours
when you reach the maximum limit of utterances.
Model evaluation is triggered automatically after training is completed successfully. The
evaluation process starts by using the trained model to run predictions on the utterances in the
testing set, and compares the predicted results with the provided labels (which establishes a
baseline of truth).

Prerequisites
An active Azure subscription. If you don't have one, you can create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
A project created in the Microsoft Foundry. For more information, see Create a Foundry
project.
Your labeled utterances tagged for your fine tuning task.

Balance training data

When it comes to training data, try to keep your schema well-balanced. Including large
quantities of one intent and few of another results in a model with bias towards particular
intents.
To address this scenario, you might need to down sample your training set. Or you might need
to add to it. To down sample, you can:
Get rid of a certain percentage of the training data randomly.
Analyze the dataset and remove overrepresented duplicate entries, which is a more
systematic manner.
To add to the training set, in Language Studio, on the Data labeling tab, select Suggest
utterances. Conversational Language Understanding sends a call to Azure OpenAI to generate
similar utterances.

ï Š

You should also look for unintentional patterns in the training set. For example, look to see if
the training set for a particular intent is all lowercase or starts with a particular phrase. In such
cases, the model you train might learn these unintended biases in the training set instead of
being able to generalize.
We recommend that you introduce casing and punctuation diversity in the training set. If your
model is expected to handle variations, be sure to have a training set that also reflects that
diversity. For example, include some utterances in proper casing and some in all lowercase.

Data splitting

Before you start the training process, labeled utterances in your project are divided into a
training set and a testing set. Each one of them serves a different function:
The training set is used in training the model, the set from which the model learns the
labeled utterances.
The testing set is a blind set that isn't introduced to the model during training but only
during evaluation.
After the model is trained successfully, the model can be used to make predictions from the
utterances in the testing set. These predictions are used to calculate evaluation metrics. We
recommend that you make sure that all your intents and entities are adequately represented in
both the training and testing set.
Conversational language understanding supports two methods for data splitting:
Automatically splitting the testing set from training data: The system splits your tagged
data between the training and testing sets, according to the percentages you choose. The
recommended percentage split is 80% for training and 20% for testing.
ï¼— Note
If you choose the Automatically splitting the testing set from training data option, only
the data assigned to a training set is split according to the percentages provided.
Use a manual split of training and testing data: This method enables users to define
which utterances should belong to which set. This step is only enabled if you added
utterances to your testing set during labeling.

Training modes
Conversational Language Understanding (CLU) supports two modes for training your models
Standard training uses fast machine learning algorithms to quickly train your models.
This training level is currently only available for English and is disabled for any project
that doesn't use English (US), or English (UK) as its primary language. This training option
is free of charge. Standard training allows you to add utterances and test them quickly
free of charge. The evaluation scores shown should guide you on where to make changes
in your project and add more utterances. While standard training is best for testing and
updating your model quickly, you should see better model quality when using advanced
training. While standard training is best for testing and updating your model quickly, you
should see better model quality when using advanced training. Once you iterate a few

times and made incremental improvements, you can consider using advanced training to
train another version of your model.
Advanced training uses the latest in machine learning technology to customize models
with your data. This training level is expected to show better performance scores for your
models and enables you to use the multilingual capabilities of CLU as well. Advanced
training is priced differently. See the pricing information

for details.

Use the evaluation scores to guide your decisions. There may be times where a specific
example is predicted incorrectly in advanced training as opposed to when you used standard
training mode. However, if the overall evaluation results are better using advanced training,
then we recommend that you use that model as your final model. If that isn't the case and you
aren't looking to use any multilingual capabilities, you can continue to use model trained with
standard mode.
ï¼— Note
You should expect to see a difference in behaviors in intent confidence scores between
the training modes as each algorithm calibrates their scores differently.

Train your model
Foundry

1. Navigate to the Foundry

.

2. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
3. Once signed in, you can create or access your existing projects within Foundry.
4. If you're not already at your project for this task, select it.
5. Select Fine-tuning from the left navigation pane.

6. Select the AI Service fine-tuning tab and then + Fine-tune button.

7. From Create service fine-tuning window, choose the Conversational language
understanding tab then select Next.

8. In Create CLU fine tuning task window, select your Connected service from the
drop-down menu, then complete the Name and Language fields. If you're using the
free Standard Training mode, select English for the language field.
9. Select the Create button. It may take a few minutes for the operation to complete.
ï¼— Note
Standard training enables faster training times and quicker iterations;
however it's only available for English.
Advanced training includes longer training durations and is supported for
English, other languages, and multilingual projects.
For more information, see Training modes.
10. From the immediate left navigation pane, choose Train model.

11. Next, select the + Train model button from the main window.

12. In the Train a new model window, select one of the following:
Create a new training model. Enter a new Model name
Overwrite an existing model name. Replace an existing model trained on the
new data.
13. Select Your current training version. The training version is the algorithm that
determines how your model learns from your data. The machine learning used to
train models is regularly updated. We recommend using the latest version for
training, as it underwent thorough testing and provides the most balanced model
predictions from your data.

14. Select Next.
15. Select one of the Data splitting methods presented in the Train a new model
window:
Automatically split the testing set from training data enables the system to
split your utterances between the training and testing sets, according to the
specified percentages.
Use a manual split of training and testing data enables the system to use the
training and testing sets that you assigned and labeled to create your custom
model. *This option is only available if you added utterances to your testing
set when you labeled your utterances.

16. Select Next and then select Create.
17. Choose the training job ID from the list. A panel appears that details the training
progress, job status, and other details for this job.
ï¼— Note
Only successfully completed training jobs generate models.
Training can take from a few minutes to a few hours based on the count of
utterances.
You can only have one training job running at a time. You can't start other
training jobs within the same project until the running job is completed.

Cancel training job
Foundry

When you're done with your custom model, you can delete the deployment and model.
You can also delete the training and validation files you uploaded to the service, if needed:
To delete your custom model, on the left navigation pane select My assets â†’ Models
+ endpoints. Choose the custom model to delete from the Model deployments tab,
and then select Delete.
To delete your training and validation files uploaded for training, on the left
navigation pane select Data + indexes. Choose the file to delete, and then select
Delete.

Next steps
Review your model's performance with model evaluation metrics.

Last updated on 01/11/2026

View conversational language
understanding model details
After model training is completed, you can view your model details and see how well it
performs against the test set.
ï¼— Note
Using the Automatically split the testing set from training data option may result in
different model evaluation result every time you train a new model, as the test set is
selected randomly from your utterances. To make sure that the evaluation is calculated on
the same test set every time you train a model, make sure to use the Use a manual split of
training and testing data option when starting a training job and define your Testing set
when add your utterances.

Prerequisites
Before viewing a model's evaluation, you need:
A successfully created project.
A successfully trained model.
See the project development lifecycle for more information.

Model details
Create a GET request using the following URL, headers, and JSON body to get the trained
model evaluation summary.

Model Summary
This API returns the summary of your model's evaluation results, including the precision, recall,
F1, and confusion matrix of your intents and entities.

Request URL
rest
{ENDPOINT}/language/authoring/analyzeconversations/projects/{projectName}/models/{trainedModelLabel}/evaluation/summary-

result?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for
authenticating your API

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

request.
{PROJECT-NAME}

The name for your project.

myProject

This value is case-sensitive.
{trainedModelLabel}

The name for your trained

Model1

model. This value is casesensitive.
{API-VERSION}

The version of the API you're

2023-04-01

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"entitiesEvaluation": {
"confusionMatrix": {
"additionalProp1": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0

},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp2": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp3": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
}
},
"entities": {
"additionalProp1": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp2": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp3": {

"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
}
},
"microF1": 0,
"microPrecision": 0,
"microRecall": 0,
"macroF1": 0,
"macroPrecision": 0,
"macroRecall": 0
},
"intentsEvaluation": {
"confusionMatrix": {
"additionalProp1": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp2": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp3": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},

"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
}
},
"intents": {
"additionalProp1": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp2": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp3": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
}
},
"microF1": 0,
"microPrecision": 0,
"microRecall": 0,
"macroF1": 0,
"macroPrecision": 0,
"macroRecall": 0
},
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 0,
"testingSplitPercentage": 0
}
}

Model Results

This API returns the individual results for each utterance including their expected and actual
predictions for intents and entities.

Request URL
rest
{ENDPOINT}/language/authoring/analyzeconversations/projects/{projectName}/models/{trainedModelLabel}/evaluation/result?
top={top}&skip={skip}&maxpagesize={maxpagesize}&api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

The name for your project. This

myProject

{PROJECT-NAME}

value is case-sensitive.
{trainedModelLabel}

The name for your trained
model. This value is casesensitive.

Model1

{API-VERSION}

The version of the API you're
calling.

2023-04-01

{top}

The maximum number of

100

utterances to return from the
collection. Optional.
{skip}

An offset into the collection of
the first utterance to be
returned. Optional.

100

{maxpagesize}

The maximum number of
utterances to include in a single

100

response. Optional

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"value": [
{
"text": "send the email",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [],
"predictedEntities": []
},
"intentsResult": {
"expectedIntent": "SendEmail",
"predictedIntent": "SendEmail"
}
},
{
"text": "send a mail to daniel",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 15,
"length": 6
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 15,
"length": 6
}
]
},
"intentsResult": {
"expectedIntent": "SendEmail",
"predictedIntent": "SendEmail"
}
},
{
"text": "i forgot to add an important part to that email to james . please set
it up to edit",
"language": "en-us",

"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 51,
"length": 5
}
],
"predictedEntities": [
{
"category": "Category",
"offset": 19,
"length": 9
},
{
"category": "ContactName",
"offset": 51,
"length": 5
}
]
},
"intentsResult": {
"expectedIntent": "AddMore",
"predictedIntent": "AddMore"
}
},
{
"text": "send email to a and tian",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 14,
"length": 1
},
{
"category": "ContactName",
"offset": 20,
"length": 4
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 14,
"length": 1
},
{
"category": "ContactName",
"offset": 20,
"length": 4
}
]
},

"intentsResult": {
"expectedIntent": "SendEmail",
"predictedIntent": "SendEmail"
}
},
{
"text": "send thomas an email",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 5,
"length": 6
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 5,
"length": 6
}
]
},
"intentsResult": {
"expectedIntent": "SendEmail",
"predictedIntent": "SendEmail"
}
},
{
"text": "i need to add more to the email message i am sending to vincent",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 56,
"length": 7
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 56,
"length": 7
}
]
},
"intentsResult": {
"expectedIntent": "AddMore",
"predictedIntent": "AddMore"
}
},
{
"text": "send an email to lily roth and abc123@microsoft.com",

"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 17,
"length": 9
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 17,
"length": 9
}
]
},
"intentsResult": {
"expectedIntent": "SendEmail",
"predictedIntent": "SendEmail"
}
},
{
"text": "i need to add something else to my email to cheryl",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 44,
"length": 6
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 44,
"length": 6
}
]
},
"intentsResult": {
"expectedIntent": "AddMore",
"predictedIntent": "AddMore"
}
},
{
"text": "send an email to larry , joseph and billy larkson",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 17,
"length": 5

},
{
"category": "ContactName",
"offset": 25,
"length": 6
},
{
"category": "ContactName",
"offset": 36,
"length": 13
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 17,
"length": 5
},
{
"category": "ContactName",
"offset": 25,
"length": 6
},
{
"category": "ContactName",
"offset": 36,
"length": 13
}
]
},
"intentsResult": {
"expectedIntent": "SendEmail",
"predictedIntent": "SendEmail"
}
},
{
"text": "send mail to dorothy",
"language": "en-us",
"entitiesResult": {
"expectedEntities": [
{
"category": "ContactName",
"offset": 13,
"length": 7
}
],
"predictedEntities": [
{
"category": "ContactName",
"offset": 13,
"length": 7
}
]
},
"intentsResult": {

"expectedIntent": "SendEmail",
"predictedIntent": "SendEmail"
}
}
],
"nextLink": "{Endpoint}/language/authoring/analyzeconversations/projects/{projectName}/models/{trainedModelLabel}/evaluation/result/?
api-version=2022-10-01-preview&top={top}&skip={skip}&maxpagesize={maxpagesize}"
}

Load or export model data
Load model data
Create a POST request using the following URL, headers, and JSON body to load your model
data to your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/models/{MODEL-NAME}:load-snapshot?stringIndexType=Utf16CodeUnit&api-version=
{API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

EmailApp

NAME}

value is case-sensitive.

{API-

The version of the API you're
calling.

2022-10-01-preview

The name of your model. This
value is case-sensitive.

v1

VERSION}
{MODEL-NAME}

Headers

Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/models/{MODEL-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the status of your model data loading, using the same authentication method.

Export model data
Create a POST request using the following URL, headers, and JSON body to export your model
data.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}&trainedModelLabel={MODELNAME}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}

The name for your project. This
value is case-sensitive.

Expand table

subdomain>.cognitiveservices.azure.com
EmailApp

Placeholder

Value

Example

{API-

The version of the API you're
calling.

2022-10-01-preview

The name of your model. This
value is case-sensitive.

v1

VERSION}
{MODEL-NAME}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the exported project JSON, using the same authentication method.

Delete model
Create a DELETE request using the following URL, headers, and JSON body to delete a model.

Request URL
rest
{ENDPOINT}/language/authoring/analyzeconversations/projects/{projectName}/models/{trainedModelLabel}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{YOUR-ENDPOINT}

The endpoint for

https://<your-custom-

authenticating your API
request.

subdomain>.cognitiveservices.azure.com

The name for your project.

myProject

{PROJECT-NAME}

This value is case-sensitive.
{trainedModelLabel}

The name for your model

model1

name. This value is casesensitive.
{API-VERSION}

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 204 response indicating success, which means
your model is deleted.

Next steps
As you review your how your model performs, learn about the evaluation metrics that are
used.
If you're happy with your model performance, you can deploy your model

Last updated on 12/18/2025

Deploy a model
Once you're satisfied with how your model performs, it's ready to be deployed, and query it for predictions from utterances. Deploying a model
makes it available for use through the prediction API.

Prerequisites
A created project
Labeled utterances and successfully trained model
Reviewed the model performance to determine how your model is performing.
For more information, see project development lifecycle.

Deploy model
After you review the model's performance and decide it can be used in your environment, you need to assign it to a deployment to be able to
query it. Assigning the model to a deployment makes it available for use through the prediction API. We recommend creating a deployment
named production to which you assign the best model you built so far and use it in your system. You can create another deployment called
staging to which you can assign the model you're currently working on to be able to test it. You can have a maximum on 10 deployments in your

project.

Submit deployment job
Create a PUT request using the following URL, headers, and JSON body to start deploying a conversational language understanding model.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{DEPLOYMENT-NAME}

The name for your deployment. This value is case-sensitive.

staging

{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request Body
JSON
{
"trainedModelLabel": "{MODEL-NAME}",
}

Expand table

ï¾‰

Key
trainedModelLabel

Expand table

Placeholder

Value

Example

{MODEL-

The model name that is assigned to your deployment. You can only assign successfully trained models. This value is
case-sensitive.

myModel

NAME}

Once you send your API request, you receive a 202 response indicating success. In the response headers, extract the operation-location value
formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?apiversion={API-VERSION}

You can use this URL to get the deployment job status.

Get deployment job status
When you send a successful deployment request, the full request URL for checking the job's status (including your endpoint, project name, and
job ID) is contained in the response's operation-location header.
Use the following GET request to get the status of your deployment job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?apiversion={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{DEPLOYMENT-NAME}

The name for your deployment. This value is case-sensitive.

staging

{JOB-ID}

The ID for locating your model's training status.

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Response Body
Once you send the request, you'll get the following response. Keep polling this endpoint until the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",

"status":"running"
}

Swap deployments
After you're done testing a model assigned to one deployment, you might want to assign it to another deployment. Swapping deployments
involves:
Taking the model assigned to the first deployment, and assigning it to the second deployment.
taking the model assigned to second deployment and assign it to the first deployment.
For example, you can swap your production and staging deployments when you want to take the model assigned to staging and assign it to
production .

Create a POST request using the following URL, headers, and JSON body to start a swap deployments job.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/:swap?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

ï¾‰

Expand table

ï¾‰

Expand table

Request Body
JSON
{
"firstDeploymentName": "{FIRST-DEPLOYMENT-NAME}",
"secondDeploymentName": "{SECOND-DEPLOYMENT-NAME}"
}

Key

Placeholder

Value

Example

firstDeploymentName

{FIRST-DEPLOYMENT-NAME}

The name for your first deployment. This value is case-sensitive.

production

secondDeploymentName

{SECOND-DEPLOYMENT-NAME}

The name for your second deployment. This value is case-sensitive.

staging

Once you send your API request, you receive a 202 response indicating success.

Delete deployment
Create a DELETE request using the following URL, headers, and JSON body to delete a conversational language understanding deployment.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{projectName}/deployments/{deploymentName}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{DEPLOYMENT-NAME}

The name for your deployment name. This value is case-sensitive.

staging

{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Once you send your API request, you receive a 202 response indicating success, which means your deployment is deleted.

Assign deployment resources
You can deploy your project to multiple regions by assigning different Language resources that exist in different regions.
Assigning deployment resources programmatically requires Microsoft Entra authentication**. Microsoft Entra ID is used to confirm you have
access to the resources you're interested in assigning to your project for multi-region deployment. To programmatically use Microsoft Entra
authentication when making REST API calls, see the Foundry Tools authentication documentation.

Assign resource
Submit a POST request using the following URL, headers, and JSON body to assign deployment resources.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/:assign?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2022-10-01-preview

Headers
Use Microsoft Entra authentication to authenticate this API.

Body
Use the following sample JSON as your body.
JSON
{
"resourcesMetadata": [
{
"azureResourceId": "{AZURE-RESOURCE-ID}",
"customDomain": "{CUSTOM-DOMAIN}",
"region": "{REGION-CODE}"
}
]
}

ï¾‰

Key
azureResourceId

Expand table

Placeholder

Value

{AZURE-

The full resource ID path you want to

/subscriptions/aaaa0a0a-bb1b-cc2c-dd3d-

RESOURCE-

assign. Found in the Azure portal under
the Properties tab for the resource,

eeeeee4e4e4e/resourceGroups/ContosoResourceGroup/providers/Microsoft.CognitiveServices

ID}

Example

within the Resource ID field.
customDomain

{CUSTOM-

The custom subdomain of the resource

DOMAIN}

you want to assign. Found in the Azure

contosoresource

portal under the Keys and Endpoint tab
for the resource, part of the Endpoint
field in the URL https://<your-customsubdomain>.cognitiveservices.azure.com/
region

{REGIONCODE}

A region code specifying the region of
the resource you want to assign. Found in

eastus

the Azure portal under the Keys and
Endpoint tab for the resource, as part of
the Location/Region field.

Get assign resource status
Use the following GET request to get the status of your assign deployment resource job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/assign/jobs/{JOB-ID}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is case-sensitive.

myProject

The job ID for getting your assign deployment status. It's in the operation-location header

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

NAME}
{JOB-ID}

value you received from the API in response to your assign deployment resource request.
{API-

The version of the API you're calling.

VERSION}

Headers
Use the following header to authenticate your request.

2022-10-01-preview

ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Unassign deployment resources
When unassigning or removing a deployment resource from a project, you also delete all the deployments that are deployed to the resource's
region.

Unassign resource
Submit a POST request using the following URL, headers, and JSON body to unassign or remove deployment resources from your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/:unassign?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2022-10-01-preview

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following sample JSON as your body.
JSON

Expand table

{
"assignedResourceIds": [
"{AZURE-RESOURCE-ID}"
]
}

ï¾‰

Key
assignedResourceIds

Expand table

Placeholder

Value

{AZURE-

The full

/subscriptions/aaaa0a0a-bb1b-cc2c-dd3d-

RESOURCE-

resource
ID path
you want

eeeeee4e4e4e/resourceGroups/ContosoResourceGroup/providers/Microsoft.CognitiveServices/accounts/ContosoResource

ID}

Example

to
unassign.
Found in
the Azure
portal
under the
Properties
tab for
the
resource
as the
Resource
ID field.

Get unassign resource status
Use the following GET request to get the status of your unassign deployment resources job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/unassign/jobs/{JOB-ID}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is case-sensitive.

myProject

The job ID for getting your assign deployment status. It's in the operation-location header

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

NAME}
{JOB-ID}

value you received from the API in response to your unassign deployment resource request.
{API-

The version of the API you're calling.

2022-10-01-preview

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body

Expand table

Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Next steps
Use prediction API to query your model

Last updated on 12/18/2025

Send prediction requests to a deployment
After the deployment is added successfully, you can query the deployment for intent and
entities predictions from your utterance based on the model you assigned to the deployment.
You can query the deployment programmatically through the prediction API

or through the

client libraries (Azure SDK).

Test deployed model
Once your model is deployed, you can test it by sending prediction requests to evaluate its
performance with real utterances. Testing helps you verify that the model accurately identifies
intents and extracts entities as expected before integrating it into your production applications.
You can test your deployment using either the REST API or the Azure SDK client libraries.

Send a conversational language understanding request
First you need to get your resource key and endpoint:
Go to your resource overview page in the Azure portal

. From the menu on the left side,

select Keys and Endpoint. You use the endpoint and key for the API requests

ï Š

Query your model
Create a POST request using the following URL, headers, and JSON body to start testing a
conversational language understanding model.

Request URL
rest
{ENDPOINT}/language/:analyze-conversations?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{APIVERSION}

The version of the API you're
calling.

Expand table

subdomain>.cognitiveservices.azure.com
2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
JSON
{
"kind": "Conversation",
"analysisInput": {
"conversationItem": {
"id": "1",
"participantId": "1",
"text": "Text 1"
}
},
"parameters": {
"projectName": "{PROJECT-NAME}",
"deploymentName": "{DEPLOYMENT-NAME}",
"stringIndexType": "TextElement_V8"
}
}

ï¾‰

Key

Placeholder

participantId

{JOB-NAME}

"MyJobName

id

{JOB-NAME}

"MyJobName

text

{TEST-

The utterance that you want to predict its intent

"Read Matt's

UTTERANCE}

and extract entities from.

email

{PROJECT-NAME}

The name of your project. This value is case-

myProject

projectName

Value

Expand table

Example

sensitive.
deploymentName

{DEPLOYMENT-

The name of your deployment. This value is case-

NAME}

sensitive.

Once you send the request, you get the following response for the prediction

Response body
JSON
{
"kind": "ConversationResult",
"result": {
"query": "Text1",
"prediction": {
"topIntent": "inten1",
"projectKind": "Conversation",
"intents": [
{
"category": "intent1",
"confidenceScore": 1
},
{
"category": "intent2",
"confidenceScore": 0
},
{
"category": "intent3",
"confidenceScore": 0
}
],
"entities": [
{
"category": "entity1",
"text": "text1",
"offset": 29,
"length": 12,
"confidenceScore": 1

staging

}
]
}
}
}

ï¾‰

Expand table

Key

Sample Value

Description

query

"Read Matt's
email"

the text you submitted for query.

topIntent

"Read"

The predicted intent with highest confidence score.

intents

[]

A list of all the intents that were predicted for the query text each of them
with a confidence score.

entities

[]

array containing list of extracted entities from the query text.

API response for a conversations project
In a conversations project, you'll get predictions for both your intents and entities that are
present within your project.
The intents and entities include a confidence score between 0.0 to 1.0 associated with
how confident the model is about predicting a certain element in your project.
The top scoring intent is contained within its own parameter.
Only predicted entities show up in your response.
Entities indicate:
The text of the entity that was extracted
Its start location denoted by an offset value
The length of the entity text denoted by a length value.
You can also use the client libraries provided by the Azure SDK to send requests to your model.
ï¼— Note
The client library for conversational language understanding is only available for:
.NET
Python
1. Go to your resource overview page in the Azure portal

2. From the menu on the left side, select Keys and Endpoint. Use endpoint for the API
requests and you need the key for Ocp-Apim-Subscription-Key header.

ï Š

3. Download and install the client library package for your language of choice:
ï¾‰

Language

Package version

.NET

1.0.0

Python

1.0.0

Expand table

4. After you install the client library, use the following samples on GitHub to start calling the
API.
C#
Python
5. For more information, see the following reference documentation:
C#
Python

Next steps
Conversational language understanding overview

Last updated on 12/18/2025

Back up and recover your conversational
language understanding models
When you create a Language resource in the Azure portal, you specify a region for it to be
created in. From then on, your resource and all of the operations related to it take place in the
specified Azure server region. It's rare, but not impossible, to encounter a network issue that
hits an entire region. If your solution needs to always be available, then you should design it to
either fail-over into another region. This process requires two Azure Language in Foundry Tools
resources in different regions and the ability to sync your CLU models across regions.
If your app or business depends on the use of a CLU model, we recommend that you create a
replica of your project into another supported region. So that if a regional outage occurs, you
can then access your model in the other fail-over region where you replicated your project.
Replicating a project means that you export your project metadata and assets and import them
into a new project. This action only makes a copy of your project settings, intents, entities, and
utterances. You still need to train and deploy the models to be available for use with runtime
APIs

.

In this article, you learn to use the export and import APIs to replicate your project from one
resource to another existing in different supported geographical regions. We also provide
guidance for keeping your projects in sync and the changes needed to your runtime
consumption.

Prerequisites
Two Language resources in different Azure regions, each of them in a different region.

Get your resource keys endpoint
Use the following steps to get the keys and endpoint for your primary and secondary
resources.
Go to your resource overview page in the Azure portal

. From the menu on the left side,

select Keys and Endpoint. You use the endpoint and key for the API requests

ï Š

îª€ Tip
Keep a note of keys and endpoints for both primary and secondary resources. Use these
values to replace the following placeholders: {PRIMARY-ENDPOINT} , {PRIMARY-RESOURCEKEY} , {SECONDARY-ENDPOINT} , and {SECONDARY-RESOURCE-KEY} . Also take note of your project

name, your model name, and your deployment name. Use these values to replace the
following placeholders: {PROJECT-NAME} , {MODEL-NAME} , and {DEPLOYMENT-NAME} .

Export your primary project assets
Start by exporting the project assets from the project in your primary resource.

Submit export job
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Create a POST request by using the following URL, headers, and JSON body to export your
project.

Request URL
Use the following URL when you create your API request. Replace the placeholder values with
your own values.

rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

EmailApp

NAME}

value is case sensitive.

{API-

The version of the API that you're
calling.

VERSION}

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

After you send your API request, you receive a 202 response that indicates success. In the
response headers, extract the operation-location value. The value is formatted like this
example:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request because this operation is asynchronous. Use this URL to

get the exported project JSON by using the same authentication method.

Get export job status
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to query the status of your export job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/export/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This value is
case-sensitive.

{JOB-ID}

The ID for locating your export job

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

status. It's in the location header value
you received in the previous step.
{API-

The version of the API you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating

{YOUR-PRIMARY-

Subscription-Key

your API requests.

RESOURCE-KEY}

Response body
JSON
{
"resultUrl": "{Endpoint}/language/authoring/analyzeconversations/projects/{PROJECT-NAME}/export/jobs/xxxxxx-xxxxx-xxxxx-xx/result?apiversion={API-VERSION}",
"jobId": "xxxx-xxxxx-xxxxx-xxx",
"createdDateTime": "2022-04-18T15:23:07Z",

"lastUpdatedDateTime": "2022-04-18T15:23:08Z",
"expirationDateTime": "2022-04-25T15:23:07Z",
"status": "succeeded"
}

Use the url from the resultUrl key in the body and view the exported assets from this job.

Get export results
Submit a GET request using the {RESULT-URL} you received from the previous step to view the
results of the export job.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-Subscription-

The key to your resource. Used for authenticating your
API requests.

{PRIMARY-RESOURCE-

Key

KEY}

Copy the response body to use as the body for the next import job.

Import to a new project
Now go ahead and import the exported project assets in your new project in the secondary
region so you can replicate it.

Submit import job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Submit a POST request by using the following URL, headers, and JSON body to import your
project.

Request URL
Use the following URL when you create your API request. Replace the placeholder values with
your own values.

rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:import?
api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}

{API-

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This value is
case sensitive and must match the
project name in the JSON file that you're
importing.

EmailAppDemo

The version of the API that you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
The JSON body you send is similar to the following example. For more information about the
JSON object, see the reference documentation.
JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "Conversation",
"settings": {
"confidenceThreshold": 0.7
},
"projectName": "{PROJECT-NAME}",
"multilingual": true,

"description": "Trying out CLU",
"language": "{LANGUAGE-CODE}"
},
"assets": {
"projectKind": "Conversation",
"intents": [
{
"category": "intent1"
},
{
"category": "intent2"
}
],
"entities": [
{
"category": "entity1"
}
],
"utterances": [
{
"text": "text1",
"dataset": "{DATASET}",
"intent": "intent1",
"entities": [
{
"category": "entity1",
"offset": 5,
"length": 5
}
]
},
{
"text": "text2",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"intent": "intent2",
"entities": []
}
]
}
}

ï¾‰

Key

Placeholder

Value

{API-

2023-04-01

VERSION}

The version of
the API that
you're calling.

projectName

{PROJECT-NAME}

The name of your project. This value is case sensitive.

Expand table
Example

EmailAppDemo

Key

Placeholder

Value

Example

language

{LANGUAGE-CODE}

A string that specifies the language code for the
utterances used in your project. If your project is a
multilingual project, choose the language code of
most of the utterances.

en-us

multilingual

true

A Boolean value that enables you to have documents
in multiple languages in your dataset. When your
model is deployed, you can query the model in any
supported language, including languages that aren't
included in your training documents.

true

dataset

{DATASET}

For information on how to split your data between a

Train

testing and training set, see Label your utterances in
Foundry. Possible values for this field are Train and
Test .

After a successful request, the API response contains an operation-location header with a URL
that you can use to check the status of the import job. The header is formatted like this
example:
HTTP
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/import/jobs/{JOB-ID}?api-version={API-VERSION}

Get import job status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

When you send a successful project import request, the full request URL for checking the
import job's status (including your endpoint, project name, and job ID) is contained in the
response's operation-location header.
Use the following GET request to query the status of your import job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/import/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

myProject

NAME}

value is case-sensitive.

{JOB-ID}

The ID for locating your import
job status.

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{API-

The version of the API you're
calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating
your API requests.

{YOUR-PRIMARY-

Subscription-Key

RESOURCE-KEY}

Response body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded."
JSON
{
"jobId": "xxxxx-xxxxx-xxxx-xxxxx",
"createdDateTime": "2022-04-18T15:17:20Z",
"lastUpdatedDateTime": "2022-04-18T15:17:22Z",
"expirationDateTime": "2022-04-25T15:17:20Z",
"status": "succeeded"
}

Train your model
After importing your project, you only have copies of the project's assets and metadata. You
still need to train your model, which incurs usage on your account.

Submit training job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Create a POST request using the following URL, headers, and JSON body to submit a training
job.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:train?
api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

EmailApp

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body

Use the following object in your request. The model will be named after the value you use for
the modelLabel parameter once training is complete.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingMode": "{TRAINING-MODE}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"testingSplitPercentage": 20,
"trainingSplitPercentage": 80
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-

Your Model name.

Model1

{CONFIG-

The training configuration model version. By

2022-05-01

VERSION}

default, the latest model version is used.

{TRAINING-

The training mode to be used for training.

MODE}

Supported modes are Standard training, faster
training, but only available for English and

NAME}
trainingConfigVersion

trainingMode

standard

Advanced training supported for other
languages and multilingual projects, but
involves longer training times. Learn more about
training modes.
kind

percentage

Split methods. Possible Values are percentage

percentage

or manual . See how to train a model for more
information.
trainingSplitPercentage

80

Percentage of your tagged data to be included
in the training set. Recommended value is 80 .

80

testingSplitPercentage

20

Percentage of your tagged data to be included

20

in the testing set. Recommended value is 20 .

ï¼— Note

The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set
to percentage and the sum of both percentages should be equal to 100.
Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to get the training job status.

Get Training Status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

When you send a successful training request, the full request URL for checking the job's status
(including your endpoint, project name, and job ID) is contained in the response's operationlocation header.

Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{YOUR-

The endpoint for authenticating
your API request.

https://<your-custom-

ENDPOINT}

Expand table

subdomain>.cognitiveservices.azure.com
EmailApp

NAME}

The name for your project. This
value is case-sensitive.

{JOB-ID}

The ID for locating your model's

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

Placeholder

Value

Example

training status.
{API-VERSION}

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".
JSON
{
"result": {
"modelLabel": "{MODEL-LABEL}",
"trainingConfigVersion": "{TRAINING-CONFIG-VERSION}",
"trainingMode": "{TRAINING-MODE}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},
"jobId": "xxxxx-xxxxx-xxxx-xxxxx-xxxx",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

ï¾‰

Key

Value

Example

modelLabel

The model name

Model1

trainingConfigVersion

The training configuration version. By default, the
latest version is used.

2022-05-01

trainingMode

Your selected training mode.

standard

startDateTime

The time training started

2022-04-

Expand table

14T10:23:04.2598544Z
status

The status of the training job

running

estimatedEndDateTime

Estimated time for the training job to finish

2022-0414T10:29:38.2598544Z

jobId

Your training job ID

xxxxx-xxxx-xxxx-xxxxxxxxxxxxx

createdDateTime

Training job creation date and time

2022-04-14T10:22:42Z

lastUpdatedDateTime

Training job last updated date and time

2022-04-14T10:23:45Z

expirationDateTime

Training job expiration date and time

2022-04-14T10:22:42Z

Deploy your model
This step is where you make your trained model available form consumption via the runtime
prediction API .
îª€ Tip
Use the same deployment name as your primary project for easier maintenance and
minimal changes to your system to handle redirecting your traffic.

Submit deployment job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Create a PUT request using the following URL, headers, and JSON body to start deploying a
conversational language understanding model.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This
value is case-sensitive.

myProject

{DEPLOYMENT-

staging

NAME}

The name for your deployment.
This value is case-sensitive.

{API-VERSION}

The version of the API you're

2023-04-01

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request Body
JSON
{
"trainedModelLabel": "{MODEL-NAME}",
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

trainedModelLabel

{MODEL-

The model name that is assigned to your deployment.
You can only assign successfully trained models. This

myModel

NAME}

value is case-sensitive.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to get the deployment job status.

Get the deployment status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

When you send a successful deployment request, the full request URL for checking the job's
status (including your endpoint, project name, and job ID) is contained in the response's
operation-location header.

Use the following GET request to get the status of your deployment job. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

The name for your project. This

myProject

{PROJECT-NAME}

value is case-sensitive.

Placeholder

Value

Example

{DEPLOYMENT-

The name for your deployment.
This value is case-sensitive.

staging

{JOB-ID}

The ID for locating your model's
training status.

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{API-VERSION}

The version of the API you're

2023-04-01

NAME}

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you'll get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Changes in calling the runtime
Within your system, at the step where you call runtime API

check for the response code

returned from the submitted task API. If you observe a consistent failure in submitting the
request, it could indicate an outage in your primary region. Failure once doesn't mean an
outage. It may also be a transient issue. Retry submitting the job through the secondary
resource you created. For the second request use your {YOUR-SECONDARY-ENDPOINT} and

secondary key, if you followed the previous steps, {PROJECT-NAME} and {DEPLOYMENT-NAME} are
the same, so no changes are required to the request body.
In case you revert to using your secondary resource, you may observe a slight increase in
latency because of the difference in regions where your model is deployed.

Check if your projects are out of sync
Maintaining the freshness of both projects is an important part of process. You need to
frequently check if any updates were made to your primary project so that you move them
over to your secondary project. This way if your primary region fail and you move into the
secondary region you should expect similar model performance since it already contains the
latest updates. Setting the frequency of checking if your projects are in sync is an important
choice, we recommend that you do this check daily in order to guarantee the freshness of data
in your secondary model.

Get project details
Use the following url to get your project details, one of the keys returned in the body indicates
the last modified date of the project. Repeat the following step twice, one for your primary
project and another for your secondary project. Them compare the timestamp returned for
both to check if they're in or out of sync.
Use the following GET request to get your project details. You can use the URL you received
from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}?apiversion={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This
value is case-sensitive.

{API-

The version of the API you're

2023-04-01

VERSION}

calling.

{PROJECT-

Expand table

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating

{YOUR-PRIMARY-

Subscription-Key

your API requests.

RESOURCE-KEY}

Response body
JSON
{
"createdDateTime": "2022-04-18T13:53:03Z",
"lastModifiedDateTime": "2022-04-18T13:53:03Z",
"lastTrainedDateTime": "2022-04-18T14:14:28Z",
"lastDeployedDateTime": "2022-04-18T14:49:01Z",
"projectKind": "Conversation",
"projectName": "{PROJECT-NAME}",
"multilingual": true,
"description": "This is a sample conversation project.",
"language": "{LANGUAGE-CODE}"
}

Repeat the same steps for your replicated project using {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} . Compare the returned lastModifiedDateTime from both projects. If

your primary project was modified sooner than your secondary one, you need to repeat the
steps of exporting, importing, training, and deploying your model.

Next steps
In this article, you learned how to use the export and import APIs to replicate your project to a
secondary Language resource in other region. Next, explore the API reference docs to see what
else you can do with authoring APIs.
Authoring REST API reference
Runtime prediction REST API reference

Last updated on 11/18/2025

CLU multi-turn conversations
Entity slot filling in Conversational Language Understanding (CLU) enables your applications to
facilitate seamless multi-turn conversations across multiple exchanges. Instead of
overwhelming users with complex forms or lengthy questions, CLU progressively extracts and
organizes the details it needs as users provide them throughout these multi-turn
conversations.
This capability transforms rigid question-and-answer interactions into fluid dialogues. Your CLU
model is prompted to ask for missing information when needed, creating conversations that
feel more natural and intuitive.

Understand entity slot filling
Entity slot filling works by identifying the structured information your application requires and
systematically collecting it from user input. Think of slots as containers for specific types of
information that your application needs to complete a task. When users provide some details
but not others, your CLU model is prompted for the missing pieces.
The process centers around three key elements:
Entities are the specific data points you need to extract.
Slots are the predefined containers that hold those entities.
Conversational continuity guarantees that information is preserved and accessible
throughout the entire dialogue.
Instead of requiring users to provide every detail at the outset, CLU enables progressive
disclosure where information emerges naturally as the conversation unfolds.

Multi-turn conversation mechanics
With CLU multi-turn conversations, you enable users to share information in whichever
sequence they prefer. The model begins by recognizing the user's intent, then extracts any
available entities from their initial input.
This approach accommodates how people naturally communicate. Users might correct
previous information, add details as they remember them, or provide partial information that
gets completed over several turns. The conversation remains coherent because CLU tracks the
relationship between all collected entities.

Usage scenario

Consider a restaurant reservation system that needs three pieces of information: party size,
date, and time.
User: "I'd like to make a reservation for four people" System: "I can help you with scheduling
that reservation! What date would you prefer?"
User: "How about this Friday?" System: "Perfect! And what time works best for your party of
four on Friday?"
User: "7 PM would be great" System: "Excellent! I reserved a table for four people this Friday at
7 PM."
In this exchange, CLU model extracts the party size, date, and time from the user input. Each
piece of information is added, allowing for a natural dialogue flow rather than a rigid formfilling experience.

Benefits of entity slot filling
Entity slot filling creates more natural user experiences by allowing people to provide
information as they think of it, rather than forcing them to remember all requirements upfront.
Users can correct or clarify information throughout the conversation, and the system
accommodates various communication styles and preferences. This process reduces cognitive
burden and makes interactions feel more conversational and less transactional.
Entity slot filling improves accuracy because conversation context helps disambiguate entities
and intents. This approach efficiently collects only the information necessary for task
completion while providing a scalable architecture that handles complex scenarios with
multiple dependent entities. The system also manages incomplete or contradictory information
gracefully, making applications more robust and user-friendly.

Microsoft Foundry
Foundry

provides a comprehensive platform for developing and deploying CLU models with

entity slot filling capabilities. The platform streamlines the process of defining your entity types
and slot requirements while helping you create multi-turn conversation examples for training.
You validate slot filling behavior across various scenarios and track accuracy and user
satisfaction metrics to ensure your model performs well in real-world conditions.
Foundry enables you to build and refine your CLU models iteratively, ensuring they effectively
handle the complexity of multi-turn conversations in your specific domain. The platform's tools
support the entire development lifecycle from initial schema design through deployment and
ongoing performance monitoring.

To get started implementing entity slot filling in your applications, see Build multi-turn models
in Foundry.

Related content
Build multi-turn model
Build a fine-tuning schema

Last updated on 11/18/2025

Best practices for conversational language
understanding
Use the following guidelines to create the best possible projects in conversational language
understanding.

Choose a consistent schema
Schema is the definition of your intents and entities. There are different approaches you could
take when you define what you should create as an intent versus an entity. Ask yourself these
questions:
What actions or queries am I trying to capture from my user?
What pieces of information are relevant in each action?
You can typically think of actions and queries as intents, while the information required to fulfill
those queries are entities.
For example, assume that you want your customers to cancel subscriptions for various
products that you offer through your chatbot. You can create a cancel intent with various
examples like "Cancel the Contoso service" or "Stop charging me for the Fabrikam
subscription." The user's intent here is to cancel, and the Contoso service or Fabrikam
subscription are the subscriptions they want to cancel.
To proceed, you create an entity for subscriptions. Then you can model your entire project to
capture actions as intents and use entities to fill in those actions. This approach allows you to
cancel anything you define as an entity, such as other products. You can then have intents for
signing up, renewing, and upgrading that all make use of the subscriptions and other entities.
The preceding schema design makes it easy for you to extend existing capabilities (canceling,
upgrading, or signing up) to new targets by creating a new entity.
Another approach is to model the information as intents and the actions as entities. Let's take
the same example of allowing your customers to cancel subscriptions through your chatbot.
You can create an intent for each subscription available, such as Contoso, with utterances like
"Cancel Contoso," "Stop charging me for Contoso services," and "Cancel the Contoso
subscription." You then create an entity to capture the cancel action. You can define different
entities for each action or consolidate actions as one entity with a list component to
differentiate between actions with different keys.
This schema design makes it easy for you to extend new actions to existing targets by adding
new action entities or entity components.

Make sure to avoid trying to funnel all the concepts into intents. For example, don't try to
create a Cancel Contoso intent that only has the purpose of that one specific action. Intents and
entities should work together to capture all the required information from the customer.
You also want to avoid mixing different schema designs. Don't build half of your application
with actions as intents and the other half with information as intents. To get the possible
results, ensure that it's consistent.

Balance training data
When it comes to training data, try to keep your schema well balanced. Including large
quantities of one intent and few of another results in a model with bias towards particular
intents.
To address this scenario, you might need to downsample your training set. Or you might need
to add to it. To downsample, you can:
Get rid of a certain percentage of the training data randomly.
Analyze the dataset and remove overrepresented duplicate entries, which is a more
systematic manner.
To add to the training set, in Language Studio, on the Data labeling tab, select Suggest
utterances. Conversational Language Understanding sends a call to Azure OpenAI to generate
similar utterances.

ï Š

You should also look for unintended "patterns" in the training set. For example, look to see if
the training set for a particular intent is all lowercase or starts with a particular phrase. In such
cases, the model you train might learn these unintended biases in the training set instead of
being able to generalize.
We recommend that you introduce casing and punctuation diversity in the training set. If your
model is expected to handle variations, be sure to have a training set that also reflects that
diversity. For example, include some utterances in proper casing and some in all lowercase.

Clearly label utterances
Ensure that the concepts that your entities refer to are well defined and separable. Check
if you can easily determine the differences reliably. If you can't, this lack of distinction
might indicate difficulty for the learned component.
Ensure that some aspect of your data can provide a signal for differences when there's a
similarity between entities.
For example, if you built a model to book flights, a user might use an utterance like "I
want a flight from Boston to Seattle." The origin city and destination city for such
utterances would be expected to be similar. A signal to differentiate origin city might be
that the word from often precedes it.
Ensure that you label all instances of each entity in both your training and testing data.
One approach is to use the search function to find all instances of a word or phrase in
your data to check if they're correctly labeled.
Ensure that you label test data for entities without learned components and also for the
entities with them. This practice helps to ensure that your evaluation metrics are accurate.

Use standard training before advanced training
Standard training is free and faster than advanced training. It can help you quickly understand
the effect of changing your training set or schema while you build the model. After you're
satisfied with the schema, consider using advanced training to get the best model quality.

Use the evaluation feature
When you build an app, it's often helpful to catch errors early. It's usually a good practice to
add a test set when you build the app. Training and evaluation results are useful in identifying
errors or issues in your schema.

Machine-learning components and composition
For more information, see Component types.

Use the None score threshold
If you see too many false positives, such as out-of-context utterances being marked as valid
intents, see Confidence threshold for information on how it affects inference.
Non-machine-learned entity components, like lists and regex, are by definition not
contextual. If you see list or regex entities in unintended places, try labeling the list
synonyms as the machine-learned component.
For entities, you can use learned component as the Required component, to restrict when
a composed entity should fire.
For example, suppose you have an entity called Ticket Quantity that attempts to extract the
number of tickets you want to reserve for booking flights, for utterances such as "Book two
tickets tomorrow to Cairo."
Typically, you add a prebuilt component for Quantity.Number that already extracts all numbers
in utterances. However, if your entity was only defined with the prebuilt component, it also
extracts other numbers as part of the Ticket Quantity entity, such as "Book two tickets
tomorrow to Cairo at 3 PM."
To resolve this issue, you label a learned component in your training data for all the numbers
that are meant to be a ticket quantity. The entity now has two components:
The prebuilt component that can interpret all numbers.
The learned component that predicts where the ticket quantity is located in a sentence.
If you require the learned component, make sure that Ticket Quantity is only returned when
the learned component predicts it in the right context. If you also require the prebuilt
component, you can then guarantee that the returned Ticket Quantity entity is both a number
and in the correct position.

Address model inconsistencies
If your model is overly sensitive to small grammatical changes, like casing or diacritics, you can
systematically manipulate your dataset directly in Microsoft Foundry. To use these features,
select the Settings tab on the left pane and locate the Advanced project settings section.

ï Š

First, you can enable the setting for Enable data transformation for casing, which normalizes
the casing of utterances when training, testing, and implementing your model. If you migrated
from LUIS, you might recognize that LUIS did this normalization by default. To access this
feature via the API, set the normalizeCasing parameter to true . See the following example:
JSON
{
"projectFileVersion": "2022-10-01-preview",
...
"settings": {
...
"normalizeCasing": true
...
}
...

Second, you can also enable the setting for Enable data augmentation for diacritics to
generate variations of your training data for possible diacritic variations used in natural
language. This feature is available for all languages. It's especially useful for Germanic and
Slavic languages, where users often write words by using classic English characters instead of
the correct characters. For example, the phrase "Navigate to the sports channel" in French is
AccÃ©dez Ã  la chaÃ®ne sportive. When this feature is enabled, the phrase Accedez a la chaine
sportive (without diacritic characters) is also included in the training dataset.

If you enable this feature, the utterance count of your training set increases. For this reason,
you might need to adjust your training data size accordingly. The current maximum utterance
count after augmentation is 25,000. To access this feature via the API, set the
augmentDiacritics parameter to true . See the following example:

JSON
{
"projectFileVersion": "2022-10-01-preview",
...
"settings": {
...
"augmentDiacritics": true
...

}
...

Address model overconfidence
Customers can use the LoraNorm training configuration version if the model is being
incorrectly overconfident. An example of this behavior can be like the following scenario where
the model predicts the incorrect intent with 100% confidence. This score makes the confidence
threshold project setting unusable.
ï¾‰

Expand table

Text

Predicted intent

Confidence score

"Who built the Eiffel Tower?"

Sports

1.00

"Do I look good to you today?"

QueryWeather

1.00

"I hope you have a good evening."

Alarm

1.00

To address this scenario, use the 2023-04-15 configuration version that normalizes confidence
scores. The confidence threshold project setting can then be adjusted to achieve the desired
result.
Console
curl --location 'https://<yourresource>.cognitiveservices.azure.com/language/authoring/analyzeconversations/projects/<your-project>/:train?api-version=2022-10-01-preview' \
--header 'Ocp-Apim-Subscription-Key: <your subscription key>' \
--header 'Content-Type: application/json' \
--data '{
"modelLabel": "<modelLabel>",
"trainingMode": "advanced",
"trainingConfigVersion": "2023-04-15",
"evaluationOptions": {
"kind": "percentage",
"testingSplitPercentage": 0,
"trainingSplitPercentage": 100
}
}

After the request is sent, you can track the progress of the training job in Microsoft Foundry.
ï¼— Note

You have to retrain your model after you update the confidenceThreshold project setting.
Afterward, you need to republish the app for the new threshold to take effect.

Normalization in model version 2023-04-15
With model version 2023-04-15, conversational language understanding provides
normalization in the inference layer that doesn't affect training.
The normalization layer normalizes the classification confidence scores to a confined range.
The range selected currently is from [-a,a] where "a" is the square root of the number of
intents. As a result, the normalization depends on the number of intents in the app. If the
number of intents is low, the normalization layer has a small range to work with. With a large
number of intents, the normalization is more effective.
If this normalization doesn't seem to help intents that are out of scope to the extent that the
confidence threshold can be used to filter out-of-scope utterances, it might be related to the
number of intents in the app. Consider adding more intents to the app. Or, if you're using an
orchestrated architecture, consider merging apps that belong to the same domain together.

Debug composed entities
Entities are functions that emit spans in your input with an associated type. One or more
components define the function. You can mark components as needed, and you can decide
whether to enable the Combine components setting. When you combine components, all
spans that overlap are merged into a single span. If the setting isn't used, each individual
component span is emitted.
To better understand how individual components are performing, you can disable the setting
and set each component to Not required. This setting lets you inspect the individual spans that
are emitted and experiment with removing components so that only problematic components
are generated.

Evaluate a model by using multiple test sets
Data in a conversational language understanding project can have two datasets: a testing set
and a training set. If you want to use multiple test sets to evaluate your model, you can:
Give your test sets different names (for example, "test1" and "test2").
Export your project to get a JSON file with its parameters and configuration.
Use the JSON to import a new project. Rename your second desired test set to "test."

Train the model to run the evaluation by using your second test set.

Custom parameters for target apps and child apps
If you're using orchestrated apps, you might want to send custom parameter overrides for
various child apps. The targetProjectParameters field allows users to send a dictionary
representing the parameters for each target project. For example, consider an orchestrator app
named Orchestrator orchestrating between a conversational language understanding app
named CLU1 and a custom question answering app named CQA1 . If you want to send a
parameter named "top" to the question answering app, you can use the preceding parameter.
Console
curl --request POST \
--url 'https://<your-languageresource>.cognitiveservices.azure.com/language/:analyze-conversations?apiversion=2022-10-01-preview' \
--header 'ocp-apim-subscription-key: <your subscription key>' \
--data '{
"kind": "Conversation",
"analysisInput": {
"conversationItem": {
"id": "1",
"text": "Turn down the volume",
"modality": "text",
"language": "en-us",
"participantId": "1"
}
},
"parameters": {
"projectName": "Orchestrator",
"verbose": true,
"deploymentName": "std",
"stringIndexType": "TextElement_V8",
"targetProjectParameters": {
"CQA1": {
"targetProjectKind": "QuestionAnswering",
"callingOptions": {
"top": 1
}
}
}
}
}'

Copy projects across language resources

Often you can copy conversational language understanding projects from one resource to
another by using the Copy button in Microsoft Foundry. In some cases, it might be easier to
copy projects by using the API.
First, identify the:
Source project name.
Target project name.
Source language resource.
Target language resource, which is where you want to copy it to.
Call the API to authorize the copy action and get accessTokens for the actual copy operation
later.
Console
curl --request POST \
--url 'https://<target-languageresource>.cognitiveservices.azure.com//language/authoring/analyzeconversations/projects/<source-project-name>/:authorize-copy?api-version=2023-04-15preview' \
--header 'Content-Type: application/json' \
--header 'Ocp-Apim-Subscription-Key: <Your-Subscription-Key>' \
--data '{"projectKind":"Conversation","allowOverwrite":false}'

Call the API to complete the copy operation. Use the response you got earlier as the payload.
Console
curl --request POST \
--url 'https://<source-languageresource>.cognitiveservices.azure.com/language/authoring/analyzeconversations/projects/<source-project-name>/:copy?api-version=2023-04-15-preview' \
--header 'Content-Type: application/json' \
--header 'Ocp-Apim-Subscription-Key: <Your-Subscription-Key>\
--data '{
"projectKind": "Conversation",
"targetProjectName": "<target-project-name>",
"accessToken": "<access-token>",
"expiresAt": "<expiry-date>",
"targetResourceId": "<target-resource-id>",
"targetResourceRegion": "<target-region>"
}'

Address out-of-domain utterances

Customers can use the newly updated training configuration version 2024-08-01-preview
(previously 2024-06-01-preview ) if the model has poor quality on out-of-domain utterances. An
example of this scenario with the default training configuration can be like the following
example where the model has three intents: Sports , QueryWeather , and Alarm . The test
utterances are out-of-domain utterances and the model classifies them as InDomain with a
relatively high confidence score.
ï¾‰

Expand table

Text

Predicted intent

Confidence score

"Who built the Eiffel Tower?"

Sports

0.90

"Do I look good to you today?"

QueryWeather

1.00

"I hope you have a good evening."

Alarm

0.80

To address this scenario, use the 2024-08-01-preview configuration version that's built
specifically to address this issue while also maintaining reasonably good quality on InDomain
utterances.
Console
curl --location 'https://<yourresource>.cognitiveservices.azure.com/language/authoring/analyzeconversations/projects/<your-project>/:train?api-version=2022-10-01-preview' \
--header 'Ocp-Apim-Subscription-Key: <your subscription key>' \
--header 'Content-Type: application/json' \
--data '{
"modelLabel": "<modelLabel>",
"trainingMode": "advanced",
"trainingConfigVersion": "2024-08-01-preview",
"evaluationOptions": {
"kind": "percentage",
"testingSplitPercentage": 0,
"trainingSplitPercentage": 100
}
}

After the request is sent, you can track the progress of the training job in Microsoft Foundry.
Caveats:
The None score threshold for the app (confidence threshold below which topIntent is
marked as None ) when you use this training configuration should be set to 0. This setting
is used because this new training configuration attributes a certain portion of the indomain probabilities to out of domain so that the model isn't incorrectly overconfident

about in-domain utterances. As a result, users might see slightly reduced confidence
scores for in-domain utterances as compared to the production training configuration.
We don't recommend this training configuration for apps with only two intents, such as
IntentA and None , for example.

We don't recommend this training configuration for apps with a low number of
utterances per intent. We highly recommend a minimum of 25 utterances per intent.

Last updated on 12/17/2025

Multilingual projects
Conversational language understanding makes it easy for you to extend your project to several
languages at once. When you enable multiple languages in projects, you can add languagespecific utterances and synonyms to your project. You can get multilingual predictions for your
intents and entities.

Multilingual intent and learned entity components
When you enable multiple languages in a project, you can train the project primarily in one
language and immediately get predictions in other languages.
For example, you can train your project entirely with English utterances and query it in French,
German, Mandarin, Japanese, Korean, and others. Conversational language understanding
makes it easy for you to scale your projects to multiple languages by using multilingual
technology to train your models.
Whenever you identify that a particular language isn't performing as well as other languages,
you can add utterances for that language in your project. In the tag utterances page in
Language Studio, you can select the language of the utterance you're adding. When you
introduce examples for that language to the model, it's introduced to more of the syntax of
that language and learns to predict it better.
You aren't expected to add the same number of utterances for every language. You should
build most of your project in one language and only add a few utterances in languages that
you observe aren't performing well. If you create a project that's primarily in English and start
testing it in French, German, and Spanish, you might observe that German doesn't perform as
well as the other two languages. In that case, consider adding 5% of your original English
examples in German, train a new model, and test in German again. You should see better
results for German queries. The more utterances you add, the more likely the results are going
to get better.
When you add data in another language, you shouldn't expect it to negatively affect other
languages.

List and prebuilt components in multiple
languages
Projects with multiple languages enabled allow you to specify synonyms per language for every
list key. Depending on the language you query your project with, you only get matches for the

list component with synonyms of that language. When you query your project, you can specify
the language in the request body:
JSON
"query": "{query}"
"language": "{language code}"

If you don't provide a language, it falls back to the default language of your project. For a list
of different language codes, see Language support.
Prebuilt components are similar, where you should expect to get predictions for prebuilt
components that are available in specific languages. The request's language again determines
which components are attempting to be predicted. For information on the language support of
each prebuilt component, see the Supported prebuilt entity components.

Related content
Tag utterances
Train a model

Last updated on 11/18/2025

Entity components
In conversational language understanding, entities are relevant pieces of information that are
extracted from your utterances. You can extract an entity using several different methods.
Entities can be detected through context, matched from a list, or detected by a prebuilt
recognized entity. Every entity in your project is composed of one or more of these methods,
which are defined as your entity's components.
When more than one component defines an entity, predictions can overlap. You can determine
the behavior of an entity prediction when its components overlap by using a fixed set of
options in the entity options.

Component types
An entity component determines a way that you can extract the entity. An entity can contain
one component, which determines the only method to be used to extract the entity. An entity
can also contain multiple components to expand the ways in which the entity is defined and
extracted.

Learned component
The learned component uses the entity tags you label your utterances with to train a machinelearned model. The model learns to predict where the entity is based on the context within the
utterance. Your labels provide examples of where the entity is expected to be present in an
utterance. This determination is based on the meaning of the words around it and as the words
that were labeled.
This component is only defined if you add labels by tagging utterances for the entity. If you
don't tag any utterances with the entity, it doesn't have a learned component.

ï Š

List component

The list component represents a fixed, closed set of related words along with their synonyms.
The component performs an exact text match against the list of values you provide as
synonyms. Each synonym belongs to a list key, which can be used as the normalized, standard
value for the synonym that returns in the output if the list component is matched. List keys
aren't used for matching.
In multilingual projects, you can specify a different set of synonyms for each language. When
you use the prediction API, you can specify the language in the input request, which only
matches the synonyms associated to that language.

ï Š

Prebuilt component
The prebuilt component allows you to select from a library of common types such as numbers,
datetimes, and names. When added, a prebuilt component is automatically detected. You can
have up to five prebuilt components per entity. For more information, see the list of supported
prebuilt components.

ï Š

Screenshot that shows an example of prebuilt components for entities.

Regex component
The regex component matches regular expressions to capture consistent patterns. When
added, any text that matches the regular expression is extracted. You can have multiple regular
expressions within the same entity, each with a different key identifier. A matched expression
returns the key as part of the prediction response.
In multilingual projects, you can specify a different expression for each language. When you
use the prediction API, you can specify the language in the input request, which only matches
the regular expression associated to that language.

ï Š

Entity options
If multiple components define an entity, their predictions may overlap. When overlap happens,
one of the following options determines each entity's final prediction:

Combine components
Combine components as one entity when they overlap by taking the union of all the
components.
Use this option to combine all components when they overlap. When components are
combined, you get all the extra information associated with a list or prebuilt component if
present.

Example
Suppose you have an entity called Software that has a list component, which contains
"Proseware OS" as an entry. In your utterance data, you have "I want to buy Proseware OS 9"
with "Proseware OS 9" tagged as Software:

ï Š

By using combined components, the entity returns with the full context as "Proseware OS 9"
along with the key from the list component:

ï Š

Suppose you had the same utterance, but only "OS 9" predicts the learned component:

ï Š

With combined components, the entity still returns as "Proseware OS 9" with the key from the
list component:

ï Š

Don't combine components
Each overlapping component returns as a separate instance of the entity. Apply your own logic
after prediction with this option.

Example
Suppose you have an entity called Software that has a list component, which contains
"Proseware Desktop" as an entry. In your utterance data, you have "I want to buy Proseware
Desktop Pro" with "Proseware Desktop Pro" tagged as Software:

ï Š

When you don't combine components, the entity returns twice:

ï Š

Required components
Sometimes, you can define an entity using multiple components, but the entity requires at
least one or more of them to be present. You can mark any component as required, which
means the system doesn't return the entity unless that component is present. For example, if an
entity has a list component and a required learned component, the system guarantees that any
returned entity includes a learned component. If an entity doesn't have the required
component, the system doesn't return it.
Required components are most frequently used with learned components because they can
restrict the other component types to a specific context, which is commonly associated to roles.
You can also require all components to make sure that every component is present for an
entity.
In Language Studio, every component in an entity has a toggle next to it that allows you to set
it as required.

Example
Suppose you have an entity called Ticket Quantity that attempts to extract the number of
tickets you want to reserve for flights, for utterances such as "Book two tickets tomorrow to
Cairo."
Typically, you add a prebuilt component for Quantity.Number that already extracts all numbers.
If your entity was only defined with the prebuilt component, it also extracts other numbers as
part of the Ticket Quantity entity, such as "Book two tickets tomorrow to Cairo at 3 PM."
To resolve this scenario, you label a learned component in your training data for all the
numbers that are meant to be Ticket Quantity. The entity now has two components: the
prebuilt component that knows all numbers, and the learned one that predicts where the ticket
quantity is in a sentence. If you require the learned component, you make sure that Ticket
Quantity only returns when the learned component predicts it in the right context. If you also
require the prebuilt component, you can then guarantee that the returned Ticket Quantity
entity is both a number and in the correct position.

Use components and options
Components give you the flexibility to define your entity in more than one way. When you
combine components, you make sure that each component is represented and you reduce the
number of entities returned in your predictions.
A common practice is to extend a prebuilt component with a list of values that the prebuilt
might not support. For example, if you have an Organization entity, which has a
General.Organization prebuilt component added to it, the entity might not predict all the

organizations specific to your domain. You can use a list component to extend the values of the
Organization entity and extend the prebuilt component with your own organizations.
Other times, you might be interested in extracting an entity through context, such as a Product
in a retail project. You label the learned component of the product to learn where a product is
based on its position within the sentence. You might also have a list of products that you
already know beforehand that you want to always extract. Combining both components in one
entity allows you to get both options for the entity.
When you don't combine components, you allow every component to act as an independent
entity extractor. One way of using this option is to separate the entities extracted from a list to
the ones extracted through the learned or prebuilt components to handle and treat them
differently.
ï¼— Note
Previously during the public preview of the service, there were four available options:
Longest overlap, Exact overlap, Union overlap, and Return all separately. Longest
overlap and Exact overlap are deprecated and are only supported for projects that
previously had those options selected. Union overlap is renamed to Combine
components, while Return all separately is renamed to Do not combine components.

Related content
Supported prebuilt components

Last updated on 11/18/2025

Evaluation metrics for conversational
language understanding models
Your dataset is split into two parts: a set for training and a set for testing. The training set is
used to train the model, while the testing set is used as a test for model after training to
calculate the model performance and evaluation. The testing set isn't introduced to the model
through the training process to make sure that the model is tested on new data.
Model evaluation is triggered automatically after training is completed successfully. The
evaluation process starts by using the trained model to predict user-defined intents and
entities for utterances in the test set. Then the process compares them with the provided tags
to establish a baseline of truth. The results are returned so that you can review the model's
performance. For evaluation, conversational language understanding uses the following
metrics:
Precision: Measures how precise or accurate your model is. It's the ratio between the
correctly identified positives (true positives) and all identified positives. The precision
metric reveals how many of the predicted classes are correctly labeled.
Precision = #True_Positive / (#True_Positive + #False_Positive)

Recall: Measures the model's ability to predict actual positive classes. It's the ratio
between the predicted true positives and what was tagged. The recall metric reveals how
many of the predicted classes are correct.
Recall = #True_Positive / (#True_Positive + #False_Negatives)

F1 score: The F1 score is a function of precision and recall. It's needed when you seek a
balance between precision and recall.
F1 Score = 2 * Precision * Recall / (Precision + Recall)

Precision, recall, and the F1 score are calculated for:
Each entity separately (entity-level evaluation).
Each intent separately (intent-level evaluation).
For the model collectively (model-level evaluation).
The definitions of precision, recall, and evaluation are the same for entity-level, intent-level,
and model-level evaluations. However, the counts for true positives, false positives, and false
negatives can differ. For example, consider the following text.

Example

Make a response with "thank you very much."
Reply with saying "yes."
Check my email please.
Email to Cynthia that dinner last week was splendid.
Send an email to Mike.
The intents used are Reply , sendEmail , and readEmail . The entities are contactName and
message .

The model could make the following predictions:
ï¾‰

Expand table

Utterance

Predicted
intent

Actual
intent

Predicted entity

Actual entity

Make a response with
"thank you very

Reply

Reply

thank you very

thank you very much as

much as message

message

much"
Reply with saying

sendEmail

Reply

--

yes as message

readEmail

readEmail

--

--

Reply

sendEmail

dinner last week

cynthia as contactName ,

dinner last week was

was splendid as

dinner last week was

splendid

message

splendid as message

mike as message

mike as contactName

"yes"
Check my email
please
Email to Cynthia that

Send an email to Mike

sendEmail

sendEmail

Intent-level evaluation for Reply intent
ï¾‰

Key

Count

Explanation

True positive

1

Utterance 1 was correctly predicted as Reply .

False positive

1

Utterance 4 was mistakenly predicted as Reply .

False negative

1

Utterance 2 was mistakenly predicted as sendEmail .

Expand table

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 1) = 0.5

Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5
F1 score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.5 * 0.5) / (0.5 + 0.5) =
0.5

Intent-level evaluation for sendEmail intent
ï¾‰

Key

Count

Explanation

True positive

1

Utterance 5 was correctly predicted as sendEmail .

False positive

1

Utterance 2 was mistakenly predicted as sendEmail .

False negative

1

Utterance 4 was mistakenly predicted as Reply .

Expand table

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 1) = 0.5
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5
F1 score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.5 * 0.5) / (0.5 + 0.5) =
0.5

Intent-level evaluation for readEmail intent
ï¾‰

Key

Count

Explanation

True positive

1

Utterance 3 was correctly predicted as readEmail .

False positive

0

--

False negative

0

--

Expand table

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 0) = 1
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 0) = 1
F1 score = 2 * Precision * Recall / (Precision + Recall) = (2 * 1 * 1) / (1 + 1) = 1

Entity-level evaluation for contactName entity

ï¾‰

Expand table

Key

Count

Explanation

True positive

1

cynthia was correctly predicted as contactName in utterance 4.

False positive

0

--

False negative

1

mike was mistakenly predicted as message in utterance 5.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 0) = 1
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5
F1 score = 2 * Precision * Recall / (Precision + Recall) = (2 * 1 * 0.5) / (1 + 0.5) =
0.67

Entity-level evaluation for message entity
ï¾‰

Expand table

Key

Count

Explanation

True
positive

2

thank you very much was correctly predicted as message in utterance 1 and dinner

False
positive

1

mike was mistakenly predicted as message in utterance 5.

False

1

yes wasn't predicted as message in utterance 2.

last week was splendid was correctly predicted as message in utterance 4.

negative

Precision = #True_Positive / (#True_Positive + #False_Positive) = 2 / (2 + 1) = 0.67
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 2 / (2 + 1) = 0.67
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.67 * 0.67) / (0.67 +
0.67) = 0.67

Model-level evaluation for the collective model
ï¾‰

Expand table

Key

Count

Explanation

True positive

6

Sum of true positives for all intents and entities.

False positive

3

Sum of false positives for all intents and entities.

False negative

4

Sum of false negatives for all intents and entities.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 6 / (6 + 3) = 0.67
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 6 / (6 + 4) = 0.60
F1 score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.67 * 0.60) / (0.67 +
0.60) = 0.63

Confusion matrix
A confusion matrix is an N x N matrix used for model performance evaluation, where N is the
number of entities or intents. The matrix compares the expected labels with the ones predicted
by the model. The matrix gives a holistic view of how well the model is performing and what
kinds of errors it's making.
You can use the confusion matrix to identify intents or entities that are too close to each other
and often get mistaken (ambiguity). In this case, consider merging these intents or entities
together. If merging isn't possible, consider adding more tagged examples of both intents or
entities to help the model differentiate between them.
The highlighted diagonal in the following image shows the correctly predicted entities, where
the predicted tag is the same as the actual tag.

ï Š

You can calculate the intent-level or entity-level and model-level evaluation metrics from the
confusion matrix:
The values in the diagonal are the true positive values of each intent or entity.
The sum of the values in the intent or entities rows (excluding the diagonal) is the false
positive of the model.
The sum of the values in the intent or entities columns (excluding the diagonal) is the
false negative of the model.
Similarly:
The true positive of the model is the sum of true positives for all intents or entities.
The false positive of the model is the sum of false positives for all intents or entities.
The false negative of the model is the sum of false negatives for all intents or entities.

Guidance
After you train your model, you see some guidance and recommendations on how to improve
the model. We recommend that you have a model covering every point in the guidance
section.
Training set has enough data: When an intent or entity has fewer than 15 labeled
instances in the training data, it can lead to lower accuracy because the model isn't
adequately trained on that intent. In this case, consider adding more labeled data in the
training set. You should only consider adding more labeled data to your entity if your

entity has a learned component. If your entity is defined only by list, prebuilt, and regex
components, this recommendation doesn't apply.
All intents or entities are present in test set: When the testing data lacks labeled
instances for an intent or entity, the model evaluation is less comprehensive because of
untested scenarios. Consider having test data for every intent and entity in your model to
ensure that everything is being tested.
Unclear distinction between intents or entities: When data is similar for different intents
or entities, it can lead to lower accuracy because they might be frequently misclassified as
each other. Review the following intents and entities and consider merging them if they're
similar. Otherwise, add more examples to better distinguish them from each other. You
can check the Confusion matrix tab for more guidance. If you're seeing two entities
constantly being predicted for the same spans because they share the same list, prebuilt,
or regex components, make sure to add a learned component for each entity and make it
required. Learn more about entity components.

Related content
Train a model in Language Studio

Last updated on 11/18/2025

Data formats accepted by conversational
language understanding
If you're uploading your data into conversational language understanding, it must follow a
specific format. Use this article to learn more about accepted data formats.

Import project file format
If you're importing a project into conversational language understanding, the file uploaded
must be in the following format:
JSON
{
"projectFileVersion": "2022-10-01-preview",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "Conversation",
"projectName": "{PROJECT-NAME}",
"multilingual": true,
"description": "DESCRIPTION",
"language": "{LANGUAGE-CODE}",
"settings": {
"confidenceThreshold": 0
}
},
"assets": {
"projectKind": "Conversation",
"intents": [
{
"category": "intent1"
}
],
"entities": [
{
"category": "entity1",
"compositionSetting": "{COMPOSITION-SETTING}",
"list": {
"sublists": [
{
"listKey": "list1",
"synonyms": [
{
"language": "{LANGUAGE-CODE}",
"values": [
"{VALUES-FOR-LIST}"
]
}
]

}
]
},
"prebuilts": [
{
"category": "{PREBUILT-COMPONENTS}"
}
],
"regex": {
"expressions": [
{
"regexKey": "regex1",
"language": "{LANGUAGE-CODE}",
"regexPattern": "{REGEX-PATTERN}"
}
]
},
"requiredComponents": [
"{REQUIRED-COMPONENTS}"
]
}
],
"utterances": [
{
"text": "utterance1",
"intent": "intent1",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"entities": [
{
"category": "ENTITY1",
"offset": 6,
"length": 4
}
]
}
]
}
}

ï¾‰

Key

Placeholder

Value

{API-VERSION}

The version of

2023-04-01

Example

the API you're
calling.
confidenceThreshold

{CONFIDENCE-

The threshold score for which the intent

THRESHOLD}

is predicted as None intent. Values are
from 0 to 1 .

Expand table

0.7

Key

Placeholder

Value

Example

projectName

{PROJECT-NAME}

The name of your project. This value is

EmailApp

case sensitive.
multilingual

true

A Boolean value that enables you to have

true

utterances in multiple languages in your
dataset. When your model is deployed,
you can query the model in any
supported language (not necessarily
included in your training documents. For
more information about supported
language codes, see Language support.
sublists

[]

Array that contains sublists. Each sublist is

[]

a key and its associated values.
compositionSetting

{COMPOSITION-

Rule that defines how to manage multiple

SETTING}

components in your entity. Options are

combineComponents

combineComponents or
separateComponents .
synonyms

[]

Array that contains all the synonyms.

synonym

language

{LANGUAGE-

A string specifying the language code for
the utterances, synonyms, and regular

en-us

CODE}

expressions used in your project. If your
project is a multilingual project, choose
the language code of most the
utterances.
intents

[]

Array that contains all the intents you

[]

have in the project. These intents are
classified from your utterances.
entities

[]

Array that contains all the entities in your
project. These entities are extracted from

[]

your utterances. Every entity can have
other optional components defined with
them: list, prebuilt, or regex.
dataset

{DATASET}

The test set that this utterance is assigned
to when the data is split before training.
To learn more about data splitting, see

Train

Train your conversational language
understanding model. Possible values for
this field are Train and Test .
category

The type of entity associated with the
span of text specified.

Entity1

Key

Placeholder

Value

Example

offset

The inclusive character position of the
start of the entity.

5

length

The character length of the entity.

5

listKey

A normalized value for the list of
synonyms to map back to in prediction.

Microsoft

"msft",

LIST}

A list of comma-separated strings that
are matched exactly for extraction and
map to the list key.

"microsoft", "MS"

{REGEX-

A normalized value for the regular

ProductPattern1

PATTERN}

expression to map back to in prediction.

{REGEX-

A regular expression.

^pre

{PREBUILT-

The prebuilt components that can extract

Quantity.Number

COMPONENTS}

common types. For the list of prebuilts
you can add, see Supported prebuilt
entity components.

{REQUIRED-

A setting that specifies a requirement that
a specific component must be present to
return the entity. To learn more, see Entity

values

{VALUES-FOR-

regexKey

regexPattern

PATTERN}
prebuilts

requiredComponents

COMPONENTS}

"learned",
"prebuilt"

components. The possible values are
learned , regex , list , or prebuilts .

Utterance file format
Conversational language understanding offers the option to upload your utterances directly to
the project rather than typing them in one by one. You can find this option on the data labeling
page for your project.
JSON
[
{
"text": "{Utterance-Text}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"intent": "{intent}",
"entities": [
{
"category": "{entity}",
"offset": 19,

"length": 10
}
]
},
{
"text": "{Utterance-Text}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"intent": "{intent}",
"entities": [
{
"category": "{entity}",
"offset": 20,
"length": 10
},
{
"category": "{entity}",
"offset": 31,
"length": 5
}
]
}
]

ï¾‰

Expand table

Key

Placeholder

Value

Example

text

{Utterance-

Your utterance text.

Testing

A string that specifies the language code for the utterances used
in your project. If your project is a multilingual project, choose the

en-us

Text}
language

{LANGUAGECODE}

language code of most of the utterances. For more information
about supported language codes, see Language support.
dataset

{DATASET}

The test set that this utterance is assigned to when the data is

Train

split before training. To learn more about data splitting, see Train
your conversational language understanding model. Possible
values for this field are Train and Test .
intent

{intent}

The assigned intent.

intent1

entity

{entity}

The entity to be extracted.

entity1

category

The type of entity associated with the span of text specified.

Entity1

offset

The inclusive character position of the start of the text.

0

length

The length of the bounding box in terms of UTF16 characters.
Training only considers the data in this region.

500

Related content
For more information on importing your labeled data into your project directly, see
Import project.
For more information about labeling your data, see Label your utterances in Language
Studio. After you label your data, you can train your model.

Last updated on 11/18/2025

None intent
Every project in conversational language understanding includes a default None intent. The
None intent is a required intent and can't be deleted or renamed. The intent is meant to
categorize utterances that don't belong to any of your other custom intents.
An utterance can be predicted as the None intent if the top scoring intent's score is lower than
the None score threshold. It can also be predicted if the utterance is similar to examples added
to the None intent.

None score threshold
You can go to the project settings of any project and set the None score threshold. The
threshold is a decimal score from 0.0 to 1.0.
For any query and utterance, the highest scoring intent ends up lower than the threshold score,
so the top intent is automatically replaced with the None intent. The scores of all the other
intents remain unchanged.
The score should be set according to your own observations of prediction scores because they
might vary by project. A higher threshold score forces the utterances to be more similar to the
examples you have in your training data.
When you export a project's JSON file, the None score threshold is defined in the settings
parameter of the JSON as the confidenceThreshold . The threshold accepts a decimal value
between 0.0 and 1.0.
ï¼— Note
During model evaluation of your test set, the None score threshold isn't applied.

Add examples to the None intent
The None intent is also treated like any other intent in your project. If there are utterances that
you want predicted as None, consider adding similar examples to them in your training data. If
you want to categorize utterances that aren't important to your project as None, add those
utterances to your intent. Examples might include greetings, yes-and-no answers, and
responses to questions such as providing a number.
You should also consider adding false positive examples to the None intent. For example, in a
flight booking project it's likely that the utterance "I want to buy a book" could be confused

with a Book Flight intent. You can add "I want to buy a book" or "I love reading books" as None
training utterances. They help to alter the predictions of those types of utterances toward the
None intent instead of Book Flight.

Related content
Conversational language understanding overview

Last updated on 11/18/2025

Deploy custom language projects to
multiple regions
ï¼— Note
This article applies to the following custom features in Azure Language in Foundry Tools:
Conversational language understanding
Custom text classification
Custom named entity recognition (NER)
Orchestration workflow
Custom Language features enable you to deploy your project to more than one region. This
capability makes it much easier to access your project globally while you manage only one
instance of your project in one place. Beginning in November 2024, custom Language features
allow you to deploy your project to multiple resources within a single region using the API.
Thus, you can access and utilize your custom model wherever needed.
Before you deploy a project, you can assign deployment resources in other regions. Each
deployment resource is a different Language resource from the one that you use to author
your project. You deploy to those resources and then target your prediction requests to that
resource in their respective regions and your queries are served directly from that region.
When you create a deployment, you can select which of your assigned deployment resources
and their corresponding regions you want to deploy to. The model you deploy is then
replicated to each region and accessible with its own endpoint dependent on the deployment
resource's custom subdomain.

Example
Suppose you want to make sure your project, which is used as part of a customer support
chatbot, is accessible by customers across the United States and India. You author a project
with the name ContosoSupport by using a West US 2 Language resource named MyWestUS2 .
Before deployment, you assign two deployment resources to your project: MyEastUS and
MyCentralIndia in East US and Central India, respectively.

When you deploy your project, you select all three regions for deployment: the original West
US 2 region and the assigned ones through East US and Central India.
You now have three different endpoint URLs to access your project in all three regions:

West US 2: https://mywestus2.cognitiveservices.azure.com/language/:analyzeconversations

East US: https://myeastus.cognitiveservices.azure.com/language/:analyzeconversations

Central India: https://mycentralindia.cognitiveservices.azure.com/language/:analyzeconversations

The same request body to each of those different URLs serves the exact same response directly
from that region.

Validations and requirements
Assigning deployment resources requires Microsoft Entra authentication. Microsoft Entra ID is
used to confirm that you have access to the resources that you want to assign to your project
for multiregion deployment. In Language Studio, you can automatically enable Microsoft Entra
authentication

by assigning yourself the Azure Cognitive Services Language Owner role to

your original resource. To programmatically use Microsoft Entra authentication, learn more
from the Foundry Tools documentation.
Your project name and resource are used as its main identifiers. A Language resource can only
have a specific project name in each resource. Any other projects with the same name can't be
deployed to that resource.
For example, if a project ContosoSupport was created via the resource MyWestUS2 in West US 2
and deployed to the resource MyEastUS in East US, the resource MyEastUS can't create a
different project called ContosoSupport and deploy a project to that region. Similarly, your
collaborators can't then create a project ContosoSupport with the resource MyCentralIndia in
Central India and deploy it to either MyWestUS2 or MyEastUS .
You can only swap deployments that are available in the exact same regions. Otherwise,
swapping fails.
If you remove an assigned resource from your project, all of the project deployments to that
resource are deleted.
Some regions are only available for deployment and not for authoring projects.

Related content
Learn how to deploy models for:
Conversational language understanding

Custom text classification
Custom NER
Orchestration workflow

Last updated on 12/05/2025

When to use conversational language
understanding or orchestration workflow
apps
When you create large applications, you should consider whether your use case is best served
by a single conversational app (flat architecture) or by multiple apps that are orchestrated.

Orchestration overview
Orchestration workflow is a feature that allows you to connect different projects from LUIS,
conversational language understanding, and custom question answering in one project. You
can then use this project for predictions by using one endpoint. The orchestration project
makes a prediction on which child project should be called, automatically routes the request,
and returns with its response.
Orchestration involves two steps:
1. Predicting which child project to call.
2. Routing the utterance to the destination child app and returning the child app's response.

Orchestration advantages
Clear decomposition and faster development:
If your overall schema has a substantial number of domains, the orchestration
approach can help decompose your application into several child apps (each serving a
specific domain). For example, an automotive conversational app might have a
navigation domain or a media domain.
Developing each domain app in parallel is easier. People and teams with specific
domain expertise can work on individual apps collaboratively and in parallel.
Because each domain app is smaller, the development cycle becomes faster. Smallersized domain apps take much less time to train than a single large app.
More flexible confidence score thresholds:
Because separate child apps serve each domain, it's easy to set separate thresholds for
different child apps.
AI-quality improvements where appropriate:
Some applications require that certain entities must be domain restricted.
Orchestration makes this task easy to achieve. After the orchestration project predicts
which child app should be called, the other child apps aren't called.

For example, if your app contains a Person.Name prebuilt entity, consider the utterance
"How do I use a jack?" in the context of a vehicle question. In this context, jack is an
automotive tool and shouldn't be recognized as a person's name. When you use
orchestration, this utterance can be redirected to a child app created to answer such a
question, which doesn't have a Person.Name entity.

Orchestration disadvantages
Redundant entities in child apps:
If you need a particular prebuilt entity being returned in all utterances irrespective of
the domain, for example Quantity.Number or Geography.Location , there's no way of
adding an entity to the orchestration app (it's an intent-only model). You would need
to add it to all individual child apps.
Efficiency:
Orchestration apps take two model inferences. One for predicting which child app to
call, and another for the prediction in the child app. Inference times are typically slower
than single apps with a flat architecture.
Train/test split for orchestrator:
Training an orchestration app doesn't allow you to granularly split data between the
testing and training sets. For example, you can't train a 90-10 split for child app A, and
then train an 80-20 split for child app B. This limitation might be minor, but it's worth
keeping in mind.

Flat architecture overview
Flat architecture is the other method of developing conversational apps. Instead of using an
orchestration app to send utterances to one of multiple child apps, you develop a singular (or
flat) app to handle utterances.

Flat architecture advantages
Simplicity:
For small-sized apps or domains, the orchestrator approach can be overly complex.
Because all intents and entities are at the same app level, it might be easier to make
changes to all of them together.
It's easier to add entities that should always be returned:
If you want certain prebuilt or list entities to be returned for all utterances, you only
need to add them alongside other entities in a single app. If you use orchestration, as

mentioned, you need to add it to every child app.

Flat architecture disadvantages
Unwieldy for large apps:
For large apps (say, more than 50 intents or entities), it can become difficult to keep
track of evolving schemas and datasets. This difficulty is evident in cases where the app
has to serve several domains. For example, an automotive conversational app might
have a navigation domain or a media domain.
Limited control over entity matches:
In a flat architecture, there's no way to restrict entities to be returned only in certain
cases. When you use orchestration, you can assign those specific entities to particular
child apps.

Related content
Orchestration workflow overview
Conversational language understanding overview

Last updated on 11/18/2025

Integrate conversational language
understanding with Bot Framework
A dialog is the interaction that occurs between user queries and an application. Dialog
management is the process that defines the automatic behavior that should occur for different
customer interactions. While conversational language understanding can classify intents and
extract information through entities, the Bot Framework SDK allows you to configure the
applied logic for the responses returned from it.
This tutorial will explain how to integrate your own conversational language understanding
(CLU) project for a flight booking project in the Bot Framework SDK that includes three intents:
Book Flight, Get Weather, and None.

Prerequisites
Create a Language resource

in the Azure portal to get your key and endpoint. After it

deploys, select Go to resource.
You will need the key and endpoint from the resource you create to connect your bot
to the API. You'll paste your key and endpoint into the code below later in the tutorial.
Download the CoreBotWithCLU sample

.

Clone the entire samples repository to get access to this solution.

Import a project in conversational language
understanding
1. Download the FlightBooking.json

file in the Core Bot with CLU sample, in the Cognitive

Models folder.
2. Sign into the Microsoft Foundry

with your Azure credentials and select your project.

3. On the left side navigation pane, select Playgrounds, navigate to the Language
playground tile, and choose Try Azure Language playground. Select the Conversational
language understanding tile, then select Fine-tune. Create a new fine-tuning task and
import the FlightBooking.json file with the project name as FlightBooking. This will
automatically import the CLU project with all the intents, entities, and utterances.

ï Š

4. Once the project is loaded, select Training jobs on the left. Press on Start a training job,
provide the model name v1 and press Train. All other settings such as Standard Training
and the evaluation settings can be left as is.

ï Š

5. Once training is complete, click to Deploying a model on the left. Select Add Deployment
and create a new deployment with the name Testing, and assign model v1 to the
deployment.

ï Š

A screenshot of the deployment page within the deploy model screen in C L U.

Update the settings file
Now that your CLU project is deployed and ready, update the settings that will connect to the
deployment.
In the Core Bot sample, update your appsettings.json

with the appropriate values.

The CluProjectName is FlightBooking.
The CluDeploymentName is Testing
The CluAPIKey can be either of the keys in the Keys and Endpoint section for your
Language resource in the Azure portal

. You can also copy your key from the Project

Settings tab in CLU.
The CluAPIHostName is the endpoint found in the Keys and Endpoint section for your
Language resource in the Azure portal. Note the format should be
<Language_Resource_Name>.cognitiveservices.azure.com without https:// .

JSON
{
"MicrosoftAppId": "",
"MicrosoftAppPassword": "",
"CluProjectName": "",
"CluDeploymentName": "",
"CluAPIKey": "",
"CluAPIHostName": ""
}

Identify integration points
In the Core Bot sample, you can check out the FlightBookingRecognizer.cs file. Here is where
the CLU API call to the deployed endpoint is made to retrieve the CLU prediction for intents
and entities.
C#
public FlightBookingRecognizer(IConfiguration configuration)
{
var cluIsConfigured =
!string.IsNullOrEmpty(configuration["CluProjectName"]) &&
!string.IsNullOrEmpty(configuration["CluDeploymentName"]) &&
!string.IsNullOrEmpty(configuration["CluAPIKey"]) &&
!string.IsNullOrEmpty(configuration["CluAPIHostName"]);
if (cluIsConfigured)
{
var cluApplication = new CluApplication(
configuration["CluProjectName"],
configuration["CluDeploymentName"],
configuration["CluAPIKey"],
"https://" + configuration["CluAPIHostName"]);
// Set the recognizer options depending on which endpoint version
you want to use.
var recognizerOptions = new CluOptions(cluApplication)
{
Language = "en"
};
_recognizer = new CluRecognizer(recognizerOptions);
}

Under the Dialogs folder, find the MainDialog which uses the following to make a CLU
prediction.
C#
var cluResult = await _cluRecognizer.RecognizeAsync<FlightBooking>
(stepContext.Context, cancellationToken);

The logic that determines what to do with the CLU result follows it.
C#
switch (cluResult.TopIntent().intent)
{
case FlightBooking.Intent.BookFlight:
// Initialize BookingDetails with any entities we may have found
in the response.
var bookingDetails = new BookingDetails()
{
Destination = cluResult.Entities.toCity,
Origin = cluResult.Entities.fromCity,
TravelDate = cluResult.Entities.flightDate,
};
// Run the BookingDialog giving it whatever details we have from
the CLU call, it will fill out the remainder.
return await stepContext.BeginDialogAsync(nameof(BookingDialog),
bookingDetails, cancellationToken);
case FlightBooking.Intent.GetWeather:
// We haven't implemented the GetWeatherDialog so we just
display a TODO message.
var getWeatherMessageText = "TODO: get weather flow here";
var getWeatherMessage =
MessageFactory.Text(getWeatherMessageText, getWeatherMessageText,
InputHints.IgnoringInput);
await stepContext.Context.SendActivityAsync(getWeatherMessage,
cancellationToken);
break;
default:
// Catch all for unhandled intents
var didntUnderstandMessageText = $"Sorry, I didn't get that.
Please try asking in a different way (intent was {cluResult.TopIntent().intent})";
var didntUnderstandMessage =
MessageFactory.Text(didntUnderstandMessageText, didntUnderstandMessageText,
InputHints.IgnoringInput);
await
stepContext.Context.SendActivityAsync(didntUnderstandMessage, cancellationToken);
break;
}

Run the bot locally
Run the sample locally on your machine OR run the bot from a terminal or from Visual Studio:

Run the bot from a terminal
From a terminal, navigate to the cognitive-service-language-samples/CoreBotWithCLU folder.
Then run the following command
Bash
# run the bot
dotnet run

Run the bot from Visual Studio
1. Launch Visual Studio
2. From the top navigation menu, select File, Open, then Project/Solution
3. Navigate to the cognitive-service-language-samples/CoreBotWithCLU folder
4. Select the CoreBotCLU.csproj file
5. Press F5 to run the project

Testing the bot using Bot Framework Emulator
Bot Framework Emulator

is a desktop application that allows bot developers to test and

debug their bots on localhost or running remotely through a tunnel.
Install the latest Bot Framework Emulator

.

Connect to the bot using Bot Framework Emulator
1. Launch Bot Framework Emulator
2. Select File, then Open Bot
3. Enter a Bot URL of http://localhost:3978/api/messages and press Connect and wait for it
to load
4. You can now query for different examples such as "Travel from Cairo to Paris" and observe
the results
If the top intent returned from CLU resolves to "Book flight". Your bot will ask additional
questions until it has enough information stored to create a travel booking. At that point it

returns this booking information back to your user.

Next steps
Learn more about the Bot Framework SDK.

Last updated on 11/18/2025

Supported prebuilt entity components
Conversational Language Understanding allows you to add prebuilt components to entities.
Prebuilt components automatically predict common types from utterances. See the entity
components article for information on components.

Reference
The following prebuilt components are available in Conversational Language Understanding.
ï¾‰

Expand table

Type

Description

Supported languages

Quantity.Age

Age of a person or thing. For example: "30 years
old," "nine months old"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

Quantity.Number

A cardinal number in numeric or text form. For
example: "30," "23," "14.5," "Two and a half"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

Quantity.Percentage

A percentage using the symbol % or the word
"percent". For example: "10%", "5.6 percent"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

Quantity.Ordinal

An ordinal number in numeric or text form. For
example: "first," "second," "last," "10th"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

Quantity.Dimension

Special dimensions such as length, distance,

English, Chinese, French,

volume, area, and speed. For example: "2 miles",
"650 square kilometers," "35 km/h"

Italian, German, Brazilian
Portuguese, Spanish

Quantity.Temperature

A temperature in Celsius or Fahrenheit. For
example: "32 F," "34 degrees celsius," "2 deg C"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

Quantity.Currency

Monetary amounts including currency. For example
"1000.00 US dollars", "Â£20.00," "$67.5B"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

Quantity.NumberRange

Datetime

A numeric interval. For example: "between 25 and

English, Chinese, French,

35"

Italian, German, Brazilian
Portuguese, Spanish

Dates and times. For example: "June 23, 1976," "7
AM," "6:49 PM," "Tomorrow at 7 PM," "Next Week"

English, Chinese, French,
Italian, German, Brazilian

Type

Description

Supported languages
Portuguese, Spanish

Person.Name

Email

Phone Number

URL

The name of an individual. For example: "Joe,"

English, Chinese, French,

"Ann"

Italian, German, Brazilian
Portuguese, Spanish

Email Addresses. For example:
"user@contoso.com", "user_name@contoso.com",

English, Chinese, French,
Italian, German, Brazilian

"user.name@contoso.com"

Portuguese, Spanish

US Phone Numbers. For example: "123-456-7890",

English, Chinese, French,

"+1 123 456 7890", "(123)456-7890"

Italian, German, Brazilian
Portuguese, Spanish

Website URLs and Links.

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

General.Organization

Companies and corporations. For example:

English, Chinese, French,

"Microsoft"

Italian, German, Brazilian
Portuguese, Spanish

Geography.Location

The name of a location. For example: "Tokyo"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

IP Address

An IP address. For example: "192.168.0.4"

English, Chinese, French,
Italian, German, Brazilian
Portuguese, Spanish

Prebuilt components in multilingual projects
In multilingual conversation projects, you can enable any of the prebuilt components. The
component is only predicted if the language of the query supports the prebuilt entity. The
language is either specified in the request or defaults to the primary language of the
application if not provided.

Next steps
Entity components

Last updated on 11/18/2025

Conversational language understanding
limits
Use this article to learn about the data and service limits when using conversational language
understanding.

Language resource limits
Your Language resource must be one of the following pricing tiers

:
ï¾‰

Expand table

Tier

Description

Limit

F0

Free tier

You're only allowed one F0 Language resource per subscription.

S

Paid tier

You can have up to 100 Language resources in the S tier per region.

For more information, see pricing

.

You can have up to 500 projects per resource.
Project names have to be unique within the same resource across all custom features.

Regional availability
See Language regional availability.

API limits
ï¾‰

Item

Request type

Maximum limit

Authoring API

POST

10 per minute

Authoring API

GET

100 per minute

Prediction API

GET/POST

1,000 per minute

Quota limits

Expand table

ï¾‰

Pricing tier

Item

Limit

F

Training time

One hour per month

S

Training time

Unlimited, Standard

F

Prediction Calls

5,000 request per month

S

Prediction Calls

Unlimited, Standard

Expand table

Data limits
The following limits are observed for the conversational language understanding.
ï¾‰

Expand table

Item

Lower Limit

Upper Limit

Number of utterances per project

1

50,000

Utterance length in characters (authoring)

1

500

Utterance length in characters (prediction)

1

1000

Number of intents per project

1

500

Number of entities per project

0

350

Number of list synonyms per entity

0

20,000

Number of list synonyms per project

0

2,000,000

Number of prebuilt components per entity

0

7

Number of regular expressions per project

0

20

Number of trained models per project

0

10

Number of deployments per project

0

10

Naming limits
ï¾‰

Expand table

Item

Limits

Project name

You can only use letters (a-z, A-Z) , and numbers (0-9) , symbols _ . - , with no
spaces. Maximum allowed length is 50 characters.

Model name

You can only use letters (a-z, A-Z) , numbers (0-9) , and symbols _ . - . Maximum
allowed length is 50 characters.

Deployment
name

You can only use letters (a-z, A-Z) , numbers (0-9) , and symbols _ . - . Maximum

Intent name

You can only use letters (a-z, A-Z) , numbers (0-9) , and all symbols except ":", $ & %

allowed length is 50 characters.

* ( ) + ~ # / ? . Maximum allowed length is 50 characters.

Entity name

You can only use letters (a-z, A-Z) , numbers (0-9) , and all symbols except ":", $ & %
* ( ) + ~ # / ? . Maximum allowed length is 50 characters.

Next steps
Conversational language understanding overview

Last updated on 11/18/2025

Terms and definitions used in conversation
language understanding
Use this article to learn about some of the definitions and terms you may encounter when
using conversation language understanding.

Entity
Entities are words in utterances that describe information used to fulfill or identify an intent. If
your entity is complex and you would like your model to identify specific parts, you can break
your model into subentities. For example, you might want your model to predict an address,
but also the subentities of street, city, state, and zipcode.

F1 score
The F1 score is a function of Precision and Recall and needed when you seek a balance
between precision and recall.

Intent
An intent represents a task or action the user wants to perform. It's a purpose or goal
expressed in a user's input, such as booking a flight, or paying a bill.

List entity
A list entity represents a fixed, closed set of related words along with their synonyms. List
entities are exact matches, unlike machined learned entities.
An entity is predicted whenever a word from the entity's list appears in the input. For instance,
consider an entity list named "size" that contains the words "small," "medium," and "large." If
any of these words are used in an utterance, the size entity is recognized. This prediction
occurs regardless of the surrounding context in which these words are mentioned.

Model
A model is an object trained to do a certain task, in this case conversation understanding tasks.
Models are trained by providing labeled data to learn from so they can later be used to
understand utterances.

Model evaluation is the process that happens right after training to know how well does
your model perform.
Deployment is the process of assigning your model to a deployment to make it available
for use via the prediction API

.

Overfitting
Overfitting happens when the model is fixated on the specific examples and isn't able to
generalize well.

Precision
Measures how precise/accurate your model is. It's the ratio between the correctly identified
positives (true positives) and all identified positives. The precision metric reveals how many of
the predicted classes are correctly labeled.

Project
A project is a work area for building your custom ML models based on your data. Your project
is only accessible to you and others who have access to the Azure resource being used.

Recall
Measures the model's ability to predict actual positive classes. It's the ratio between the
predicted true positives and what was tagged. The recall metric reveals how many of the
predicted classes are correct.

Regular expression
A regular expression entity represents a regular expression. Regular expression entities are
exact matches.

Schema
Schema is defined as the combination of intents and entities within your project. Schema
design is a crucial part of your project's success. When creating a schema, you want to think
about which intents and entities should be included in your project.

Training data
Training data is the set of information that is needed to train a model.

Utterance
An utterance is user input that's short text representative of a sentence in a conversation. It's a
natural language phrase such as "book 2 tickets to Seattle next Tuesday." Example utterances
are added to train the model and the model predicts on new utterance at runtime

Next steps
Data and service limits.
Conversation language understanding overview.

Last updated on 11/18/2025

What is custom question answering?
Custom question answering (CQA) is a cloud-based Natural Language Processing (NLP) service
that creates conversational AI applications over your data. Build knowledge bases from FAQs,
manuals, and documents to deliver accurate answers through chat bots, virtual assistants, and
interactive interfaces.

Key capabilities
Custom question answering provides enterprise-grade features for building and maintaining
conversational AI solutions:
Knowledge base creation - Import content from URLs, files, and documents. The service
automatically extracts question-answer pairs from structured and semi-structured
sources.
Multi-turn conversations - Create guided conversation flows with follow-up prompts that
navigate users through complex information.
Metadata filtering - Tag answers by content type, domain, or freshness to deliver
contextually relevant responses.
Active learning - Improve answer quality based on real-world usage patterns and user
queries.
Deep learning ranking - Multi-stage ranking architecture combines Azure AI Search with
NLP reranking for optimal answer selection.

Architecture and workflow
The service follows a structured pipeline from project creation to production deployment:
1. Create a project - Build a knowledge base by importing content sources or manually
adding question-answer pairs in Microsoft Foundry (classic)

.

2. Test and refine - Use the test interface to validate responses and adjust answer quality
before deployment.
3. Deploy - Publish your project to create a REST API endpoint accessible by client
applications.
4. Integrate - Client applications send queries and receive JSON responses with answers,
confidence scores, and follow-up prompts.

Development options

Choose from multiple development approaches based on your technical requirements and
expertise:
Microsoft Foundry (classic) - Low-code authoring with automatic QA extraction,
markdown support, and chit-chat integration. Deploy directly to Azure Bot Service

.

REST APIs - Programmatic access for custom integrations and automated workflows. See
the Azure Language REST API reference for endpoint documentation.
Client libraries - SDK packages for .NET and Python enable programmatic project
management and query integration:
.NET (C#) packages

- Runtime and authoring SDKs for C# applications

Python packages

- Runtime and authoring SDKs for Python applications

Next steps
Quickstart: Create and deploy a CQA project
Manage knowledge bases
Configure multi-turn conversations

Last updated on 12/13/2025

Quickstart: custom question answering
(CQA)
This quickstart guides you through the essential steps needed to create, test, and deploy a
custom question answering (CQA) project in the Microsoft Foundry. Whether you're
transitioning from Language Studio or starting from scratch, this quickstart is for you. It
provides clear and actionable instructions to achieve a fast and successful CQA project
deployment.
ï¼— Note
If you already have an Azure Language in Foundry Tools or multi-service resourceâ€”
whether used on its own or through Language Studioâ€”you can continue to use
those existing Language resources within the Foundry portal. For more information,
see How to use Foundry Tools in the Foundry portal.
We highly recommend that you use a Foundry resource in the Foundry; however, you
can also follow these instructions using a Language resource.

Prerequisites
Before you get started, you need the following resources and permissions:
An active Azure subscription. If you don't have one, create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
A Foundry resource or a Language resource
An Azure AI Search resource

.

(required for accessing CQA). For more information on

how to connect your Azure AI Search resource, see Configure connections in Foundry
A Foundry project created in the Foundry. For more information, see Create a Foundry
project.

Get started
1. Navigate to the Foundry

.

2. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
3. Once signed in, you can create or access your existing projects within Foundry.
4. If you're not already at your project for this task, select it.

Create your CQA fine tuning task
In the Foundry, a fine-tuning task serves as your workspace for your CQA solutions. Previously,
a fine-tuning task was referred to as a CQA project. You might encounter both terms used
interchangeably in older CQA documentation.
1. After you select the Foundry project to use for this quickstart, select fine-tuning from the
left navigation menu.

2. From the main window, select the AI Service fine-tuning tab and then the + Fine-tune
button.

3. From the Create service fine-tuning window, choose the Custom question answering tab
and then select Next.

4. Select your Connected Azure AI Search resource from the Create CQA fine tuning task
window. For more information, see Configure Azure resource connections.
5. Next, complete the Name and Language fields. For this project, you can leave the Default
answer when no answer is returned field as is (No answer found).
6. Select the Create button.

Add a CQA knowledge base source
A CQA knowledge base is a structured set of question-and-answer pairs optimized for
conversational AI. The knowledge base uses natural language processing to interpret user
queries and return context-aware, accurate answers from a specific dataset.
1. From the Getting Started menu, select Manage sources.

2. From the main window, select the + Add source drop-down menu.
3. From the drop-down menu you can select Add chit chat, Add URLs, or Add Files.

4. For this project, let's choose Add chitchat.
5. From the Add new source window, let's choose Friendly.

6. Finally, select Add. It may take a few minutes for the source to be created.
7. Once created, the source is listed in the Manage sources window.

Test your knowledge base
1. Select Test knowledge base from the Getting Started menu.
2. Type the following in the Type your question field and then select Run.

text
Hello! How are you doing today?

3. In the inspection interface, you can review the response confidence level and choose the
most suitable answer.
Screenshot of the inspection interface in the Foundry.

Deploy your knowledge base
Deploying a CQA knowledge base means publishing your curated question-and-answer
content as a live, searchable endpoint. This process moves your project from a testing phase to
a production environment enabling client applications to use it for various projects and
solutions, including chatbots.
1. Once your inspection is complete, choose the Deploy knowledge base section from the
Getting Started menu.
2. Select the Deploy button first from the Deploy knowledge base main window and then
from the Deploy this project pop-up window. It takes a few minutes to deploy.
3. After deployment is complete, your deployed project is listed in the Deploy knowledge
base window.
That's it! Your Custom Question Answering (CQA) knowledge base provides a natural language
interface to your data, allowing users to interact with information in a conversational manner.
By deploying this solution, you can create advanced chatbots and interactive agents that
comprehend user questions, supply precise answers, and adjust to changing informational
requirements.

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Explore the REST API

Authoring API reference
Authoring API cURL examples
Runtime API reference

Next steps
Tutorial: Create an FAQ bot

Last updated on 11/18/2025

Language support for custom question
answering and projects
This article describes the language support options for custom question answering enabled
resources and projects.
In custom question answering, you have the option to either select the language each time you
add a new project to a resource allowing multiple language support, or you can select a
language that will apply to all future projects for a resource.

Supporting multiple languages in one custom
question answering enabled resource

When you're creating the first project in your service, you get a choice pick the language
each time you create a new project. Select this option, to create projects belonging to
different languages within one service.
The language setting option cannot be modified for the service once the first project is
created.

If you enable multiple languages for the project, then instead of having one test index for
the service you will have one test index per project.

Supporting multiple languages in one project
If you need to support a project system, which includes several languages, you can:
Use the Translator service to translate a question into a single language before sending
the question to your project. This allows you to focus on the quality of a single language
and the quality of the alternate questions and answers.
Create a custom question answering enabled language resource, and a project inside that
resource, for every language. This allows you to manage separate alternate questions and
answer text that is more nuanced for each language. This provides more flexibility but
requires a much higher maintenance cost when the questions or answers change across
all languages.

Single language per resource
If you select the option to set the language used by all projects associated with the resource,
consider the following:
A language resource, and all its projects, will support one language only.
The language is explicitly set when the first project of the service is created.
The language can't be changed for any other projects associated with the resource.
The language is used by the Azure AI Search service (ranker #1) and custom question
answering (ranker #2) to generate the best answer to a query.

Languages supported
The following list contains the languages supported for a custom question answering resource.
Arabic
Armenian
Bangla
Basque
Bulgarian
Catalan
Chinese_Simplified
Chinese_Traditional
Croatian
Czech

Danish
Dutch
English
Estonian
Finnish
French
Galician
German
Greek
Gujarati
Hebrew
Hindi
Hungarian
Icelandic
Indonesian
Irish
Italian
Japanese
Kannada
Korean
Latvian
Lithuanian
Malayalam
Malay
Norwegian
Polish
Portuguese
Punjabi
Romanian
Russian
Serbian_Cyrillic
Serbian_Latin
Slovak
Slovenian
Spanish
Swedish
Tamil
Telugu
Thai
Turkish
Ukrainian

Urdu
Vietnamese

Query matching and relevance
Custom question answering depends on Azure AI Search language analyzers for providing
results.
While the Azure AI Search capabilities are on par for supported languages, custom question
answering has an additional ranker that sits above the Azure search results. In this ranker
model, we use some special semantic and word-based features in the following languages.
Chinese
Czech
Dutch
English
French
German
Hungarian
Italian
Japanese
Korean
Polish
Portuguese
Spanish
Swedish
This ranking is an internal working of the custom question answering's ranker.

Next steps
Question answering quickstart

Last updated on 11/18/2025

Transparency note and use cases for
question answering
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance.
Microsoft provides transparency notes to help you understand how our AI technology works.
This includes the choices system owners can make that influence system performance and
behavior, and the importance of thinking about the whole system, including the technology,
the people, and the environment. You can use transparency notes when developing or
deploying your own system, or share them with the people who will use or be affected by your
system.
Transparency notes are part of a broader effort at Microsoft to put our AI principles into
practice. To find out more, see Microsoft's AI principles

.

Introduction to question answering
Question answering is a cloud-based, natural language processing service that easily creates a
natural conversational layer over your data. It can be used to find the most appropriate answer
for a specified natural language input, from your custom knowledge base of information. See
the list of supported languages here.
Question answering is commonly used to build conversational client applications, which
include social media applications, chat bots, and speech-enabled desktop applications. A client
application based on question answering can be any conversational application that
communicates with a user in natural language to answer a question.
Question Answering uses several Azure resources, each for a different purpose: Azure Cognitive
Search, and Azure Monitor. All customer data (question answers and chatlogs) is stored in the

region where the customer deploys the dependent service instances. For more details on
dependent services see here.

The basics of question answering
The first step in using question answering is training and preparing the QnA service to
recognize the questions and answers that may be developed from your content. Question
answering imports your content into a knowledge base of question and answer pairs. The
import process extracts information about the relationship between the parts of your
structured and semi-structured content to infer relationships between the question and answer
pairs.
The extracted QnA pairs are displayed in the following way:

You can edit these question and answer pairs, and add new pairs yourself. When you're
satisfied with the content of your knowledge base, you can publish it, which will make it ready
to be used to respond to questions sent to your client applications. At the second step, your
client application sends the user's question to your question answering service API. Your
question answering service processes the question and responds with the best answer.

For more details, see the question answering documentation.

Terms and definitions
ï¾‰

Expand table

Term

Definition

Knowledge
base

A collection of questions, answers, and metadata that have been extracted from content
sources or added manually. The collection is then used to develop question and answer
pairs. Queries to the QnA service are matched against the contents of the knowledge base.

Active

Consumes the feedback from use of the system to provide suggestions (in the form of

learning

new questions) to the knowledge base owner to improve the contents of their knowledge
base. Learn more here.

Multi-turn

Sometimes additional information is needed for question answering to determine the best
answer to a user question. Question answering asks a follow-up question to the user.

Metadata

Additional information in the form of a name and value that you can associate with each
QnA pair in your knowledge base. Metadata can be used to pass context and filter results.

Synonyms

Alternate terms that can be used interchangeably in the knowledge base.

Example use cases
You can use question answering in multiple scenarios and across a variety of industries.
Typically information retrieval use cases are best suited for question answering where there are
usually one or only a few correct responses to a user question. Scenarios or topics that have a
wide variety of viewpoints, worldviews, geopolitical views, controversial content, etc. will be
more difficult to answer correctly. Customers should be aware that providing this type of
content via question answering can create negative sentiment and reactions, and result in
negative publicity. If you do provide this type of content, consider adding source attribution to
allow your users to evaluate the answers for themselves.
Some typical scenarios where question answering is recommended are:
Customer support: In most customer support scenarios, common questions get asked
frequently. Question Answering lets you instantly create a chat bot from existing support
content, and this bot can act as the front line system for handling customer queries. If the
questions can't be answered by the bot, then additional components can help identify
and flag the question for human intervention.
Enterprise FAQ bot: Information retrieval is a challenge for enterprise employees. Internal
FAQ bots are a great tool for helping employees get answers to their common questions.
Question answering enables various departments, such as human resources or payroll, to
build FAQ chat bots to help employees.

Instant answers over search: Many search systems augment their search results with
instant answers, which provide the user with immediate access to information relevant to
their query. Answers from question answering can be combined with the results from
document search to offer an instant answer experience to the end user.

Considerations when choosing other use cases
Avoid high-risk scenarios: The machine learnt algorithm used by question answering
optimizes the performance based on the data it is trained on, however there will always
be edge cases where the correct answer isn't returned for a user query which the system
doesn't understand well. When you design your scenarios with question answering, be
aware of the possibility of false positive results. It is advisable to create a dataset of the
top queries asked in your scenario and the corresponding expected answers, and
periodically test the service for the correctness of the responses. For example:
Healthcare: This often requires high precision, and wrong information can have lifethreatening consequences. Consider the example of a Doctor Assistant bot that uses
question answering to understand the patient's symptoms and match it to common
illnesses. Likewise, any bots that are designed to converse with patients with mental
health issues, such as depression or anxiety, must be very careful of the responses
returned. question answering can be helpful in parsing through clinical terminology
and deriving useful question and answer pairs, but is not designed, intended or made
available to create medical devices, and is not designed or intended and should not be
used as a substitute for professional medical advice, diagnosis, treatment, or judgment.
Customer is solely responsible for displaying and/or obtaining appropriate consents,
warnings, disclaimers and acknowledgements to end users of their implementation.
Avoid open domain scenarios: question answering is meant to answer questions from a
particular domain knowledge base, not open-ended questions, or out-of-domain
questions. Using out-of-domain questions with question answering runs the risk of
returning incorrect responses. For example:
Social bots: Bots that are meant for generic chit-chat, not related to a particular
domain, are difficult to design with question answering. In these scenarios, the user
intents and viewpoints can range widely (for example, sports, fashion, politics, and
religion). Building a question answering knowledge base is best used for facts and/or
discovery of content. Using question answering for diverse worldview topics may be
challenging and we recommend customers consider more careful review or curating of
such content.
Handling inappropriate conversations: It's possible that users will initiate
inappropriate conversations with the bot, including expletives or hate speech. The bot
designer must be very careful about how to handle these conversations, and make

sure that these intents are detected with high accuracy and the appropriate response
given. It's difficult to build a comprehensive knowledge base in question answering
containing every variation of inappropriate utterances possible. It is therefore better to
handle such cases with a rule based system, for example the user utterances can be
quickly checked for the presence of any words from a pre-processed blocklist of
inappropriate keywords. This is not part of the question answering service and would
need to be developed on top of the question answering service.
Legal and regulatory considerations: Organizations need to evaluate potential specific
legal and regulatory obligations when using any AI services and solutions, which may not
be appropriate for use in every industry or scenario. Additionally, AI services or solutions
are not designed for and may not be used in ways prohibited in applicable terms of
service and relevant codes of conduct.

Next steps
Microsoft AI principles
Microsoft responsible AI resources
Microsoft principles for developing and deploying facial recognition technology
Identify principles and practices for responsible AI
Building responsible bots

Last updated on 08/17/2025

Guidance for integration and responsible
use of question answering
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use question
answering. We're taking a principled approach to upholding personal agency and dignity by
considering the AI systems' fairness, reliability and safety, privacy and security, inclusiveness,
transparency, and human accountability. These considerations reflect our commitment to
developing Responsible AI.

General deployment principles
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context, by thoroughly testing it with real-life conditions and data.
Synthetic data and tests that don't reflect your end-to-end scenario won't be sufficient.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within, and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.
System review: If you're planning to deploy an AI-powered product or feature into an
existing system of software, customers, and organizational processes, take the time to
understand how each part of your system will be affected. Consider how your AI solution
aligns with Microsoft's AI principles

.

Human in the loop: Keep a human in the loop. This means ensuring constant human
oversight of the AI-powered product or feature, and maintaining the role of humans in
decision making. Ensure that you can have real-time human intervention in the solution

to prevent harm. This enables you to manage situations when the AI model does not
perform as required.
Security: Ensure your solution is secure, and that it has adequate controls to preserve the
integrity of your content and prevent any unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service after it has been deployed. Monitor and improve the AIpowered product or feature on an ongoing basis.

Specific deployment guidance for question
answering
Common use cases of question answering include customer support chat bots and internal
enterprise FAQ chat bots. When you're deploying an application that uses question answering,
you should ask the following questions:
How is the data processed? All the customer data is stored in Azure Cognitive Search and
Azure Monitor in the customer's Azure subscription. Question answering processes the
data when extracting questions and answers from sources and serving the correct
answers for a particular query. The question answering service doesn't retain customer
data after responding to a client application's query.
Where is the data stored? Some countries or domains might have restrictions on the data
being stored in a particular geographic area. Choose the appropriate regions for Azure
Cognitive Search, and Azure Monitor, keeping in mind the data residency requirements of
your scenario.
How is user privacy handled? In some scenarios, users can be asked for additional
information before the response is returned from question answering. These scenarios are
called multi-turn conversations. Some of the information collected from the users can
include personal information or other sensitive information. For more information about
best practices for data privacy, see the Responsible bots guidelines

for developers.

Inform users up front about the data that is collected and how it is used, and obtain
their consent beforehand. Provide easy access to a valid privacy statement and
applicable service agreement, and include a "profile page" for users to obtain
information about the bot, with links to relevant privacy and legal information.
Collect no more personal data than you need, limit access to it, and store it for no
longer than needed. Collect only the personal data that is essential for your bot to
operate effectively. If your bot will share data (such as with another bot), be sure only

to share the minimum amount of user data necessary in order to complete the
requested function on behalf of the user. If you enable access by other agents to your
bot's user data, do so only for the time necessary in order to complete the requested
function. Always give users the opportunity to choose which agents your bot will share
data with, and what data is suitable for sharing. Consider whether you can purge
stored user data from time to time, while still enabling your bot to learn. Shorter
retention periods minimize security risks for users.
Provide privacy-protecting user controls. For bots that store personal information,
such as authenticated bots, consider providing easily discoverable buttons to protect
privacy. For example: Show me all you know about me, Forget my last interaction
and Delete all you know about me. In some cases, such controls might be legally
required.
Obtain legal and privacy review. The privacy aspects of bot design are subject to
important and increasingly stringent legal requirements. Be sure to obtain both a legal
and a privacy review of your bot's privacy practices through the appropriate channels
in your organization.

Next steps
Microsoft AI principles
Microsoft responsible AI resources
Microsoft principles for developing and deploying facial recognition technology
Identify principles and practices for responsible AI
Building responsible bots

Last updated on 08/17/2025

Data and privacy for question answering
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides some high level details regarding how data is processed by question
answering. Its important to remember that you are responsible for your use and the
implementation of this technology, including complying with all applicable laws and
regulations that apply to you. For example, it's your responsibility to:
Understand where your data is processed and stored by the question answering service in
order to meet regulatory obligations for your application.
Inform the users of your applications that information like chat logs will be logged and
can be used for further processing.
Ensure that you have all necessary licenses, proprietary rights or other permissions
required to the content in your knowledge base that is used as the basis for developing
the QnAs.

What data does question answering process?
question answering uses several Azure services, each with a different purpose. For a detailed
explanation of how these services are used read the documentation here.
Question answering handles two kinds of customer data:
Data sources: Any sources (documents or URLs) added to question answering via the
portal or APIs are parsed to extract the QnA pairs. These QnAs are stored in a Azure
Cognitive Search service

in the customer's subscription. After extracting QnA pairs the

management service discards the data sources, so no customer data is stored with the
question answering service.
Chat logs: If diagnostic logs are turned on, all chat logs are stored in the Azure Monitor
service in the customer's subscription.
In both of these cases, Microsoft acts as a data processor. Data is stored and served directly
from the customer's subscription.

How does question answering process data?
There are two main parts in the question answering stack that process data:
Extraction of question and answer pairs: Any data sources added by the user to the
knowledge base are parsed to extract these pairs. The algorithm looks for a repeating
pattern in the source documents, or for a particular layout of the content, to determine
which sections constitute a question and answer. question answering optimizes the
extraction for display in a chat bot, which typically has a small surface area. The extracted
QnAs are stored in Azure Cognitive Search.
Search for the best answer match: When the Azure Cognitive Search index is built, the
ranking looks for the best match for any incoming user question. It does so by applying
natural language processing techniques.

How is data retained and what customer controls are
available?
The question answering knowledge base and the user chat logs are stored in Azure Cognitive
Search and Azure Monitor in the user's subscription itself.
Only users who have access to the customer's Azure subscription can view the chat logs
stored in Azure Monitor. The owner of the subscription can control who has access by
using role-based access control.
To control access to a question answering knowledge base, you can assign the
appropriate roles to users by using question answering specific roles.
To learn more about privacy and security commitments, see the Microsoft Trust Center

Next steps
Microsoft AI principles
Microsoft responsible AI resources
Microsoft principles for developing and deploying facial recognition technology
Identify principles and practices for responsible AI
Building responsible bots

.

Last updated on 08/17/2025

Create, test, and deploy: CQA knowledge
base
This guide walks you through the essential steps needed to create, test, and deploy a custom
question answering (CQA) knowledge base in the Microsoft Foundry. Whether you're
transitioning from Language Studio or starting from scratch, this guide is for you. It provides
clear and actionable instructions to achieve a fast and successful CQA deployment in the
Foundry.
ï¼— Note
If you already have an Azure Language in Foundry Tools or multi-service resourceâ€”
whether used on its own or through Language Studioâ€”you can continue to use
those existing Language resources within the Foundry portal. For more information,
see How to use Foundry Tools in the Foundry portal.
In Foundry, a fine-tuning task serves as your workspace for your CQA solutions.
Previously, a fine-tuning task was referred to as a CQA project. You might encounter
both terms used interchangeably in older CQA documentation.
We highly recommend that you use a Foundry resource in the Foundry; however, you
can also follow these instructions using a Language resource.

Prerequisites
Before you get started, you need the following resources and permissions:
An active Azure subscription. If you don't have one, create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
A Foundry resource or a Language resource
An Azure AI Search resource

.

(required for accessing CQA). For more information on

how to connect your Azure AI Search resource, see Configure connections in Foundry
A Foundry project created in the Foundry. For more information, see Create a Foundry
project.

Get started
1. Navigate to the Foundry

.

2. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
3. Once signed in, you can create or access your existing projects within Foundry.
4. If you're not already at your project for this task, select it.

Create your CQA fine tuning task
In the Foundry, a fine-tuning task serves as your workspace for your CQA solutions. Previously,
a fine-tuning task was referred to as a CQA project. You might encounter both terms used
interchangeably in older CQA documentation.
1. After you select the Foundry project to use for this project, select fine-tuning from the
left navigation menu.

2. From the main window, select the AI Service fine-tuning tab and then the + Fine-tune
button.

3. From the Create service fine-tuning window, choose the Custom question answering tab
and then select Next.

4. Select your Connected Azure AI Search resource from the Create CQA fine tuning task
window. For more information, see Configure Azure resource connections.
5. Next, complete the Name and Language fields. For this project, you can leave the Default
answer when no answer is returned field as is (No answer found).
6. Select the Create button.

Add a CQA knowledge base source
A CQA knowledge base is a structured set of question-and-answer pairs optimized for
conversational AI. The knowledge base uses natural language processing to interpret user
queries and return context-aware, accurate answers from a specific dataset.
1. From the Getting Started menu, select Manage sources.

2. From the main window, select the + Add source drop-down menu.
3. From the drop-down menu you can select Add chit chat, Add URLs, or Add Files.

4. For this project, choose Add URLS.
5. In the Add URLs window, add the following values:
URL name: Surface Book User Guide
URL: https://download.microsoft.com/download/7/B/1/7B10C82E-F520-40808516-5CF0D803EEE0/surface-book-user-guide-EN.pdf
Classify file structure: Leave the default setting (Auto-detect)

6. Finally, select the Add URLs button.
The extraction process requires a short amount of time to analyze the document and
detect questions and answers. During this step, the service evaluates whether the content
is structured or unstructured.
Once the source is successfully added, you can edit its contents and include added
custom question-and-answer pairs.
7. Once the source is successfully added, it appears in the Manage sources window. There
you have the option to edit its contents and include additional custom question-andanswer pairs.

Test your knowledge base
1. Select Test knowledge base from the Getting Started menu.
2. In the main window, Enter the question How do I set up my Surface Book? and then
select the Run button. Answers are returned using the question-and-answer pairs that

were automatically detected and taken from the source URL:

ï Š

Deploy your knowledge base
Deploying a CQA knowledge base means publishing your curated question-and-answer
content as a live, searchable endpoint. This process moves your project from a testing phase to
a production environment enabling client applications to use it for various projects and
solutions, including chatbots.
1. Once your inspection is complete, choose the Deploy knowledge base section from the
Getting Started menu.
2. Select the Deploy button first from the Deploy knowledge base main window and then
from the Deploy this project pop-up window. It takes a few minutes to deploy.
3. After deployment is complete, your deployed project is listed in the Deploy knowledge
base window.
That's it! Your Custom Question Answering (CQA) knowledge base provides a natural language
interface to your data, allowing users to interact with information in a conversational manner.
By deploying this solution, you can create advanced chatbots and interactive agents that
comprehend user questions, supply precise answers, and adjust to changing informational
requirements.

Clean up resources

To clean up and remove an Azure AI subscription, you can delete either the individual resource
or the entire resource group. If you delete the resource group, all resources contained within it
will also be deleted.

Next steps
Last updated on 12/15/2025

Create and deploy a CQA agent
This article gives you clear steps and important tips for building and deploying a CQA agent.
Whether you're new to this process or updating your skills, this guide helps you set up and
launch your agent successfully.
ï¼— Note
If you already have an Azure Language in Foundry Tools or multi-service resourceâ€”
whether used on its own or through Language Studioâ€”you can continue to use
those existing Language resources within the Microsoft Foundry portal. For more
information, see How to use Foundry Tools in the Foundry portal.
In Foundry, a fine-tuning task serves as your workspace for your CQA solutions.
Previously, a fine-tuning task was referred to as a CQA project. You might encounter
both terms used interchangeably in older CQA documentation.
We highly recommend that you use a Foundry resource in the Foundry; however, you
can also follow these instructions using a Language resource.

Prerequisites
Before you get started, you need the following resources and permissions:
An active Azure subscription. If you don't have one, create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
A Foundry resource or a Language resource
An Azure AI Search resource

.

(required for accessing CQA). For more information on

how to connect your Azure AI Search resource, see Configure connections in Foundry
A Foundry project created in the Foundry with a deployed CQA knowledge base. For
more information, see Create and deploy a CQA project

Step 1: Get started
Let's begin:
1. Navigate to the Foundry.

2. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
3. Once signed in, you can create or access your existing projects within Foundry.
4. If you're not already in your Foundry project with your deployed CQA knowledge base,
select it now.

Step 2: Deploy an OpenAI model in Foundry
(required)
An OpenAI model serves as the foundational source of intelligence and advanced reasoning for
your agent.
1. Select Models + endpoints from the My assets section of the navigation menu:
Screenshot of the deploy model button menu in Foundry.
2. From the main window, select the + Deploy model button.
3. Select Deploy base model from the drop-down menu.
Screenshot of the connected resources menu selection in Foundry.
4. From the Select a model window, select the gpt-4 base model for this project.
Screenshot of the select a model selection in Foundry.
5. Next, select the Confirm button.
6. In the Deploy gpt-4 window, keep the default values and select the Deploy button.
Screenshot of the gpt-4 deployment window in Foundry.
7. Great! The model deployment step is complete.

Step 3: Connect a custom key (required)
A custom key serves as an enhanced security credential for your agent.
1. Navigate to Management Center â†’ Connected Resources.
Screenshot of Management center navigation menu's connected resources selection in
Foundry.

2. From the main window, select the + New connection button.
3. From the Add a connection to external assets window, under Other resource types,
select Custom keys.
Screenshot of add your custom keys selection in Foundry.
4. In the Connect a custom resource window, configure the connection as follows:
Authentication. Leave this field set to Custom (default).
Custom keys. Select + Add key value pairs and complete the two fields as follows:
First field. Complete this field with the following key name:
text
Ocp-Apim-Subscription-Key

Second field. Complete this field with the key value from your Azure portal Foundry
or Language resource used to create your CQA knowledge base. Make sure to check
the is secret box.
Screenshot of the connect a custom resource window in the Foundry.
Next, add a Connection name.
Finally, select the Add connection button.
5. Your new Custom key connection is listed on the Manage connected resources in this
project page.
6. We now provisioned all the necessary resources to create an agent. Select the Go to
project button to return to your project with your deployed CQA knowledge base.
Screenshot of go to project button in Foundry.

Step 4: Create an agent
With your OpenAI deployment and custom key in place, you're ready to begin building your
agent, grounded in the knowledge base you chose for this project.
1. From your project's overview page, select Fine tuning from the left navigation menu.

2. In the main window, select the model name, created with your deployed CQA knowledge
base, from the displayed list.
3. Select Deploy knowledge base from the Getting started menu.
4. Under next steps, select the Create an agent button.
Screenshot of the create agent button in Foundry.
5. In the Create new CQA agent window, complete the fields as follows:
Foundry Project Name. The name of your project should already appear in this field
by default.
Deployment Model. The name of the model that you deployed in Step 2 should
already appear in this field by default.
Agent Name. Name your agent (don't use dashes or underscores).
Custom Connection. The name of the custom key that you connected in Step 3
should already appear in this field by default.
6. Select the Next button:
Screenshot of create CQA agent window in Foundry.
7. Review the details of your new agent, and then select the Create agent button.
8. Once your agent is successfully created, select the Try in playground button.
Screenshot of create CQA agent review and create window in Foundry.

Step 5: Test in agents playground
The agents playground provides a sandbox to test and configure a deployed agentâ€”adding
knowledge and defining actionsâ€”before deploying it to production, all without writing code.
1. From the Create and debug your agents window select the Try in Playground button.
Screenshot of the **Create and debug your agents** window and **Try in playground**
button in Foundry
2. Once the agent is successfully uploaded to the playground, you can test it by sending test
queries.
Screenshot of the agents playground in Foundry.

That's it! The agent creation and deployment processes are complete. You now know how to
deploy a CQA agent using your own custom knowledge base.

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.

Next Steps
Learn about CQA supported languages

Last updated on 11/18/2025

Export-import-refresh in custom question
answering
You might want to create a copy of your custom question answering project or related
question and answer pairs for several reasons:
To implement a backup and restore process
To integrate with your CI/CD pipeline
To move your data to different regions

Prerequisites
An Azure subscription. You can create one for free
A language resource

before you begin.

with the custom question answering feature enabled. Remember

your Microsoft Entra ID, Subscription, language resource name you selected when you
created the resource.

Export a project programmatically
To automate the export process, use the export functionality of the authoring API

Import a project programmatically
To automate the import process, use the import functionality of the authoring API

Refresh a URL programmatically
To automate the URL refresh process, use the update sources functionality of the authoring API
The update sources example in the Authoring API docs shows the syntax for adding a new
URL-based source. An example query for an update would be as follows:
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region-

specific portion of southcentral . The endpoint path is already present.

Variable
name

Value

API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project where you would like to update sources.

NAME

Bash
curl -X PATCH -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '[
{
"op": "replace",
"value": {
"displayName": "source5",
"sourceKind": "url",
"sourceUri": https://download.microsoft.com/download/7/B/1/7B10C82E-F520-40808516-5CF0D803EEE0/surface-book-user-guide-EN.pdf,
"refresh": "true"
}
}
]' -i 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/sources?api-version=2021-10-01'

Next steps
Learn how to use the Authoring API

Last updated on 12/13/2025

Use smart URL refresh with a project
Custom question answering allows you to keep your source content up to date by retrieving
the latest information from a source URL. With just one selection, you can update the
corresponding project to reflect these changes. The service ingests content from the URL and
either creates, merges, or deletes question-and-answer pairs in the project.
This functionality is provided to support scenarios where the content in the source URL
changes frequently, such as product FAQ page updates. The service refreshes the source and
update the project to the latest content while retaining any manual edits made previously.
ï¼— Note
This feature is only applicable to URL sources, and they must be refreshed individually, not
in bulk.

ï¼‰ Important
This feature is only available in the 2021-10-01 version of Azure Language API.

How it works
If you have a project with a URL source that changed, you can trigger a smart URL refresh to
keep your project up to date. The service scans the URL for updated content and generates
QnA pairs. It adds any new QnA pairs to your project and also delete any pairs that
disappeared from the source (with exceptionsâ€”). It also merges old and new QnA pairs in
some situations.
ï¼‰ Important
Because smart URL refresh can involve deleting old content from your project, you might
want to create a backup of your project before you do any refresh operations.
You can trigger a refresh programmatically using the REST API. See the Update Sources
reference documentation for parameters and a sample request.

Smart refresh behavior

When the user refreshes content using this feature, the project of QnA pairs may be updated in
the following ways:

Delete old pair
If the content at the source URL changes and an existing QnA pair from the previous version is
no longer present, that pair is removed from the updated project. This process ensures that
your refreshed project only contains QnA pairs that match the current source content. For
example, if a QnA pair like Q1A1 existed in the previous version of the project, but after
refreshing, the updated source no longer generates the A1 answer, that pair is considered
outdated. As a result, Q1A1 is removed from the project entirely.
However, if the old QnA pairs are manually edited in the authoring portal, they aren't deleted.

Add new pair
If the URL has new content, and a new QnA pair appears in the old knowledge base, the new
pair is added. This addition ensures your knowledge base always includes the latest
information from the source. For example, if the service finds that a new answer A2 can be
generated, then the QnA pair Q2A2 is inserted into the KB.

Merge pairs
If the answer of a new QnA pair matches the answer of an old QnA pair, the two pairs are
merged. The new pair's question is added as an alternate question to the old QnA pair. For
example, consider Q3A3 exists in the old source. When you refresh the source, a new QnA pair
Q3'A3 is introduced. In that case, the two QnA pairs are merged: Q3' is added to Q3 as an
alternate question.
If the old QnA pair has a metadata value, that data is retained and persisted in the newly
merged pair.
If the old QnA pair has follow-up prompts associated with it, then the following scenarios may
arise:
If the prompt attached to the old pair is from the source being refreshed, that prompt is
deleted, and the prompt of the new pair (if any exists) is appended to the newly merged
QnA pair.
If the prompt attached to the old pair is from a different source, then that prompt is
maintained as-is. The prompt from the new question (if any exists) is appended to the
newly merged QnA pair.

Merge example
See the following example of a merge operation with differing questions and prompts:
ï¾‰

Expand table

Source
iteration

Question

Answer

Prompts

old

"What is the new HR
policy?"

"You might have to choose among the
following options:"

P1, P2

new

"What is the new payroll
policy?"

"You might have to choose among the
following options:"

P3, P4

The prompts P1 and P2 come from the original source and are different from prompts P3 and
P4 of the new QnA pair. They both have the same answer, You might have to choose among the
following options: , but it leads to different prompts. In this case, the resulting QnA pair would

look like this:
ï¾‰

Expand table

Question

Answer

Prompts

"What is the new HR policy?"

"You might have to choose among the

P3, P4

(Alternate question: "What is the new

following options:"

payroll policy?")

Duplicate answers scenario
When the original source has two or more QnA pairs with the same answer (as in, Q1A1 and
Q2A1), the merge behavior may be more complex.
If each QnA pair has its own prompt (like Q1A1 with P1 and Q2A1 with P2), the updated source
might make a new QnA pair with the same answer but a new prompt, such as Q1'A1 with P3. In
this case, the new question is added as an alternate to the originals. This process helps keep
the QnA pairs up to date with the latest source content. However, all of the original prompts
are replaced via the new prompt from the refreshed content. So the final pair set looks like this:
ï¾‰

Question

Answer

Prompts

Q1
(alternate question: Q1')

A1

P3

Expand table

Question

Answer

Prompts

Q2
(alternate question: Q1')

A1

P3

Next steps
Custom question answering quickstart
Update Sources API reference

Last updated on 12/13/2025

Custom question answering encryption of
data at rest
Custom question answering automatically encrypts your data when persisted to the cloud,
helping to meet your organizational security and compliance goals.

About encryption key management
By default, your subscription uses Microsoft-managed encryption keys. There's also the option
to manage your resource with your own keys called customer-managed keys (CMK). CMK
offers greater flexibility to create, rotate, disable, and revoke access controls. You can also audit
the encryption keys used to protect your data. If CMK is configured for your subscription,
double encryption is provided, which offers a second layer of protection, while allowing you to
control the encryption key through your Azure Key Vault.
Custom question answering uses CMK support from Azure search, and associates the provided
CMK to encrypt the data stored in Azure search index. Follow the steps listed in this article and
configure Key Vault access for the Azure search service.
ï¼— Note
Whenever the CMK is being rotated, ensure that there's a period of overlap between the
old and new versions of the key. During this time, both versions should be enabled and
not expired.

ï¼‰ Important
Your Azure Search service resource must be created after January 2019 and can't be in the
free (shared) tier. There's no support to configure customer-managed keys in the Azure
portal.

Enable customer-managed keys
Follow these steps for enabling CMKs:
1. Go to the Encryption tab of your language resource with custom question answering
enabled.
2. Select the Customer Managed Keys option. Provide the details of your customermanaged keys and select Save.

3. On a successful save, the CMK is used to encrypt the data stored in the Azure Search
Index.
ï¼‰ Important
We recommend that you set your CMK in a fresh Azure AI Search service before any
projects are created. If you set CMK in a language resource with existing projects, you
might lose access to them. Read more about working with encrypted content in Azure AI
Search.

Regional availability
Customer-managed keys are available in all Azure Search regions.

Encryption of data in transit
Every action triggers a direct call to the respective Foundry Tools API. Hence, custom question
answering is compliant for data in transit.

Next steps
Encryption in Azure Search using CMKs in Azure Key Vault
Data encryption at rest

Learn more about Azure Key Vault

Last updated on 12/15/2025

Move projects and question answer pairs
ï¼— Note

This article deals with the process to export and move projects and sources from one
Language resource to another.
You might want to create copies of your projects or sources for several reasons:
To implement a backup and restore process
Integrate with your CI/CD pipeline
When you wish to move your data to different regions

Prerequisites
If you don't have an Azure subscription, create a free account
A language resource

before you begin.

with the custom question answering feature enabled in the Azure

portal. Remember your Microsoft Entra ID, Subscription, and Azure Language resource
name you selected when you created the resource.

Export a project
Exporting a project allows you to back up all the question answer sources that are contained
within a single project.
1. Sign in to the Language Studio

.

2. Select Azure Language resource you want to move a project from.
3. Go to Custom Question Answering service. On the Projects page, you have the options to
export in two formats, Excel or TSV. This will determine the contents of the file. The file
itself will be exported as a .zip containing the contents of your project.
4. You can export only one project at a time.

Import a project
1. Select Azure Language resource, which will be the destination for your previously
exported project.
2. Go to Custom Question Answering service. On the Projects page, select Import and
choose the format used when you selected export. Then browse to the local .zip file

containing your exported project. Enter a name for your newly imported project and
select Done.

Export sources
1. Select the language resource you want to move an individual question answer source
from.
2. Select the project that contains the question and answer source you wish to export.
3. On the Edit project page, select the ellipsis ( ... ) icon to the right of Enable rich text in
the toolbar. You have the option to export in either Excel or TSV.

Import question and answers
1. Select the language resource, which will be the destination for your previously exported
question and answer source.
2. Select the project where you want to import a question and answer source.
3. On the Edit project page, select the ellipsis ( ... ) icon to the right of Enable rich text in
the toolbar. You have the option to import either an Excel or TSV file.
4. Browse to the local location of the file with the Choose File option and select Done.

Test
Test the question answer source by selecting the Test option from the toolbar in the Edit
project page which will launch the test panel. Learn how to test your project.

Deploy
Deploy the project and create a chat bot. Learn how to deploy your project.

Chat logs
There is no way to move chat logs with projects. If diagnostic logs are enabled, chat logs are
stored in the associated Azure Monitor resource.

Next steps
Last updated on 11/18/2025

Use chitchat with a project
Adding chitchat to your bot makes it more conversational and engaging. The chitchat feature
in custom question answering allows you to easily add a prepopulated set of the top chitchat,
into your project. This can be a starting point for your bot's personality, and it will save you the
time and cost of writing them from scratch.
This dataset has about 100 scenarios of chitchat in the voice of multiple personas, like
Professional, Friendly and Witty. Choose the persona that most closely resembles your bot's
voice. Given a user query, custom question answering tries to match it with the closest known
chitchat question and answer.
Some examples of the different personalities are below. You can see all the personality
datasets

along with details of the personalities.

For the user query of When is your birthday? , each personality has a styled response:

Personality

Example

Professional

Age doesn't really apply to me.

Friendly

I don't really have an age.

Witty

I'm age-free.

Caring

I don't have an age.

Enthusiastic

I'm a bot, so I don't have an age.

ï¾‰

Expand table

ï¾‰

Expand table

Language support
Chitchat data sets are supported in the following languages:

Language
Chinese
English
French

Language
Germany
Italian
Japanese
Korean
Portuguese
Spanish

Add chitchat source
After you create your project you can add sources from URLs, files, as well as chitchat from the
Manage sources pane.

Choose the personality that you want as your chitchat base.

Edit your chitchat questions and answers
When you edit your project, you can see a new source for chitchat, based on the personality
you selected. You can now add altered questions or edit the responses, just like with any other
source.

To turn the views for context and metadata on and off, select Show columns in the toolbar.

Add more chitchat questions and answers
You can add a new chitchat question pair that is not in the predefined data set. Ensure that
you're not duplicating a question pair that is already covered in the chitchat set. When you add
any new chitchat question pair, it gets added to your Editorial source. To ensure the ranker
understands that this is chitchat, add the metadata key/value pair "Editorial: chitchat," as seen
in the following image:

ï Š

Delete chitchat from your project
Select the manage sources pane, and choose your chitchat source. Your specific chitchat
source is listed as a tsv file, with the selected personality name. Select Delete from the toolbar.

Next steps
Last updated on 11/18/2025

Change default answer for custom question
answering
The default answer for a project is meant to be returned when an answer is not found. If you're
using a client application, such as the Azure AI Bot Service, it may also have a separate default
answer, indicating no answer met the score threshold.

Default answer
ï¾‰

Expand table

Default answer

Description of answer

KB answer when no
answer is
determined

No good match found in KB. - When the custom question answering API finds no

matching answer to the question it displays a default text response. In custom
question answering, you can set this text in the Settings of your project.

Client application integration
For a client application, such as a bot with the Azure AI Bot Service, you can choose from the
following scenarios:
Use your project's setting
Use different text in the client application to distinguish when an answer is returned but
doesn't meet the score threshold. This text can either be static text stored in code, or can
be stored in the client application's settings list.

Next steps
Create a project

Last updated on 12/13/2025

Configure your environment for Azure AI
resources
In this guide, we walk you through configuring your Azure AI resources and permissions for
custom question and answering projects, enabling you to fine-tune models with Azure AI
Search and Custom Question Answering (CQA). Completing this setup is essential for fully
integrating your environment with Foundry Tools resources. You only need to perform this
setup onceâ€”afterward, you have seamless access to advanced, AI-powered question
answering capabilities.
In addition, we show you how to assign the correct roles and permissions within the Azure
portal. These steps help you get started quickly and effectively with Azure Language in Foundry
Tools.

Prerequisites
Before you can set up your resources, you need:
An active Azure subscription. If you don't have one, you can create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
A Microsoft Foundry resource or a Language resource .
An Azure AI Search resource

(required for accessing CQA)

ï¼— Note
We highly recommend that you use a Foundry resource in the Foundry; however, you can
also follow these instructions using a Language resource.

Step 1: Assign the correct role to your Search
resource
Azure RBAC is an authorization system built on Azure Resource Manager. It provides finegrained access management to Azure resources.
1. Navigate to the Azure portal .

2. Go to your Azure Search resource. (See All resources to find your Search resource.)
3. In the Settings section on the left panel, select Keys.
4. Make sure the API Access control button is set to API keys.
Screenshot of API access control selector in the Azure portal.
5. Next, select Access Control (IAM) on the left panel, then select Add role assignment.

6. Search and select the Azure AI Administrator role. Select Next.

ï Š

7. Navigate to the Members tab and then select Assign access to User, group, or service
principal.

8. Select Select members, then add your account name, and choose Select.
9. Finally, select Review + assign to confirm your selection.

Step 2: Configure connections in Foundry
Foundry offers a unified platform where you can easily build, manage, and deploy AI solutions
using a wide range of models and tools. Connections enable authentication and access to both
Microsoft and external resources within your Foundry projects.
1. Sign into Foundry

using your account and required subscription. Then, select the

project containing your Foundry resource.
2. Next, navigate to the Management Center in the bottom left corner of the page.
3. Scroll to the Connected resources section of the Management center.
Screenshot of the management center selector in the Foundry.
4. Select the + New connection button.

5. In the new window, select Azure AI Search as the resource type.
6. Search for and display the Azure AI Search resource you connected in Step 1.
7. Ensure the Authentication is set to API key.
8. Select Add connection then select Close.

Screenshot of connect search resource selector in the Foundry.

Step 3: Create a fine-tuning task with connected
resources
1. Navigate to Go to project at the end of the left navigation panel.
Screenshot the go-to-project button in the Foundry.
2. Select Fine-tuning from the left navigation panel, choose the AI Service fine-tuning tab,
and then select the + Fine-tune button.

3. Choose Custom question answering as the task type from the new window, then select
Next.
4. Under Connected service, select your selected Foundry resource. Then select your newly
connected search resource.
5. Your resources are now set up properly. Continue with setting up the fine-tuning task and
customizing your CQA project.

Change Azure AI Search resource
ï¼’ Warning
If you change the Azure Search service associated with your language resource, you lose
access to all the projects already present in it. Make sure you export the existing projects
before you change the Azure Search service.
If you create a language resource and its dependencies (such as Search) through the Azure
portal, a Search service is created for you and linked to the language resource. After these
resources are created, you can update the Search resource in the Features tab.

1. Go to your language resource in the Azure portal.
2. Select Features and select the Azure AI Search service you want to link with your
language resource.
ï¼— Note
Your Language resource retains your Azure AI Search keys. If you update your search
resource (for example, regenerating your keys), you need to select Update Azure AI
Search keys for the current search service.

3. Select Save.

Next Steps
Create, test, and deploy a custom question answering project

Last updated on 11/18/2025

Get analytics for your project
Custom question answering uses Azure diagnostic logging to store the telemetry data and chat
logs. Follow the below steps to run sample queries to get analytics on the usage of your
custom question answering project.
1. Enable diagnostics logging for your language resource with custom question answering
enabled.
2. In the previous step, select Trace in addition to Audit, RequestResponse and AllMetrics
for logging

Kusto queries
Chat log
Kusto
// All QnA Traffic
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where OperationName=="CustomQuestionAnswering QueryKnowledgebases" // This
OperationName is valid for custom question answering enabled resources
| extend answer_ = tostring(parse_json(properties_s).answer)
| extend question_ = tostring(parse_json(properties_s).question)
| extend score_ = tostring(parse_json(properties_s).score)

| extend kbId_ = tostring(parse_json(properties_s).kbId)
| project question_, answer_, score_, kbId_

Traffic count per project and user in a time period
Kusto
// Traffic count per KB and user in a time period
let startDate = todatetime('2019-01-01');
let endDate = todatetime('2020-12-31');
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where OperationName=="CustomQuestionAnswering QueryKnowledgebases" // This
OperationName is valid for custom question answering enabled resources
| where TimeGenerated <= endDate and TimeGenerated >=startDate
| extend kbId_ = tostring(parse_json(properties_s).kbId)
| extend userId_ = tostring(parse_json(properties_s).userId)
| summarize ChatCount=count() by bin(TimeGenerated, 1d), kbId_, userId_

Latency of GenerateAnswer API
Kusto
// Latency of GenerateAnswer
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where OperationName=="Generate Answer"
| project TimeGenerated, DurationMs
| render timechart

Average latency of all operations
Kusto
// Average Latency of all operations
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| project DurationMs, OperationName
| summarize count(), avg(DurationMs) by OperationName
| render barchart

Unanswered questions
Kusto

// All unanswered questions
AzureDiagnostics
| where ResourceProvider == "MICROSOFT.COGNITIVESERVICES"
| where OperationName=="CustomQuestionAnswering QueryKnowledgebases" // This
OperationName is valid for custom question answering enabled resources
| extend answer_ = tostring(parse_json(properties_s).answer)
| extend question_ = tostring(parse_json(properties_s).question)
| extend score_ = tostring(parse_json(properties_s).score)
| extend kbId_ = tostring(parse_json(properties_s).kbId)
| where score_ == 0
| project question_, answer_, score_, kbId_

Prebuilt custom question answering inference calls
Kusto
// Show logs from AzureDiagnostics table
// Lists the latest logs in AzureDiagnostics table, sorted by time (latest first).
AzureDiagnostics
| where OperationName == "CustomQuestionAnswering QueryText"
| extend answer_ = tostring(parse_json(properties_s).answer)
| extend question_ = tostring(parse_json(properties_s).question)
| extend score_ = tostring(parse_json(properties_s).score)
| extend requestid = tostring(parse_json(properties_s)["apim-request-id"])
| project TimeGenerated, requestid, question_, answer_, score_

Next steps
Last updated on 11/18/2025

Create and manage project settings
Custom question answering allows you to manage your projects by providing access to the
project settings and data sources. If you haven't created a custom question answering project
before we recommend starting with the getting started article.

Prerequisites
If you don't have an Azure subscription, create a free account
A Language resource

before you begin.

with the custom question answering feature enabled in the

Azure portal. Remember your Microsoft Entra ID, Subscription, and language resource
name you selected when you created the resource.

Create a project
1. Sign in to the Language Studio

portal with your Azure credentials.

2. Open the question answering

page.

3. Select create new project.
4. If you're creating the first project associated with your language resource, you have the
option of creating future projects with multiple languages for the same resource. If you
choose to explicitly set the language to a single language in your first project, you will not
be able to modify this setting later and all subsequent projects for that resource will use
the language selected during the creation of your first project.
Screenshot of language selection UI.
5. Enter basic project settings:
ï¾‰

Expand table

Setting

Value

Name

Enter your unique project name here

Description

Enter a description for your project

Source
language

Whether or not this value is greyed out, is dependent on the selection that was
made when the first project associated with the language resource was created.

Default
answer

The default answer the system will send if there was no answer found for the
question. You can change this at any time in Project settings.

Manage projects
From the main custom question answering page in Language Studio you can:
Create projects
Delete projects
Export existing projects for backup or to migrate to other language resources
Import projects. (The expected file format is a .zip file containing a project that was
exported in excel or .tsv format).
Projects can be ordered by either Last modified or Last published date.

Manage sources
1. Select Manage sources in the left pane.
2. There are three types of sources: URLS, Files, and Chitchat
ï¾‰

Expand table

Goal

Action

Add Source

You can add new sources and FAQ content to your project by selecting Add
source > and choosing URLs, Files, or Chitchat

Delete Source

You can delete existing sources by selecting to the left of the source, which
will cause a blue circle with a checkmark to appear > select the trash can
icon.

Mark content as

If you want to mark the uploaded file content as unstructured select

unstructured

Unstructured content from the dropdown when adding the source.

Auto-detect

Allow question and answering to attempt to determine if content is
structured versus unstructured.

Manage large projects
From the Edit project page you can:
Search project: You can search the project by typing in the text box at the top of question
answer panel. Hit enter to search on the question, answer, or metadata content.
Pagination: Quickly move through data sources to manage large projects. Page numbers
appear at the bottom of the UI and are sometimes off screen.

Delete project
Deleting a project is a permanent operation. It can't be undone. Before deleting a project, you
should export the project from the main custom question answering page within Language
Studio.
If you share your project with collaborators and then later delete it, everyone loses access to
the project.

Next steps
Configure resources

Last updated on 11/18/2025

Network isolation and private endpoints
The following steps describe how to restrict public access to custom question answering
resources as well as how to enable Azure Private Link. Protect a Foundry resource from public
access by configuring the virtual network.

Private Endpoints
Azure Private Endpoint is a network interface that connects you privately and securely to a
service powered by Azure Private Link. Custom question answering provides you support to
create private endpoints to the Azure Search Service.
Private endpoints are provided by Azure Private Link, as a separate service. For more
information about costs, see the pricing page.

Steps to enable private endpoint
1. Assign the contributor role to your resource in the Azure Search Service instance. This
operation requires Owner access to the subscription. Go to Identity tab in the service
resource to get the identity.

2. Add the above identity as Contributor by going to the Azure Search Service access control
tab.

3. Select on Add role assignments, add the identity and select Save.

4. Now, go to Networking tab in the Azure Search Service instance and switch Endpoint
connectivity data from Public to Private. This operation is a long running process and can
take up to 30 mins to complete.

5. Go to Networking tab of language resource and under the Allow access from, select the
Selected Networks and private endpoints option and select save.

This will establish a private endpoint connection between language resource and Azure AI
Search service instance. You can verify the Private endpoint connection on the Networking tab
of the Azure AI Search service instance. Once the whole operation is completed, you're good to
use your language resource with question answering enabled.

Support details
We don't support changes to Azure AI Search service once you enable private access to
your language resources. If you change the Azure AI Search service via 'Features' tab after
you have enabled private access, the language resource will become unusable.
After establishing Private Endpoint Connection, if you switch Azure AI Search Service
Networking to 'Public', you won't be able to use the language resource. Azure Search
Service Networking needs to be 'Private' for the Private Endpoint Connection to work.

Restrict access to Azure AI Search resource
Follow these steps to restrict public access to custom question answering language resources.
Protect a Foundry resource from public access by configuring the virtual network.

ï Š

Last updated on 12/13/2025

Authoring API
The custom question answering Authoring API is used to automate common tasks like adding
new question answer pairs, and creating, publishing, and maintaining projects.
ï¼— Note
Authoring functionality is available via the REST API and Authoring SDK (preview). This
article provides examples of using the REST API with cURL. For full documentation of all
parameters and functionality available, consult the REST API reference content.

Prerequisites
The current version of cURL . Several command-line switches are used in this article,
which are noted in the cURL documentation

.

The commands in this article are executed in a Bash shell. These commands don't always
work in a Windows command prompt or in PowerShell without modification. If you don't
have a Bash shell installed locally, you can use the Azure Cloud Shell's bash environment.

Create a project
To create a project programmatically:
ï¾‰

Variable

Expand table

Value

name
ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ . If the prior example was your

endpoint in the following code sample, you would only need to add the region specific
portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

NEWPROJECTNAME

The name for your new custom question answering project.

You can also adjust more values, such as the project language. Another option is to set the
default answer that is provided when no response meets or exceeds the confidence threshold.
In addition, you can specify whether this language resource supports multiple languages.

Example query
Bash
curl -X PATCH -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '{
"description": "proj1 is a test project.",
"language": "en",
"settings": {
"defaultAnswer": "No good match found for your question in the project."
},
"multilingualResource": true
}
}' 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{NEW-PROJECT-NAME}?api-version=2021-10-01'

Example response
JSON
{
"200": {
"headers": {},
"body": {
"projectName": "proj1",
"description": "proj1 is a test project.",
"language": "en",
"settings": {
"defaultAnswer": "No good match found for your question in the project."
},
"multilingualResource": true,
"createdDateTime": "2021-05-01T15:13:22Z",
"lastModifiedDateTime": "2021-05-01T15:13:22Z",
"lastDeployedDateTime": "2021-05-01T15:13:22Z"
}
}
}

Delete Project
To delete a project programmatically:

ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ . If the prior example was your

endpoint in the following code sample, you would only need to add the region specific
portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to delete.

NAME

Example query
Bash
curl -X DELETE -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -i 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}?api-version=2021-10-01'

A successful call to delete a project results in an Operation-Location header being returned,
which can be used to check the status of the deleted project job. In most our examples, we
don't view the response headers. To retrieve the response headers our curl command uses -i .
Without this parameter preceding the endpoint address, the response to this command would
appear empty as if no response occurred.

Example response
Bash
HTTP/2 202
content-length: 0
operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/deletion-jobs/{JOB-ID-GUID}
x-envoy-upstream-service-time: 324
apim-request-id:
strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Tue, 23 Nov 2021 20:56:18 GMT

If the project was already deleted or couldn't be found, you would receive a message like:
JSON
{
"error": {
"code": "ProjectNotFound",
"message": "The specified project was not found.",
"details": [
{
"code": "ProjectNotFound",
"message": "{GUID}"
}
]
}
}

Get project deletion status
To check on the status of your delete project request:
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to check on the deployment status for.

NAME
JOB-ID

When you delete a project programmatically, a JOB-ID is generated as part of the operationlocation response header to the deletion request. The JOB-ID is the guid at the end of the
operation-location . For example: operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/deletion-jobs/{THIS GUID IS YOUR JOB ID}

Example query
Bash

curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/deletion-jobs/{JOB-ID}?api-version=2021-10-01'

Example response
JSON
{
"createdDateTime": "2021-11-23T20:56:18+00:00",
"expirationDateTime": "2021-11-24T02:56:18+00:00",
"jobId": "GUID",
"lastUpdatedDateTime": "2021-11-23T20:56:18+00:00",
"status": "succeeded"
}

Get project settings
To retrieve information about a given project, update the following values in the query:
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region
specific portion of southcentral . The rest of the endpoint path is already present.

API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to retrieve information about.

NAME

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}?api-version=2021-10-01'

Example response
JSON
{
"200": {
"headers": {},
"body": {
"projectName": "proj1",
"description": "proj1 is a test project.",
"language": "en",
"settings": {
"defaultAnswer": "No good match found for your question in the project."
},
"createdDateTime": "2021-05-01T15:13:22Z",
"lastModifiedDateTime": "2021-05-01T15:13:22Z",
"lastDeployedDateTime": "2021-05-01T15:13:22Z"
}
}
}

Get question answer pairs
To retrieve question answer pairs and related information for a given project, update the
following values in the query:
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region
specific portion of southcentral . The rest of the endpoint path is already present.

API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to retrieve all the question answer pairs for.

NAME

Example query
Bash

curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/qnas?api-version=2021-10-01'

Example response
JSON
{
"200": {
"headers": {},
"body": {
"value": [
{
"id": 1,
"answer": "ans1",
"source": "source1",
"questions": [
"question 1.1",
"question 1.2"
],
"metadata": {
"k1": "v1",
"k2": "v2"
},
"dialog": {
"isContextOnly": false,
"prompts": [
{
"displayOrder": 1,
"qnaId": 11,
"displayText": "prompt 1.1"
},
{
"displayOrder": 2,
"qnaId": 21,
"displayText": "prompt 1.2"
}
]
},
"lastUpdatedDateTime": "2021-05-01T17:21:14Z"
},
{
"id": 2,
"answer": "ans2",
"source": "source2",
"questions": [
"question 2.1",
"question 2.2"
],
"lastUpdatedDateTime": "2021-05-01T17:21:14Z"

}
]
}
}
}

Get sources
To retrieve the sources and related information for a given project, update the following values
in the query:
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region
specific portion of southcentral . The rest of the endpoint path is already present.

API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to retrieve all the source information for.

NAME

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT_NAME}/sources?api-version=2021-10-01'

Example response
JSON
{
"200": {
"headers": {},
"body": {
"value": [
{

"displayName": "source1",
"sourceUri": "https://learn.microsoft.com/azure/aiservices/qnamaker/overview/overview",
"sourceKind": "url",
"lastUpdatedDateTime": "2021-05-01T15:13:22Z"
},
{
"displayName": "source2",
"sourceUri": "https://download.microsoft.com/download/2/9/B/29B20383302C-4517-A006-B0186F04BE28/surface-pro-4-user-guide-EN.pdf",
"sourceKind": "file",
"contentStructureKind": "unstructured",
"lastUpdatedDateTime": "2021-05-01T15:13:22Z"
}
]
}
}
}

Get synonyms
To retrieve synonyms and related information for a given project, update the following values
in the query:
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to retrieve synonym information for.

NAME

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:

application/json" 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/synonyms?api-version=2021-10-01'

Example response
JSON
{
"200": {
"headers": {},
"body": {
"value": [
{
"alterations": [
"qnamaker",
"qna maker"
]
},
{
"alterations": [
"botframework",
"bot framework"
]
}
]
}
}
}

Deploy project
To deploy a project to production, update the following values in the query:
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

Variable
name

Value

PROJECT-

The name of project you would like to deploy to production.

NAME

Example query
Bash
curl -X PUT -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '' -i
'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/deployments/production?api-version=2021-1001'

A successful call to deploy a project results in an Operation-Location header being returned
which can be used to check the status of the deployment job. In most our examples, we don't
view the response headers. To retrieve the response headers our curl command uses -i .
Without this parameter preceding the endpoint address, the response to this command would
appear empty as if no response occurred.

Example response
Bash
0HTTP/2 202
content-length: 0
operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/deployments/production/jobs/{JOB-ID-GUID}
x-envoy-upstream-service-time: 31
apim-request-id:
strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Tue, 23 Nov 2021 20:35:00 GMT

Get project deployment status
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to check on the deployment status for.

NAME
JOB-ID

When you deploy a project programmatically, a JOB-ID is generated as part of the operationlocation response header to the deployment request. The JOB-ID is the guid at the end of

the operation-location . For example: operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/deployments/production/jobs/{THIS GUID IS YOUR JOB
ID}

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d ''
'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/deployments/production/jobs/{JOB-ID}?apiversion=2021-10-01'

Example response
JSON
{
"200": {
"headers": {},
"body": {
"errors": [],
"createdDateTime": "2021-05-01T17:21:14Z",
"expirationDateTime": "2021-05-01T17:21:14Z",
"jobId": "{JOB-ID-GUID}",
"lastUpdatedDateTime": "2021-05-01T17:21:14Z",
"status": "succeeded"
}

}
}

Export project metadata and assets
ï¾‰

Variable

Expand table

Value

name
ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to export.

NAME

Example query
Bash
curl -X POST -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '{exportAssetTypes": ["qnas","synonyms"]}' -i
'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/:export?api-version=2021-10-01&format=tsv'

Example response
Bash
HTTP/2 202
content-length: 0
operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/Sample-project/export/jobs/{JOB-ID_GUID}
x-envoy-upstream-service-time: 214
apim-request-id:
strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Tue, 23 Nov 2021 21:24:03 GMT

Check export status
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to check on the export status for.

NAME
JOB-ID

When you export a project programmatically, a JOB-ID is generated as part of the operationlocation response header to the export request. The JOB-ID is the guid at the end of the
operation-location . For example: operation-location:
https://southcentralus.cognitiveservices.azure.com/language/queryknowledgebases/projects/sample-proj1/export/jobs/{THIS GUID IS YOUR JOB ID}

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d ''
'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/sample-proj1/export/jobs/{JOB-ID}?api-version=2021-10-01'

Example response
JSON
{
"createdDateTime": "2021-11-23T21:24:03+00:00",
"expirationDateTime": "2021-11-24T03:24:03+00:00",
"jobId": "JOB-ID-GUID",
"lastUpdatedDateTime": "2021-11-23T21:24:08+00:00",
"status": "succeeded",
"resultUrl":
"https://southcentralus.cognitiveservices.azure.com:443/language/query-

knowledgebases/projects/sample-proj1/export/jobs/{JOB-ID_GUID}/result"
}

If you try to access the resultUrl directly, you get a 404 error. You must append ?apiversion=2021-10-01 to the path to make it accessible by an authenticated request:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/export/jobs/{JOB-ID_GUID}/result?api-version=202110-01

Import project
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you would only need to add the

region specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to be the destination for the import.

NAME
FILE-

When you export a project programmatically, and then check the status the export a

URI-PATH

resultUrl is generated as part of the response. For example: "resultUrl":
"https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/export/jobs/{JOB-ID_GUID}/result" you can use the

resultUrl with the API version appended as a source file to import a project from:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/export/jobs/{JOB-ID_GUID}/result?api-version=202110-01 .

Example query
Bash
curl -X POST -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '{
"fileUri": "FILE-URI-PATH"

}' -i 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/:import?api-version=2021-10-01&format=tsv'

A successful call to import a project results in an Operation-Location header being returned,
which can be used to check the status of the import job. In many of our examples, we view the
response headers. To retrieve the response headers our curl command uses -i . Without this
added parameter preceding the endpoint address, the response to this command would
appear empty as if no response occurred.

Example response
Bash
HTTP/2 202
content-length: 0
operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/sample-proj1/import/jobs/{JOB-ID-GUID}
x-envoy-upstream-service-time: 417
apim-request-id:
strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Wed, 24 Nov 2021 00:35:11 GMT

Check import status
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to be the destination for the import.

NAME
JOB-ID

When you import a project programmatically, a JOB-ID is generated as part of the operationlocation response header to the export request. The JOB-ID is the GUID at the end of the
operation-location . For example: operation-location:

Variable

Value

name
https://southcentralus.cognitiveservices.azure.com/language/queryknowledgebases/projects/sample-proj1/import/jobs/{THIS GUID IS YOUR JOB ID}

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d ''
'https://southcentralus.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME/import/jobs/{JOB-ID-GUID}?api-version=2021-1001'

Example query response
Bash
{
"errors": [],
"createdDateTime": "2021-05-01T17:21:14Z",
"expirationDateTime": "2021-05-01T17:21:14Z",
"jobId": "JOB-ID-GUID",
"lastUpdatedDateTime": "2021-05-01T17:21:14Z",
"status": "succeeded"
}

List deployments
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

Variable
name

Value

PROJECT-

The name of project you would like to generate a deployment list for.

NAME

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d ''
'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/deployments?api-version=2021-10-01'

Example response
JSON
[
{
"deploymentName": "production",
"lastDeployedDateTime": "2021-10-26T15:12:02Z"
}
]

List Projects
Retrieve a list of all question answering projects your account has access to.
ï¾‰

Variable

Expand table

Value

name
ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d ''
'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects?api-version=2021-10-01'

Example response
JSON
{
"value": [
{
"projectName": "Sample-project",
"description": "My first question answering project",
"language": "en",
"multilingualResource": false,
"createdDateTime": "2021-10-07T04:51:15Z",
"lastModifiedDateTime": "2021-10-27T00:42:01Z",
"lastDeployedDateTime": "2021-11-24T01:34:18Z",
"settings": {
"defaultAnswer": "No good match found in KB"
}
}
]
}

Update sources
In this example, we add a new source to an existing project. You can also replace and delete
existing sources with this command depending on what kind of operations you pass as part of
your query body.
ï¾‰

Variable

Expand table

Value

name
ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.

Variable

Value

name
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project where you would like to update sources.

NAME
METHOD

PATCH

Example query
Bash
curl -X PATCH -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '[
{
"op": "add",
"value": {
"displayName": "source5",
"sourceKind": "url",
"sourceUri": "https://download.microsoft.com/download/7/B/1/7B10C82E-F5204080-8516-5CF0D803EEE0/surface-book-user-guide-EN.pdf",
"sourceContentStructureKind": "semistructured"
}
}
]' -i '{LanguageServiceName}.cognitiveservices.azure.com//language/queryknowledgebases/projects/{projectName}/sources?api-version=2021-10-01'

A successful call to update a source results in an Operation-Location header being returned
which can be used to check the status of the import job. In many of our examples, we don't
view the response headers. To retrieve the response headers our curl command uses -i .
Without this parameter preceding the endpoint address, the response to this command would
appear empty as if no response occurred.

Example response
Bash
HTTP/2 202
content-length: 0
operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/Sample-project/sources/jobs/{JOB_ID_GUID}
x-envoy-upstream-service-time: 412
apim-request-id: dda23d2b-f110-4645-8bce-1a6f8d504b33

strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Wed, 24 Nov 2021 02:47:53 GMT

Get update source status
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to be the destination for the import.

NAME
JOB-ID

When you update a source programmatically, a JOB-ID is generated as part of the operationlocation response header to the update source request. The JOB-ID is the GUID at the end of

the operation-location . For example: operation-location:
https://southcentralus.cognitiveservices.azure.com/language/queryknowledgebases/projects/sample-proj1/sources/jobs/{THIS GUID IS YOUR JOB ID}

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d ''
'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/sources/jobs/{JOB-ID}?api-version=2021-10-01'

Example response
Bash
{
"createdDateTime": "2021-11-24T02:47:53+00:00",
"expirationDateTime": "2021-11-24T08:47:53+00:00",
"jobId": "{JOB-ID-GUID}",

"lastUpdatedDateTime": "2021-11-24T02:47:56+00:00",
"status": "succeeded",
"resultUrl": "/knowledgebases/Sample-project"
}

Update question and answer pairs
In this example, we add a question answer pair to an existing source. You can also modify, or
delete existing question answer pairs with this query depending on what operation you pass in
the query body. If you don't have a source named source5 , this example query fails. You can
adjust the source value in the body of the query to a source that exists for your target project.
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to be the destination for the import.

NAME

Bash
curl -X PATCH -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '[
{
"op": "add",
"value":{
"id": 1,
"answer": "The latest question answering docs are on
https://learn.microsoft.com",
"source": "source5",
"questions": [
"Where do I find docs for question answering?"
],
"metadata": {},
"dialog": {
"isContextOnly": false,
"prompts": []

}
}
}
]' -i 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/qnas?api-version=2021-10-01'

A successful call to update a question answer pair results in an Operation-Location header
being returned which can be used to check the status of the update job. In many of our
examples, we don't view the response headers. To retrieve the response headers our curl
command uses -i . Without this parameter preceding the endpoint address, the response to
this command would appear empty as if no response occurred.

Example response
Bash
HTTP/2 202
content-length: 0
operation-location:
https://southcentralus.cognitiveservices.azure.com:443/language/queryknowledgebases/projects/Sample-project/qnas/jobs/{JOB-ID-GUID}
x-envoy-upstream-service-time: 507
apim-request-id:
strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Wed, 24 Nov 2021 03:16:01 GMT

Get update question answer pairs status
ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region
specific portion of southcentral . The rest of the endpoint path is already present.

API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to be the destination for the question answer pairs
updates.

NAME

Variable

Value

name
JOB-ID

When you update a question answer pair programmatically, a JOB-ID is generated as part of
the operation-location response header to the update request. The JOB-ID is the GUID at
the end of the operation-location . For example: operation-location:
https://southcentralus.cognitiveservices.azure.com/language/queryknowledgebases/projects/sample-proj1/qnas/jobs/{THIS GUID IS YOUR JOB ID}

Example query
Bash
curl -X GET -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d ''
'https://southcentralus.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/qnas/jobs/{JOB-ID}?api-version=2021-10-01'

Example response
Bash
"createdDateTime": "2021-11-24T03:16:01+00:00",
"expirationDateTime": "2021-11-24T09:16:01+00:00",
"jobId": "{JOB-ID-GUID}",
"lastUpdatedDateTime": "2021-11-24T03:16:06+00:00",
"status": "succeeded",
"resultUrl": "/knowledgebases/Sample-project"

Update Synonyms
ï¾‰

Variable

Expand table

Value

name
ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region

specific portion of southcentral . The rest of the endpoint path is already present.
API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

Variable
name

Value

PROJECT-

The name of project you would like to add synonyms.

NAME

Example query
Bash
curl -X PUT -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '{
"value": [
{
"alterations": [
"qnamaker",
"qna maker"
]
},
{
"alterations": [
"botframework",
"bot framework"
]
}
]
}' -i 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/synonyms?api-version=2021-10-01'

Example response
Bash
0HTTP/2 200
content-length: 17
content-type: application/json; charset=utf-8
x-envoy-upstream-service-time: 39
apim-request-id: 5deb2692-dac8-43a8-82fe-36476e407ef6
strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Wed, 24 Nov 2021 03:59:09 GMT
{
"value": []
}

Update active learning feedback

ï¾‰

Expand table

Variable
name

Value

ENDPOINT

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. An example endpoint is:
https://southcentralus.cognitiveservices.azure.com/ and you only need to add the region
specific portion of southcentral . The rest of the endpoint path is already present.

API-KEY

This value can be found in the Keys & Endpoint section when examining your resource from
the Azure portal. You can use either Key1 or Key2. Always having two valid keys allows for
secure key rotation with zero downtime. The key value is part of the sample request.

PROJECT-

The name of project you would like to be the destination for the active learning feedback
updates.

NAME

Example query
Bash
curl -X POST -H "Ocp-Apim-Subscription-Key: {API-KEY}" -H "Content-Type:
application/json" -d '{
records": [
{
"userId": "user1",
"userQuestion": "hi",
"qnaId": 1
},
{
"userId": "user1",
"userQuestion": "hello",
"qnaId": 2
}
]
}' -i 'https://{ENDPOINT}.cognitiveservices.azure.com/language/queryknowledgebases/projects/{PROJECT-NAME}/feedback?api-version=2021-10-01'

Example response
Bash
HTTP/2 204
x-envoy-upstream-service-time: 37
apim-request-id: 92225e03-e83f-4c7f-b35a-223b1b0f29dd
strict-transport-security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
date: Wed, 24 Nov 2021 04:02:56 GMT

Last updated on 12/13/2025

Prebuilt API
The custom question answering prebuilt API provides you with the capability to answer
questions based on a passage of text without having to create projects, maintain question and
answer pairs, or incurring costs for underutilized infrastructure. This functionality is provided as
an API and can be used to meet question and answering needs without having to learn the
details about custom question answering.
Given a user query and a block of text/passage the API returns an answer and precise answer (if
available).

Example API usage
Imagine that you have one or more blocks of text from which you would like to get answers for
a given question. Normally you have to create as many sources as the number of blocks of text.
However, now with the prebuilt API you can query the blocks of text without having to define
content sources in a project.
Some other scenarios where this API can be used are:
You're developing an ebook reader app for end users, which allows them to highlight text,
enter a question and find answers over a highlighted passage of text.
A browser extension that allows users to ask a question over the content being currently
displayed on the browser page.
A health bot that takes queries from users and provides answers based on the medical
content that the bot identifies as most relevant to the user query.
Here's an example of a sample request:

Sample request
POST https://{Unique-to-your-endpoint}.cognitiveservices.azure.com/language/:querytext

Sample query over a single block of text
Request Body
JSON

{
"parameters": {
"Endpoint": "{Endpoint}",
"Ocp-Apim-Subscription-Key": "{API key}",
"Content-Type": "application/json",
"api-version": "2021-10-01",
"stringIndexType": "TextElements_v8",
"textQueryOptions": {
"question": "how long it takes to charge surface?",
"records": [
{
"id": "1",
"text": "Power and charging. It takes two to four hours to charge the
Surface Pro 4 battery fully from an empty state. It can take longer if you're using
your Surface for power-intensive activities like gaming or video streaming while
you're charging it."
},
{
"id": "2",
"text": "You can use the USB port on your Surface Pro 4 power supply to
charge other devices, like a phone, while your Surface charges. The USB port on the
power supply is only for charging, not for data transfer. If you want to use a USB
device, plug it into the USB port on your Surface."
}
],
"language": "en"
}
}
}

Sample response
In the request body, we query over a single block of text. Here's a sample response received for
the query:
JSON
{
"responses": {
"200": {
"headers": {},
"body": {
"answers": [
{
"answer": "Power and charging. It takes two to four hours to charge the
Surface Pro 4 battery fully from an empty state. It can take longer if you're using
your Surface for power-intensive activities like gaming or video streaming while
you're charging it.",
"confidenceScore": 0.93,
"id": "1",
"answerSpan": {

"text": "two to four hours",
"confidenceScore": 0,
"offset": 28,
"length": 45
},
"offset": 0,
"length": 224
},
{
"answer": "It takes two to four hours to charge the Surface Pro 4
battery fully from an empty state. It can take longer if you're using your Surface
for power-intensive activities like gaming or video streaming while you're charging
it.",
"confidenceScore": 0.92,
"id": "1",
"answerSpan": {
"text": "two to four hours",
"confidenceScore": 0,
"offset": 8,
"length": 25
},
"offset": 20,
"length": 224
},
{
"answer": "It can take longer if you're using your Surface for powerintensive activities like gaming or video streaming while you're charging it.",
"confidenceScore": 0.05,
"id": "1",
"answerSpan": null,
"offset": 110,
"length": 244
}
]
}
}
}

We see that multiple answers are received as part of the API response. Each answer has a
specific confidence score that helps understand the overall relevance of the answer. Answer
span represents whether a potential short answer was also detected. Users can make use of
this confidence score to determine which answers to provide in response to the query.

Prebuilt API limits
API call limits
If you need to use larger documents than the limit allows, you can break the text into smaller
chunks of text before sending them to the API. In this context, a document is a defined single

string of text characters.
These numbers represent the per individual API call limits:
Number of documents: 5.
Maximum size of a single document: 5,120 characters.
Maximum three responses per document.

Language codes supported
The following language codes support the Prebuilt API. These language codes are in
accordance to the ISO 639-1 codes standard

.
ï¾‰

Language code

Language

af

Afrikaans

am

Amharic

ar

Arabic

as

Assamese

az

Azerbaijani

ba

Bashkir

be

Belarusian

bg

Bulgarian

bn

Bengali

ca

Catalan

ckb

Central Kurdish

cs

Czech

cy

Welsh

da

Danish

de

German

el

Greek, Modern (1453â€“)

Expand table

Language code

Language

en

English

eo

Esperanto

es

Spanish, Castilian

et

Estonian

eu

Basque

fa

Persian

fi

Finnish

fr

French

ga

Irish

gl

Galician

gu

Gujarati

he

Hebrew

hi

Hindi

hr

Croatian

hu

Hungarian

hy

Armenian

id

Indonesian

is

Icelandic

it

Italian

ja

Japanese

ka

Georgian

kk

Kazakh

km

Central Khmer

kn

Kannada

ko

Korean

ky

Kirghiz, Kyrgyz

Language code

Language

la

Latin

lo

Lao

lt

Lithuanian

lv

Latvian

mk

Macedonian

ml

Malayalam

mn

Mongolian

mr

Marathi

ms

Malay

mt

Maltese

my

Burmese

ne

Nepali

nl

Dutch, Flemish

nn

Norwegian Nynorsk

no

Norwegian

or

Odia

pa

Punjabi, Punjabi

pl

Polish

ps

Pashto, Pushto

pt

Portuguese

ro

Romanian

ru

Russian

sa

Sanskrit

sd

Sindhi

si

Sinhala, Singhalese

sk

Slovak

Language code

Language

sl

Slovenian

sq

Albanian

sr

Serbian

sv

Swedish

sw

Swahili

ta

Tamil

te

Telugu

tg

Tajik

th

Thai

tl

Tagalog

tr

Turkish

tt

Tatar

ug

Uighur, Uyghur

uk

Ukrainian

ur

Urdu

uz

Uzbek

vi

Vietnamese

yi

Yiddish

zh

Chinese

Prebuilt API reference
Visit the full prebuilt API samples

documentation to understand the input and output

parameters required for calling the API.

Last updated on 11/18/2025

Project best practices
The following list of QnA pairs are used to represent a project to highlight best practices when
authoring in custom question answering.
ï¾‰

Expand table

Question

Answer

I want to buy a car.

There are three options for buying a car.

I want to purchase software license.

Software licenses can be purchased online at no cost.

How to get access to WPA?

WPA can be accessed via the company portal.

What is the price of Microsoft stock?

$200.

How do I buy Microsoft Services?

Microsoft services can be bought online.

I want to sell car.

Please send car pictures and documents.

How do I get an identification card?

Apply via company portal to get an identification card.

How do I use WPA?

WPA is easy to use with the provided manual.

What is the utility of WPA?

WPA provides a secure way to access company resources.

When should you add alternate questions to a
QnA?
Custom question answering employs a transformer-based ranker that takes care of user
queries that are semantically similar to questions in the project. For example, consider the
following question answer pair:
Question: "What is the price of Microsoft Stock?"
Answer: "$200".
The service can return expected responses for semantically similar queries such as:
"How much is Microsoft stock worth?"
"How much is Microsoft's share value?"
"How much does a Microsoft share cost?"
"What is the market value of Microsoft stock?"

"What is the market value of a Microsoft share?"
The system's confidence score depends on the input query and how closely it matches
the original question-answer pair. Greater differences between them can lead to changes
in the confidence level.
There are certain scenarios that require the customer to add an alternate question. When
a query doesn't return the correct answer despite it being present in the project, we
advise adding that query as an alternate question to the intended QnA pair.

How many alternate questions per QnA is optimal?
Users can add up to 10 alternate questions depending on their scenario. Alternate
questions beyond the first 10 aren't considered via our core ranker. However, they're
evaluated in the other processing layers resulting in better output overall. All the alternate
questions are considered in the preprocessing step to look for an exact match.
Semantic understanding in custom question answering should be able to take care of
similar alternate questions.
The return on investment starts diminishing once you exceed 10 questions. Even if you're
adding more than 10 alternate questions, try to make the initial 10 questions as
semantically dissimilar as possible so that all intents for the answer are captured via these
10 questions. For the project in QNA #1, adding alternate questions such as "How can I
buy a car?", "I wanna buy a car." aren't required. Whereas adding alternate questions such
as "How to purchase a car.", "What are the options for buying a vehicle?" can be useful.

When to add synonyms to a project
Custom question answering provides the flexibility to use synonyms at the project level,
unlike QnA Maker where synonyms are shared across projects for the entire service.
For better relevance, the customer needs to provide a list of acronyms that the end user
intends to use interchangeably. For instance, the following list provides acceptable
acronyms:
MSFT â€“ Microsoft
ID â€“ Identification
ETA â€“ Estimated time of Arrival

Apart from acronyms, if you think your words are similar in context of a particular domain
and generic language models don't consider them similar, it's better to add them as
synonyms. For instance, if an auto company producing a car model X receives queries
such as "my car's audio isn't working" and the project has questions on "fixing audio for
car X," then we need to add "X" and "car" as synonyms.
The Transformer based model already takes care of most of the common synonym cases,
for example- Purchase â€“ Buy, Sell - Auction, Price â€“ Value. For example, consider the
following QnA pair: Q: "What is the price of Microsoft Stock?" A: "$200".
Users should receive accurate answers to queries like "Microsoft stock value," "Microsoft share
value," or "stock worth," even if terms such as "share," "value," or "worth" aren't present in the
knowledge base.

How are lowercase/uppercase characters treated?
Custom question answering takes casing into account but it's intelligent enough to understand
when it's to be ignored. You shouldn't be seeing any perceivable difference due to wrong
casing.

How are QnAs prioritized for multi-turn questions?
When a knowledge base has hierarchical relationships, and the previous answer related to
other QnAs, the system slightly favors, in order: child QnAs, sibling QnAs, then grandchild QnAs
for the next query. Along with any query, the [custom question Answering API]
(/rest/api/cognitiveservices/questionanswering/question-answering/get-answers) expects a
"context" object with the property "previousQnAId" that denotes the last top answer. Based on
this previous QnA ID, all the related QnAs are boosted.

How are accents treated?
Accents are supported for all major European languages. If the query has an incorrect accent,
confidence score might be slightly different, but the service still returns the relevant answer
and takes care of minor errors by using fuzzy search.

How is punctuation in a user query treated?
Punctuation is ignored in user query before sending it to the ranking stack. Ideally it shouldn't
impact the relevance scores. Punctuation that is ignored: ,?:;"'(){}[]-+ã€‚./!*â€«ØŸâ€¬

Next steps
Last updated on 11/18/2025

Troubleshooting for custom question
answering
The curated list of the most frequently asked questions regarding custom question answering
helps you adopt the feature faster and with better results.

Manage predictions
How can I improve the throughput performance for query predictions?
Answer: Throughput performance issues indicate you need to scale up your Azure AI Search.
Consider adding a replica to your Azure AI Search and improve performance.
Learn more about pricing tiers.

Manage your project
Why is my URL(s)/file(s) not extracting question-answer pairs?
Answer: It's possible that custom question answering can't autoextract some question-andanswer (QnA) content from valid FAQ URLs. In such cases, you can paste the QnA content in a
.txt file and see if the tool can ingest it.
How large a project can I create?
Answer: The size of the project depends on the SKU of Azure search you choose when creating
the QnA Maker service. Read here for more details.
How do I share a project with others?
Answer: Project sharing works at the level of the language resource, that is, all projects
associated a language resource can be shared.
Can you share a project with a contributor that is not in the same Microsoft Entra tenant,
to modify a project?
Answer: Sharing is based on Azure role-based access control (Azure Role-base access control).
If you can share any resource in Azure with another user, you can also share custom question
answering.
Can you assign read/write rights to 5 different users so each of them can access only 1
custom question answering project?
Answer: You can share an entire language resource, not individual projects.
The updates that I made to my project are not reflected in production. Why not?

Answer: Every edit operation, whether in a table update, test, or setting, needs to be saved
before it can be deployed. Be sure to select Save after making changes and then redeploy your
project for those changes to be reflected in production.
Does the project support rich data or multimedia?
Answer:

Multimedia autoextraction for files and URLs
Links - limited HTML-to-Markdown conversion capability.
Files - not supported

Answer text in markdown
Once QnA pairs are in the project, you can edit an answer's markdown text to include links to
media available from public URLs.
Does custom question answering support non-English languages?
Answer: See more details about supported languages.
If you have content from multiple languages, be sure to create a separate project for each
language.

Manage service
I deleted my existing Search service. How can I fix this?
Answer: If you delete an Azure AI Search index, the operation is final and the index can't be
recovered.
I deleted my `testkbv2` index in my Search service. How can I fix this?
Answer: In case you deleted the testkbv2 index in your Search service, you can restore the
data from the last published KB. Use the recovery tool RestoreTestKBIndex

available on

GitHub.
Can I use the same Azure AI Search resource for projects using multiple languages?
Answer: To use multiple language and multiple projects, the user has to create a project for
each language and the first project created for the language resource has to select the option I
want to select the language when I create a project in this resource. This requirement creates
a separate Azure search service per language.

Integrate with other services including Bots

Do I need to use Bot Framework in order to use custom question answering?
Answer: No, you don't need to use the Bot Framework

with custom question answering.

However, custom question answering is offered as one of several templates in Azure AI Bot
Service. Bot Service enables rapid intelligent bot development through Microsoft Bot
Framework, and it runs in a server-less environment.
How can I create a new bot with custom question answering?
Answer: Follow the instructions in this documentation to create your Bot with Azure AI Bot
Service.
How do I use a different project with an existing Azure AI Bot Service?
Answer: You need to have the following information about your project:
Project ID.
Project's published endpoint custom subdomain name, known as host , found on
Settings page after you publish.
Project's published endpoint key - found on Settings page after you publish.
With this information, go to your bot's app service in the Azure portal. Under Settings ->
Configuration -> Application settings, change those values.
The project's endpoint key is labeled QnAAuthkey in the ABS service.
Can two or more client applications share a project?
Answer: Yes, the project can be queried from any number of clients.
How do I embed custom question answering in my website?
Answer: Follow these steps to embed the custom question answering service as a web-chat
control in your website:
1. Create your FAQ bot by following the instructions here.
2. Enable the web chat by following the steps here

Data storage
What data is stored and where is it stored?
Answer:
When you create your language resource for custom question answering, you selected an
Azure region. Your projects and log files are stored in this region.

Last updated on 12/13/2025

What is custom question answering?
Custom question answering (CQA) is a cloud-based Natural Language Processing (NLP) service
that creates conversational AI applications over your data. Build knowledge bases from FAQs,
manuals, and documents to deliver accurate answers through chat bots, virtual assistants, and
interactive interfaces.

Key capabilities
Custom question answering provides enterprise-grade features for building and maintaining
conversational AI solutions:
Knowledge base creation - Import content from URLs, files, and documents. The service
automatically extracts question-answer pairs from structured and semi-structured
sources.
Multi-turn conversations - Create guided conversation flows with follow-up prompts that
navigate users through complex information.
Metadata filtering - Tag answers by content type, domain, or freshness to deliver
contextually relevant responses.
Active learning - Improve answer quality based on real-world usage patterns and user
queries.
Deep learning ranking - Multi-stage ranking architecture combines Azure AI Search with
NLP reranking for optimal answer selection.

Architecture and workflow
The service follows a structured pipeline from project creation to production deployment:
1. Create a project - Build a knowledge base by importing content sources or manually
adding question-answer pairs in Microsoft Foundry (classic)

.

2. Test and refine - Use the test interface to validate responses and adjust answer quality
before deployment.
3. Deploy - Publish your project to create a REST API endpoint accessible by client
applications.
4. Integrate - Client applications send queries and receive JSON responses with answers,
confidence scores, and follow-up prompts.

Development options

Choose from multiple development approaches based on your technical requirements and
expertise:
Microsoft Foundry (classic) - Low-code authoring with automatic QA extraction,
markdown support, and chit-chat integration. Deploy directly to Azure Bot Service

.

REST APIs - Programmatic access for custom integrations and automated workflows. See
the Azure Language REST API reference for endpoint documentation.
Client libraries - SDK packages for .NET and Python enable programmatic project
management and query integration:
.NET (C#) packages

- Runtime and authoring SDKs for C# applications

Python packages

- Runtime and authoring SDKs for Python applications

Next steps
Quickstart: Create and deploy a CQA project
Manage knowledge bases
Configure multi-turn conversations

Last updated on 12/13/2025

Custom question answering project
lifecycle
Custom question answering learns best in an iterative cycle of model changes, utterance
examples, deployment, and gathering data from endpoint queries.

Creating a project
Custom question answering projects provide a best-match answer to a user query based on
the content of the project. Creating a project is a one-time action to setting up a content
repository of questions, answers, and associated metadata. A project can be created by
crawling pre-existing content such the following sources:
FAQ pages
Product manuals
Q-A pairs
Learn how to create a project.

Testing and updating your project
The project is ready for testing once it is populated with content, either editorially or through
automatic extraction. Then you verify that the responses returned with both the correct
response and a sufficient confidence score.
To fix low confidence scores: add alternate questions.
When a query incorrectly returns the default response: add new answers to the correct
question.
This tight loop of test-update continues until you're satisfied with the results.

Deploy your project
Once you're done testing the project, you can deploy it to production. Deployment pushes the
latest version of the tested project to a dedicated Azure AI Search index representing the
published project. It also creates an endpoint that can be called in your application or chat bot.
Due to the deployment action, any further changes made to the test version of the project
leave the published version unaffected. The published version can be live in a production
application.

Each of these projects can be targeted for testing separately.

Monitor usage
To be able to log the chat logs of your service and get additional analytics, you would need to
enable Azure Monitor Diagnostic Logs after you create your language resource.
Based on what you learn from your analytics, make appropriate updates to your project.

Version control for data in your project
Version control for data is provided through the import/export features on your project page.
You can back up a project by exporting the project, in either .tsv or .xls format. Once
exported, include this file as part of your regular source control check.
When you need to go back to a specific version, you need to import that file from your local
system. An exported must only be used via import on the project page. It can't be used as a file
or URL document data source. This will replace questions and answers currently in the project
with the contents of the imported file.

Test and production project
A project is the repository of questions and answer sets created, maintained, and used through
custom question answering. Each language resource can hold multiple projects.
A project has two states: test and published.

Production project
The published project is the version that's used in your chat bot or application. Publishing a
project puts the content of its test version into its published version. The published project is
the version that the application uses through the endpoint. Make sure that the content is
correct and well tested. The published project is known as prod in the HTTP request.

Next steps
Last updated on 12/13/2025

Azure resources for custom question
answering
Custom question answering uses several Azure sources, each with a different purpose.
Understanding how they're used individually allows you to plan for and select the correct
pricing tier or know when to change your pricing tier. Understanding how resources are used
in combination allows you to find and fix problems when they occur.

Resource planning
îª€ Tip
"Knowledge base" and "project" are equivalent terms in custom question answering and
can be used interchangeably.
When you first develop a project, in the prototype phase, it's common to have a single
resource for both testing and production.
When you move into the development phase of the project, you should consider:
How many languages will your project hold?
How many regions you need your project to be available in?
How many documents will your system hold in each domain?

Pricing tier considerations
Typically there are three parameters you need to consider:
The throughput you need:
The throughput for custom question answering is currently capped at 10 text records
per second for both management APIs and prediction APIs.
The throughput cap should also influence your Azure AI Search selection. For more
information, see Azure AI Search. Additionally, you might need to adjust Azure AI
Search capacity with replicas.
Size and the number of projects: Choose the appropriate Azure search SKU

for your

scenario. Typically, you decide the number of projects you need based on number of
different subject domains. One subject domain (for a single language) should be in one
project.

With custom question answering, you have a choice to set up your language resource in a
single language or multiple languages.
ï¼‰ Important
You can publish N-1 projects with a single language resource or N-2 projects with
multiple language resources in a single tier. The N notation is the maximum indexes
allowed in the tier. Also, check the maximum size and the number of documents
allowed per tier.
For example, if your tier has 15 allowed indexes, you can publish 14 projects of the same
language (one index per published project). The 15th index is used for all the projects for
authoring and testing. If you choose to have projects in different languages, then you can
only publish seven projects.
Number of documents as sources: There are no limits to the number of documents you
can add as sources in custom question answering.
The following table gives you some high-level guidelines.
ï¾‰

Expand table

Azure AI Search

Limitations

Experimentation

Free Tier

Publish Up to 2-KBs, 50-MB size

Dev/Test Environment

Basic

Publish Up to 14-KBs, 2-GB size

Production Environment

Standard

Publish Up to 49-KBs, 25-GB size

Recommended settings
The throughput for custom question answering is currently capped at 10 text records per
second for both management APIs and prediction APIs. To target 10 text records per second
for your service, we recommend the S1 (one instance) tier of Azure AI Search.

Keys in custom question answering
Your custom question answering feature deals with two kinds of keys: authoring keys and
Azure AI Search keys used to access the service in the customer's subscription.
Use these keys when making requests to the service through APIs.

ï¾‰

Expand table

Name

Location

Purpose

Authoring/Subscription

Azure

These keys are used to access Azure Language APIs). These APIs

key

portal

let you edit the questions and answers in your project, and
publish your project. These keys are created when you create a
new resource.
Find these keys on the Foundry Tools resource on the Keys and
Endpoint page.

Azure AI Search Admin

Azure

These keys are used to communicate with the Azure AI Search

Key

portal

service deployed in the user's Azure subscription. When you
associate an Azure AI Search resource with the custom question
answering feature, the admin key is automatically passed to
custom question answering.
You can find these keys on the Azure AI Search resource on the
Keys page.

Find authoring keys in the Azure portal
You can view and reset your authoring keys from the Azure portal, where you added the
custom question answering feature in your language resource.
1. Go to the language resource in the Azure portal and select the resource that has the
Foundry Tools type:

2. Go to Keys and Endpoint:

Management service region
In custom question answering, both the management and the prediction services are colocated
in the same region.

Resource purposes
Each Azure resource created with custom question answering feature has a specific purpose:
Language resource (Also referred to as a Text Analytics resource depending on the
context of where you're evaluating the resource.)
Azure AI Search resource

Language resource
The language resource with custom question answering feature provides access to the
authoring and publishing APIs, hosts the ranking runtime and provides telemetry.

Azure AI Search resource
The Azure AI Search resource is used to:
Store the question and answer pairs
Provide the initial ranking (ranker #1) of the question and answer pairs at runtime

Index usage
You can publish N-1 projects of a single language or N/2 projects of different languages in a
particular tier, where N is the maximum number of indexes allowed in the Azure AI Search tier.
Also check the maximum size and the number of documents allowed per tier.
For example, if your tier has 15 allowed indexes, you can publish 14 projects of the same
language (one index per published project). The 15th index is used for all the projects for
authoring and testing. If you choose to have projects in different languages, then you can only
publish seven projects.

Language usage
With custom question answering, you have a choice to set up your service for projects in a
single language or multiple languages. You make this choice during the creation of the first
project in your language resource.

Next steps
Learn about the custom question answering projects

Last updated on 12/11/2025

Plan your custom question answering app
To plan your custom question answering app, you need to understand how custom question
answering works and interacts with other Azure services. You should also have a solid grasp of
project concepts.

Azure resources
Each Azure resource created with custom question answering has a specific purpose. Each
resource has its own purpose, limits, and pricing tier. It's important to understand the function
of these resources so that you can use that knowledge into your planning process.
ï¾‰

Resource

Purpose

Language resource resource

Authoring, query prediction endpoint and telemetry

Azure AI Search resource

Data storage and search

Expand table

Resource planning
Custom question answering throughput is currently capped at 10 text records per second for
both management APIs and prediction APIs. To target 10 text records per second for your
service, we recommend the S1 (one instance) SKU of Azure AI Search.

Language resource
A single language resource with the custom question answering feature enabled can host more
than one project. The number of projects is determined via the Azure AI Search pricing tier's
quantity of supported indexes. Learn more about the relationship of indexes to projects.

Project size and throughput
When you build a real app, plan sufficient resources for the size of your project and your
expected query prediction requests.
Project size control factors:
Azure AI Search resource pricing tier limits
Custom question answering limits

The project query prediction request is controlled via the web app plan and web app. Refer to
recommended settings to plan your pricing tier.

Understand the impact of resource selection
Proper resource selection means your project answers query predictions successfully.
If your project isn't functioning properly, it's typically an issue of improper resource
management.
Improper resource selection requires investigation to determine which resource needs to
change.

Project
A project is directly tied its language resource. It holds the question and answer (QnA) pairs
that are used to answer query prediction requests.

Language considerations
You can now have projects in different languages within the same language resource where the
custom question answering feature is enabled. When you create your first project, you can
decide whether to set a single language for all future projects or to select a language each time
you start a new one. This choice determines if the resource applies to projects in one language
or allow for language selection with each new project.

Ingest data sources
Custom question answering also supports unstructured content. You can upload a file that has
unstructured content.
Currently we don't support URLs for unstructured content.
The ingestion process converts supported content types to markdown. All further editing of the
answer is done with markdown. After you create a project, you can edit QnA pairs with rich text
authoring.

Data format considerations
Because the final format of a QnA pair is markdown, it's important to understand markdown
support.

Bot personality
Add a bot personality to your project with chit-chat. This personality comes through with
answers provided in a certain conversational tone such as professional and friendly. This chitchat is provided as a conversational set, which you have total control to add, edit, and remove.
A bot personality is recommended if your bot connects to your project. You can include chitchat in your project even if you're connecting to other services. However, it's important to
review how the bot service interacts with these integrations to ensure this approach fits your
overall architectural design.

Conversation flow with a project
Conversation flow usually begins with a salutation from a user, such as Hi or Hello . Your
project can answer with a general answer, such as Hi, how can I help you , and it can also
provide a selection of follow-up prompts to continue the conversation.
Design your conversational flow so that users always know how to interact with your bot and
are never left without guidance. By including a loop or clear navigation, you ensure users aren't
abandoned during the conversation. Follow-up prompts provide linking between QnA pairs,
which allow for the conversational flow.

Authoring with collaborators
Collaborators may be other developers who share the full development stack of the project
application or may be limited to just authoring the project.
Project authoring supports several role-based access permissions you apply in the Azure portal
to limit the scope of a collaborator's abilities.

Integration with client applications
Integration with client applications is accomplished by sending a query to the prediction
runtime endpoint. A query is sent to your specific project with an SDK or REST-based request
to your custom question answering web app endpoint.
To authenticate a client request correctly, the client application must send the correct
credentials and project ID. If you're using an Azure AI Bot Service, configure these settings as
part of the bot configuration in the Azure portal.

Conversation flow in a client application

Conversation flow in a client application, such as an Azure bot, may require functionality before
and after interacting with the project.
Does your client application support conversation flow, either by providing alternate means to
handle follow-up prompts or including chit-chit? If so, design these features early and make
sure the client application query is handled correctly via another service or when sent to your
project.

Active learning from a client application
Custom question answering uses active learning to improve your project by suggesting
alternate questions to an answer. The client application is responsible for a part of this active
learning. Through conversational prompts, the client application can determine that the project
returned an answer that's not useful to the user, and it can determine a better answer. The
client application needs to send that information back to the project to improve the prediction
quality.

Providing a default answer
If your project doesn't find an answer, it returns the default answer. This answer is configurable
on the Settings page.
This default answer is different from the Azure bot default answer. You configure the default
answer for your Azure bot in the Azure portal as part of configuration settings. The default
answer is then returned when the score threshold isn't met.

Prediction
The prediction is the response from your project, and it includes more information than just the
answer. To get a query prediction response, use the custom question answering API.

Prediction score fluctuations
A score can change based on several factors:
Number of answers you requested in response with the top property
Variety of available alternate questions
Filtering for metadata
Query sent to test or production project.

Analytics with Azure Monitor

In custom question answering, telemetry is offered through the Azure Monitor service. Use our
top queries to understand your metrics.

Development lifecycle
The development lifecycle of a project is ongoing: editing, testing, and publishing your project.

Project development of question answer pairs
Your QnA pairs should be designed and developed based on your client application usage.
Each pair can contain:
Metadata - filterable when querying to allow you to tag your QnA pairs with additional
information about the source, content, format, and purpose of your data.
Follow-up prompts - helps to determine a path through your project so the user arrives at
the correct answer.
Alternate questions - important to allow search to match to your answer from different
forms of the question. Active learning suggestions turn into alternate questions.

DevOps development
Developing a project to insert into a DevOps pipeline requires that the project is isolated
during batch testing.
A project shares the Azure AI Search index with all other projects on the language resource.
While the project is isolated via a partition, sharing the index can cause a difference in the
score when compared to the published project.
To have the same score on the test and production projects, isolate a language resource to a
single project. In this architecture, the resource only needs to live as long as the isolated batch
test.

Next steps
Azure resources

Last updated on 12/13/2025

Precise answering
The precise answering feature introduced, allows you to get the precise short answer from the
best candidate answer passage present in the project for any user query. This feature uses a
deep learning model at runtime, which understands the intent of the user query and detects
the precise short answer from the answer passage, if there is a short answer present as a fact in
the answer passage.
This feature is beneficial for both content developers as well as end users. Now, content
developers don't need to manually curate specific question answer pairs for every fact present
in the project, and the end user doesn't need to look through the whole answer passage
returned from the service to find the actual fact that answers the user's query.

Precise answering via the portal
When you open the test pane, you can see an option to Include short answer response on the
top above show advanced options.
When you enter a query in the test pane, you can see a short-answer along with the answer
passage, if there is a short answer present in the answer passage.

ï Š

You can unselect the Include short answer response option, if you want to see only the Long
answer passage in the test pane.
The service also returns back the confidence score of the precise answer as an Answer-span
confidence score which you can check by selecting the Inspect option and then selection
additional information.

ï Š

Deploying a bot
When you publish a bot, you get the precise answer enabled experience by default in your
application, where you can see short answer along with the answer passage. Refer to the API
reference for REST API to see how to use the precise answer (called AnswerSpan) in the
response. User has the flexibility to choose other experiences by updating the template
through the Bot app service.

Last updated on 12/13/2025

Confidence score
When a user query is matched against a project (also known as a knowledge base), Custom
question answering returns relevant answers, along with a confidence score. This score
indicates the confidence that the answer is the right match for the given user query.
The confidence score is a number between 0 and 100. A score of 100 is likely an exact match,
while a score of 0 means, that no matching answer was found. The higher the score- the
greater the confidence in the answer. For a given query, there could be multiple answers
returned. In that case, the answers are returned in order of decreasing confidence score.
The following table indicates typical confidence associated for a given score.
ï¾‰

Score
Value

Score Meaning

0.90 1.00

A near exact match of user query and a KB question

> 0.70

High confidence - typically a good answer that completely answers the
user's query

0.50 0.70

Medium confidence - typically a fairly good answer that should answer the
main intent of the user query

0.30 0.50

Low confidence - typically a related answer, that partially answers the user's
intent

< 0.30

Very low confidence - typically does not answer the user's query, but has

Expand table
Example
Query

some matching words or phrases
0

No match, so the answer is not returned.

Choose a score threshold
The table above shows the range of scores that can occur when querying with Custom
question answering. However, since every project is different, and has different types of words,
intents, and goals- we recommend you test and choose the threshold that best works for you.
By default the threshold is set to 0 , so that all possible answers are returned. The
recommended threshold that should work for most projects, is 50.
When choosing your threshold, keep in mind the balance between Accuracy and Coverage,
and adjust your threshold based on your requirements.

If Accuracy (or precision) is more important for your scenario, then increase your
threshold. This way, every time you return an answer, it will be a much more CONFIDENT
case, and much more likely to be the answer users are looking for. In this case, you might
end up leaving more questions unanswered.
If Coverage (or recall) is more important- and you want to answer as many questions as
possible, even if there is only a partial relation to the user's question- then LOWER the
threshold. This means there could be more cases where the answer does not answer the
user's actual query, but gives some other somewhat related answer.

Set threshold
Set the threshold score as a property of the REST API JSON body. This means you set it for
each call to REST API.

Improve confidence scores
To improve the confidence score of a particular response to a user query, you can add the user
query to the project as an alternate question on that response. You can also use caseinsensitive synonyms to add synonyms to keywords in your project.

Similar confidence scores
When multiple responses have a similar confidence score, it is likely that the query was too
generic and therefore matched with equal likelihood with multiple answers. Try to structure
your QnAs better so that every QnA entity has a distinct intent.

Confidence score differences between test and
production
The confidence score of an answer may change negligibly between the test and deployed
version of the project even if the content is the same. This is because the content of the test
and the deployed project are located in different Azure AI Search indexes.
The test index holds all the question and answer pairs of your project. When querying the test
index, the query applies to the entire index then results are restricted to the partition for that
specific project. If the test query results are negatively impacting your ability to validate the
project, you can:
Organize your project using one of the following:

One resource restricted to one project: restrict your single language resource (and the
resulting Azure AI Search test index) to a project.
Two resources - one for test, one for production: have two language resources, using
one for testing (with its own test and production indexes) and one for production (also
having its own test and production indexes)
Always use the same parameters when querying both your test and production projects.
When you deploy a project, the question and answer contents of your project moves from the
test index to a production index in Azure search.
If you have a project in different regions, each region uses its own Azure AI Search index.
Because different indexes are used, the scores will not be exactly the same.

No match found
When no good match is found by the ranker, the confidence score of 0.0 or "None" is returned
and the default response is returned. You can change the default response.

Next steps
Last updated on 11/18/2025

Custom question answering best practices
Use these best practices to improve your project and provide better results to your client
application or chat bot's end users.

Extraction
Custom question answering is continually improving the algorithms that extract question
answer pairs from content and expanding the list of supported file and HTML formats. In
general, FAQ pages should be stand-alone and not combined with other information. Product
manuals should have clear headings and preferably an index page.

Creating good questions and answers
We use the following list of question and answer pairs to represent a project. This approach
helps highlight best practices when creating projects for custom question answering.
ï¾‰

Expand table

Question

Answer

I want to buy a car

There are three options to buy a car.

I want to purchase software license

Software license can be purchased online at no cost.

What is the price of Microsoft stock?

$200.

How to buy Microsoft Services

Microsoft services can be bought online.

Want to sell car

Send car pics and document.

How to get access to identification card?

Apply via company portal to get identification card.

When should you add alternate questions to question and
answer pairs?
Custom question answering employs a transformer-based ranker that takes care of user
queries that are semantically similar to the question in the project. For example, consider the
following question answer pair:
Question: What is the price of Microsoft Stock? Answer: $200.
The service can return the expected response for semantically similar queries such as:

"How much is Microsoft stock worth? "How much is Microsoft share value?" "How much does a
Microsoft share cost?" "What is the market value of a Microsoft stock?" "What is the market
value of a Microsoft share?"
The confidence score that the system assigns to its response can vary. This variation depends
on the input query and how much it differs from the original question-answer pair.
There are certain scenarios that require the customer to add an alternate question. If you
already verified that a specific query doesn't return the correct answer, even though the answer
exists in the project, we recommend taking further action. Add that query as an alternate
question to the intended question and answer pair. This step can help ensure users receive the
correct response in the future.

How many alternate questions per question answer pair is
optimal?
Users can add as many alternate questions as they want, but only first 5 are considered for core
ranking. However, the rest is useful for exact match scenarios. We also recommended keeping
the different intent/distinct alternate questions at the top for better relevance and score.
Semantic understanding in custom question answering should be able to take care of similar
alternate questions.
The return on investment decreases after you exceed 10 questions. Even if you include more
than 10 alternate questions, focus on making the first 10 questions as semantically different as
possible. By doing so, you ensure that these 10 questions capture all possible intents for the
answer. For the project at the beginning of this section, in question answer pair #1, adding
alternate questions such as How can I buy a car or I want to buy a car aren't required. Whereas
adding alternate questions such as How to purchase a car, What are the options of buying a
vehicle can be useful.

When to add synonyms to a project?
Custom question answering provides the flexibility to use synonyms at the project level, unlike
QnA Maker where synonyms are shared across projects for the entire service.
For better relevance, you need to provide a list of acronyms that the end user intends to use
interchangeably. The following list details acceptable acronyms:
MSFT â€“ Microsoft
ID â€“ Identification
ETA â€“ Estimated time of Arrival

Other than acronyms, if you think your words are similar in context of a particular domain and
generic language models doesn't consider them similar, it's better to add them as synonyms.
For instance, an auto company producing a car model X receives queries such as my car's audio
isn't working and the project has questions on fixing audio for car X. Then, we need to add 'X'
and 'car' as synonyms.
The transformer-based model already takes care of most of the common synonym cases, for
example: Purchase â€“ Buy , Sell - Auction , Price â€“ Value . For another example, consider the
following question answer pair: Q: What is the price of Microsoft Stock? A: $200.
If users ask questions like Microsoft stock value, Microsoft share value, Microsoft stock worth,
Microsoft share worth, or just stock value, you should still be able to provide the correct answer.
It's important to maintain this clarity, even though terms such as share, value, and worth aren't
originally included in the project.
Special characters aren't allowed in synonyms.

How are lowercase/uppercase characters treated?
Question answering takes casing into account but it's intelligent enough to understand when
it's to be ignored. You shouldn't be seeing any perceivable difference due to wrong casing.

How are question answer pairs prioritized for multi-turn
questions?
When a project includes hierarchical relationshipsâ€”whether they're added manually or
through extractionâ€”special handling is applied. If the previous response addressed a question
that belongs to a related set of question-answer pairs, it affects how we handle subsequent
queries. For the next query, we give slight preference to all child question-answer pairs first.
Preference is then given to sibling question-answer pairs, followed by grandchild questionanswer pairs, in that order. Along with any query, the custom question answering REST API
expects a context object with the property previousQnAId , which denotes the last top answer.
Based on this previous QnAID , all the related QnAs are boosted.

How are accents treated?
Accents are supported for all major European languages. If the query has an incorrect accent,
the confidence score might be slightly different, but the service still returns the relevant answer
and takes care of minor errors by using fuzzy search.

How is punctuation in a user query treated?

Punctuation is ignored in a user query before sending it to the ranking stack. Ideally it
shouldn't impact the relevance scores. Punctuation that is ignored is as follows: ,?:;\"'(){}[]+ã€‚./!*â€«ØŸâ€¬

Chit-Chat
Add chit-chat to your bot, to make your bot more conversational and engaging, with low
effort. You can easily add chit-chat data sources from predefined personalities when creating
your project, and change them at any time. Learn how to add chit-chat to your KB.
Chit-chat is supported in many languages.

Choosing a personality
Chit-chat is supported for several predefined personalities:
ï¾‰

Personality

Custom question answering dataset file

Professional

qna_chitchat_professional.tsv

Friendly

qna_chitchat_friendly.tsv

Witty

qna_chitchat_witty.tsv

Caring

qna_chitchat_caring.tsv

Enthusiastic

qna_chitchat_enthusiastic.tsv

Expand table

The responses range from formal to informal and irreverent. You should select the personality
that is closest aligned with the tone you want for your bot. You can view the datasets, and
choose one that serves as a base for your bot, and then customize the responses.

Edit bot-specific questions
There are some bot-specific questions that are part of the chit-chat data set, and are
completed with generic answers. Change these answers to best reflect your bot details.
We recommend making the following chit-chat question answer pairs more specific:
Who are you?
What can you do?
What is your age?

Who created you?

Adding custom chit-chat with a metadata tag
If you add your own chit-chat question answer pairs, make sure to add metadata so these
answers are returned. The metadata name/value pair is editorial:chitchat .

Searching for answers
The custom question answering REST API uses both questions and the answer to search for
best answers to a user's query.

Searching questions only when answer isn't relevant
Use the RankerType=QuestionOnly if you don't want to search answers.
An example is when the project is a catalog of acronyms as questions with their full form as the
answer. The value of the answer doesn't help to search for the appropriate answer.

Ranking/Scoring
Make sure you're making the best use of the supported ranking features. Doing so improves
the likelihood that a given user query is answered with an appropriate response.

Choosing a threshold
The default confidence score that is used as a threshold is 0, however you can change the
threshold for your project based on your needs. Since every project is different, you should test
and choose the threshold that is best suited for your project.

Choosing Ranker type
By default, custom question answering searches through questions and answers. If you want to
search through questions only, to generate an answer, use the RankerType=QuestionOnly in the
POST body of the REST API request.

Add alternate questions
Alternate questions to improve the likelihood of a match with a user query. Alternate questions
are useful when there are multiple ways in which the same question may be asked. The

alternate questions can include changes in the sentence structure and word-style.
ï¾‰

Expand table

Original query

Alternate queries

Change

Is parking available?

Do you have a car park?

sentence structure

Hi

Yo
Hey there

word-style or slang

Use metadata tags to filter questions and answers
Metadata adds the ability for a client application to know it shouldn't take all answers but
instead to narrow down the results of a user query based on metadata tags. The project answer
can differ based on the metadata tag, even if the query is the same. For example, the answer to
where is parking located can vary depending on the branch location. If the metadata is Location:
Seattle, the answer is different than if the metadata is Location: Redmond.

Use synonyms
While there's some support for synonyms in the English language, use case-insensitive word
alterations to add synonyms to keywords that take different forms.
ï¾‰

Original word

Synonyms

buy

purchase
Net-banking
Net banking

Expand table

Use distinct words to differentiate questions
The ranking algorithm, which matches a user query with a question in the project, works best if
each question addresses a different need. Repetition of the same word set between questions
reduces the likelihood that the right answer is chosen for a given user query with those words.
For example, you might have two separate question answer pairs with the following questions:
ï¾‰

Expand table

Questions
where is the parking location
where is the ATM location

Since these two questions are phrased with similar words, this similarity could cause similar
scores for many user queries that are phrased like where is the <x> location. Instead, try to
clearly differentiate your queries. For example, use specific questions like where is the parking
lot and where is the ATM. Avoid using general words like location, since they could appear in
many different questions throughout your project.

Collaborate
Custom question answering allows users to collaborate on a project. Users need access to the
associated Azure resource group in order to access the projects. Some organizations may want
to outsource the project editing and maintenance, and still be able to protect access to their
Azure resources. This editor-approver model is done by setting up two identical language
resources with identical custom question answering projects in different subscriptions and
selecting one for the edit-testing cycle. Once testing is finished, the project contents are
exported. They're then transferred using an import-export process. This process moves the
contents to the language resource of the approver, who deploys the project and updates the
endpoint.

Active learning
Active learning does the best job of suggesting alternative questions when it has a wide range
of quality and quantity of user-based queries. It's important to allow client-applications' user
queries to participate in the active learning feedback loop without censorship. Once questions
are suggested you can review and accept or reject those suggestions.

Next steps
Last updated on 12/12/2025

Project limits and boundaries
The following custom question answering limits are a combination of the Azure AI Search
pricing tier limits and custom question answering limits. Both sets of limits affect how many
projects you can create per resource and how large each project can grow.

Projects
The maximum number of projects is based on Azure AI Search tier limits.
Choose the appropriate Azure search SKU

for your scenario. Typically, you decide the

number of projects you need based on number of different subject domains. One subject
domain (for a single language) should be in one project.
With custom question answering, you have a choice to set up your language resource in a
single language or multiple languages.
ï¼‰ Important
You can publish N-1 projects with a single language resource or N-2 projects with multiple
language resources in a single tier. The N notation is the maximum indexes allowed in the
tier. Also check the maximum size and the number of documents allowed per tier.
For example, if your tier has 15 allowed indexes, you can publish 14 projects of the same
language (one index per published project). The 15th index is used for all the projects for
authoring and testing. If you choose to have projects in different languages, then you can only
publish seven projects.

Extraction limits
File naming constraints
File names may not include the following characters:
ï¾‰

Don't use character
Single quote '
Double quote "

Expand table

Maximum file size
ï¾‰

Format

Max file size (MB)

.docx

10

.pdf

25

.tsv

10

.txt

10

.xlsx

3

Expand table

Maximum number of files
ï¼— Note
Custom question answering currently has no limits on the number of sources that can be
added. Throughput is currently capped at 10 text records per second for both
management APIs and prediction APIs. If you use the F0 tier, upload is limited to three
files.

Maximum number of deep-links from URL
The maximum number of deep-links that can be crawled for extraction of question answer
pairs from a URL page is 20.

Metadata limits
Metadata is presented as a text-based key:value pair, such as product:windows 10 . Metadata is
stored and compared in lower case. The maximum number of metadata fields is based on your
Azure AI Search tier limits.
If you choose to projects with multiple languages in a single language resource, there's a
dedicated test index per project. So the limit is applied per project in Azure Language.
ï¾‰

Expand table

Azure AI Search tier

Free

Basic

S1

S2

S3

S3 HD

Maximum metadata fields per Language (per project)

1,000

100*

1,000

1,000

1,000

1,000

If you don't choose the option to have projects with multiple different languages, then the
limits are applied across all projects in Azure Language.
ï¾‰

Azure AI Search tier

Free

Basic

S1

S2

Expand table
S3

S3
HD

Maximum metadata fields per Language (across all

1,000

100*

1,000

1,000

1,000

1,000

projects)

By name and value
The length and acceptable characters for metadata name and value are listed in the following
table.
ï¾‰

Expand table

Item

Allowed chars

Regex pattern match

Max chars

Name (key)

Allows

^[a-zA-Z0-9_]+$

100

^[^:|]+$

500

Alphanumeric (letters and digits)
_ (underscore)
Must not contain spaces.
Value

Allows everything except
: (colon)
| (vertical pipe)

Only one value allowed.

Project content limits
Overall limits on the content in the project:
Length of answer text: 25,000 characters
Length of question text: 1,000 characters
Length of metadata key text: 100 characters
Length of metadata value text: 500 characters
Supported characters for metadata name: Alphabets, digits, and _

Supported characters for metadata value: All except : and |
Length of file name: 200
Supported file formats: ".tsv", ".pdf", ".txt", ".docx", ".xlsx".
Maximum number of alternate questions: 300
Maximum number of question-answer pairs: Depends on the Azure AI Search tier
chosen. A question and answer pair maps to a document on Azure AI Search index.
URL/HTML page: 1 million characters

Create project call limits:
The limits for each create project action, that is, selecting Create new project or calling the REST
API to create a project are as follows:
Recommended maximum number of alternate questions per answer: 300
Maximum number of URLs: 10
Maximum number of files: 10
Maximum number of QnAs permitted per call: 1000

Update project call limits
The limits for each update action, that is, selecting Save or calling the REST API with an update
request are as follows:
Length of each source name: 300
Recommended maximum number of alternate questions added or deleted: 300
Maximum number of metadata fields added or deleted: 10
Maximum number of URLs that can be refreshed: 5
Maximum number of QnAs permitted per call: 1000

Add unstructured file limits
ï¼— Note
If you need to use larger files than the limit allows, you can break the file into smaller
files before sending them to the API.
The limits when unstructured files are used to Create new project or call the REST API to create
a project are as follows:
Length of file: The service extracts the first 32,000 characters.

Maximum three responses per file.

Prebuilt custom question answering limits
ï¼— Note
A document is a single string of text characters.
To use larger documents than the limit allows, you can break the text into smaller
chunks before sending them to the API.
The limits when the REST API is used to answer a question without having to create a project is
as follows:
Number of documents: 5
Maximum size of a single document: 5,120 characters
Maximum three responses per document.

Last updated on 12/11/2025

When to use conversational language
understanding or orchestration workflow
apps
When you create large applications, you should consider whether your use case is best served
by a single conversational app (flat architecture) or by multiple apps that are orchestrated.

Orchestration overview
Orchestration workflow is a feature that allows you to connect different projects from LUIS,
conversational language understanding, and custom question answering in one project. You
can then use this project for predictions by using one endpoint. The orchestration project
makes a prediction on which child project should be called, automatically routes the request,
and returns with its response.
Orchestration involves two steps:
1. Predicting which child project to call.
2. Routing the utterance to the destination child app and returning the child app's response.

Orchestration advantages
Clear decomposition and faster development:
If your overall schema has a substantial number of domains, the orchestration
approach can help decompose your application into several child apps (each serving a
specific domain). For example, an automotive conversational app might have a
navigation domain or a media domain.
Developing each domain app in parallel is easier. People and teams with specific
domain expertise can work on individual apps collaboratively and in parallel.
Because each domain app is smaller, the development cycle becomes faster. Smallersized domain apps take much less time to train than a single large app.
More flexible confidence score thresholds:
Because separate child apps serve each domain, it's easy to set separate thresholds for
different child apps.
AI-quality improvements where appropriate:
Some applications require that certain entities must be domain restricted.
Orchestration makes this task easy to achieve. After the orchestration project predicts
which child app should be called, the other child apps aren't called.

For example, if your app contains a Person.Name prebuilt entity, consider the utterance
"How do I use a jack?" in the context of a vehicle question. In this context, jack is an
automotive tool and shouldn't be recognized as a person's name. When you use
orchestration, this utterance can be redirected to a child app created to answer such a
question, which doesn't have a Person.Name entity.

Orchestration disadvantages
Redundant entities in child apps:
If you need a particular prebuilt entity being returned in all utterances irrespective of
the domain, for example Quantity.Number or Geography.Location , there's no way of
adding an entity to the orchestration app (it's an intent-only model). You would need
to add it to all individual child apps.
Efficiency:
Orchestration apps take two model inferences. One for predicting which child app to
call, and another for the prediction in the child app. Inference times are typically slower
than single apps with a flat architecture.
Train/test split for orchestrator:
Training an orchestration app doesn't allow you to granularly split data between the
testing and training sets. For example, you can't train a 90-10 split for child app A, and
then train an 80-20 split for child app B. This limitation might be minor, but it's worth
keeping in mind.

Flat architecture overview
Flat architecture is the other method of developing conversational apps. Instead of using an
orchestration app to send utterances to one of multiple child apps, you develop a singular (or
flat) app to handle utterances.

Flat architecture advantages
Simplicity:
For small-sized apps or domains, the orchestrator approach can be overly complex.
Because all intents and entities are at the same app level, it might be easier to make
changes to all of them together.
It's easier to add entities that should always be returned:
If you want certain prebuilt or list entities to be returned for all utterances, you only
need to add them alongside other entities in a single app. If you use orchestration, as

mentioned, you need to add it to every child app.

Flat architecture disadvantages
Unwieldy for large apps:
For large apps (say, more than 50 intents or entities), it can become difficult to keep
track of evolving schemas and datasets. This difficulty is evident in cases where the app
has to serve several domains. For example, an automotive conversational app might
have a navigation domain or a media domain.
Limited control over entity matches:
In a flat architecture, there's no way to restrict entities to be returned only in certain
cases. When you use orchestration, you can assign those specific entities to particular
child apps.

Related content
Orchestration workflow overview
Conversational language understanding overview

Last updated on 11/18/2025

Enrich your project with active learning
In this tutorial, you learn how to:
ï¼‚ Download an active learning test file
ï¼‚ Import the test file to your existing project
ï¼‚ Accept/reject active learning suggestions
ï¼‚ Add alternate questions
This tutorial shows you how to enhance your Custom question answering project with active
learning. If you notice that customers are asking questions that are not covered in your project,
they may be paraphrased variations of questions.
These variations, when added as alternate questions to the relevant question answer pair, help
to optimize the project to answer real world user queries. You can manually add alternate
questions to question answer pairs through the editor. At the same time, you can also use the
active learning feature to generate active learning suggestions based on user queries. The
active learning feature, however, requires that the project receives regular user traffic to
generate suggestions.

Use active learning
Active learning is turned on by default for Custom question answering enabled resources.
To try out active learning suggestions, you can import the following file as a new project:
SampleActiveLearning.tsv

.

Download file
Run the following command from the command prompt to download a local copy of the
SampleActiveLearning.tsv file.

Windows Command Prompt
curl "https://github.com/Azure-Samples/cognitive-services-sample-datafiles/blob/master/qna-maker/knowledge-bases/SampleActiveLearning.tsv" --output
SampleActiveLearning.tsv

Import file
From the edit project pane for your project, select the ... (ellipsis) icon from the menu >
Import questions and answers > Import as TSV. Then, select Choose file to browse to the

copy of SampleActiveLearning.tsv that you downloaded to your computer in the previous
step, and then select done.

ï Š

Screenshot of edit project menu bar with import as TSV option displayed.

View and add/reject active learning suggestions
Once the import of the test file is complete, active learning suggestions can be viewed on the
review suggestions pane:

ï Š

ï¼— Note
Active learning suggestions are not real time. There is an approximate delay of 30 minutes
before the suggestions can show on this pane. This delay is to ensure that we balance the
high cost involved for real time updates to the index and service performance.
We can now either accept these suggestions or reject them using the options on the menu bar
to Accept all suggestions or Reject all suggestions.
Alternatively, to accept or reject individual suggestions, select the checkmark (accept) symbol
or trash can (reject) symbol that appears next to individual questions in the Review
suggestions page.

ï Š

Add alternate questions
While active learning automatically suggests alternate questions based on the user queries
hitting the project, we can also add variations of a question on the edit project page by
selecting Add alternate phrase to question answer pairs.
By adding alternate questions along with active learning, we further enrich the project with
variations of a question that helps to provide consistent answers to user queries.
ï¼— Note
When alternate questions have many stop words, they might negatively impact the
accuracy of responses. So, if the only difference between alternate questions is in the stop
words, these alternate questions are not required. To examine the list of stop words
consult the stop words article .

Next steps
Last updated on 11/18/2025

Improve quality of response with synonyms
In this tutorial, you learn how to:
ï¼‚ Add synonyms to improve the quality of your responses
ï¼‚ Evaluate the response quality via the inspect option of the Test pane
This tutorial shows you how you can improve the quality of your responses by using synonyms.
Let's assume that users aren't getting an accurate response to their queries, when they use
alternate forms, synonyms or acronyms of a word. So, they decide to improve the quality of the
response by using Authoring API to add synonyms for keywords.

Add synonyms using Authoring API
Let improve the results by adding the following words and their alterations:
ï¾‰

Word

Alterations

fix problems

troubleshoot , diagnostic

whiteboard

white board , white canvas

bluetooth

blue tooth , BT

JSON
{
"synonyms": [
{
"alterations": [
"fix problems",
"troubleshoot",
"diagnostic",
]
},
{
"alterations": [
"whiteboard",
"white board",
"white canvas"
]
},
{
"alterations": [
"bluetooth",
"blue tooth",

Expand table

"BT"
]
}
]
}

For the question and answer pair â€œFix problems with Surface Pen,â€ we compare the response
for a query made using its synonym â€œtroubleshoot.â€

Response before addition of synonym

ï Š

Response after addition of synonym

ï Š

As you can see, when troubleshoot was not added as a synonym, we got a low confidence
response to the query â€œHow to troubleshoot your surface pen.â€ However, after we add
troubleshoot as a synonym to â€œfix problemsâ€, we received the correct response to the query

with a higher confidence score. Once these synonyms were added, the relevance of results is
improved.
ï¼‰ Important
Synonyms are case insensitive. Synonyms also might not work as expected if you add stop
words as synonyms. The list of stop words can be found here: List of stop words . For
instance, if you add the abbreviation IT for Information technology, the system might not
be able to recognize Information Technology because IT is a stop word and is filtered
when a query is processed.

Notes
Synonyms can be added in any order. The ordering is not considered in any
computational logic.
Synonyms can only be added to a project that has at least one question and answer pair.

Synonyms can be added only when there is at least one question and answer pair present
in a project.
In case of overlapping synonym words between two sets of alterations, it can have
unexpected results and it isn't recommended to use overlapping sets.
Special characters are not allowed for synonyms. For hyphenated words like "COVID-19,"
they are treated the same as "COVID 19," and "space" can be used as a term separator.
Following is the list of special characters not allowed:
ï¾‰

Special character

Symbol

Comma

,

Question mark

?

Colon

:

Semicolon

;

Double quotation mark

"

Single quotation mark

'

Open parenthesis

(

Close parenthesis

)

Open brace

{

Close brace

}

Open bracket

[

Close bracket

]

Hyphen/dash

-

Plus sign

+

Period

.

Forward slash

/

Exclamation mark

!

Asterisk

*

Underscore

_

Ampersand

@

Expand table

Special character

Symbol

Hash

#

Next steps
Last updated on 11/18/2025

Add multiple categories to your FAQ bot
In this tutorial, you learn how to:
ï¼‚ Create a project and tag question answer pairs into distinct categories with metadata
ï¼‚ Create a separate project for each domain
ï¼‚ Create a separate language resource for each domain
When building an FAQ bot, you may encounter use cases that require you to address queries
across multiple domains. Let's say the marketing team at Microsoft wants to build a customer
support bot that answers common user queries on multiple Surface Products. For the sake of
simplicity here, we use two FAQ URLs, Surface Pen

, and Surface Earbuds

to create the

project.

Create project with domain specific metadata
The content authors can use documents to extract question answer pairs or add custom
question answer pairs to the project. In order to group the question and answers into specific
domains or categories, you can add metadata.
For the bot on Surface products, you can take the following steps to create a bot that answers
queries for both product types:
1. Add the following FAQ URLs as sources by selecting Add source > URLs > and then Add
all once you add each of the URLs:
Surface Pen FAQ
Surface Earbuds FAQ

ï Š

2. In this project, we have sets of question-and-answer pairs for two different products. Our
goal is to clearly differentiate between these products so that users can search for

relevant responses within the question and answer sets for a specific product.
In the example, we added a metadata with product as key and surface_pen or surface_earbuds
as values wherever applicable. You can extend this example to extract data on multiple
products and add a different value for each product.

ï Š

1. Now, in order to restrict the system to search for the response across a particular product
you would need to pass that product as a filter in the custom question answering REST
API.
The REST API prediction URL can be retrieved from the Deploy project pane:

ï Š

In the JSON body for the API call, we passed surface_pen as value for the metadata
product. So, the system only looks for the response among the QnA pairs with the same
metadata.
JSON
{
"question": "What is the price?",
"top": 3
},
"answerSpanRequest": {
"enable": true,
"confidenceScoreThreshold": 0.3,
"topAnswersWithSpan": 1
},
"filters": {
"metadataFilter": {
"metadata": [
{
"key": "product",
"value": "surface_pen"

}
]
}
}

You can obtain metadata value based on user input in the following ways:
Explicitly take the domain as input from the user through the bot client. For
instance, you can take the product category as input from the user when the
conversation is initiated.

Implicitly identify domain based on bot context. For instance, in case the previous
question was on a particular Surface product, it can be saved as context by the
client. If the user doesn't specify the product in the next query, you could pass on
the bot context as metadata to the Generate Answer API.

Extract entity from user query to identify domain to be used for metadata filter. You
can use other Foundry Tools such as Named Entity Recognition (NER) and
conversational language understanding for entity extraction.

How large can our projects be?
You can add up to 50000 question answer pairs to a single project. If your data exceeds 50,000
question answer pairs, you should consider splitting the project.

Create a separate project for each domain
You can also create a separate project for each domain and maintain the projects separately.
All APIs require for the user to pass on the project name to make any update to the project or

fetch an answer to the user's question.
When the service receives the user question, you need to pass on the projectName in the REST
API endpoint shown to fetch a response from the relevant project. You can locate the URL in
the Deploy project page under Get prediction URL:
https://southcentralus.cognitiveservices.azure.com/language/:query-knowledgebases?
projectName=Test-Project-English&api-version=2021-10-01&deploymentName=production

Create a separate language resource for each
domain
Let's say the marketing team at Microsoft wants to build a customer support bot that answers
user queries on Surface and Xbox products. They plan to assign distinct teams to access
projects on Surface and Xbox. In this case, you should create two custom question answering
resourcesâ€”one for Surface and another for Xbox. You can define distinct roles for users
accessing the same resource.

Last updated on 11/18/2025

Markdown format supported in answer
text
Custom question answering stores answer text as markdown. There are many flavors of
markdown. In order to make sure the answer text is returned and displayed correctly, use this
reference.
Use the CommonMark

tutorial to validate your markdown. The tutorial has a Try it feature

for quick copy/paste validation.

When to use rich-text editing versus markdown
Use a formatting toolbar for rich-text editing of answers, allowing you, as the author, to quickly
select and format text.
Markdown is a better tool when you need to autogenerate content to create projects to be
imported as part of a CI/CD pipeline or for batch testing.

Supported markdown format
Following is the list of markdown formats that you can use in your answer text.
ï¾‰

Expand table

Purpose

Format

Example markdown

A new line
between two
sentences.

\n\n

How can I create a bot with \n\n custom

Headers from h1
to h6, the

\n# text \n## text \n### text

## Creating a bot \n ...text.... \n###

\n####text \n#####text

Important news\n ...text... \n### Related

question answering?

number of #

Information\n ....text...

denotes which
header. One # is
the h1.

\n# my h1 \n## my h2\n### my h3 \n#### my

Italics

h4 \n##### my h5
*text*

How do I create a bot with *custom question
answering*?

Strong (bold)

**text**

How do I create a bot with **custom
question answering***?

Purpose

Format

Example markdown

URL for link

[text](https://www.my.com)

How do I create a bot with [custom question
answering]
(https://language.cognitive.azure.com/)?

*URL for public

![text]

How can I create a bot with ![custom

image

(https://www.my.com/image.png)

question answering](path-to-your-image.png)

Strikethrough

~~text~~

some ~~questions~~ questions need to be
asked

Bold and italics

***text***

How can I create a ***custom question
answering**** bot?

Bold URL for link

[**text**](https://www.my.com)

How do I create a bot with [**custom
question answering**]
(https://language.cognitive.azure.com/)?

Italics URL for
link

[*text*](https://www.my.com)

How do I create a bot with [*custom
question answering*]
(https://language.cognitive.azure.com/)?

Escape

\*text\*

markdown

How do I create a bot with \*custom
question answering*\*?

symbols
Ordered list

\n 1. item1 \n 1. item2

This is an ordered list: \n 1. List item 1
\n 1. List item 2

The preceding example uses automatic
numbering built into markdown.
This is an ordered list: \n 1. List item 1
\n 2. List item 2

The preceding example uses explicit
numbering.
Unordered list

\n * item1 \n * item2

This is an unordered list: \n * List item 1

or

\n * List item 2

\n - item1 \n - item2

Nested lists

\n * Parent1 \n\t * Child1 \n\t *

This is an unordered list: \n * List item 1

Child2 \n * Parent2

\n\t * Child1 \n\t * Child2 \n * List item
2

\n * Parent1 \n\t 1. Child1 \n\t *
Child2 \n 1. Parent2

This is an ordered nested list: \n 1.
Parent1 \n\t 1. Child1 \n\t 1. Child2 \n 1.

You can nest ordered and unordered
lists together. The tab, \t , indicates

Parent2

Purpose

Format

Example markdown

the indentation level of the child
element.

Custom question answering doesn't process the image in any way. It's the client
application's role to render the image.
If you're adding content through the update or replace project APIs and your content or file
includes HTML tags, be sure to preserve the HTML within your file. This step ensures that all
opening and closing tags are properly converted to their encoded format.
ï¾‰

Expand table

Preserve HTML

Representation in the API request

Representation in KB

Yes

&lt;br&gt;

<br>

Yes

&lt;h3&gt;header&lt;/h3&gt;

<h3>header</h3>

Additionally, CR LF(\r\n) are converted to \n in the KB. LF(\n) is kept as is. If you want to
escape any escape sequence like a \t or \n you can use backslash, for example: '\\r\\n' and '\\t'

Next steps
Import a project

Last updated on 12/15/2025

Format guidelines for custom question
answering
Review these formatting guidelines to get the best results for your content.

Formatting considerations
After you import a file or URL, custom question answering converts and stores your content in
the markdown format

. The conversion process adds new lines in the text, such as \n\n . A

knowledge of the markdown format helps you to understand the converted content and
manage your project content.
If you add or edit your content directly in your project, use markdown formatting to create
rich text content or change the markdown format content that is already in the answer. Custom
question answering supports much of the markdown format to bring rich text capabilities to
your content. However, the client application, such as a chat bot may not support the same set
of markdown formats. It's important to test the client application's display of answers.

Basic document formatting
Custom question answering identifies sections and subsections and relationships in the file
based on visual clues like:
font size
font style
numbering
colors
ï¼— Note
We don't support extraction of images from uploaded documents currently.

Product manuals
A manual is typically guidance material that accompanies a product. It helps the user to set up,
use, maintain, and troubleshoot the product. When custom question answering processes a
manual, it extracts the headings and subheadings as questions and the subsequent content as
answers. See an example here

.

To follow is an example of a manual with an index page, and hierarchical content

ï¼— Note
Extraction works best on manuals that have a table of contents and/or an index page, and
a clear structure with hierarchical headings.

Brochures, guidelines, papers, and other files
Many other types of documents can also be processed to generate question answer pairs,
provided they have a clear structure and layout. These documents include: Brochures,
guidelines, reports, white papers, scientific papers, policies, books, etc. See an example here
To follow is an example of a semi-structured doc, without an index:

.

Unstructured document support
Custom question answering now supports unstructured documents. A document that doesn't
have its content organized in a hierarchical manner, is missing a set structure or has its content
free flowing can be considered as an unstructured document.
To follow is an example of an unstructured PDF document:

ï¼— Note
QnA pairs aren't extracted in the "Edit sources" tab for unstructured sources.

ï¼‰ Important
Support for unstructured file/content is available only in custom question answering.

Structured custom question answering document
The format for structured question-answers in DOC files is in the form of alternating questions
and answers per line. It's one question per line followed by its answer in the following line, as
shown:
text
Question1
Answer1
Question2
Answer2

To follow is an example of a structured custom question answering word document:

Structured TXT, TSV and XLS Files
Custom question answering in the form of structured .txt, .tsv or .xls files can also be uploaded
to custom question answering to create or augment a project. These files can either be plain
text, or can have content in RTF or HTML. Question answer pairs have an optional metadata
field that can be used to group question answer pairs into categories.
ï¾‰

Expand table

Question

Answer

Metadata (one key: One value)

Question1

Answer1

Key1:Value1 | Key2:Value2

Question2

Answer2

Key:Value

Any other columns in the source file are ignored.

Structured data format through import
Importing a project replaces the content of the existing project. Import requires a structured
.tsv file that contains data source information. This information helps group the questionanswer pairs and attributes them to a particular data source. Question answer pairs have an
optional metadata field that can be used to group question answer pairs into categories. The
import format needs to be similar to the exported knowledgebase format.
ï¾‰

Expand table

Question

Answer

Source

Metadata (one key: one value)

QnaId

Question1

Answer1

Url1

Key1:Value1 | Key2:Value2

QnaId 1

Question2

Answer2

Editorial

Key:Value

QnaId 2

Multi-turn document formatting
Use headings and subheadings to denote hierarchy. For example, You can h1 to denote
the parent question answer and h2 to denote the question answer that should be taken
as prompt. Use small heading size to denote subsequent hierarchy. Don't use style, color,
or some other mechanism to imply structure in your document, custom question
answering doesn't extract the multi-turn prompts.
First character of heading must be capitalized.
Don't end a heading with a question mark, ? .
Sample documents:
Surface Pro (docx)
Contoso Benefits (docx)
Contoso Benefits (pdf)

FAQ URLs
Custom question answering can support FAQ web pages in three different forms:

Plain FAQ pages
FAQ pages with links
FAQ pages with a Topics Homepage

Plain FAQ pages
This type is the most common type of FAQ page, in which the answers immediately follow the
questions in the same page.

FAQ pages with links
In this type of FAQ page, questions are aggregated together and are linked to answers that are
either in different sections of the same page, or in different pages.
To follow is an example of an FAQ page with links in sections that are on the same page:

Parent Topics page links to child answers pages
This type of FAQ has a Topics page where each subject is linked to a corresponding set of
questions and answers on a different page. Question answer crawls all the linked pages to
extract the corresponding questions & answers.

To follow is an example of a Topics page with links to FAQ sections in different pages.

Support URLs
Custom question answering works with semi-structured support web pages. These web pages
include articles that explain how to do a task, how to solve a problem, or what best practices to
follow. Extraction works best when the content has a clear structure with headings.
ï¼— Note
Extraction for support articles is a new feature and is in early stages. It works best for
simple pages that are well structured, and don't contain complex headers/footers.

Import and export project
To migrate your Azure Language Studio project to your Microsoft Foundry project, link your
existing Azure Language resource as a Connected Resource within your Foundry project's
Management Center . For more information, see Connect Foundry Tools to a Foundry project

Next steps
Tutorial: Create an FAQ bot

Last updated on 12/13/2025

What is custom text classification?
Custom text classification is one of the custom features offered by Azure Language in Foundry
Tools. It's a cloud-based API service that applies machine-learning intelligence to enable you to
build custom models for text classification tasks.
Custom text classification enables users to build custom AI models to classify text into custom
classes predefined by the user. By creating a custom text classification project, developers can
iteratively label data, train, evaluate, and improve model performance before making it
available for consumption. The quality of the labeled data greatly impacts model performance.
To simplify building and customizing your model, the service offers a unified platform for
building, managing, and deploying AI solutions that can be accessed through the Microsoft
Foundry

. You can easily get started with the service by following the steps in this quickstart.

Custom text classification supports two types of projects:
Single label classification - you can assign a single class for each document in your
dataset. For example, a movie script could only be classified as "Romance" or "Comedy."
Multi label classification - you can assign multiple classes for each document in your
dataset. For example, a movie script could be classified as "Comedy" or "Romance" and
"Comedy."
This documentation contains the following article types:
Quickstarts are getting-started instructions to guide you through making requests to the
service.
Concepts provide explanations of the service functionality and features.
How-to guides contain instructions for using the service in more specific or customized
ways.

Example usage scenarios
Custom text classification can be used in multiple scenarios across various industries:

Automatic emails or ticket triage
Support centers of all types receive a high volume of emails or tickets containing unstructured,
freeform text, and attachments. Timely review, acknowledgment, and routing to subject matter
experts within internal teams is critical. Email triage at this scale requires people to review and
route to the right departments, which takes time and resources. Custom text classification can
be used to analyze incoming text, and triage and categorize the content to be automatically
routed to the relevant departments for further action.

Knowledge mining to enhance/enrich semantic search
Search is foundational to any app that surfaces text content to users. Common scenarios
include catalog or document searches, retail product searches, or knowledge mining for data
science. Many enterprises across various industries are seeking to build a rich search
experience over private, heterogeneous content, which includes both structured and
unstructured documents. As a part of their pipeline, developers can use custom text
classification to categorize their text into classes that are relevant to their industry. The
predicted classes can be used to enrich the indexing of the file for a more customized search
experience.

Project development lifecycle
Creating a custom text classification project typically involves several different steps.

ï Š

Follow these steps to get the most out of your model:
1. Define your schema: Know your data and identify the classes you want differentiate
between, to avoid ambiguity.
2. Label your data: The quality of data labeling is a key factor in determining model
performance. Documents that belong to the same class should always have the same
class, if you have a document that can fall into two classes use Multi label classification
projects. Avoid class ambiguity, make sure that your classes are clearly separable from
each other, especially with single label classification projects.
3. Train the model: Your model starts learning from your labeled data.
4. View the model's performance: View the evaluation details for your model to determine
how well it performs when introduced to new data.
5. Deploy the model: Deploying a model makes it available for use via the Analyze API

.

6. Classify text: Use your custom model for custom text classification tasks.

Reference documentation and code samples
As you use custom text classification, see the following reference documentation and samples
for Azure Language in Foundry Tools:

Development option /

Reference

language

documentation

REST APIs (Authoring)

REST API

ï¾‰

Expand table

C# samples - Single label classification

C# samples

Samples

documentation
REST APIs (Runtime)

REST API
documentation

C# (Runtime)

C# documentation

- Multi label classification
Java (Runtime)

Java documentation

Java Samples - Single label classification

Java

Samples - Multi label classification
JavaScript (Runtime)

Python (Runtime)

JavaScript

JavaScript samples - Single label classification

documentation

JavaScript samples - Multi label classification

Python

Python samples - Single label classification

documentation

samples - Multi label classification

Python

Responsible AI
An AI system includes not only the technology, but also the people who use it, the people
affected by it, and the deployment environment. Read the transparency note for custom text
classification to learn about responsible AI use and deployment in your systems.
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Next steps
Use the quickstart article to start using custom text classification.

As you go through the project development lifecycle, review the glossary to learn more
about the terms used throughout the documentation for this feature.
Remember to view the service limits for information such as regional availability.

Last updated on 12/15/2025

Quickstart: Custom text classification REST
API
Use this article to get started with creating a custom text classification project where you can train
custom models for text classification. A model is AI software trained to do a certain task. For this
system, the models classify text, and are trained by learning from tagged data.
Custom text classification supports two types of projects:
Single label classification - you can assign a single class for each document in your dataset.
For example, a movie script could only be classified as "Romance" or "Comedy."
Multi label classification - you can assign multiple classes for each document in your dataset.
For example, a movie script could be classified as "Comedy" or "Romance" and "Comedy."
In this quickstart, you can use the sample datasets provided to build a multi label classification to
classify movie scripts into one or more categories. Alternatively, you can use single label
classification dataset to classify abstracts of scientific papers into one of the defined domains.

Prerequisites
Azure subscription - Create one for free

.

Create a new Azure Language in Foundry Tools
resource and Azure storage account
Before you can use custom text classification, you'll need to create a Language resource, which will
give you the credentials that you need to create a project and start training a model. You'll also need
an Azure storage account, where you can upload your dataset used in building your model.
ï¼‰ Important
To get started quickly, we recommend creating a new Language resource using the steps
provided in this article, which lets you create Azure Language resource, and create and/or
connect a storage account at the same time, which is easier than doing it later.
If you have a pre-existing resource that you'd like to use, you need to connect it to storage
account.

Create a new resource from the Azure portal
1. Go to the Azure portal

to create a new Azure Language in Foundry Tools resource.

2. In the window that appears, select Custom text classification & custom named entity
recognition from the custom features. Select Continue to create your resource at the bottom
of the screen.

ï Š

3. Create a Language resource with following details.
ï¾‰

Expand table

Name

Required value

Subscription

Your Azure subscription.

Resource
group

A resource group that will contain your resource. You can use an existing one, or create
a new one.

Region

One of the supported regions. For example "West US 2".

Name

A name for your resource.

Pricing tier

One of the supported pricing tiers. You can use the Free (F0) tier to try the service.

If you get a message saying "your login account is not an owner of the selected storage account's
resource group", your account needs to have an owner role assigned on the resource group
before you can create a Language resource. Contact your Azure subscription owner for
assistance.
You can determine your Azure subscription owner by searching your resource group
following the link to its associated subscription. Then:
a. Select the Access Control (IAM) tab
b. Select Role assignments

and

c. Filter by Role:Owner.
4. In the Custom text classification & custom named entity recognition section, select an
existing storage account or select New storage account. Note that these values are to help you
get started, and not necessarily the storage account values youâ€™ll want to use in production
environments. To avoid latency during building your project connect to storage accounts in the
same region as your Language resource.
ï¾‰

Storage account value

Recommended value

Storage account name

Any name

Storage account type

Standard LRS

Expand table

5. Make sure the Responsible AI Notice is checked. Select Review + create at the bottom of the
page.

Upload sample data to blob container
After you create an Azure storage account and connected it to your Language resource, you need to
upload the documents from the sample dataset to the root directory of your container. These
documents are used to train your model.
Multi label classification

1. Download the sample dataset for multi label classification projects .
2. Open the .zip file, and extract the folder containing the documents.
The provided sample dataset contains about 200 documents, each of which is a summary for a
movie. Each document belongs to one or more of the following classes:
"Mystery"
"Drama"
"Thriller"
"Comedy"
"Action"

Azure portal

1. In the Azure portal

, navigate to the storage account you created, and select it by selecting

Storage accounts and typing your storage account name into Filter for any field.
if your resource group doesn't show up, make sure the Subscription equals filter is set to All.
2. In your storage account, select Containers from the left menu, located below Data storage. On
the screen that appears, select + Container. Give the container the name example-data and
leave the default Public access level.

ï Š

3. After your container is created, select it. Then select Upload button to select the .txt and
.json files you downloaded earlier.

ï Š

Get your resource keys and endpoint
Go to your resource overview page in the Azure portal
From the menu on the left side, select Keys and Endpoint. The endpoint and key are used for
API requests.

ï Š

Create a custom text classification project
Once your resource and storage container are configured, create a new custom text classification
project. A project is a work area for building your custom ML models based on your data. Your
project can only be accessed by you and others who have access to Azure Language resource being
used.

Trigger import project job
Submit a POST request using the following URL, headers, and JSON body to import your labels file.
Make sure that your labels file follow the accepted format.
If a project with the same name already exists, the data of that project is replaced.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}/:import?api-version=
{API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

The name for your project. This value is casesensitive.

myProject

{PROJECTNAME}

Placeholder

Value

Example

{API-

The version of the API you're calling. The value

2022-05-01

VERSION}

referenced is for the latest version released.
Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request. Replace the placeholder values with your own values.
Multi label classification

JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectName": "{PROJECT-NAME}",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectKind": "customMultiLabelClassification",
"description": "Trying out custom multi label text classification",
"language": "{LANGUAGE-CODE}",
"multilingual": true,
"settings": {}
},
"assets": {
"projectKind": "customMultiLabelClassification",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
],
"documents": [
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",

"dataset": "{DATASET}",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
]
},
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"classes": [
{
"category": "Class2"
}
]
}
]
}
}

ï¾‰

Key

Placeholder

Value

Example

api-version

{API-VERSION}

The version
of the API

2022-05-01

you're
calling. The
version
used here
must be
the same
API version
in the URL.
Learn more
about
other
available
API
versions
projectName

{PROJECT-NAME}

The name
of your
project.
This value
is casesensitive.

myProject

Expand table

Key

Placeholder

Value

Example

projectKind

customMultiLabelClassification

Your
project

customMultiLabelClassification

kind.
language

{LANGUAGE-CODE}

A string
specifying

en-us

the
language
code for
the
documents
used in
your
project. If
your
project is a
multilingual
project,
choose the
language
code for
most of the
documents.
See
language
support to
learn more
about
multilingual
support.
multilingual

true

A boolean
value that
enables
you to have
documents
in multiple
languages
in your
dataset and
when your
model is
deployed
you can
query the
model in
any
supported
language
(not

true

Key

Placeholder

Value

Example

necessarily
included in
your
training
documents.
See
language
support to
learn more
about
multilingual
support.
storageInputContainerName

{CONTAINER-NAME}

The name
of your
Azure
storage
container

myContainer

for your
uploaded
documents.
classes

[]

Array
containing
all the

[]

classes you
have in the
project.
documents

[]

Array
containing
all the
documents

[]

in your
project and
what the
classes
labeled for
this
document.
location

{DOCUMENT-NAME}

The
location of
the
documents
in the
storage
container.
Since all
the
documents

doc1.txt

Key

Placeholder

Value

Example

are in the
root of the
container,
it should
be the
document
name.
dataset

{DATASET}

The test set
to which

Train

this
document
goes to
when split
before
training.
See How to
train a
model.
Possible
values for
this field
are Train
and Test .

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOB-ID}?
api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You use this URL to

get the import job status.
Possible error scenarios for this request:
The selected resource doesn't have proper permissions for the storage account.
The storageInputContainerName specified doesn't exist.
Invalid language code is used, or if the language code type isn't string.
multilingual value is a string and not a boolean.

Get import job Status

Use the following GET request to get the status of your importing your project. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOB-ID}?
api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com
myProject

NAME}

The name of your project. This value is casesensitive.

{JOB-ID}

The ID for locating your model's training

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

status. This value is in the location header
value you received in the previous step.
{API-

The version of the API you're calling. The value

VERSION}

referenced is for the latest version released.

2022-05-01

Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Train your model
Typically after you create a project, you go ahead and start tagging the documents you have in the
container connected to your project. For this quickstart, you have imported a sample tagged dataset
and initialized your project with the sample JSON tags file.

Start training your model

After your project has been imported, you can start training your model.
Submit a POST request using the following URL, headers, and JSON body to submit a training job.
Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:train?api-version=
{API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is case-

myProject

NAME}

sensitive.

{API-

The version of the API you're calling. The value
referenced is for the latest version released.

VERSION}

2022-05-01

Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in your request body. The model will be given the {MODEL-NAME} once
training is complete. Only successful training jobs will produce models.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 80,
"testingSplitPercentage": 20

}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-NAME}

The model name that is assigned to your model once

myModel

trained successfully.
trainingConfigVersion

{CONFIG-

This is the model version used to train the model.

2022-05-01

Option to split your data across training and testing

{}

VERSION}

evaluationOptions

sets.
kind

percentage

Split methods. Possible values are percentage or

percentage

manual . See How to train a model for more

information.
trainingSplitPercentage

80

Percentage of your tagged data to be included in the

80

training set. Recommended value is 80 .
testingSplitPercentage

20

Percentage of your tagged data to be included in the

20

testing set. Recommended value is 20 .

ï¼— Note
The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set to
percentage and the sum of both percentages should be equal to 100.

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOB-ID}?
api-version={API-VERSION}

{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this URL
to get the training status.

Get training job status
Training could take sometime between 10 and 30 minutes. You can use the following request to
keep polling the status of the training job until it's successfully completed.

Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOB-ID}?
api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com
myProject

NAME}

The name of your project. This value is casesensitive.

{JOB-ID}

The ID for locating your model's training

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

status. This value is in the location header
value you received in the previous step.
{API-

The version of the API you're calling. The value

VERSION}

referenced is for the latest version released.

2022-05-01

For more information, see Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"result": {
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",

"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},
"jobId": "{JOB-ID}",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

Deploy your model
Generally after training a model you would review it's evaluation details and make improvements if
necessary. In this quickstart, you will just deploy your model, and make it available for you to try in
Language Studio, or you can call the prediction API .

Submit deployment job
Submit a PUT request using the following URL, headers, and JSON body to submit a deployment
job. Replace the placeholder values with your own values.
rest
{Endpoint}/language/authoring/analyzetext/projects/{projectName}/deployments/{deploymentName}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is case-

myProject

NAME}

sensitive.

{DEPLOYMENT-

The name of your deployment. This value is

NAME}

case-sensitive.

{API-VERSION}

The version of the API you're calling. The
value referenced is for the latest version

https://<your-custom-

staging

2022-05-01

Placeholder

Value

Example

released. Learn more about other available
API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in the body of your request. Use the name of the model you to assign to the
deployment.
JSON
{
"trainedModelLabel": "{MODEL-NAME}"
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

trainedModelLabel

{MODEL-

The model name that is assigned to your deployment. You can

myModel

NAME}

only assign successfully trained models. This value is casesensitive.

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this URL
to get the deployment status.

Get deployment job status

Use the following GET request to query the status of the deployment job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com
myProject

NAME}

The name of your project. This value is casesensitive.

{DEPLOYMENT-

The name of your deployment. This value is

staging

NAME}

case-sensitive.

{JOB-ID}

The ID for locating your model's training

{PROJECT-

Expand table

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

status. It's in the location header value you
received in the previous step.
{API-VERSION}

The version of the API you're calling. The
value referenced is for the latest version

2022-05-01

released. Learn more about other available
API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the
status parameter changes to "succeeded". You should get a 200 code to indicate the success of the
request.
JSON

{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Classify text
After your model is deployed successfully, you can start using it to classify your text via Prediction
API

. In the sample dataset you downloaded earlier you can find some test documents that you can

use in this step.

Submit a custom text classification task
Use this POST request to start a text classification task.
rest
{ENDPOINT}/language/analyze-text/jobs?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{API-

The version of the API you're calling. The value

2022-05-01

VERSION}

referenced is for the latest version released.
For more information, see Model lifecycle.

Headers
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

Your key that provides access to this API.

Body
Multi label classification

Expand table

JSON
{
"displayName": "Classifying documents",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "{LANGUAGE-CODE}",
"text": "Text1"
},
{
"id": "2",
"language": "{LANGUAGE-CODE}",
"text": "Text2"
}
]
},
"tasks": [
{
"kind": "CustomMultiLabelClassification",
"taskName": "Multi Label Classification",
"parameters": {
"projectName": "{PROJECT-NAME}",
"deploymentName": "{DEPLOYMENT-NAME}"
}
}
]
}

ï¾‰

Key

Placeholder

Value

Example

displayName

{JOB-NAME}

Your job name.

MyJobName

documents

[{},{}]

List of documents to

[{},{}]

run tasks on.
id

{DOC-ID}

Document name or ID.

doc1

language

{LANGUAGE-CODE}

A string specifying the
language code for the

en-us

document. If this key
isn't specified, the
service will assume the
default language of the
project that was
selected during project
creation. See language
support for a list of

Expand table

Key

Placeholder

Value

Example

supported language
codes.
text

{DOC-TEXT}

tasks

Document task to run
the tasks on.

Lorem ipsum dolor sit amet

List of tasks we want to

[]

perform.
taskName

CustomMultiLabelClassification

{PROJECT-NAME}

The name for your

myProject

project. This value is
case-sensitive.

name

deployment-

CustomMultiLabelClassification

List of parameters to
pass to the task.

parameters

project-

The task name

{DEPLOYMENT-NAME}

The name of your

prod

deployment. This value
is case-sensitive.

name

Response
You receive a 202 response indicating success. In the response headers, extract operation-location .
operation-location is formatted like this:
{ENDPOINT}/language/analyze-text/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to query the task completion status and get the results when task is completed.

Get task results
Use the following GET request to query the status/results of the text classification task.
rest
{ENDPOINT}/language/analyze-text/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

Placeholder

Value

Example

{API-

The version of the API you're calling. The
value referenced is for the latest released

2022-05-01

VERSION}

model version version.

Headers
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

Your key that provides access to this API.

Response body
The response will be a JSON document with the following parameters.
Multi label classification

JSON
{
"createdDateTime": "2021-05-19T14:32:25.578Z",
"displayName": "MyJobName",
"expirationDateTime": "2021-05-19T14:32:25.578Z",
"jobId": "xxxx-xxxxxx-xxxxx-xxxx",
"lastUpdateDateTime": "2021-05-19T14:32:25.578Z",
"status": "succeeded",
"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "customMultiClassificationTasks",
"taskName": "Classify documents",
"lastUpdateDateTime": "2020-10-01T15:01:03Z",
"status": "succeeded",
"results": {
"documents": [
{
"id": "{DOC-ID}",
"classes": [
{
"category": "Class_1",
"confidenceScore": 0.0551877357
}
],

Expand table

"warnings": []
}
],
"errors": [],
"modelVersion": "2020-04-01"
}
}
]
}
}

Clean up resources
When you no longer need your project, you can delete it with the following DELETE request. Replace
the placeholder values with your own values.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This value is casesensitive.

myProject

The version of the API you're calling. The value
referenced is for the latest version released.

2022-05-01

Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success, which means your
project is deleted. A successful call results with an Operation-Location header used to check the
status of the job.

Next steps
After you create a custom text classification model, you can:
Use the runtime API to classify text
When you start to create your own custom text classification projects, use the how-to articles to
learn more about developing your model in greater detail:
Data selection and schema design
Tag data
Train a model
View model evaluation

Last updated on 12/17/2025

Language support for custom text
classification
Use this article to learn about the languages currently supported by custom text classification
feature.

Multi-lingual option
With custom text classification, you can train a model in one language and use to classify
documents in another language. This feature is useful because it helps save time and effort.
Instead of building separate projects for every language, you can handle multi-lingual dataset
in one project. Your dataset doesn't have to be entirely in the same language but you should
enable the multi-lingual option for your project while creating or later in project settings. If you
notice your model performing poorly in certain languages during the evaluation process,
consider adding more data in these languages to your training set.
You can train your project entirely with English documents, and query it in: French, German,
Mandarin, Japanese, Korean, and others. Custom text classification makes it easy for you to
scale your projects to multiple languages by using multilingual technology to train your
models.
Whenever you identify that a particular language is not performing as well as other languages,
you can add more documents for that language in your project. When you introduce more
documents for that language to the model, it is introduced to more of the syntax of that
language, and learns to predict it better.
You aren't expected to add the same number of documents for every language. You should
build the majority of your project in one language, and only add a few documents in languages
you observe aren't performing well. If you create a project that is primarily in English, and start
testing it in French, German, and Spanish, you might observe that German doesn't perform as
well as the other two languages. In that case, consider adding 5% of your original English
documents in German, train a new model and test in German again. You should see better
results for German queries. The more labeled documents you add, the more likely the results
are going to get better.
When you add data in another language, you shouldn't expect it to negatively affect other
languages.

Languages supported by custom text classification
Custom text classification supports .txt files in the following languages:

ï¾‰

Language

Language Code

Afrikaans

af

Amharic

am

Arabic

ar

Assamese

as

Azerbaijani

az

Belarusian

be

Bulgarian

bg

Bengali

bn

Breton

br

Bosnian

bs

Catalan

ca

Czech

cs

Welsh

cy

Danish

da

German

de

Greek

el

English (US)

en-us

Esperanto

eo

Spanish

es

Estonian

et

Basque

eu

Persian

fa

Finnish

fi

French

fr

Western Frisian

fy

Expand table

Language

Language Code

Irish

ga

Scottish Gaelic

gd

Galician

gl

Gujarati

gu

Hausa

ha

Hebrew

he

Hindi

hi

Croatian

hr

Hungarian

hu

Armenian

hy

Indonesian

id

Italian

it

Japanese

ja

Javanese

jv

Georgian

ka

Kazakh

kk

Khmer

km

Kannada

kn

Korean

ko

Kurdish (Kurmanji)

ku

Kyrgyz

ky

Latin

la

Lao

lo

Lithuanian

lt

Latvian

lv

Malagasy

mg

Language

Language Code

Macedonian

mk

Malayalam

ml

Mongolian

mn

Marathi

mr

Malay

ms

Burmese

my

Nepali

ne

Dutch

nl

Norwegian (Bokmal)

nb

Odia

or

Punjabi

pa

Polish

pl

Pashto

ps

Portuguese (Brazil)

pt-br

Portuguese (Portugal)

pt-pt

Romanian

ro

Russian

ru

Sanskrit

sa

Sindhi

sd

Sinhala

si

Slovak

sk

Slovenian

sl

Somali

so

Albanian

sq

Serbian

sr

Sundanese

su

Language

Language Code

Swedish

sv

Swahili

sw

Tamil

ta

Telugu

te

Thai

th

Filipino

tl

Turkish

tr

Uyghur

ug

Ukrainian

uk

Urdu

ur

Uzbek

uz

Vietnamese

vi

Xhosa

xh

Yiddish

yi

Chinese (Simplified)

zh-hans

Zulu

zu

Next steps
Custom text classification overview
Service limits

Last updated on 12/15/2025

Frequently asked questions
Find answers to commonly asked questions about concepts, and scenarios related to custom
text classification in Azure Language in Foundry Tools.

How do I get started with the service?
See the quickstart to quickly create your first project, or view how to create projects.

What are the service limits?
See the service limits article.

Which languages are supported?
See the language support article.

How many tagged files are needed?
Generally, diverse and representative tagged data leads to better results, given that the
tagging is done precisely, consistently and completely. There's no set number of tagged classes
that make every model perform well. Performance is highly dependent on your schema and the
ambiguity of your schema. Ambiguous classes need more tags. Performance also depends on
the quality of your tagging. The recommended number of tagged instances per class is 50.

Training is taking a long time, is it to be expected?
The training process can take some time. As a rough estimate, the expected training time for
files with a combined length of 12,800,000 chars is 6 hours.

How do I build my custom model
programmatically?
You can use the REST APIs

to build your custom models. Follow this quickstart to get started

with creating a project and creating a model through APIs for examples of how to call the
Authoring API.

When you're ready to start using your model to make predictions, you can use the REST API, or
the client library.

What is the recommended CI/CD process?
You can train multiple models on the same dataset within the same project. After you train
your model successfully, you can view its evaluation. You can deploy and test your model
within Language studio . You can add or remove tags from your data and train a new model
and test it as well. View service limitsto learn about maximum number of trained models with
the same project. When you tag your data, you can determine how your dataset is split into
training and testing sets.

Does a low or high model score guarantee bad or
good performance in production?
Model evaluation may not always be comprehensive, depending on:
If the test set is too small, the good/bad scores aren't representative of model's actual
performance. Also if a specific class is missing or under-represented in your test set it
affects model performance.
Data diversity if your data only covers few scenarios/examples of the text you expect in
production, your model isn't exposed to all possible scenarios and might perform poorly
on the scenarios it isn't trained on.
Data representation if the dataset used to train the model isn't representative of the data
that would be introduced to the model in production, model performance is affected
greatly.
See the data selection and schema design article.

How do I improve model performance?
View the model confusion matrix, if you notice that a certain class is frequently classified
incorrectly, consider adding more tagged instances for this class. If you notice that two
classes are frequently classified as each other, it means the schema is ambiguous,
consider merging them both into one class for better performance.
Examine Data distribution If one of the classes has many more tagged instances than the
others, your model may be biased towards this class. Add more data to the other classes
or remove most of the examples from the dominating class.
Review the data selection and schema design article.

Review your test set to see predicted and tagged classes side-by-side. Then you can get a
better idea of your model performance, and decide if any changes in the schema or the
tags are necessary.

When I retrain my model, I get different results.
Why?
When you tag your data, you can determine how your dataset is split into training and
testing sets. You can also have your data split randomly into training and testing sets.
However, there's no guarantee that the reflected model evaluation is on the same test set,
so results aren't comparable.
If you're retraining the same model, your test set is the same, but you might notice a
slight change in predictions made by the model. It's because the trained model isn't
robust enough, which is a factor of how representative and distinct your data is, and the
quality of your tagged data.

How do I get predictions in different languages?
First, you need to enable the multilingual option when creating your project or you can enable
it later from the project settings page. After you train and deploy your model, you can start
querying it in multiple languages. You may get varied results for different languages. To
improve the accuracy of any language, add more tagged instances to your project in that
language to introduce the trained model to more syntax of that language. See language
support.

I trained my model, but I can't test it
You need to deploy your model before you can test it.

How do I use my trained model to make
predictions?
After deploying your model, you call the prediction API, using either the REST API or client
libraries.

Data privacy and security

For more information, see Data, privacy, and security for Azure Language in Foundry Tools.

How to clone my project?
To clone your project, you need to use the export API to export the project assets, and then
import them into a new project. See REST APIs

Next steps
Custom text classification overview
Quickstart

Last updated on 11/18/2025

reference for both operations.

Use cases for custom text classification
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a Transparency Note?
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance. Microsoft's Transparency
Notes are intended to help you understand how our AI technology works, the choices system
owners can make that influence system performance and behavior, and the importance of
thinking about the whole system, including the technology, the people, and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who will use or be affected by your system.
Microsoft's Transparency Notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Microsoft AI Principles

.

Introduction to custom text classification
Custom text classification is a cloud-based API service that applies machine-learning
intelligence to enable you to build custom models for text classification tasks.
Custom text classification supports two types of projects:
Single label classification: You assign only one label for each file in your dataset. For
example, if a file is a movie script, it could only be classified as "Action," "Thriller," or
"Romance."
Multiple label classification: You assign multiple labels for each file in your dataset. For
example, if a file is a movie script, it could be classified as "Action" or "Action" and
"Thriller."

The basics of custom text classification
Custom text classification is offered as part of the custom features within Azure Language in
Foundry Tools. This feature enables its users to build custom AI models to classify text into

custom categories predefined by the user. By creating a custom text classification project,
developers can iteratively tag data and train, evaluate, and improve model performance before
they make it available for consumption. The quality of the tagged data greatly affects model
performance.
To simplify building and customizing your model, the service offers a custom web portal that
can be accessed through the Language Studio

. You can easily get started with the service by

following the steps in this quickstart.

Custom text classification terminology
The following terms are commonly used within custom text classification:
ï¾‰

Expand table

Term

Definition

Project

A project is a work area for building your custom AI models based on your data. Your project
can only be accessed by you and others who have contributor access to the Azure resource
being used. Within a project, you can tag data, build models, evaluate and improve them where
necessary, and eventually deploy a model to be ready for consumption. You can build multiple
models within your project on the same dataset.

Model

A model is an object that's trained to do a certain task. For this system, the models classify text.
Models are trained by learning from tagged data.

Class

A class is a user-defined category that indicates the overall classification of the text. Developers
tag their data with their assigned classes before they pass it to the model for training.

Example use cases for custom text classification
Custom text classification can be used in multiple scenarios across a variety of industries. Some
examples are:
Automatic emails or ticket triaging: Support centers of all types receive a high volume of
emails or tickets containing unstructured, freeform text and attachments. Timely review,
acknowledgment, and routing to subject matter experts within internal teams is critical.
Email triage at this scale requires people to review and route to the right departments,
which takes time and resources. Custom text classification can be used to analyze
incoming text, and triage and categorize the content to be automatically routed to the
relevant departments for further action.
Knowledge mining to enhance and enrich semantic search: Search is foundational to
any app that surfaces text content to users. Common scenarios include catalog or

document searches, retail product searches, or knowledge mining for data science. Many
enterprises across various industries are seeking to build a rich search experience over
private, heterogeneous content, which includes both structured and unstructured
documents. As a part of their pipeline, developers can use custom text classification to
categorize their text into classes that are relevant to their industry. The predicted classes
can be used to enrich the indexing of the file for a more customized search experience.

Considerations when you choose a use case
Avoid using custom text classification for decisions that might have serious adverse
impacts. Include human review of decisions that have the potential for serious impacts on
individuals. For example, identifying whether to accept or reject an insurance claim based
on a user's description of an incident.
Avoid creating classes that are ambiguous and not representative. When you design
your schema, avoid classes that are so similar to each other that there might be difficulty
differentiating them from each other. For example, if you're classifying movie scripts,
avoid creating a class for romance, comedy, and rom-com. Instead, consider using a
multiple-label classification model with romance and comedy classes. Then, for rom-com
movies, assign both classes.
Legal and regulatory considerations: Organizations need to evaluate potential specific
legal and regulatory obligations when using any AI services and solutions, which may not
be appropriate for use in every industry or scenario. Additionally, AI services or solutions
are not designed for and may not be used in ways prohibited in applicable terms of
service and relevant codes of conduct.

Next steps
Introduction to custom text classification
Characteristics and limitations for using custom text classification
Data privacy and security
Guidance for integration and responsible use
Microsoft AI principles

Last updated on 08/17/2025

Guidance for integration and responsible
use with custom text classification
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft works to help customers responsibly develop and deploy solutions by using custom
text classification. Our principled approach upholds personal agency and dignity by
considering the AI system's:
Fairness, reliability, and safety.
Privacy and security.
Inclusiveness.
Transparency.
Human accountability.
These considerations reflect our commitment to developing responsible AI.

General guidelines for integration and responsible
use principles
When you get ready to integrate and responsibly use AI-powered products or features, the
following activities help to set you up for success:
Understand what it can do. Fully assess the potential of any AI system to understand its
capabilities and limitations. Understand how it will perform in your particular scenario and
context by thoroughly testing it with real-life conditions and data.
Respect an individual's right to privacy. Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Obtain legal review. Obtain appropriate legal advice to review your solution, particularly
if you'll use it in sensitive or high-risk applications. Understand what restrictions you
might need to work within and your responsibility to resolve any issues that might come
up in the future.
Have a human in the loop. Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. Ensure constant human oversight of the AI-powered

product or feature. Maintain the role of humans in decision making. Make sure you can
have real-time human intervention in the solution to prevent harm and manage situations
when the AI system doesnâ€™t perform as expected.
Maintain security. Ensure your solution is secure and has adequate controls to preserve
the integrity of your content and prevent unauthorized access.
Build trust with affected stakeholders. Communicate the expected benefits and potential
risks to affected stakeholders. Help people understand why the data is needed and how
the use of the data will lead to their benefit. Describe data handling in an understandable
way.
Create a customer feedback loop. Provide a feedback channel that allows users and
individuals to report issues with the service after it's deployed. After you've deployed an
AI-powered product or feature, it requires ongoing monitoring and improvement. Be
ready to implement any feedback and suggestions for improvement. Establish channels
to collect questions and concerns from affected stakeholders. People who might be
directly or indirectly affected by the system include employees, visitors, and the general
public. For example, consider using:
Feedback features built into app experiences.
An easy-to-remember email address for feedback.
Anonymous feedback boxes placed in semi-private spaces.
Knowledgeable representatives in the lobby.
Always plan to have the user confirm an action before being processed. Plan to have
your user confirm an action before being processed by your client application to avoid
incorrect responses that might come from the custom text classification models. For
example, suppose your custom text classification model is integrated in an insurance
claim approval system to classify nonurgent and urgent cases. Have someone on your
side confirm the model's prediction before processing it.
Always plan to have a correction path for the user. After a certain action is taken by the
client application, show a confirmation message to the user of the action that was
processed. Plan that the response of the custom text classification model might not be
accurate and that your user might end up in an error state. In that case, always have a
fallback plan or a correction path that the user can use to exit from that state.

Next steps
Introduction to custom text classification
Custom text classification Transparency Note
Microsoft AI principles

Last updated on 08/17/2025

Characteristics and limitations for using
custom text classification
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Performance of custom text classification models will vary based on the scenario and input
data. The following sections are designed to help you understand key concepts about
performance and evaluation of custom text classification models.

Performance evaluation metrics
Reviewing model evaluation is an important step in the custom text classification model's
development life cycle. It helps you determine how well your model is performing and to
gauge the expected performance when the model is used in production.
In the process of building a model, training and testing sets are either defined during tagging
or chosen at random during training. Either way, the training and testing sets are essential for
training and evaluating custom text classification models. The training set is used to train the
custom machine learning model. The test set is used as a blind set to evaluate model
performance.
The model evaluation process is triggered after training is completed successfully. The
evaluation process takes place by using the trained model to predict user-defined classes for
files in the test set and compare the predictions with the provided data tags (ground truth).
The results are returned to you to review the model's performance.
The first step in calculating the model's evaluation is categorizing the predicted labels in one of
the following categories: true positives, false positives, or false negatives. The following table
further explains these terms.
ï¾‰

Expand table

Term

Correct/Incorrect

Definition

Example

True

Correct

The model predicts a

For a comedy movie script, the class comedy is

class, and it's the same
as the text has been
tagged.

predicted.

positive

Term

Correct/Incorrect

Definition

Example

False

Incorrect

The model predicts the

For a comedy movie script, the class drama is

wrong class for a
specific text.

predicted.

The system doesn't
return a result when a
human judge would

For a drama movie script, the class comedy is

return a correct result.

comedy is predicted, but the class romance is

positive

False
negative

Incorrect

predicted. In multiclassification only, for a
romance and comedy movie script, the class
not predicted.

For single-label classification, it is not possible to have a false negative, because single-label
classification models will always predict one class for each file. For a multi-label classification it
is counted as both a false negative and false positive, false negative for the tagged class and
false positive for the predicted class.
The preceding categories are then used to calculate precision, recall and an F1 score. These
metrics are provided as part of the service's model evaluation. Here are the metric definitions
and how they're calculated:
Precision: The measure of the model's ability to predict actual positive classes. It's the ratio
between the predicted true positives and the actually tagged positives. Recall returns how
many predicted classes are correct.
Recall: The measure of the model's ability to predict actual positive classes. It's the ratio
between the predicted true positives and the actually tagged positives. Recall returns how
many predicted classes are correct.
F1 score: A function of precision and recall. An F1 score is needed when you are seeking a
balance between precision and recall.
ï¼— Note
For single classification, because the count of false positives and false negatives is always
equal, it follows that precision, recall, and the F1 score are always equal to each other.
Model evaluation scores might not always be comprehensive, especially if a specific class is
missing or underrepresented in the training data. This can occur if an insufficient number of
tagged files were provided in the training phase. This situation would affect the quantity and
quality of the testing split, which may affect the quality of the evaluation.
Any custom text classification model is expected to experience both false negative and false
positive errors. You need to consider how each type of error affects the overall system and

carefully think through scenarios where true events won't be recognized and incorrect events
will be recognized. Depending on your scenario, precision or recall could be a more suitable
metric for evaluating your model's performance. For example, if your scenario is about ticket
triaging, predicting the wrong class would cause it to be forwarded to the wrong team, which
costs time and effort. In this case, your system should be more sensitive to false positives and
precision would then be a more relevant metric for evaluation.
If your scenario is about categorizing email as important or spam, failing to predict that a
certain email is important would cause you to miss it. But if spam email was mistakenly marked
important, you would simply disregard it. In this case, the system should be more sensitive to
false negatives and recall would then be a more relevant evaluation metric.
If you want to optimize for general purpose scenarios or when precision and recall are equally
important, the F1 score would be the most relevant metric. Evaluation scores are dependent on
your scenario and acceptance criteria. There's no absolute metric that will work for all
scenarios.

System limitations and best practices for
enhancing system performance
Understand service limitations: There are some limits enforced on the user, such as the
number of files and classes contained in your data or entity length. Learn more about
system limitations.
Plan your schema: Identify the categories that you want to classify your data into. You
need to plan your schema to avoid ambiguity and to take the complexity of classes into
consideration. Learn more about recommended practices.
Select training data: The quality of training data is an important factor in model quality.
Using diverse and real-life data similar to the data you expect during production will
make the model more robust and better able to handle real-life scenarios. Make sure to
include all layouts and formats of text that will be used in production. If the model isn't
exposed to a certain scenario or class during training, it won't be able to recognize it in
production. Learn more about recommended practices.
Tag data accurately: The quality of your tagged data is a key factor in model
performance, and it's considered the ground truth from which the model learns. Tag
precisely and consistently. When you tag a specific file, make sure you assign it to the
most relevant class. Make sure similar files in your data are always tagged with the same
class. Make sure all classes are well represented and that you have a balanced data
distribution across all entities. Examine data distribution to make sure all your classes are
adequately represented. If a certain class is tagged less frequently than the others, this

class may be underrepresented and may not be recognized properly by the model during
production. In this case, consider adding more files from the underrepresented class to
your training data and then train a new model.
Review evaluation and improve model: After the model is successfully trained, check the
model evaluation and confusion matrix. This review helps you understand where your
model went wrong and learn about classes that aren't performing well. It's also
considered a best practice to review the test set and view the predicted and tagged
classes side by side. It gives you a better idea of the model's performance and helps you
decide if any changes in the schema or the tags are necessary. You can also review the
confusion matrix to identify classes that are often mistakenly predicted to see if anything
can be done to improve model performance.

General guidelines to understand and improve
performance
The following guidelines will help you to understand and improve performance in custom text
classification.

Understand confidence scores
After you've tagged data and trained your model, you'll need to deploy it to be consumed in a
production environment. Deploying a model means making it available for use via the runtime
API

to predict classes for a submitted text. The API returns a JSON object that contains the

predicted class or classes and the confidence score. The confidence score is a decimal number
between zero (0) and one (1). It serves as an indicator of how confident the system is with its
prediction. A higher value indicates higher confidence in the accuracy of that result. The
returned score is directly affected by the data you tagged when you built the custom model. If
the user's input is similar to the data used in training, higher scores and more accurate
predictions can be expected. If a certain class is consistently predicted with a low confidence
score, you might want to examine the tagged data and add more instances for this class, and
then retrain the model.

Set confidence score thresholds
The confidence score threshold can be adjusted based on your scenario. You can automate
decisions in your scenario based on the confidence score the system returns. You can also set a
certain threshold so that predicted classes with confidence scores higher or lower than this
threshold are treated differently. For example, if a prediction is returned with a confidence
score below the threshold, the file can be flagged for additional review.

Different scenarios call for different approaches. If the actions based on the predicted class will
have high-impacts, you might decide to set a higher threshold to ensure accuracy of
classification. In this case, you would expect fewer false positives but more false negatives
resulting in higher precision. If no high-impact decision based on the predicted class will be
made in your scenario, you might accept a lower threshold because you would want to predict
all possible classes that might apply to the submitted text (in multi-label classification cases). In
this case, you would expect more false positives but fewer false negatives. The result is a higher
recall.
It's very important to evaluate your system with the set thresholds by using real data that the
system will process in production to determine the effects on precision and recall.

Different training sessions and changes in evaluation
Retraining the same model without any changes in tagged data will result in the same model
output, and as a result, the same evaluation scores. If you add or remove tags, the model
performance changes accordingly. Provided that no new files were added during tagging, the
evaluation scores can be compared with the previous version of the model because both have
the same files in the training and testing sets.
Adding new files or training a different model with random set splits leads to different files in
training and testing sets. Although changes in evaluation scores might occur, they can't be
directly compared to other models because performance is calculated on different splits for
test sets.

Review incorrect predictions to improve performance
After you've trained your model, you can review model evaluation details to identify areas for
improvement. The model-level metrics provide information on the overall model performance.
By observing the class-level performance metrics, you can identify if there are any issues within
a specific class.
If you notice that a specific class has low performance, it means the model is having trouble
predicting it. This issue could be due to an ambiguous schema, which means the class can't be
differentiated from other classes. It could also be caused by a data imbalance, which means
this class is underrepresented. In this instance you will need to add more tagged examples for
the model to better predict this class.
You can also review the confusion matrix to identify classes that are often mistakenly predicted
to see if anything can be done to improve model performance. If you notice that a specific
class is often predicted as another class, it's a strong indicator that these two classes are similar

to each other. You might need to rethink your schema. Or you can add more tagged examples
to your dataset to help the model differentiate these classes.
After you've viewed evaluation details for your model, you can improve your model. This
process enables you to view the predicted and tagged classes side by side to determine what
went wrong during model evaluation. If you find that some classes are interchangeably
repeated, consider adding them all to a higher order which represents multiple classes for
better prediction.

Performance varies across features and languages
Custom text classification gives you the option to use data in multiple languages. You can have
multiple files in your dataset of different languages. Also, you can train your model in one
language and use it to query text in other languages. If you want to use the multilingual
option, you have to enable this option during project creation.
If you notice low scores in a certain language, consider adding more data in this language to
your dataset. To learn more about supported languages, see [this website/azure/aiservices/language-service/custom-text-classification/language-support).

Next steps
Introduction to custom text classification
Custom text classification Transparency Note
Data privacy and security
Guidance for integration and responsible use
Microsoft AI principles

Last updated on 08/17/2025

Data and privacy for Custom text
classification
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides high-level details about how data is processed by custom text
classification. Remember that you're responsible for your use and the implementation of this
technology, which includes complying with all laws and regulations that apply to you. For
example, it's your responsibility to:
Understand where your data is processed and stored by the custom text classification
service to meet regulatory obligations for your application.
Ensure you have all necessary licenses, proprietary rights, or other permissions required
for the content in your dataset that's used as the basis for building your custom text
classification models.
It's your responsibility to comply with all applicable laws and regulations in your jurisdiction.

What data does Custom text classification process?
Custom text classification processes the following data:
User's dataset and tags file: As a prerequisite to creating a custom text classification
project, users need to upload their dataset to their Azure Blob Storage container. A tags
file is a JSON-formatted file that contains references to a user's tagged data and classes.
The user can either bring their own tags or they can tag their data through the UI
experience in the Language Studio

. Either way, a tags file that contains tagged data and

classes is essential for the training.
A user's dataset is split into train and test sets, where the split can either be predefined by
developers in a tags file or chosen at random during training. The train set and the tags
file are processed during training to create the custom text classification model. The test
set is later processed by the trained model to evaluate its performance.
Custom text classification models: Based on the user's request to train the model,
custom text classification processes the provided tagged data to output a trained model.
The user can choose to train a new model or overwrite an existing one. The trained model
is then stored on the service's side and used for processing the model evaluation. After

the developer is content with the model's performance, they request to deploy the model
for consumption use. The deployed model is also stored on the service's side, which is
used to process the user's requests for prediction through the Analyze API.
Data sent for classification: This data is the user's text sent from a customer's client
application through the Analyze API

to be processed for text classification by the

custom machine learning model. Output of the processed data contains the predicted
classes along with their confidence scores. This output is returned to the client's
application to perform an action to fulfill the user's request.
Custom text classification doesn't collect or store any customer data to improve its machinelearned models or for product improvement purposes. We use aggregate telemetry, such as
which APIs are used and the number of calls from each subscription and resource, for service
monitoring purposes.

How does Custom text classification process data?
The following diagram illustrates how your data is processed.

How is data retained, and what customer controls
are available?
Custom text classification is a data processor for General Data Protection Regulation (GDPR)
purposes. In compliance with GDPR policies, custom text classification users have full control to
view, export, or delete any user content either through the Language Studio

or

programmatically by using Language APIs.
Your data is only stored in your Azure Storage account. custom text classification only has
access to read from it during training.
Customer controls include:
Tagged data provided by the user as a prerequisite to train the model is saved in the
customer's Azure Storage account that's connected to the project during creation.
Customers can edit or remove tags whenever they want through the Language Studio.
Custom text classification projects metadata is stored in the service's side until the
customer deletes the project. The project's metadata are the fields that you fill in when
you create your project, such as project name, description, language, name of connected
blob container, and tags file location.
Trained custom text classification models are stored in the service's Azure Storage
accounts until the customer deletes them. The model is overwritten each time the user
retrains it.
Deployed custom text classification models persist in the service's Azure Storage accounts
until the customer deletes the deployment or deletes the model itself. The model is
overwritten each time the user deploys to the same deployment name.

Optional: Security for customers' data
Azure services are implemented while maintaining appropriate technical and organizational
measures to protect customer data in the cloud.
To learn more about Microsoft's privacy and security commitments, see the Microsoft Trust
Center

.

Next steps
Introduction to Custom text classification
Custom text classification Transparency Note
Guidance for integration and responsible use
Microsoft AI principles

Last updated on 08/17/2025

How to create custom text classification
project
Use this article to learn how to set up the requirements for starting with custom text classification
and create a project.

Prerequisites
Before you start using custom text classification, you need:
An Azure subscription - Create one for free

.

Create a Language resource
Before you start using custom text classification, you need an Azure Language in Foundry Tools
resource. We recommended that you create your Language resource and connect a storage account
to it in the Azure portal. Creating a resource in the Azure portal lets you create an Azure storage
account at the same time, with all of the required permissions preconfigured. You can also read
further in the article to learn how to use a preexisting resource, and configure it to work with custom
text classification.
You also need an Azure storage account where to upload your .txt documents that are used to
train a model to classify text.
ï¼— Note
You need to have an owner role assigned on the resource group to create a Language
resource.
If you connect a preexisting storage account, you should have an owner role assigned to
it.

Create Language resource and connect storage
account
ï¼— Note
You shouldn't move the storage account to a different resource group or subscription once it's
linked with Azure Language resource.

Using the Azure portal

Create a new resource from the Azure portal
1. Go to the Azure portal

to create a new Azure Language in Foundry Tools resource.

2. In the window that appears, select Custom text classification & custom named entity
recognition from the custom features. Select Continue to create your resource at the
bottom of the screen.

ï Š

3. Create a Language resource with following details.
ï¾‰

Expand table

Name

Required value

Subscription

Your Azure subscription.

Resource
group

A resource group that will contain your resource. You can use an existing one, or
create a new one.

Region

One of the supported regions. For example "West US 2".

Name

A name for your resource.

Pricing tier

One of the supported pricing tiers. You can use the Free (F0) tier to try the service.

If you get a message saying "your login account is not an owner of the selected storage
account's resource group", your account needs to have an owner role assigned on the

resource group before you can create a Language resource. Contact your Azure
subscription owner for assistance.
You can determine your Azure subscription owner by searching your resource group

and

following the link to its associated subscription. Then:
a. Select the Access Control (IAM) tab
b. Select Role assignments
c. Filter by Role:Owner.
4. In the Custom text classification & custom named entity recognition section, select an
existing storage account or select New storage account. Note that these values are to
help you get started, and not necessarily the storage account values youâ€™ll want to use in
production environments. To avoid latency during building your project connect to
storage accounts in the same region as your Language resource.
ï¾‰

Storage account value

Recommended value

Storage account name

Any name

Storage account type

Standard LRS

Expand table

5. Make sure the Responsible AI Notice is checked. Select Review + create at the bottom of
the page.

ï¼— Note
The process of connecting a storage account to your Language resource is irreversibleâ€”it
can't be disconnected later.
You can only connect your language resource to one storage account.

Using a preexisting Language resource
ï¾‰

Expand table

Requirement

Description

Regions

Make sure your existing resource is provisioned in one of the supported regions. If you don't
have a resource, you need to create a new one in a supported region.

Pricing tier

The pricing tier for your resource.

Requirement

Description

Managed

Make sure that the resource's managed identity setting is enabled. Otherwise, read the next

identity

section.

To use custom text classification, you'll need to create an Azure storage account if you don't have
one already.

Enable identity management for your resource
Your Language resource must have identity management, to enable it using Azure portal :
1. Go to your Language resource
2. From left hand menu, under Resource Management section, select Identity
3. From System assigned tab, make sure to set Status to On

Enable custom text classification feature
Make sure to enable Custom text classification / Custom Named Entity Recognition feature from
Azure portal.
1. Go to your Language resource in Azure portal
2. From the left side menu, under Resource Management section, select Features
3. Enable Custom text classification / Custom Named Entity Recognition feature
4. Connect your storage account
5. Select Apply
ï¼‰ Important
Make sure that your Language resource has storage blob data contributor role assigned
on the storage account you're connecting.

Set roles for your Azure Language resource and storage account
Use the following steps to set the required roles for your Language resource and storage account.

ï Š

Roles for your Azure Language in Foundry Tools resource
1. Go to your storage account or Language resource in the Azure portal

.

2. Select Access Control (IAM) in the left pane.
3. Select Add to Add Role Assignments, and choose the appropriate role for your account.
You should have the owner or contributor role assigned on your Language resource.
4. Within Assign access to, select User, group, or service principal
5. Select Select members
6. Select your user name. You can search for user names in the Select field. Repeat this for all
roles.
7. Repeat these steps for all the user accounts that need access to this resource.

Roles for your storage account
1. Go to your storage account page in the Azure portal

.

2. Select Access Control (IAM) in the left pane.
3. Select Add to Add Role Assignments, and choose the Storage blob data contributor role on
the storage account.
4. Within Assign access to, select Managed identity.

5. Select Select members
6. Select your subscription, and Language as the managed identity. You can search for user
names in the Select field.
ï¼‰ Important
If you have a virtual network or private endpoint, be sure to select Allow Azure services on the
trusted services list to access this storage account in the Azure portal.

Enable CORS for your storage account
Make sure to allow (GET, PUT, DELETE) methods when enabling Cross-Origin Resource Sharing
(CORS). Set allowed origins field to https://language.cognitive.azure.com . Allow all header by
adding * to the allowed header values, and set the maximum age to 500 .

ï Š

Create a custom text classification project (REST API)
Once your resource and storage container are configured, create a new custom text classification
project. A project is a work area for building your custom AI models based on your data. Your
project is only accessible by you and others who have access to the Azure resource being used. If
you labeled data, you can import it to get started.
To start creating a custom text classification model, you need to create a project. Creating a project
lets you label data, train, evaluate, improve, and deploy your models.
ï¼— Note

The project name is case-sensitive for all operations.
Create a PATCH request using the following URL, headers, and JSON body to create your project.

Request URL
Use the following URL to create a project. Replace the placeholder values with your own values.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is case-

myProject

NAME}

sensitive.

{API-

The version of the API you're calling. The value
referenced is for the latest version released. See
Model lifecycle to learn more about other
available API versions.

VERSION}

2022-05-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request. Replace the placeholder values with your own values.
Multi label classification

JSON
{
"projectName": "{PROJECT-NAME}",
"language": "{LANGUAGE-CODE}",
"projectKind": "customMultiLabelClassification",
"description": "Project description",
"multilingual": "True",
"storageInputContainerName": "{CONTAINER-NAME}"
}

ï¾‰

Key

Placeholder

Value

Example

projectName

{PROJECT-NAME}

The name
of your
project.

myProject

Expand table

This value
is casesensitive.
language

{LANGUAGE-CODE}

A string
specifying
the

en-us

language
code for
the
documents
used in
your
project. If
your
project is a
multilingual
project,
choose the
language
code of
most the
documents.
See
language
support to
learn more
about
supported
language
codes.
projectKind

customMultiLabelClassification

Your
project

customMultiLabelClassification

Key

Placeholder

Value

Example

kind.
multilingual

true

A boolean
value that
enables

true

you to have
documents
in multiple
languages
in your
dataset and
when your
model is
deployed
you can
query the
model in
any
supported
language
(not
necessarily
included in
your
training
documents.
See
language
support to
learn more
about
multilingual
support.
storageInputContainerName

{CONTAINER-NAME}

The name
of your
Azure
storage

myContainer

container
for your
uploaded
documents.

This request returns a 201 response, which means that the project is created.
This request returns an error if:
The selected resource doesn't have proper permission for the storage account.

Import a custom text classification project (REST API)
If you already labeled data, you can use it to get started with the service. Make sure that your
labeled data follows the accepted data formats.
Submit a POST request using the following URL, headers, and JSON body to import your labels file.
Make sure that your labels file follow the accepted format.
If a project with the same name already exists, the data of that project is replaced.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}/:import?api-version=
{API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This value is casesensitive.

myProject

The version of the API you're calling. The value
referenced is for the latest version released.

2022-05-01

Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request. Replace the placeholder values with your own values.
Multi label classification

JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectName": "{PROJECT-NAME}",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectKind": "customMultiLabelClassification",
"description": "Trying out custom multi label text classification",
"language": "{LANGUAGE-CODE}",
"multilingual": true,
"settings": {}
},
"assets": {
"projectKind": "customMultiLabelClassification",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
],
"documents": [
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
]
},
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"classes": [
{
"category": "Class2"
}
]
}
]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

api-version

{API-VERSION}

The version

2022-05-01

of the API
you're
calling. The
version
used here
must be
the same
API version
in the URL.
Learn more
about
other
available
API
versions
projectName

{PROJECT-NAME}

The name

myProject

of your
project.
This value
is casesensitive.
projectKind

customMultiLabelClassification

Your

customMultiLabelClassification

project
kind.
language

{LANGUAGE-CODE}

A string
specifying
the
language
code for
the
documents
used in
your
project. If
your
project is a
multilingual
project,
choose the
language
code for
most of the
documents.
See
language
support to

en-us

Key

Placeholder

Value

Example

learn more
about
multilingual
support.
multilingual

true

A boolean

true

value that
enables
you to have
documents
in multiple
languages
in your
dataset and
when your
model is
deployed
you can
query the
model in
any
supported
language
(not
necessarily
included in
your
training
documents.
See
language
support to
learn more
about
multilingual
support.
storageInputContainerName

{CONTAINER-NAME}

The name
of your

myContainer

Azure
storage
container
for your
uploaded
documents.
classes

[]

Array
containing
all the
classes you

[]

Key

Placeholder

Value

Example

have in the
project.
documents

[]

Array

[]

containing
all the
documents
in your
project and
what the
classes
labeled for
this
document.
location

{DOCUMENT-NAME}

The
location of

doc1.txt

the
documents
in the
storage
container.
Since all
the
documents
are in the
root of the
container,
it should
be the
document
name.
dataset

{DATASET}

The test set
to which
this
document
goes to
when split
before
training.
See How to
train a
model.
Possible
values for
this field
are Train
and Test .

Train

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOB-ID}?
api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You use this URL to

get the import job status.
Possible error scenarios for this request:
The selected resource doesn't have proper permissions for the storage account.
The storageInputContainerName specified doesn't exist.
Invalid language code is used, or if the language code type isn't string.
multilingual value is a string and not a boolean.

Get project details (REST API)
To get custom text classification project details, submit a GET request using the following URL and
headers. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This value is
case-sensitive.

{API-

The version of the API you're calling. The

2022-05-01

VERSION}

value referenced is for the latest released

{PROJECT-

model version.

Headers
Use the following header to authenticate your request.

ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"createdDateTime": "2022-04-23T13:39:09.384Z",
"lastModifiedDateTime": "2022-04-23T13:39:09.384Z",
"lastTrainedDateTime": "2022-04-23T13:39:09.384Z",
"lastDeployedDateTime": "2022-04-23T13:39:09.384Z",
"projectKind": "customSingleLabelClassification",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectName": "{PROJECT-NAME}",
"multilingual": true,
"description": "Project description",
"language": "{LANGUAGE-CODE}"
}

ï¾‰

Value

placeholder

Description

Example

projectKind

customSingleLabelClassification

Your project
kind.

This value can be

Expand table

customSingleLabelClassification

or
customMultiLabelClassification .
storageInputContainerName

{CONTAINER-NAME}

The name
of your

myContainer

Azure
storage
container
for your
uploaded
documents.
projectName

{PROJECT-NAME}

The name
of your
project. This
value is
casesensitive.

myProject

Value

placeholder

multilingual

Description

Example

A boolean

true

value that
enables you
to have
documents
in multiple
languages
in your
dataset.
When your
model is
deployed,
you can
query the
model in
any
supported
language
(not
necessarily
included in
your
training
documents.
For more
information
on
multilingual
support,
see
language
support.
language

{LANGUAGE-CODE}

A string
specifying
the
language
code for the
documents
used in
your
project. If
your project
is a
multilingual
project,
choose the
language
code for
most of the

en-us

Value

placeholder

Description

Example

documents.
See
language
support to
learn more
about
supported
language
codes.

Once you send your API request, you receive a 200 response indicating success and JSON response
body with your project details.

Delete project (REST API)
When you no longer need your project, you can delete it with the following DELETE request. Replace
the placeholder values with your own values.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This value is casesensitive.

{API-

The version of the API you're calling. The value

2022-05-01

VERSION}

referenced is for the latest version released.
Learn more about other available API versions

{PROJECT-

Expand table

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success, which means your
project is deleted. A successful call results with an Operation-Location header used to check the
status of the job.

Next steps
You should plan the project schema used to label your data.
After your project is created, you can start labeling your data. Labeling informs your text
classification model how to interpret text and is used for training and evaluation.

Last updated on 12/17/2025

How to prepare data and define a text
classification schema
In order to create a custom text classification model, you need quality data to train it. This
article covers how you should select and prepare your data, along with defining a schema.
Defining the schema is the first step in project development lifecycle, and it defines the classes
that you need your model to classify your text into at runtime.

Schema design
The schema defines the classes that you need your model to classify your text into at runtime.
Review and identify: Review documents in your dataset to be familiar with their structure
and content, then identify how you want to classify your data.
For example, if you're classifying support tickets, you might need the following classes:
Sign in issue, hardware issue, connectivity issue, and new equipment request.
Avoid ambiguity in classes: Ambiguity arises when the classes you specify share similar
meaning to one another. The more ambiguous your schema is, the more labeled data you
might need to differentiate between different classes.
For example, if you're classifying food recipes, they may be similar to an extent. To
differentiate between dessert recipe and main dish recipe, you might need to label more
examples to help your model distinguish between the two classes. Avoiding ambiguity
saves time and yields better results.
Out of scope data: When using your model in production, consider adding an out of
scope class to your schema if you expect documents that don't belong to any of your
classes. Then add a few documents to your dataset to be labeled as out of scope. The
model can learn to recognize irrelevant documents, and predict their labels accordingly.

Data selection
The quality of data you train your model with affects model performance greatly.
Use real-life data that reflects your domain's problem space to effectively train your
model. You can use synthetic data to accelerate the initial model training process, but it
likely differs from your real-life data and make your model less effective when used.
Balance your data distribution as much as possible without deviating far from the
distribution in real-life.

Use diverse data whenever possible to avoid overfitting your model. Less diversity in
training data may lead to your model learning spurious correlations that may not exist in
real-life data.
Avoid duplicate documents in your data. Duplicate data has a negative effect on the
training process, model metrics, and model performance.
Consider where your data comes from. If you're collecting data from one person,
department, or part of your scenario, you're likely missing diversity that may be important
for your model to learn about.
ï¼— Note
If your documents are in multiple languages, select the multiple languages option during
project creation and set the language option to the language for most of your
documents.

Data preparation
As a prerequisite for creating a custom text classification project, your training data needs to
be uploaded to a blob container in your storage account. You can create and upload training
documents from Azure directly, or through using the Azure Storage Explorer tool. Using the
Azure Storage Explorer tool allows you to upload more data quickly.
Create and upload documents from Azure
Create and upload documents using Azure Storage Explorer
You can only use .txt documents for custom text. If your data is in other format, you can use
CLUtils parse command

to change your file format.

Test set
When defining the testing set, make sure to include example documents that aren't present in
the training set. Defining the testing set is an important step to calculate the model
performance. Also, make sure that the testing set includes documents that represent all classes
used in your project.

Next steps

If you haven't already, create a custom text classification project. If it's your first time using
custom text classification, consider following the quickstart to create an example project. For
more information, see project requirements.

Last updated on 12/17/2025

Label text data for training your model
Before training your model, you need to label your documents with the classes you want to
categorize them into. Data labeling is a crucial step in development lifecycle; in this step you
can create the classes you want to categorize your data into and label your documents with
these classes. This data is used in the next step when training your model so that your model
can learn from the labeled data. If you already labeled your data, you can directly import it into
your project but you need to make sure that your data follows the accepted data format.
Before creating a custom text classification model, you need to have labeled data first. If your
data isn't labeled already, you can label it in the Microsoft Foundry

. Labeled data informs the

model how to interpret text, and is used for training and evaluation.

Prerequisites
Before you can label data, you need:
A successfully created project with a configured Azure blob storage account,
Documents containing the uploaded text data in your storage account.
See the project development lifecycle.

Data labeling guidelines
After preparing your data, designing your schema and creating your project, you need to label
your data. Labeling your data is important so your model knows which documents will be
associated with the classes you need. When you label your data or import labeled data, these
labels are stored in the JSON file in your storage container that you've connected to this
project.
As you label your data, keep in mind:
In general, more labeled data leads to better results, provided the data is labeled
accurately.
There is no fixed number of labels that can guarantee your model performs the best.
Model performance on possible ambiguity in your schema, and the quality of your
labeled data. Nevertheless, we recommend 50 labeled documents per class.

Label your data
Use the following steps to label your data:

1. Go to your project page in Microsoft Foundry

.

2. From the left side menu, select Data labeling. You can find a list of all documents in your
storage container. See the image below.
îª€ Tip
You can use the filters in top menu to view the unlabeled files so that you can start
labeling them. You can also use the filters to view the documents that are labeled
with a specific class.
3. Change to a single file view from the left side in the top menu or select a specific file to
start labeling. You can find a list of all .txt files available in your projects to the left. You
can use the Back and Next button from the bottom of the page to navigate through your
documents.
ï¼— Note
If you enabled multiple languages for your project, you will find a Language
dropdown in the top menu, which lets you select the language of each document.
4. In the right side pane, Add class to your project so you can start labeling your data with
them.
5. Start labeling your files.
Multi label classification

Multi label classification: your file can be labeled with multiple classes. You can do so
by selecting all applicable check boxes next to the classes you want to label this
document with.

ï Š

You can also use the auto labeling feature to ensure complete labeling.
6. In the right side pane under the Labels pivot you can find all the classes in your project
and the count of labeled instances per each.
7. In the bottom section of the right side pane you can add the current file you're viewing to
the training set or the testing set. By default all the documents are added to your training
set. Learn more about training and testing sets and how they're used for model training
and evaluation.
îª€ Tip
If you're planning on using Automatic data splitting, use the default option of
assigning all the documents into your training set.
8. Under the Distribution pivot you can view the distribution across training and testing
sets. You have two options for viewing:
Total instances where you can view count of all labeled instances of a specific class.
documents with at least one label where each document is counted if it contains at
least one labeled instance of this class.
9. While you're labeling, your changes are synced periodically, if they have not been saved
yet you will find a warning at the top of your page. If you want to save manually, select
Save labels button at the bottom of the page.

Remove labels
If you want to remove a label, uncheck the button next to the class.

Delete or classes
To delete a class, select the icon next to the class you want to remove. Deleting a class will
remove all its labeled instances from your dataset.

Next steps
After you've labeled your data, you can begin training a model that will learn based on your
data.

Last updated on 12/17/2025

How to train a custom text classification
model
Training is the process where the model learns from your labeled data. After training is
completed, you can view the model's performance to determine if you need to improve your
model.
To train a model, start a training job. Only successfully completed jobs create a usable model.
Training jobs expire after seven days. After this period, you won't be able to retrieve the job
details. If your training job completed successfully and a model was created, the job expiration
isn't affected. You can only have one training job running at a time, and you can't start other
jobs in the same project.
Depending on the dataset size and the complexity of your schema, training times can vary
from a few minutes up to several hours.

Prerequisites
Before you train your model, you need:
A successfully created project with a configured Azure blob storage account,
Text data that is uploaded to your storage account.
Labeled data
See the project development lifecycle.

Data splitting
Before you start the training process, labeled documents in your project are divided into a
training set and a testing set. Each one of them serves a different function. The training set is
used in training the model and where the model learns the class/classes assigned to each
document. The testing set is a blind set that isn't introduced to the model during training but
only during evaluation. After the model is trained successfully, it can make predictions from the
documents in the testing set. Based on these predictions, the model's evaluation metrics are
calculated. We recommend making sure that all your classes are adequately represented in
both the training and testing set.
Custom text classification supports two methods for data splitting:
Automatically splitting the testing set from training data: The system splits your labeled
data between the training and testing sets, according to the percentages you choose. The

system attempts to have a representation of all classes in your training set. The
recommended percentage split is 80% for training and 20% for testing.
ï¼— Note
If you choose the Automatically splitting the testing set from training data option, only
the data assigned to training set is split according to the percentages provided.
Use a manual split of training and testing data: This method enables users to define
which labeled documents should belong to which set. This step is only enabled if you
added documents to your testing set during data labeling.

Train model with REST API
Start training job
Submit a POST request using the following URL, headers, and JSON body to submit a training
job. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:train?apiversion={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is

myProject

NAME}

case-sensitive.

{API-

The version of the API you're calling. The

VERSION}

value referenced is for the latest version

2022-05-01

released. Learn more about other
available API versions

Headers
Use the following header to authenticate your request.

ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in your request body. The model will be given the {MODEL-NAME} once
training is complete. Only successful training jobs will produce models.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 80,
"testingSplitPercentage": 20
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-NAME}

The model name that is assigned to your model
once trained successfully.

myModel

trainingConfigVersion

{CONFIG-

This is the model version used to train the

2022-05-01

VERSION}

model.

evaluationOptions

kind

percentage

Option to split your data across training and
testing sets.

{}

Split methods. Possible values are percentage or

percentage

manual . See How to train a model for more

information.
trainingSplitPercentage

80

Percentage of your tagged data to be included
in the training set. Recommended value is 80 .

80

testingSplitPercentage

20

Percentage of your tagged data to be included
in the testing set. Recommended value is 20 .

20

ï¼— Note

The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set
to percentage and the sum of both percentages should be equal to 100.
Once you send your API request, you receive a 202 response indicating that the job was
submitted correctly. In the response headers, extract the location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}?api-version={API-VERSION}

{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this
URL to get the training status.

Get training job status
Training could take sometime depending on the size of your training data and complexity of
your schema. You can use the following request to keep polling the status of the training job
until successfully completed.
Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is

myProject

NAME}

case-sensitive.

{JOB-ID}

The ID for locating your model's training
status. This value is in the location
header value you received in the previous
step.

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

Placeholder

Value

Example

{API-

The version of the API you're calling. The
value referenced is for the latest version

2022-05-01

VERSION}

released. For more information, see
Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"result": {
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},
"jobId": "{JOB-ID}",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

Cancel training job with REST API

Create a POST request by using the following URL, headers, and JSON body to cancel a
training job.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{Endpoint}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}/:cancel?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your
API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

EmailApp

NAME}

case-sensitive.

{JOB-ID}

This value is the training job ID.

XXXXX-XXXXX-XXXX-XX

{API-

The version of the API you're calling.
The value referenced is for the latest
released model version.

2022-05-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

After you send your API request, you receive a 202 response with an Operation-Location
header used to check the status of the job.

Next steps

After training is completed, you're able to view the model's performance to optionally improve
your model if needed. Once you're satisfied with your model, you can deploy it, making it
available to use for classifying text.

Last updated on 12/17/2025

View your text classification model's
evaluation and details
After your model has finished training, you can view the model performance and see the
predicted classes for the documents in the test set.
ï¼— Note
Using the Automatically split the testing set from training data option may result in
different model evaluation result every time you train a new model, as the test set is
selected randomly from the data. To make sure that the evaluation is calculated on the
same test set every time you train a model, make sure to use the Use a manual split of
training and testing data option when starting a training job and define your Test
documents when labeling data.

Prerequisites
Before viewing model evaluation you need:
A custom text classification project with a configured Azure blob storage account.
Text data that has been uploaded to your storage account.
Labeled data
A successfully trained model
See the project development lifecycle.

Model details (REST API)
Single label classification
Submit a GET request using the following URL, headers, and JSON body to get a trained model
evaluation summary.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/models/{trainedModelLabel}/evaluation/summary-result?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

The name for your project. This

myProject

{PROJECT-NAME}

value is case-sensitive.
{trainedModelLabel}

The name for your trained model.

Model1

This value is case-sensitive.
{API-VERSION}

The version of the API you're

2022-05-01

calling. The value referenced is for
the latest version released. Learn
more about other available API
versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"projectKind": "customSingleLabelClassification",
"customSingleLabelClassificationEvaluation": {
"confusionMatrix": {
"additionalProp1": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},

"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp2": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp3": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
}
},
"classes": {
"additionalProp1": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp2": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp3": {
"f1": 0,

"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
}
},
"microF1": 0,
"microPrecision": 0,
"microRecall": 0,
"macroF1": 0,
"macroPrecision": 0,
"macroRecall": 0
},
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 0,
"testingSplitPercentage": 0
}
}

Multi label classification
Submit a GET request using the following URL, headers, and JSON body to get a trained model
evaluation summary.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/models/{trainedModelLabel}/evaluation/summary-result?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This
value is case-sensitive.

myProject

{trainedModelLabel}

The name for your trained model.
This value is case-sensitive.

Model1

Placeholder

Value

Example

{API-VERSION}

The version of the API you're
calling. The value referenced is for

2022-05-01

the latest version released. Learn
more about other available API
versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"projectKind": "customMultiLabelClassification",
"customMultiLabelClassificationEvaluation": {
"classes": {
"additionalProp1": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp2": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp3": {
"f1": 0,

"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
}
},
"microF1": 0,
"microPrecision": 0,
"microRecall": 0,
"macroF1": 0,
"macroPrecision": 0,
"macroRecall": 0
},
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 0,
"testingSplitPercentage": 0
}
}

Load or export model data (REST API)
Load model data
Create a POST request using the following URL, headers, and JSON body to load your model
data to your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/models/{MODELNAME}:load-snapshot?stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

Placeholder

Value

Example

{PROJECT-

The name for your project. This

EmailApp

NAME}

value is case-sensitive.

{API-

The version of the API you're
calling.

2022-10-01-preview

The name of your model. This
value is case-sensitive.

v1

VERSION}
{MODEL-NAME}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/models/{MODELNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the status of your model data loading, using the same authentication method.

Export model data
Create a POST request using the following URL, headers, and JSON body to export your model
data.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest

{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}&trainedModelLabel={MODELNAME}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

EmailApp

NAME}

value is case-sensitive.

{API-

The version of the API you're
calling.

2022-10-01-preview

The name of your model. This
value is case-sensitive.

v1

VERSION}
{MODEL-NAME}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/jobs/{JOB-ID}?
api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the exported project JSON, using the same authentication method.

Delete model (REST API)

Create a DELETE request using the following URL, headers, and JSON body to delete a trained
model.

Request URL
rest
{Endpoint}/language/authoring/analyze-text/projects/{PROJECTNAME}/models/{trainedModelLabel}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This
value is case-sensitive.

myProject

{trainedModelLabel}

The name for your model name.
This value is case-sensitive.

model1

{API-VERSION}

The version of the API you're
calling. The value referenced is for

2022-05-01

the latest version released. Learn
more about other available API
versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 204 response indicating success, which means
your trained model is deleted.

Next steps

As you review your how your model performs, learn about the evaluation metrics that are used.
Once you know whether your model performance needs to improve, you can begin improving
the model.

Last updated on 12/17/2025

Deploy a model and classify text using the runtime API
Once you're satisfied with how your model performs, it's ready to be deployed; and use it to classify text. Deploying a model makes it available for
use through the prediction API

.

Prerequisites
A custom text classification project with a configured Azure storage account,
Text data that is uploaded to your storage account.
Labeled data and successfully trained model
Reviewed the model evaluation details to determine how your model is performing.
See the project development lifecycle.

Deploy model (REST API)
After you review your model's performance and decided it can be used in your environment, you need to assign it to a deployment to be able to
query it. Assigning the model to a deployment makes it available for use through the prediction API

. We recommend that you create a

deployment named production to which you assign the best model you built so far and use it in your system. You can create another deployment
called staging to which you can assign the model you're currently working on to be able to test it. You can have a maximum on 10 deployments
in your project.

Submit deployment job
Submit a PUT request using the following URL, headers, and JSON body to submit a deployment job. Replace the placeholder values with your
own values.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}/deployments/{deploymentName}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name of your project. This value is case-sensitive.

myProject

{DEPLOYMENT-

The name of your deployment. This value is case-sensitive.

staging

The version of the API you're calling. The value referenced is for the latest version
released. Learn more about other available API versions

2022-05-01

NAME}
{API-VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in the body of your request. Use the name of the model you to assign to the deployment.
JSON

Expand table

{
"trainedModelLabel": "{MODEL-NAME}"
}

ï¾‰

Key
trainedModelLabel

Expand table

Placeholder

Value

Example

{MODEL-

The model name that is assigned to your deployment. You can only assign successfully trained models. This value is
case-sensitive.

myModel

NAME}

Once you send your API request, you receive a 202 response indicating that the job was submitted correctly. In the response headers, extract the
operation-location value formatted like this:

rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={APIVERSION}

{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this URL to get the deployment status.

Get deployment job status
Use the following GET request to query the status of the deployment job. You can use the URL you received from the previous step, or replace the
placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name of your project. This value is case-sensitive.

myProject

{DEPLOYMENT-

The name of your deployment. This value is case-sensitive.

staging

The ID for locating your model's training status. It's in the location header value you

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

NAME}
{JOB-ID}

received in the previous step.
{API-VERSION}

The version of the API you're calling. The value referenced is for the latest version

2022-05-01

released. Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to "succeeded". You
should get a 200 code to indicate the success of the request.
JSON

{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Swap deployments (REST API)
You can swap deployments after testing a model assigned to one deployment, and want to assign it to another. Swapping deployments involves
taking the model assigned to the first deployment, and assigning it to the second deployment. Then taking the model assigned to second
deployment and assign it to the first deployment. This step could be used to swap your production and staging deployments when you want to
take the model assigned to staging and assign it to production .
Create a POST request using the following URL, headers, and JSON body to start a swap deployments job.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/deployments/:swap?api-version={API-VERSION}

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

ï¾‰

Expand table

ï¾‰

Expand table

ï¾‰

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is case-sensitive.

myProject

The version of the API you're calling. The value referenced is for the latest model
version released.

2022-05-01

NAME}
{API-VERSION}

Headers
Use the following header to authenticate your request.

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request Body
JSON
{
"firstDeploymentName": "{FIRST-DEPLOYMENT-NAME}",
"secondDeploymentName": "{SECOND-DEPLOYMENT-NAME}"
}

Key

Placeholder

Value

Example

firstDeploymentName

{FIRST-DEPLOYMENT-NAME}

The name for your first deployment. This value is case-sensitive.

production

secondDeploymentName

{SECOND-DEPLOYMENT-NAME}

The name for your second deployment. This value is case-sensitive.

staging

Once you send your API request, you receive a 202 response indicating success.

Delete deployment (REST API)
Create a DELETE request using the following URL, headers, and JSON body to delete a deployment.

Request URL
rest
{Endpoint}/language/authoring/analyze-text/projects/{PROJECT-NAME}/deployments/{deploymentName}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{DEPLOYMENT-

The name for your deployment name. This value is case-sensitive.

prod

The version of the API you're calling. The value referenced is for the latest version

2022-05-01

NAME}
{API-VERSION}

released. Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Once you send your API request, you receive a 202 response indicating success, which means your deployment is deleted. A successful call results
with an Operation-Location header used to check the status of the job.

Assign deployment resources (REST API)
You can deploy your project to multiple regions by assigning different Language resources that exist in different regions.
Assigning deployment resources programmatically requires Microsoft Entra authentication. Microsoft Entra ID is used to confirm you have access
to the resources you're interested in assigning to your project for multi-region deployment. To programmatically use Microsoft Entra
authentication when making REST API calls, learn more from the Foundry Tools documentation.

Assign resource
Submit a POST request using the following URL, headers, and JSON body to assign deployment resources.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/resources/:assign?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

Placeholder

Value

Example

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2022-10-01-preview

Headers
Use Microsoft Entra authentication to authenticate this API.

Body
Use the following sample JSON as your body.
JSON
{
"resourcesMetadata": [
{
"azureResourceId": "{AZURE-RESOURCE-ID}",
"customDomain": "{CUSTOM-DOMAIN}",
"region": "{REGION-CODE}"
}
]
}

ï¾‰

Key
azureResourceId

customDomain

Placeholder

Value

Expand table

Example

{AZURE-

The full resource ID path you want to

/subscriptions/aaaa0a0a-bb1b-cc2c-dd3d-

RESOURCE-

assign. Found in the Azure portal under

eeeeee4e4e4e/resourceGroups/ContosoResourceGroup/providers/Microsoft.CognitiveServices

ID}

the Properties tab for the resource, in the
Resource ID field.

{CUSTOM-

The custom subdomain of the resource

DOMAIN}

you want to assign. Found in the Azure

contosoresource

portal under the Keys and Endpoint tab
for the resource, as the Endpoint field in
the URL https://<your-customsubdomain>.cognitiveservices.azure.com/
region

{REGION-

A region code specifying the region of

CODE}

the resource you want to assign. Found in

eastus

the Azure portal under the Keys and
Endpoint tab for the resource, in the
Location/Region field.

Get assign resource status
Use the following GET request to get the status of your assign deployment resource job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/resources/assign/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECTNAME}

The name for your project. This value is case-sensitive.

myProject

Placeholder

Value

Example

{JOB-ID}

The job ID for getting your assign deployment status. It's in the operation-location header

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

value you received from the API in response to your assign deployment resource request.
{API-

The version of the API you're calling.

2022-10-01-preview

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to succeeded .
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Unassign deployment resources (REST API)
When you unassign or remove a deployment resource from a project, you also delete all the deployments previously deployed to that resource
region.

Unassign resource
Submit a POST request using the following URL, headers, and JSON body to unassign or remove deployment resources from your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/resources/:unassign?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2022-10-01-preview

Headers
Use the following header to authenticate your request.

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

ï¾‰

Expand table

ï¾‰

Expand table

Body
Use the following sample JSON as your body.
JSON
{
"assignedResourceIds": [
"{AZURE-RESOURCE-ID}"
]
}

Key
assignedResourceIds

Placeholder

Value

Example

{AZURE-

The full
resource

e1e1e1e1e1e1/resourceGroups/ContosoResourceGroup/providers/Microsoft.CognitiveServices/accounts/ContosoResource

RESOURCEID}

/subscriptions/a0a0a0a0-bbbb-cccc-dddd-

ID path
you want
to
unassign.
Found in
the Azure
portal
under the
Properties
tab for
the
resource
as the
Resource
ID field.

Get unassign resource status
Use the following GET request to get the status of your unassign deployment resources job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/resources/unassign/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is case-sensitive.

myProject

The job ID for getting your assign deployment status. It's in the operation-location header

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

NAME}
{JOB-ID}

value you received from the API in response to your unassign deployment resource request.
{APIVERSION}

Headers

The version of the API you're calling.

2022-10-01-preview

Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Next steps
Use prediction API to query your model

Last updated on 12/17/2025

Send text classification requests to your
model
After you successfully deploy a model, you can query the deployment to classify text based on
the model you assigned to the deployment. You can query the deployment programmatically
Prediction API or through the client libraries (Azure SDK).

Send a text classification request to your model
(REST API)
First you need to get your resource key and endpoint:
1. Go to your resource overview page in the Azure portal
2. From the menu on the left side, select Keys and Endpoint. You use the endpoint and key
for the API requests

ï Š

Submit a custom text classification task
Use this POST request to start a text classification task.
rest
{ENDPOINT}/language/analyze-text/jobs?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{API-

The version of the API you're calling. The

2022-05-01

VERSION}

value referenced is for the latest version
released. For more information, see
Model lifecycle.

Headers
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

Your key that provides access to this API.

Body
Multi label classification

JSON
{
"displayName": "Classifying documents",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "{LANGUAGE-CODE}",
"text": "Text1"
},
{
"id": "2",
"language": "{LANGUAGE-CODE}",
"text": "Text2"
}
]
},
"tasks": [
{
"kind": "CustomMultiLabelClassification",
"taskName": "Multi Label Classification",
"parameters": {
"projectName": "{PROJECT-NAME}",
"deploymentName": "{DEPLOYMENT-NAME}"

Expand table

}
}
]
}

ï¾‰

Key

Placeholder

Value

Example

displayName

{JOB-NAME}

Your job name.

MyJobName

documents

[{},{}]

List of documents

[{},{}]

Expand table

to run tasks on.
id

{DOC-ID}

Document name

doc1

or ID.
language

{LANGUAGE-CODE}

A string

en-us

specifying the
language code
for the document.
If this key isn't
specified, the
service will
assume the
default language
of the project that
was selected
during project
creation. See
language support
for a list of
supported
language codes.
text

{DOC-TEXT}

tasks

Document task to
run the tasks on.

Lorem ipsum dolor sit amet

List of tasks we

[]

want to perform.
taskName

CustomMultiLabelClassification

The task name

CustomMultiLabelClassification

List of parameters

parameters

to pass to the
task.
projectname

{PROJECT-NAME}

The name for
your project. This

myProject

Key

Placeholder

Value

Example

value is casesensitive.
deployment-

{DEPLOYMENT-NAME}

The name of your

prod

deployment. This
value is casesensitive.

name

Response
You receive a 202 response indicating success. In the response headers, extract operationlocation . operation-location is formatted like this:
{ENDPOINT}/language/analyze-text/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to query the task completion status and get the results when task is
completed.

Get task results (Azure SDK)
Use the following GET request to query the status/results of the text classification task.
rest
{ENDPOINT}/language/analyze-text/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your
API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{API-

The version of the API you're calling.

2022-05-01

VERSION}

The value referenced is for the latest
released model version version.

Headers
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

Your key that provides access to this API.

Response body
The response will be a JSON document with the following parameters.
Multi label classification

JSON
{
"createdDateTime": "2021-05-19T14:32:25.578Z",
"displayName": "MyJobName",
"expirationDateTime": "2021-05-19T14:32:25.578Z",
"jobId": "xxxx-xxxxxx-xxxxx-xxxx",
"lastUpdateDateTime": "2021-05-19T14:32:25.578Z",
"status": "succeeded",
"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "customMultiClassificationTasks",
"taskName": "Classify documents",
"lastUpdateDateTime": "2020-10-01T15:01:03Z",
"status": "succeeded",
"results": {
"documents": [
{
"id": "{DOC-ID}",
"classes": [
{
"category": "Class_1",
"confidenceScore": 0.0551877357
}
],
"warnings": []
}
],
"errors": [],
"modelVersion": "2020-04-01"
}
}
]
}

}

First you need to get your resource key and endpoint:
Go to your resource overview page in the Azure portal
From the menu on the left side, select Keys and Endpoint. The endpoint and key are used
for API requests.

ï Š

1. Download and install the client library package for your language of choice:
ï¾‰

Language

Package version

.NET

5.2.0-beta.3

Java

5.2.0-beta.3

JavaScript

6.0.0-beta.1

Python

5.2.0b4

Expand table

2. After you install the client library, use the following samples on GitHub to start calling the
API.
Single label classification:
C#
Java

JavaScript
Python
Multi label classification:
C#
Java
JavaScript
Python
3. See the following reference documentation on the client, and return object:
C#
Java
JavaScript
Python

Next steps
Custom text classification overview

Last updated on 12/17/2025

Back up and recover your custom text
classification models
When you create a Language resource, you specify a region for it to be created in. From then on,
your resource and all of the operations related to it take place in the specified Azure server region.
It's rare, but not impossible, to encounter a network issue that hits an entire region. If your solution
needs to always be available, then you should design it to either fail-over into another region. Failover requires two Azure Language in Foundry Tools resources in different regions and the ability to
sync custom models across regions.
If your app or business depends on the use of a custom text classification model, we recommend
that you create a replica of your project into another supported region. So that if a regional outage
occurs, you can then access your model in the other fail-over region where you replicated your
project.
Replicating a project means that you export your project metadata and assets and import them into
a new project. Replication only makes a copy of your project settings and tagged data. You still need
to train and deploy the models to be available for use with prediction APIs

.

In this article, you learn to how to use the export and import APIs to replicate your project from one
resource to another existing in different supported geographical regions. We also provide guidance
for keeping your projects in sync and the updates needed for your runtime consumption.

Prerequisites
Two Language resources in different Azure regions. Create a Language resource and connect
them to an Azure storage account. We recommend that you connect both of your Language
resources to the same storage account. Although this step might introduce slightly higher
latency when importing your project, and training a model.

Get your resource keys endpoint
Use the following steps to get the keys and endpoint of your primary and secondary resources.
Go to your resource overview page in the Azure portal
From the menu on the left side, select Keys and Endpoint. The endpoint and key are used for
API requests.

ï Š

îª€ Tip
Keep a note of keys and endpoints for both primary and secondary resources. Use these values
to replace the following placeholders: {PRIMARY-ENDPOINT} , {PRIMARY-RESOURCE-KEY} ,
{SECONDARY-ENDPOINT} , and {SECONDARY-RESOURCE-KEY} . Also take note of your project name,

your model name, and your deployment name. Use these values to replace the following
placeholders: {PROJECT-NAME} , {MODEL-NAME} , and {DEPLOYMENT-NAME} .

Export your primary project assets
Start by exporting the project assets from the project in your primary resource.

Submit export job
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Create a POST request using the following URL, headers, and JSON body to export your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with your
own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:export?

stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This value is
case-sensitive.

MyProject

The version of the API you're calling. The
value referenced is the latest model version
released.

2022-05-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request body specifying that you want to export all the assets.
JSON
{
"assetsToExport": ["*"]
}

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/export/jobs/{JOB-ID}?
api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. Use this URL to get

the export job status.

Get export job status
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of exporting your project assets. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/export/jobs/{JOB-ID}?
api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is case-

myProject

NAME}

sensitive.

{JOB-ID}

The ID for locating your model's training
status. It's in the location header value you

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

received in the previous step.
{API-

The version of the API you're calling. The value

VERSION}

referenced is for the latest version released.
Learn more about other available API versions

2022-05-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response body
JSON

{
"resultUrl": "{RESULT-URL}",
"jobId": "string",
"createdDateTime": "2021-10-19T23:24:41.572Z",
"lastUpdatedDateTime": "2021-10-19T23:24:41.572Z",
"expirationDateTime": "2021-10-19T23:24:41.572Z",
"status": "unknown",
"errors": [
{
"code": "unknown",
"message": "string"
}
]
}

Use the url from the resultUrl key in the body and view the exported assets from this job.

Get export results
Submit a GET request using the {RESULT-URL} you received from the previous step to view the
results of the export job.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Copy the response body to use as the body for the next import job.

Import to a new project
Now go ahead and import the exported project assets in your new project in the secondary region
so you can replicate it.

Submit import job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and {SECONDARYRESOURCE-KEY} that you obtained in the first step.

Submit a POST request using the following URL, headers, and JSON body to import your labels file.
Make sure that your labels file follow the accepted format.

If a project with the same name already exists, the data of that project is replaced.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}/:import?api-version=
{API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This value is casesensitive.

myProject

The version of the API you're calling. The value
referenced is for the latest version released.

2022-05-01

Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request. Replace the placeholder values with your own values.
Multi label classification

JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectName": "{PROJECT-NAME}",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectKind": "customMultiLabelClassification",
"description": "Trying out custom multi label text classification",

"language": "{LANGUAGE-CODE}",
"multilingual": true,
"settings": {}
},
"assets": {
"projectKind": "customMultiLabelClassification",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
],
"documents": [
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
]
},
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"classes": [
{
"category": "Class2"
}
]
}
]
}
}

ï¾‰

Key

Placeholder

Value

Example

api-version

{API-VERSION}

The version
of the API

2022-05-01

you're
calling. The
version
used here
must be
the same

Expand table

Key

Placeholder

Value

Example

API version
in the URL.
Learn more
about
other
available
API
versions
projectName

{PROJECT-NAME}

The name
of your
project.

myProject

This value
is casesensitive.
projectKind

customMultiLabelClassification

Your
project
kind.

customMultiLabelClassification

language

{LANGUAGE-CODE}

A string
specifying
the

en-us

language
code for
the
documents
used in
your
project. If
your
project is a
multilingual
project,
choose the
language
code for
most of the
documents.
See
language
support to
learn more
about
multilingual
support.
multilingual

true

A boolean
value that
enables
you to have

true

Key

Placeholder

Value

Example

documents
in multiple
languages
in your
dataset and
when your
model is
deployed
you can
query the
model in
any
supported
language
(not
necessarily
included in
your
training
documents.
See
language
support to
learn more
about
multilingual
support.
storageInputContainerName

{CONTAINER-NAME}

The name
of your

myContainer

Azure
storage
container
for your
uploaded
documents.
classes

[]

Array

[]

containing
all the
classes you
have in the
project.
documents

[]

Array
containing
all the
documents
in your
project and
what the

[]

Key

Placeholder

Value

Example

classes
labeled for
this
document.
location

{DOCUMENT-NAME}

The

doc1.txt

location of
the
documents
in the
storage
container.
Since all
the
documents
are in the
root of the
container,
it should
be the
document
name.
dataset

{DATASET}

The test set
to which
this

Train

document
goes to
when split
before
training.
See How to
train a
model.
Possible
values for
this field
are Train
and Test .

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOB-ID}?
api-version={API-VERSION}

{JOB-ID} is used to identify your request, since this operation is asynchronous. You use this URL to

get the import job status.
Possible error scenarios for this request:
The selected resource doesn't have proper permissions for the storage account.
The storageInputContainerName specified doesn't exist.
Invalid language code is used, or if the language code type isn't string.
multilingual value is a string and not a boolean.

Get import job status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and {SECONDARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of your importing your project. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOB-ID}?
api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

The name of your project. This value is casesensitive.

myProject

The ID for locating your model's training
status. This value is in the location header

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECTNAME}
{JOB-ID}

value you received in the previous step.
{API-

The version of the API you're calling. The value

VERSION}

referenced is for the latest version released.
Learn more about other available API versions

Headers
Use the following header to authenticate your request.

2022-05-01

ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Train your model
After importing your project, you only copied the project's assets and metadata and assets. You still
need to train your model, which incurs usage on your account.

Submit training job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and {SECONDARYRESOURCE-KEY} that you obtained in the first step.

Submit a POST request using the following URL, headers, and JSON body to submit a training job.
Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:train?api-version=
{API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

The name of your project. This value is casesensitive.

myProject

The version of the API you're calling. The value
referenced is for the latest version released.

2022-05-01

{PROJECTNAME}
{APIVERSION}

Learn more about other available API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in your request body. The model will be given the {MODEL-NAME} once
training is complete. Only successful training jobs will produce models.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 80,
"testingSplitPercentage": 20
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-NAME}

The model name that is assigned to your model once

myModel

trained successfully.
trainingConfigVersion

{CONFIG-

This is the model version used to train the model.

2022-05-01

Option to split your data across training and testing

{}

VERSION}

evaluationOptions

sets.
kind

percentage

Split methods. Possible values are percentage or

percentage

manual . See How to train a model for more

information.
trainingSplitPercentage

80

Percentage of your tagged data to be included in the
training set. Recommended value is 80 .

80

testingSplitPercentage

20

Percentage of your tagged data to be included in the
testing set. Recommended value is 20 .

20

ï¼— Note
The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set to
percentage and the sum of both percentages should be equal to 100.

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOB-ID}?
api-version={API-VERSION}

{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this URL
to get the training status.

Get Training Status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and {SECONDARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOB-ID}?
api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is case-

myProject

NAME}

sensitive.

{JOB-ID}

The ID for locating your model's training

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

status. This value is in the location header
value you received in the previous step.
{API-

The version of the API you're calling. The value

VERSION}

referenced is for the latest version released.
For more information, see Model lifecycle.

Headers
Use the following header to authenticate your request.

2022-05-01

ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"result": {
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},
"jobId": "{JOB-ID}",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

Deploy your model
Deployment is the step where you make your trained model available form consumption via the
runtime prediction API

.

îª€ Tip
Use the same deployment name as your primary project for easier maintenance and minimal
changes to your system to handle redirecting your traffic.

Submit deployment job

Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and {SECONDARYRESOURCE-KEY} that you obtained in the first step.

Submit a PUT request using the following URL, headers, and JSON body to submit a deployment
job. Replace the placeholder values with your own values.
rest
{Endpoint}/language/authoring/analyzetext/projects/{projectName}/deployments/{deploymentName}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

The name of your project. This value is casesensitive.

myProject

staging

NAME}

The name of your deployment. This value is
case-sensitive.

{API-VERSION}

The version of the API you're calling. The

2022-05-01

{PROJECTNAME}
{DEPLOYMENT-

Expand table

value referenced is for the latest version
released. Learn more about other available
API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in the body of your request. Use the name of the model you to assign to the
deployment.
JSON
{
"trainedModelLabel": "{MODEL-NAME}"

}

ï¾‰

Expand table

Key

Placeholder

Value

Example

trainedModelLabel

{MODEL-

The model name that is assigned to your deployment. You can

myModel

NAME}

only assign successfully trained models. This value is casesensitive.

Once you send your API request, you receive a 202 response indicating that the job was submitted
correctly. In the response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this URL
to get the deployment status.

Get the deployment status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and {SECONDARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to query the status of the deployment job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is case-

myProject

NAME}

sensitive.

{DEPLOYMENT-

The name of your deployment. This value is
case-sensitive.

NAME}

staging

Placeholder

Value

Example

{JOB-ID}

The ID for locating your model's training

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

status. It's in the location header value you
received in the previous step.
{API-VERSION}

The version of the API you're calling. The

2022-05-01

value referenced is for the latest version
released. Learn more about other available
API versions

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the
status parameter changes to "succeeded". You should get a 200 code to indicate the success of the
request.
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Changes in calling the runtime
Within your system, at the step where you call runtime prediction API

check for the response code

returned from the submitted task API. If you observe a consistent failure in submitting the request, it
could indicate an outage in your primary region. Failure once doesn't mean an outage; it may be
transient issue. Retry submitting the job through the secondary resource you created. For the second
request use your {SECONDARY-ENDPOINT} , and {SECONDARY-RESOURCE-KEY} , if you followed the previous
steps, {PROJECT-NAME} and {DEPLOYMENT-NAME} would be the same so no changes are required to the
request body.

In case you revert to using your secondary resource you can observe slight increase in latency
because of the difference in regions where your model is deployed.

Check if your projects are out of sync
Maintaining the freshness of both projects is an important part of process. You need to frequently
check if any updates were made to your primary project so that you move them over to your
secondary project. This way if your primary region fail and you move into the secondary region you
should expect similar model performance since it already contains the latest updates. Setting the
frequency of checking if your projects are in sync is an important choice, we recommend that you do
this check daily in order to guarantee the freshness of data in your secondary model.

Get project details
Use the following url to get your project details, one of the keys returned in the body indicates the
last modified date of the project. Repeat the following step twice: once for your primary project, and
again for your secondary project. Compare the timestamp returned for both to check if they're out
of sync.
Use the following GET request to get your project details. Replace the placeholder values with your
own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is case-

myProject

NAME}

sensitive.

{API-

The version of the API you're calling. The value

VERSION}

referenced is for the latest version released.
Learn more about other available API versions

Headers
Use the following header to authenticate your request.

2022-05-01

ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response body
JSON
{
"createdDateTime": "2021-10-19T23:24:41.572Z",
"lastModifiedDateTime": "2021-10-19T23:24:41.572Z",
"lastTrainedDateTime": "2021-10-19T23:24:41.572Z",
"lastDeployedDateTime": "2021-10-19T23:24:41.572Z",
"projectKind": "customMultiLabelClassification",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectName": "{PROJECT-NAME}",
"multilingual": false,
"description": "Project description",
"language": "{LANGUAGE-CODE}"
}

Once you send your API request, you receive a 200 response indicating success and JSON response
body with your project details.
Repeat the same steps for your replicated project using {SECONDARY-ENDPOINT} , and {SECONDARYRESOURCE-KEY} . Compare the returned lastModifiedDateTime from both project. If your primary

project was modified sooner than your secondary one, you need to repeat the steps of exporting,
importing, training, and deploying your model.

Next steps
In this article, you learned how to use the export and import APIs to replicate your project to a
secondary Language resource in other region. Next, explore the API reference docs to see what else
you can do with authoring APIs.
Authoring REST API reference
Runtime prediction REST API reference

Last updated on 12/15/2025

Deploy custom language projects to
multiple regions
ï¼— Note
This article applies to the following custom features in Azure Language in Foundry Tools:
Conversational language understanding
Custom text classification
Custom named entity recognition (NER)
Orchestration workflow
Custom Language features enable you to deploy your project to more than one region. This
capability makes it much easier to access your project globally while you manage only one
instance of your project in one place. Beginning in November 2024, custom Language features
allow you to deploy your project to multiple resources within a single region using the API.
Thus, you can access and utilize your custom model wherever needed.
Before you deploy a project, you can assign deployment resources in other regions. Each
deployment resource is a different Language resource from the one that you use to author
your project. You deploy to those resources and then target your prediction requests to that
resource in their respective regions and your queries are served directly from that region.
When you create a deployment, you can select which of your assigned deployment resources
and their corresponding regions you want to deploy to. The model you deploy is then
replicated to each region and accessible with its own endpoint dependent on the deployment
resource's custom subdomain.

Example
Suppose you want to make sure your project, which is used as part of a customer support
chatbot, is accessible by customers across the United States and India. You author a project
with the name ContosoSupport by using a West US 2 Language resource named MyWestUS2 .
Before deployment, you assign two deployment resources to your project: MyEastUS and
MyCentralIndia in East US and Central India, respectively.

When you deploy your project, you select all three regions for deployment: the original West
US 2 region and the assigned ones through East US and Central India.
You now have three different endpoint URLs to access your project in all three regions:

West US 2: https://mywestus2.cognitiveservices.azure.com/language/:analyzeconversations

East US: https://myeastus.cognitiveservices.azure.com/language/:analyzeconversations

Central India: https://mycentralindia.cognitiveservices.azure.com/language/:analyzeconversations

The same request body to each of those different URLs serves the exact same response directly
from that region.

Validations and requirements
Assigning deployment resources requires Microsoft Entra authentication. Microsoft Entra ID is
used to confirm that you have access to the resources that you want to assign to your project
for multiregion deployment. In Language Studio, you can automatically enable Microsoft Entra
authentication

by assigning yourself the Azure Cognitive Services Language Owner role to

your original resource. To programmatically use Microsoft Entra authentication, learn more
from the Foundry Tools documentation.
Your project name and resource are used as its main identifiers. A Language resource can only
have a specific project name in each resource. Any other projects with the same name can't be
deployed to that resource.
For example, if a project ContosoSupport was created via the resource MyWestUS2 in West US 2
and deployed to the resource MyEastUS in East US, the resource MyEastUS can't create a
different project called ContosoSupport and deploy a project to that region. Similarly, your
collaborators can't then create a project ContosoSupport with the resource MyCentralIndia in
Central India and deploy it to either MyWestUS2 or MyEastUS .
You can only swap deployments that are available in the exact same regions. Otherwise,
swapping fails.
If you remove an assigned resource from your project, all of the project deployments to that
resource are deleted.
Some regions are only available for deployment and not for authoring projects.

Related content
Learn how to deploy models for:
Conversational language understanding

Custom text classification
Custom NER
Orchestration workflow

Last updated on 12/05/2025

Evaluation metrics
Your dataset is split into two parts: a set for training, and a set for testing. The training set is
used to train the model, while the testing set is used as a test for model after training to
calculate the model performance and evaluation. The testing set isn't introduced to the model
through the training process, to make sure that the model is tested on new data.
Model evaluation is triggered automatically after training is completed successfully. The
evaluation process starts by using the trained model to predict user defined classes for
documents in the test set, and compares them with the provided data tags (which establishes a
baseline of truth). The results are returned so you can review the model's performance. For
evaluation, custom text classification uses the following metrics:
Precision: Measures how precise/accurate your model is. It's the ratio between the
correctly identified positives (true positives) and all identified positives. The precision
metric reveals how many of the predicted classes are correctly labeled.
Precision = #True_Positive / (#True_Positive + #False_Positive)

Recall: Measures the model's ability to predict actual positive classes. It's the ratio
between the predicted true positives and what was tagged. The recall metric reveals how
many of the predicted classes are correct.
Recall = #True_Positive / (#True_Positive + #False_Negatives)

F1 score: The F1 score is a function of precision and recall. Needed when you seek a
balance between precision and recall.
F1 Score = 2 * Precision * Recall / (Precision + Recall)

ï¼— Note
Precision, recall, and F1 score are calculated for each class separately (class-level
evaluation) and for the model collectively (model-level evaluation).

Model-level and Class-level evaluation metrics
The definitions of precision, recall, and evaluation are the same for both class-level and modellevel evaluations. However, the count of True Positive, False Positive, and False Negative differ as
shown in the following example.
The below sections use the following example dataset:

Document

Actual classes

Predicted classes

1

action, comedy

comedy

2

action

action

3

romance

romance

4

romance, comedy

romance

5

comedy

action

ï¾‰

Expand table

ï¾‰

Expand table

Class-level evaluation for the action class

Key

Count

Explanation

True Positive

1

Document 2 was correctly classified as action.

False Positive

1

Document 5 was mistakenly classified as action.

False Negative

1

Document 1 wasn't classified as Action though it should have.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 1) = 0.5
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.5 * 0.5) / (0.5 + 0.5)
= 0.5

Class-level evaluation for the comedy class
ï¾‰

Expand table

Key

Count

Explanation

True positive

1

Document 1 was correctly classified as comedy.

False positive

0

No documents were mistakenly classified as comedy.

False negative

2

Documents 5 and 4 aren't classified as comedy though they should have.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 0) = 1

Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 2) = 0.33
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 1 * 0.67) / (1 + 0.67) =
0.80

Model-level evaluation for the collective model
ï¾‰

Expand table

Key

Count

Explanation

True Positive

4

Documents 1, 2, 3, and 4 were given correct classes at prediction.

False Positive

1

Document 5 assigned the wrong class at prediction.

False Negative

2

Documents 1 and 4 aren't given all correct class at prediction.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 4 / (4 + 1) = 0.8
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 4 / (4 + 2) = 0.67
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.8 * 0.67) / (0.8 +
0.67) = 0.73

ï¼— Note
For single-label classification models, the number of false negatives and false positives are
always equal. Custom single-label classification models always predict one class for each
document. If the prediction isn't correct, FP count of the predicted class increases by one
and FN of the actual class increases by one, overall count of FP and FN for the model is
always equal. Not the case for multi-label classification, because failing to predict one of
the classes of a document is counted as a false negative.

Interpreting class-level evaluation metrics
So what does it actually mean to have a high precision or a high recall for a certain class?
ï¾‰

Recall

Precision

Interpretation

High

High

The model successfully handled the class designation.

Expand table

Recall

Precision

Interpretation

Low

High

The model can't always predict this class but when it does it is with high confidence.
This evaluation may be because this class is underrepresented in the dataset so
consider balancing your data distribution.

High

Low

The model predicts this class well; however it is with low confidence. May be due to
class over-representation in the dataset so consider balancing your data distribution.

Low

Low

The model handled this class poorly without high confidence.

Custom text classification models are expected to experience both false negatives and false
positives. You need to consider how each affects the overall system. Carefully consider
scenarios where the model ignores correct predictions and recognize incorrect predictions.
Depending on your scenario, either precision or recall could be more suitable evaluating your
model's performance.
For example, if your scenario involves processing technical support tickets, predicting the
wrong class could cause it to be forwarded to the wrong department/team. In this example,
you should consider making your system more sensitive to false positives, and precision would
be a more relevant metric for evaluation.
As another example, if your scenario involves categorizing email as "important" or "spam," an
incorrect prediction could cause you to miss a useful email if labeled "spam." However, if a
spam email is labeled important you can disregard it. In this example, you should consider
making your system more sensitive to false negatives, and recall would be a more relevant
metric for evaluation.
If you want to optimize for general purpose scenarios or when precision and recall are both
important, you can utilize the F1 score. Evaluation scores are subjective depending on your
scenario and acceptance criteria. There's no absolute metric that works for every scenario.

Guidance
After you trained your model, you can see some guidance and recommendation on how to
improve the model. We recommend having a model covering all points in the guidance
section.
Training set has enough data: A class type with fewer than 15 labeled instances in the
training data can lead to lower accuracy due to the model having inadequate training on
cases.
All class types are present in test set: When the testing data lacks labeled instances for a
class type, the model's test performance may become less comprehensive due to

untested scenarios.
Class types are balanced within training and test sets: When sampling bias causes an
inaccurate representation of a class type's frequency, it can lead to lower accuracy due to
the model expecting that class type to occur too often or too little.
Class types are evenly distributed between training and test sets: When the mix of class
types doesn't match between training and test sets, it can lead to lower testing accuracy
due to the model being trained differently from how it's being tested.
Class types in training set are clearly distinct: When the training data is similar for multiple
class types, it can lead to lower accuracy because the class types may be frequently
misclassified as each other.

Confusion matrix
ï¼‰ Important
Confusion matrix is not available for multi-label classification projects. A Confusion matrix
is an N x N matrix used for model performance evaluation, where N is the number of
classes. The matrix compares the expected labels with the ones predicted by the model.
This gives a holistic view of how well the model is performing and what kinds of errors it is
making.
You can use the Confusion matrix to identify classes that are too close to each other and often
get mistaken (ambiguity). In this case consider merging these classes together. If that isn't
possible, consider labeling more documents with both classes to help the model differentiate
between them.
All correct predictions are located in the diagonal of the table, so it's easy to visually inspect
the table for prediction errors.

ï Š

You can calculate the class-level and model-level evaluation metrics from the confusion matrix:
The values in the diagonal are the True Positive values of each class.
The sum of the values in the class rows (excluding the diagonal) is the false positive of the
model.
The sum of the values in the class columns (excluding the diagonal) is the false Negative
of the model.
Similarly,
The true positive of the model is the sum of true Positives for all classes.
The false positive of the model is the sum of false positives for all classes.
The false Negative of the model is the sum of false negatives for all classes.

Next steps
View a model's performance in Language Studio
Train a model

Last updated on 11/18/2025

Accepted data formats
If you're trying to import your data into custom text classification, it has to follow a specific
format. If you don't have data to import, you can create your project and use Microsoft
Foundry to label your documents.

Labels file format
Your Labels file should be in the json format to enable you to import your labels into a project.
Multi label classification

JSON
{
"projectFileVersion": "2022-05-01",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "CustomMultiLabelClassification",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectName": "{PROJECT-NAME}",
"multilingual": false,
"description": "Project-description",
"language": "en-us"
},
"assets": {
"projectKind": "CustomMultiLabelClassification",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
],
"documents": [
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"classes": [
{
"category": "Class1"
},
{
"category": "Class2"
}
]
}

]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

multilingual

true

A boolean value that enables you to
have documents in multiple languages
in your dataset and when your model

true

is deployed you can query the model
in any supported language (not
necessarily included in your training
documents). See language support to
learn more about multilingual support.
projectName

{PROJECT-

Project name

myproject

Container name

mycontainer

Array containing all the classes you

[]

NAME}

storageInputContainerName

{CONTAINERNAME}

classes

[]

have in the project.
documents

[]

Array containing all the documents in

[]

your project and the classes labeled
for this document.
location

{DOCUMENTNAME}

The location of the documents in the
storage container. Since all the

doc1.txt

documents are in the root of the
container, this value should be the
document name.
dataset

{DATASET}

The test set to which this file goes to
when split before training. See How to

Train

train a model. Possible values for this
field are Train and Test .

Next steps
You can import your labeled data into your project directly. See How to create a project
to learn more about importing projects.
See the how-to article more information about labeling your data. When you're done
labeling your data, you can train your model.

Last updated on 12/15/2025

Project versioning
ï¼— Note
This article applies to the following custom features in Azure Language in Foundry Tools:
Conversational language understanding
Custom text classification
Custom named entity recognition (NER)
Orchestration workflow
Building your project typically happens in increments. You may add, remove, or edit intents,
entities, labels, and data at each stage. Every time you train, a snapshot of your current project
state is taken to produce a model. That model saves the snapshot to be loaded back at any
time. Every model acts as its own version of the project.
For example, if your project has 10 intents and/or entities, with 50 training documents or
utterances, it can be trained to create a model named v1. Afterwards, you might make changes
to the project to alter the numbers of training data. The project can be trained again to create
a new model named v2. If you don't like the changes you made in v2 and would like to
continue from where you left off in model v1, then you would just need to load the model data
from v1 back into the project. Loading a model's data is possible through both Microsoft
Foundry and API. Once complete, the project has the original amount and types of training
data.
If the project data isn't saved in a trained model, it can be lost. For example, if you loaded
model v1, your project now has the data that was used to train it. If you then made changes,
didn't train, and loaded model v2, you would lose those changes as they weren't saved to any
specific snapshot.
If you overwrite a model with a new snapshot of data, you can't revert back to any previous
state of that model.
You always can locally export the data for every model.

Data location
The data for your model versions is saved in different locations, depending on the custom
feature you're using.
Custom NER

In custom named entity recognition, the data being saved to the snapshot is the labels file.

Next steps
Learn how to load or export model data for:
Conversational language understanding
Custom text classification
Custom NER
Orchestration workflow

Last updated on 12/17/2025

Tutorial: Triage incoming emails with Power
Automate
In this tutorial, you learn to categorize and triage incoming email using custom text
classification. With this Power Automate flow, a new email is received, its contents are
classified, and, depending on the result, a message is sent to a designated channel on
Microsoft Teams

.

Prerequisites
Azure subscription - Create one for free
A Language resource
A trained custom text classification model.
You need the key and endpoint from your Language resource to authenticate your
Power Automate flow.
A successfully created and deployed single text classification custom model

Create a Power Automate flow
1. Sign in to Power Automate
2. From the left side menu, select My flows and create a Automated cloud flow

ï Š

3. Name your flow EmailTriage . Below Choose your flow's triggers, search for email and
select When a new email arrives. Then select create

ï Š

4. Add the right connection to your email account. This connection is used to access the
email content.
5. To add a Language connector, search for Azure Language in Foundry Tools.

ï Š

6. Search for CustomSingleLabelClassification.

ï Š

7. Start by adding the right connection to your connector. This connection is used to access
the classification project.
8. In the documents ID field, add 1.
9. In the documents text field, add body from dynamic content.
10. Fill in the project name and deployment name of your deployed custom text classification
model.

ï Š

11. Add a condition to send a Microsoft Teams message to the right team by:
a. Select results from dynamic content, and add the condition. For this tutorial, we're
looking for Computer_science related emails. In the Yes condition, choose your desired
option to notify a team channel. In the No condition, you can add other conditions to
perform alternative actions.

ï Š

Next steps
Use Azure Language with Power Automate
Available Language connectors

Last updated on 12/15/2025

Terms and definitions used in custom text
classification
Use this article to learn about some of the definitions and terms you may encounter when
using custom text classification.

Class
A class is a user-defined category that indicates the overall classification of the text. Developers
label their data with their classes before they pass it to the model for training.

F1 score
The F1 score is a function of Precision and Recall. Needed when you seek a balance between
precision and recall.

Model
A model is an object trained to do a certain task, in this case text classification tasks. Models
are trained by providing labeled data to learn from so they can later be used for classification
tasks.
Model training is the process of teaching your model how to classify documents based
on your labeled data.
Model evaluation is the process that happens right after training to know how well does
your model perform.
Deployment is the process of assigning your model to a deployment to make it available
for use via the prediction API

.

Precision
Measures how precise/accurate your model is. It's the ratio between the correctly identified
positives (true positives) and all identified positives. The precision metric reveals how many of
the predicted classes are correctly labeled.

Project
A project is a work area for building your custom ML models based on your data. Your project
is only accessible by you and others who have access to the Azure resource being used. As a

prerequisite to creating a custom text classification project, you have to connect your resource
to a storage account with your dataset when you create a new project. Your project
automatically includes all the .txt files available in your container.
Within your project, you can do the following actions:
Label your data: The process of labeling your data so that when you train your model it
learns what you want to extract.
Build and train your model: The core step of your project, where your model starts
learning from your labeled data.
View model evaluation details: Review your model performance to decide if there's room
for improvement, or you're satisfied with the results.
Deployment: After you review model performance and decide if it can be used in your
environment, you need to assign it to a deployment to be able to query it. Assigning the
model to a deployment makes it available for use through the prediction API .
Test model: After deploying your model, you can use this operation in Language Studio
to try it out your deployment and see how it would perform in production.

Project types
Custom text classification supports two types of projects
Single label classification - you can assign a single class for each document in your
dataset. For example, a movie script could only be classified as "Romance" or "Comedy."
Multi label classification - you can assign multiple classes for each document in your
dataset. For example, a movie script could be classified as "Comedy" or "Romance" and
"Comedy."

Recall
Measures the model's ability to predict actual positive classes. It's the ratio between the
predicted true positives and what was tagged. The recall metric reveals how many of the
predicted classes are correct.

Next steps
Data and service limits.
Custom text classification overview.

Last updated on 11/18/2025

Custom text classification limits
Use this article to learn about the data and service limits when using custom text classification.

Language resource limits
Your Language resource has to be created in one of the supported regions and pricing
tiers listed here.
You can only connect one storage account per resource. This process is irreversible. If you
connect a storage account to your resource, you can't unlink it later. Learn more about
connecting a storage account
You can have up to 500 projects per resource.
Project names have to be unique within the same resource across all custom features.

Pricing tiers
Custom text classification is available with the following pricing tiers:
ï¾‰

Expand table

Tier

Description

Limit

F0

Free tier

You're only allowed one F0 tier Language resource per subscription.

S

Paid tier

You can have unlimited Language S tier resources per subscription.

See pricing

.

Regional availability
See Language regional availability.

API limits
ï¾‰

Expand table

Item

Request
type

Maximum limit

Authoring
API

POST

10 per minute

Authoring
API

GET

100 per minute

Prediction
API

GET/POST

1,000 per minute

Document
size

--

125,000 characters. You can send up to 25 documents as long as they
collectively don't exceed 125,000 characters

îª€ Tip
If you need to send larger files than the limit allows, you can break the text into smaller
chunks of text before sending them to the API. You use can the chunk command from
CLUtils

for this process.

Quota limits
ï¾‰

Pricing tier

Item

Limit

F

Training time

One hour per month

S

Training time

Unlimited, Standard

F

Prediction Calls

5,000 text records per month

S

Prediction Calls

Unlimited, Standard

Expand table

Document limits
You can only use .txt . files. If your data is in another format, you can use the CLUtils
parse command

to open your document and extract the text.

All files uploaded in your container must contain data. Empty files aren't allowed for
training.
All files should be available at the root of your container.

Data limits
The following limits are observed for the custom text classification.
ï¾‰

Expand table

Item

Lower
Limit

Upper Limit

Documents count

10

100,000

Document length in characters

1

128,000 characters; approximately 28,000 words or
56 pages.

Count of classes

1

200

Count of trained models per project

0

10

Count of deployments per project

0

10

0

1

(paid tier)
Count of deployments per project
(free tier)

Naming limits
ï¾‰

Expand table

Item

Limits

Project name

You can only use letters (a-z, A-Z) , and numbers (0-9) , symbols _ . - , with no
spaces. Maximum allowed length is 50 characters.

Model name

You can only use letters (a-z, A-Z) , numbers (0-9) , and symbols _ . - . Maximum
allowed length is 50 characters.

Deployment
name

You can only use letters (a-z, A-Z) , numbers (0-9) , and symbols _ . - . Maximum

Class name

You can only use letters (a-z, A-Z) , numbers (0-9) and, all symbols except ":", $ & %

allowed length is 50 characters.

* ( ) + ~ # / ? . Maximum allowed length is 50 characters.

Document
name

You can only use letters (a-z, A-Z) , and numbers (0-9) with no spaces.

Next steps

Custom text classification overview

Last updated on 11/18/2025

What is entity linking in Azure Language in
Foundry Tools?
ï¼‰ Important
Entity Linking is retiring from Azure Language in Foundry Tools effective September 1,
2028. After this date, the Entity Linking feature is no longer supported. During the support
window, we recommend that users migrate existing workloads and direct all new projects
to Language Named Entity Recognition or consider other alternative solutions.
Entity linking is one of the features offered by Language, a collection of machine learning and
AI algorithms in the cloud for developing intelligent applications that involve written language.
Entity linking identifies and disambiguates the identity of entities found in text. For example, in
the sentence "We went to Seattle last week.", the word "Seattle" would be identified, with a link
to more information on Wikipedia.
This documentation contains the following types of articles:
Quickstarts are getting-started instructions to guide you through making requests to the
service.
How-to guides contain instructions for using the service in more specific ways.

Get started with entity linking
To use entity linking, you submit raw unstructured text for analysis and handle the API output
in your application. Analysis is performed as-is, with no additional customization to the model
used on your data. There are two ways to use entity linking:
ï¾‰

Expand table

Development option

Description

Language studio

Language Studio is a web-based platform that lets you try entity linking with text
examples without an Azure account, and your own data when you sign up. For
more information, see the Language Studio website .

REST API or Client
library (Azure SDK)

Integrate entity linking into your applications using the REST API, or the client
library available in a variety of languages. For more information, see the entity
linking quickstart.

Reference documentation and code samples

As you use this feature in your applications, see the following reference documentation and
samples for Azure Language in Foundry Tools:
ï¾‰

Expand table

Development option / language

Reference documentation

Samples

REST API

REST API documentation

C#

C# documentation

C# samples

Java

Java documentation

Java Samples

JavaScript

JavaScript documentation

JavaScript samples

Python

Python documentation

Python samples

Responsible AI
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Read the transparency
note for entity linking to learn about responsible AI use and deployment in your systems.
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Next steps
There are two ways to get started using the entity linking feature:
Language Studio

, which is a web-based platform that enables you to try several

Language features without needing to write code.
The quickstart article for instructions on making requests to the service using the REST
API and client library SDK.

Last updated on 11/18/2025

Quickstart: Entity Linking using the client
library and REST API
ï¼‰ Important
Entity Linking is retiring from Azure Language in Foundry Tools effective September 1,
2028. After this date, the Entity Linking feature is no longer supported. During the support
window, we recommend that users migrate existing workloads and direct all new projects
to Language Named Entity Recognition or consider other alternative solutions.
Reference documentation | More samples

| Package (NuGet)

| Library source code

Use this quickstart to create an entity linking application with the client library for .NET. In the
following example, you create a C# application that can identify and disambiguate entities
found in text.
îª€ Tip
You can use Microsoft Foundry to try summarization without needing to write code.

Prerequisites
Azure subscription - Create one for free
The Visual Studio IDE

Setting up
Create an Azure resource
To use the code sample below, you need to deploy an Azure resource. This resource will
contain a key and endpoint you use to authenticate the API calls you send to Azure Language.
1. Use the following link to create a language resource

using the Azure portal. You need

to sign in using your Azure subscription.
2. On the Select additional features screen that appears, select Continue to create your
resource.

ï Š

3. In the Create language screen, provide the following information:
ï¾‰

Expand table

Detail

Description

Subscription

The subscription account that your resource will be associated with. Select your
Azure subscription from the drop-down menu.

Resource

A resource group is a container that stores the resources you create. Select Create

group

new to create a new resource group.

Region

The location of your Language resource. Different regions may introduce latency
depending on your physical location, but have no impact on the runtime availability
of your resource. For this quickstart, either select an available region near you, or
choose East US.

Name

The name for your Language resource. This name will also be used to create an
endpoint URL that your applications will use to send API requests.

Pricing tier

The pricing tier

for your Language resource. You can use the Free F0 tier to try

the service and upgrade later to a paid tier for production.

ï Š

4. Make sure the Responsible AI Notice checkbox is checked.
5. Select Review + Create at the bottom of the page.
6. In the screen that appears, make sure the validation has passed, and that you entered
your information correctly. Then select Create.

Get your key and endpoint
Next you will need the key and endpoint from the resource to connect your application to the
API. You'll paste your key and endpoint into the code later in the quickstart.
1. After Azure Language resource deploys successfully, click the Go to Resource button
under Next Steps.

ï Š

2. On the screen for your resource, select Keys and endpoint on the left pane. You will use
one of your keys and your endpoint in the steps below.

ï Š

Create environment variables
Your application must be authenticated to send API requests. For production, use a secure way
of storing and accessing your credentials. In this example, you will write your credentials to
environment variables on the local machine running the application.
To set the environment variable for your Language resource key, open a console window, and
follow the instructions for your operating system and development environment.
To set the LANGUAGE_KEY environment variable, replace your-key with one of the keys for
your resource.
To set the LANGUAGE_ENDPOINT environment variable, replace your-endpoint with the
endpoint for your resource.

ï¼‰ Important
We recommend Microsoft Entra ID authentication with managed identities for Azure
resources to avoid storing credentials with your applications that run in the cloud.
Use API keys with caution. Don't include the API key directly in your code, and never post
it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys
regularly, and restrict access to Azure Key Vault using role based access control and
network access restrictions. For more information about using API keys securely in your
apps, see API keys with Azure Key Vault.
For more information about AI services security, see Authenticate requests to Azure AI
services.

Windows

Console
setx LANGUAGE_KEY your-key

Console
setx LANGUAGE_ENDPOINT your-endpoint

ï¼— Note
If you only need to access the environment variables in the current running console,
you can set the environment variable with set instead of setx .
After you add the environment variables, you might need to restart any running programs
that will need to read the environment variables, including the console window. For
example, if you're using Visual Studio as your editor, restart Visual Studio before running
the example.

Create a new .NET Core application
Using the Visual Studio IDE, create a new .NET Core console app. This will create a "Hello
World" project with a single C# source file: program.cs.

Install the client library by right-clicking the solution in the Solution Explorer and selecting
Manage NuGet Packages. In the package manager that opens select Browse and search for
Azure.AI.TextAnalytics . Select version 5.2.0 , and then Install. You can also use the Package

Manager Console.

Code example
Copy the following code into your program.cs file and run the code.
C#
using Azure;
using System;
using System.Globalization;
using Azure.AI.TextAnalytics;
namespace EntityLinkingExample
{
class Program
{
// This example requires environment variables named "LANGUAGE_KEY" and
"LANGUAGE_ENDPOINT"
static string languageKey =
Environment.GetEnvironmentVariable("LANGUAGE_KEY");
static string languageEndpoint =
Environment.GetEnvironmentVariable("LANGUAGE_ENDPOINT");
private static readonly AzureKeyCredential credentials = new
AzureKeyCredential(languageKey);
private static readonly Uri endpoint = new Uri(languageEndpoint);
// Example method for recognizing entities and providing a link to an online
data source.
static void EntityLinkingExample(TextAnalyticsClient client)
{
var response = client.RecognizeLinkedEntities(
"Microsoft was founded by Bill Gates and Paul Allen on April 4,
1975, " +
"to develop and sell BASIC interpreters for the Altair 8800. " +
"During his career at Microsoft, Gates held the positions of
chairman, " +
"chief executive officer, president and chief software architect, "
+
"while also being the largest individual shareholder until May
2014.");
Console.WriteLine("Linked Entities:");
foreach (var entity in response.Value)
{
Console.WriteLine($"\tName: {entity.Name},\tID:
{entity.DataSourceEntityId},\tURL: {entity.Url}\tData Source: {entity.DataSource}");
Console.WriteLine("\tMatches:");

foreach (var match in entity.Matches)
{
Console.WriteLine($"\t\tText: {match.Text}");
Console.WriteLine($"\t\tScore: {match.ConfidenceScore:F2}\n");
}
}
}
static void Main(string[] args)
{
var client = new TextAnalyticsClient(endpoint, credentials);
EntityLinkingExample(client);
Console.Write("Press any key to exit.");
Console.ReadKey();
}
}
}

Output
Console
Linked Entities:
Name: Microsoft,
ID: Microsoft, URL:
https://en.wikipedia.org/wiki/Microsoft
Data Source: Wikipedia
Matches:
Text: Microsoft
Score: 0.55
Text: Microsoft
Score: 0.55
Name: Bill Gates,
ID: Bill Gates, URL:
https://en.wikipedia.org/wiki/Bill_Gates
Data Source: Wikipedia
Matches:
Text: Bill Gates
Score: 0.63
Text: Gates
Score: 0.63
Name: Paul Allen,
ID: Paul Allen, URL:
https://en.wikipedia.org/wiki/Paul_Allen
Data Source: Wikipedia
Matches:
Text: Paul Allen
Score: 0.60
Name: April 4, ID: April 4,
Data Source: Wikipedia
Matches:

URL: https://en.wikipedia.org/wiki/April_4

Text: April 4
Score: 0.32
Name: BASIC,
ID: BASIC,
Data Source: Wikipedia
Matches:
Text: BASIC
Score: 0.33

URL: https://en.wikipedia.org/wiki/BASIC

Name: Altair 8800,
ID: Altair 8800,
URL:
https://en.wikipedia.org/wiki/Altair_8800 Data Source: Wikipedia
Matches:
Text: Altair 8800
Score: 0.88

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
Entity linking language support
How to call the entity linking API
Reference documentation
Additional samples

Last updated on 11/18/2025

Entity linking language support
ï¼‰ Important
Entity Linking is retiring from Azure Language in Foundry Tools effective September 1,
2028. After this date, the Entity Linking feature is no longer supported. During the support
window, we recommend that users migrate existing workloads and direct all new projects
to Language Named Entity Recognition or consider other alternative solutions.

ï¾‰

Language

Language code

English

en

Spanish

es

Next steps
Entity linking overview

Last updated on 11/18/2025

Expand table
Notes

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

How to use entity linking
ï¼‰ Important
Entity Linking is retiring from Azure Language in Foundry Tools effective September 1,
2028. After this date, the Entity Linking feature is no longer supported. During the support
window, we recommend that users migrate existing workloads and direct all new projects
to Language Named Entity Recognition or consider other alternative solutions.
The entity linking feature enables the detection and clarification of the specific identity of
entities mentioned within text. For instance, it can determine whether the term "Mars" refers to
the planet or to the Roman god of war. This capability helps eliminate ambiguity by associating
each entity with the correct context. It returns the entities in the text with links to Wikipedia
as a knowledge base.

Development options
To use entity linking, you submit raw unstructured text for analysis and handle the API output
in your application. Analysis is performed as-is, with no additional customization to the model
used on your data. There are two ways to use entity linking:
ï¾‰

Expand table

Development option

Description

Language studio

Language Studio is a web-based platform that lets you try entity linking with text
examples without an Azure account, and your own data when you sign up. For
more information, see the Language Studio website .

REST API or Client
library (Azure SDK)

Integrate entity linking into your applications using the REST API, or the client
library available in a variety of languages. For more information, see the entity
linking quickstart.

Determine how to process the data (optional)
Specify the entity linking model
By default, entity linking uses the latest available AI model on your text. You can also configure
your API requests to use a specific model version.

Input languages
When you submit documents to for entity linking processing, you can specify which of the
supported languages they're written in. If you don't specify a language, entity linking defaults
to English. Due to multilingual and emoji support, the response may contain text offsets.

Submitting data
Entity linking produces a higher-quality result when you give it smaller amounts of text to work
on. This characteristic is opposite from some features, like key phrase extraction that performs
better on larger blocks of text. To get the best results from both operations, consider
restructuring the inputs accordingly.
To send an API request, you need a Language resource endpoint and API key.
ï¼— Note
You can find the key and endpoint for your Language resource on the Azure portal.
They're located on the resource's Key and endpoint page, under resource management.
Analysis is performed upon receipt of the request. Using entity linking synchronously is
stateless. No data is stored in your account, and results are returned immediately in the
response.
When using this feature asynchronously, the API results are available for 24 hours from the
time the request was ingested, and is indicated in the response. After this time period, the
results are purged and are no longer available for retrieval.

Getting entity linking results
You can stream the results to an application, or save the output to a file on the local system.

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

See also
Entity linking overview

Last updated on 11/18/2025

What is language detection in Azure
Language in Foundry Tools?
Language detection is one of the features offered by Azure Language in Foundry Tools, a
collection of machine learning and AI algorithms in the cloud for developing intelligent
applications that involve written language. Language detection is able to detect more than 100
languages in their primary script. In addition, the service offers script detection for each
detected language using ISO 15924 standard

for a select number of languages. This

documentation contains the following types of articles:
Quickstarts are getting-started instructions to guide you through making requests to the
service.
How-to guides contain instructions for using the service in more specific or customized
ways.

Language detection features
Language detection: For each document, returns the main language, its ISO 639-1 code,
readable name, confidence score, script name, and ISO 15924 script code.
Script detection: To distinguish between multiple scripts used to write certain languages,
such as Kazakh, language detection returns a script name and script code according to
the ISO 15924 standard.
Ambiguous content handling: To help disambiguate language based on the input, you
can specify an ISO 3166-1 alpha-2 country/region code. For example, the word
"communication" is common to both English and French. Specifying the origin of the text
as France can help the language detection model determine the correct language.

Typical workflow
To use this feature, you submit data for analysis and handle the API output in your application.
Analysis is performed as-is, with no added customization to the model used on your data.
1. Create an Azure Language in Foundry Tools resource, which grants you access to the
features offered by Language. It generates a password (called a key) and an endpoint URL
that you use to authenticate API requests.
2. Create a request using either the REST API or the client library for C#, Java, JavaScript, and
Python. You can also send asynchronous calls with a batch request to combine API
requests for multiple features into a single call.

3. Send the request containing your text data. Your key and endpoint are used for
authentication.
4. Stream or store the response locally.

Get started with language detection
To use language detection, you submit raw unstructured text for analysis and handle the API
output in your application. Analysis is performed as-is, with no additional customization to the
model used on your data. There are three ways to use language detection:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry

Foundry (new) is a cloud-based AI platform that provides streamlined access to

(new)

Foundry models, agents, and tools through Foundry projects.

portal

Foundry (classic)

Foundry (classic) is a cloud-based platform that supports hub-based projects

portal

and other resource types. When you sign up, you can use your own data to
detect more than 100 languages in their primary script.

REST API or Client
library (Azure SDK)

Integrate language detection into your applications using the REST API, or the
client library available in various languages.

Docker container

Use the available Docker container to deploy this feature on-premises. Docker
containers enable you to bring the service closer to your data for compliance,
security, or other operational considerations.

Responsible AI
An AI system includes not only the technology, but also individuals who operate the system,
people who experience its effects, and the broader environment where the system functions.
Read the transparency note for language detection to learn about responsible AI use and
deployment in your systems.
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Next steps
There are two ways to get started using the entity linking feature:

Microsoft Foundry is a web-based platform that lets you use several Language features
without needing to write code.
The quickstart article for instructions on making requests to the service using the REST
API and client library SDK.

Last updated on 11/18/2025

Quickstart: using Azure Language
Detection client library and REST API
Prerequisites
Create a Project in Foundry in the Microsoft Foundry portal
Foundry (classic)

ï¼— Note
This content refers to the Foundry (classic)

portal, which supports hub-based

projects and other resource types. To confirm that you're using Foundry (classic),
make sure the version toggle in the portal banner is in the off position.

You can use Foundry (classic)

to:

ï¼‚ Create a project
ï¼‚ Deploy a model
ï¼‚ Run a chat completion
ï¼‚ Create and run an agent
ï¼‚ Upload files to your agent

Foundry (classic)

Playground

Using the left side pane, select Playgrounds. Then select the Try Azure Language
Playground button.

ï Š

Use Language Detection in the Foundry
Playground
The Language Playground consists of four sections:
Top banner: You can select any of the currently available Languages here.
Right pane: This pane is where you can find the Configuration options for the
service, such as the API and model version, along with features specific to the service.
Center pane: This pane is where you enter your text for processing. After the
operation is run, some results are shown here.
Right pane: This pane is where Details of the run operation are shown.
Here you can select Azure Language Detection capability by choosing the top banner tile,
Detect language.

Use Detect language

Detect language is designed to identify the language typed in text.
In Configuration there are the following options:
ï¾‰

Option

Description

Select API version

Select which version of the API to use.

Select model version

Select which version of the model to use.

Select country hint

Select the origin country/region of the input text.

Expand table

After your operation is completed, the Details section contains the following fields for the
most detected language and script:
ï¾‰

Expand table

Field

Description

ISO 639-1 Code

The ISO 639-1 code for the most detected language.

Confidence Score

How confident the model is in the correctness of identification of the most
typed language.

Script Name

The name of the most detected script in the text.

ISO 15924 Script

The ISO 15924 script code for the most detected script.

Code

ï Š

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
Language detection overview

Last updated on 11/18/2025

Language support for Language Detection
Use this article to learn which natural languages that language detection supports.
The Language Detection feature can detect a wide range of languages, variants, dialects, and
some regional/cultural languages, and return detected languages with their name and code.
The returned language code parameters conform to BCP-47
conforming to ISO-639-1

standard with most of them

identifiers.

If you have content expressed in a less frequently used language, you can try Language
Detection to see if it returns a code. The response for languages that can't be detected is
unknown .

Languages supported by Language Detection
ï¾‰

Language

Language Code

Supported Script Code

Afrikaans

af

Latn

Albanian

sq

Latn

Amharic

am

Ethi

Arabic

ar

Arab

Armenian

hy

Armn

Assamese

as

Beng , Latn

Azerbaijani

az

Latn

Bashkir

ba

Cyrl

Basque

eu

Latn

Belarusian

be

Cyrl

Bengali

bn

Beng , Latn

Bhojpuri

bho

Deva

Bodo

brx

Deva

Bosnian

bs

Latn

Bulgarian

bg

Cyrl

Expand table

Language

Language Code

Supported Script Code

Burmese

my

Mymr

Catalan

ca

Latn

Central Khmer

km

Khmr

Checheni

ce

Cyrl

Chhattisgarhi

hne

Deva

Chinese Literal

lzh

Hani

Chinese Simplified

zh_chs

Hans

Chinese Traditional

zh_cht

Hant

Chuvash

cv

Cyrl

Corsican

co

Latn

Croatian

hr

Latn

Czech

cs

Latn

Danish

da

Latn

Dari

prs

Arab

Divehi

dv

Thaa

Dogri

dgo

Deva

Dutch

nl

Latn

English

en

Latn

Esperanto

eo

Latn

Estonian

et

Latn

Faroese

fo

Latn

Fijian

fj

Latn

Finnish

fi

Latn

French

fr

Latn

Galician

gl

Latn

Georgian

ka

Gujr

Language

Language Code

Supported Script Code

German

de

Latn

Greek

el

Grek

Gujarati

gu

Gujr , Latn

Haitian

ht

Latn

Hausa

ha

Latn

Hebrew

he

Hebr

Hindi

hi

Deva , Latn

Hmong Daw

mww

Latn

Hungarian

hu

Latn

Icelandic

is

Latn

Igbo

ig

Latn

Indonesian

id

Latn

Inuktitut

iu

Cans , Latn

Inuinnaqtun

ikt

Latn

Irish

ga

Latn

Italian

it

Latn

Japanese

ja

Jpan

Javanese

jv

Latn

Kannada

kn

Knda , Latn

Kashmiri

ks

Arab , Deva , Shrd

Kazakh

kk

Cyrl

Kinyarwanda

rw

Latn

Kirghiz

ky

Cyrl

Konkani

gom

Deva

Korean

ko

Hang

Kurdish

ku

Arab

Language

Language Code

Supported Script Code

Kurdish (Northern)

kmr

Latn

Lao

lo

Laoo

Latin

la

Latn

Latvian

lv

Latn

Lithuanian

lt

Latn

Lower Siberian

dsb

Latn

Luxembourgish

lb

Latn

Macedonian

mk

Cyrl

Maithili

mai

Deva

Malagasy

mg

Latn

Malay

ms

Latn

Malayalam

ml

Mlym , Latn

Maltese

mt

Latn

Maori

mi

Latn

Marathi

mr

Deva , Latn

Meitei

mni

Mtei

Mongolian

mn

Cyrl , Mong

Nepali

ne

Deva

Norwegian

no

Latn

Norwegian Nynorsk

nn

Latn

Odia

or

Orya , Latn

Pashto

ps

Arab

Persian

fa

Arab

Polish

pl

Latn

Portuguese

pt

Latn

Punjabi

pa

Guru , Latn

Language

Language Code

Supported Script Code

Queretaro Otomi

otq

Latn

Romanian

ro

Latn

Russian

ru

Cyrl

Samoan

sm

Latn

Sanscrit

sa

Deva

Santali

sat

Olck

Serbian

sr

Latn , Cyrl

Shona

sn

Latn

Sindhi

sd

Arab

Sinhala

si

Sinh

Slovak

sk

Latn

Slovenian

sl

Latn

Somali

so

Latn

Spanish

es

Latn

Sundanese

su

Latn

Swahili

sw

Latn

Swedish

sv

Latn

Tagalog

tl

Latn

Tahitian

ty

Latn

Tajik

tg

Cyrl

Tamil

ta

Taml , Latn

Tatar

tt

Cyrl

Telugu

te

Telu , Latn

Thai

th

Thai

Tibetan

bo

Tibt

Tigrinya

ti

Ethi

Language

Language Code

Supported Script Code

Tongan

to

Latn

Turkish

tr

Latn

Turkmen

tk

Latn

Upper Sorbian

hsb

Latn

Uyghur

ug

Arab

Ukrainian

uk

Latn

Urdu

ur

Arab , Latn

Uzbek

uz

Latn

Vietnamese

vi

Latn

Welsh

cy

Latn

Xhosa

xh

Latn

Yiddish

yi

Hebr

Yoruba

yo

Latn

Yucatec Maya

yua

Latn

Zulu

zu

Latn

Romanized Indic Languages supported by
Language Detection
ï¾‰

Language

Language Code

Assamese

as

Bengali

bn

Gujarati

gu

Hindi

hi

Kannada

kn

Expand table

Language

Language Code

Malayalam

ml

Marathi

mr

Odia

or

Punjabi

pa

Tamil

ta

Telugu

te

Urdu

ur

Script detection
ï¾‰

Language

Script code

Scripts

Assamese

as

Latn , Beng

Bengali

bn

Latn , Beng

Gujarati

gu

Latn , Gujr

Hindi

hi

Latn , Deva

Kannada

kn

Latn , Knda

Malayalam

ml

Latn , Mlym

Marathi

mr

Latn , Deva

Odia

or

Latn , Orya

Punjabi

pa

Latn , Guru

Tamil

ta

Latn , Taml

Telugu

te

Latn , Telu

Urdu

ur

Latn , Arab

Tatar

tt

Latn , Cyrl

Serbian

sr

Latn , Cyrl

Expand table

Language

Script code

Scripts

Inuktitut

iu

Latn , Cans

Next steps
Language detection overview

Last updated on 11/18/2025

Transparency note for Language Detection
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
ï¼‰ Important
This article assumes that you're familiar with guidelines and best practices for Azure
Language in Foundry Tools. For more information, see Transparency note for Language.
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance. Microsoft's Transparency
Notes are intended to help you understand how our AI technology works, the choices system
owners can make that influence system performance and behavior, and the importance of
thinking about the whole system, including the technology, the people, and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who will use or be affected by your system.
Microsoft's Transparency notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Responsible AI principles from Microsoft.

Introduction to language detection
The language detection feature of Language detects the language an input text is written in
and reports a single language code for every document submitted on the request in a wide
range of languages, variants, dialects, and some regional/cultural languages. The language
code is paired with a confidence score.
Be sure to check the list of supported languages to ensure the languages you need are
supported.

Example use cases

Language detection is used in multiple scenarios across a variety of industries. Some examples
include:
Preprocessing text of other Language features. Other Language features require a
language code to be sent in the request to identify the source language. If you don't
know the source language of your text, you can use language detection as a preprocessor to obtain the language code.
Detect languages for business workflow. For example, if a company receives email in
various languages from customers, they could use language detection to route the emails
by language to native speakers that can communicate best with those customers.

Considerations when choosing a use case
Do not use
Do not use for automatic actions without human intervention for high risk scenarios. A
person should always review source data when another person's economic situation,
health or safety is affected.
Legal and regulatory considerations: Organizations need to evaluate potential specific legal
and regulatory obligations when using any AI services and solutions, which may not be
appropriate for use in every industry or scenario. Additionally, AI services or solutions are not
designed for and may not be used in ways prohibited in applicable terms of service and
relevant codes of conduct.

Characteristics and limitations
Depending on your scenario and input data, you could experience different levels of
performance. The following information is designed to help you understand key concepts
about performance as they apply to using Language's language detection.

System limitations and best practices for enhancing
performance
For inputs that include mixed-language content only a single language is returned. In
general the language with the largest representation in the content is returned, but with a
lower confidence score.
The service does not yet support the romanized versions of all languages that do not use
the Latin script. For example, Pinyin is not supported for Chinese and Franco-Arabic is not
supported for Arabic.

Some words exist in multiple languages. For example, "impossible" is common to both
English and French. For short samples that include ambiguous words, you may not get
the right language.
If you have some idea about the country or region of origin of your text, and you
encounter mixed languages, you can use the countryHint parameter to pass in a 2 letter
country/region code.
In general longer inputs are more likely to be correctly recognized. Full phrases or
sentences are more likely to be correctly recognized than single words or sentence
fragments.
Not all languages will be recognized. Be sure to check the list of supported languages
and scripts.
To distinguish between multiple scripts used to write certain languages such as Kazakh,
the language detection feature returns a script name and script code according to the ISO
15924 standard

for a limited set of scripts.

The service supports language detection of text only if it is in native script. For example,
Pinyin is not supported for Chinese and Franco-Arabic is not supported for Arabic.
Due to unknown gaps in our training data, certain dialects and language varieties less
represented in web data may not be properly recognized.

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for Health
Transparency note for Key Phrase Extraction
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

How to use language detection
The Language Detection feature can evaluate text, and return a language identifier that
indicates the language a document was written in.
Language detection is useful for content stores that collect arbitrary text, where language is
unknown. You can parse the results of this analysis to determine which language is used in the
input document. The response also returns a score between 0 and 1 that reflects the
confidence of the model.
The Language Detection feature can detect a wide range of languages, variants, dialects, and
some regional or cultural languages.

Development options
To use language detection, you submit raw unstructured text for analysis and handle the API
output in your application. Analysis is performed as-is, with no additional customization to the
model used on your data. There are three ways to use language detection:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry
(new) portal

Foundry (new) is a cloud-based AI platform that provides streamlined access to
Foundry models, agents, and tools through Foundry projects.

Foundry (classic)

Foundry (classic) is a cloud-based platform that supports hub-based projects

portal

and other resource types. When you sign up, you can use your own data to
detect more than 100 languages in their primary script.

REST API or Client
library (Azure SDK)

Integrate language detection into your applications using the REST API, or the
client library available in various languages.

Docker container

Use the available Docker container to deploy this feature on-premises. Docker
containers enable you to bring the service closer to your data for compliance,
security, or other operational considerations.

Determine how to process the data (optional)
Specify the language detection model
By default, language detection will use the latest available AI model on your text. You can also
configure your API requests to use a specific model version.

Input languages
When you submit documents to be evaluated, language detection will attempt to determine if
the text was written in any of the supported languages.
If you have content expressed in a less frequently used language, you can try Azure Language
Detection feature to see if it returns a code. The response for languages that can't be detected
is unknown .

Submitting data
îª€ Tip
You can use a Docker containerfor language detection, so you can use the API onpremises.
Analysis is performed upon receipt of the request. Using the language detection feature
synchronously is stateless. No data is stored in your account, and results are returned
immediately in the response.
When using this feature asynchronously, the API results are available for 24 hours from the
time the request was ingested, and is indicated in the response. After this time period, the
results are purged and are no longer available for retrieval.

Getting language detection results
When you get results from language detection, you can stream the results to an application or
save the output to a file on the local system.
Language detection returns one predominant language for each document you submit, along
with it's ISO 639-1

name, a human-readable name, a confidence score, script name, and

script code according to the ISO 15924 standard

. A positive score of 1 indicates the highest

possible confidence level of the analysis.

Ambiguous content
In some cases it may be hard to disambiguate languages based on the input. You can use the
countryHint parameter to specify an ISO 3166-1 alpha-2

country/region code. By default the

API uses "US" as the default country hint. To remove this behavior, you can reset this parameter
by setting this value to empty string countryHint = "" .

For example, "communication" is common to both English and French and if given with limited
context the response will be based on the "US" country/region hint. If the origin of the text is
known to be coming from France that can be given as a hint.
ï¼— Note
Ambiguous content can cause confidence scores to be lower. The countryHint in the
response is only applicable if the confidence score is less than 0.8.
Input
JSON
{
"documents": [
{
"id": "1",
"text": "communication"
},
{
"id": "2",
"text": "communication",
"countryHint": "fr"
}
]
}

With the second document, the language detection model has additional context to make a
better judgment because it contains the countryHint property in the input above. This returns
the following output.
Output
JSON
{
"documents":[
{
"detectedLanguage":{
"confidenceScore":0.62,
"iso6391Name":"en",
"name":"English"
},
"id":"1",
"warnings":[
]
},

{
"detectedLanguage":{
"confidenceScore":1.0,
"iso6391Name":"fr",
"name":"French"
},
"id":"2",
"warnings":[
]
}
],
"errors":[
],
"modelVersion":"2022-10-01"
}

If the analyzer can't parse the input, it returns (Unknown) . An example is if you submit a text
string that consists solely of numbers.
JSON
{
"documents": [
{
"id": "1",
"detectedLanguage": {
"name": "(Unknown)",
"iso6391Name": "(Unknown)",
"confidenceScore": 0.0
},
"warnings": []
}
],
"errors": [],
"modelVersion": "2023-12-01"
}

Mixed-language content
Mixed-language content within the same document returns the language with the largest
representation in the content, but with a lower positive rating. The rating reflects the marginal
strength of the assessment. In the following example, input is a blend of English, Spanish, and
French. The analyzer counts characters in each segment to determine the predominant
language.
Input

JSON
{
"documents": [
{
"id": "1",
"text": "Hello, I would like to take a class at your University. Â¿Se
ofrecen clases en espaÃ±ol? Es mi primera lengua y mÃ¡s fÃ¡cil para escribir. Que
diriez-vous des cours en franÃ§ais?"
}
]
}

Output
The resulting output consists of the predominant language, with a score of less than 1.0, which
indicates a weaker level of confidence.
JSON
{
"kind": "LanguageDetectionResults",
"results": {
"documents": [
{
"id": "1",
"detectedLanguage": {
"name": "Spanish",
"iso6391Name": "es",
"confidenceScore": 0.97,
"script": "Latin",
"scriptCode": "Latn"
},
"warnings": []
}
],
"errors": [],
"modelVersion": "2023-12-01"
}
}

Script name and script code
ï¼— Note
Script detection is currently limited to select languages.
The script detection is only available for textual input which is greater than 12
characters in length.

Language detection offers the ability to detect more than one script per language according to
the ISO 15924 standard

. Specifically, Language Detection returns two script-related

properties:
script : The human-readable name of the identified script
scriptCode : The ISO 15924 code for the identified script

The output of the API includes the value of the scriptCode property for documents that are at
least 12 characters or greater in length and matches the list of supported languages and
scripts. Script detection is designed to benefit users whose language can be transliterated or
written in more than one script, such as Kazakh or Hindi language.
Previously, language detection was designed to detect the language of documents in a wide
variety of languages, dialects, and regional variants, but was limited by "Romanization".
Romanization refers to conversion of text from one writing system to the Roman (Latin) script,
and is necessary to detect many Indo-European languages. However, there are other languages
which are written in multiple scripts, such as Kazakh, which can be written in Cyrillic, PersoArabic, and Latin scripts. There are also other cases in which users may either choose or are
required to transliterate their language in more than one script, such as Hindi transliterated in
Latin script, due to the limited availability of keyboards which support its Devanagari script.
Consequently, language detection's expanded support for script detection behaves as follows:
Input
JSON
{
"kind": "LanguageDetection",
"parameters": {
"modelVersion": "latest"
},
"analysisInput": {
"documents": [
{
"id": "1",
"text": "à¤†à¤ª à¤•à¤¹à¤¾à¤ à¤œà¤¾ à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚?"
},
{
"id": "2",
"text": "Ð¢ÑƒÒ“Ð°Ð½ Ð¶ÐµÑ€Ñ–Ð¼ Ð¼ÐµÐ½Ñ–Ò£ - ÒšÐ°Ð·Ð°Ò›ÑÑ‚Ð°Ð½Ñ‹Ð¼"
}
]
}
}

Output

The resulting output consists of the predominant language, along with a script name, script
code, and confidence score.
JSON
{
"kind": "LanguageDetectionResults",
"results": {
"documents": [
{
"id": "1",
"detectedLanguage": {
"name": "Hindi",
"iso6391Name": "hi",
"confidenceScore": 1.0,
"script": "Devanagari",
"scriptCode": "Deva"
},
"warnings": []
},
{
"id": "2",
"detectedLanguage": {
"name": "Kazakh",
"iso6391Name": "kk",
"confidenceScore": 1.0,
"script": "Cyrillic",
"scriptCode": "Cyrl"
},
"warnings": []
}
],
"errors": [],
"modelVersion": "2023-12-01"
}
}

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

See also
Language detection overview

Last updated on 11/18/2025

Use language detection Docker containers
on-premises
Containers enable you to host Azure Language Detection API on your own infrastructure. If you
have security or data governance requirements that can't be fulfilled by calling Language
Detection remotely, then containers might be a good option.

Prerequisites
If you don't have an Azure subscription, create a free account
Docker

.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts
A Language resource

.

with the free (F0) or standard (S) pricing tier

.

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:
Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the language
detection container. Each CPU core must be at least 2.6 gigahertz (GHz) or faster. The allowable
Transactions Per Second (TPS) are also listed.
ï¾‰

Expand table

Language

Minimum host

Recommended host

Minimum

Maximum

specs

specs

TPS

TPS

1 core, 5GB memory

1 core, 8GB memory

15

30

detection

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Get the container image with docker pull
The Language Detection container image can be found on the mcr.microsoft.com container
registry syndicate. It resides within the azure-cognitive-services/textanalytics/ repository
and is named language . The fully qualified container image name is, mcr.microsoft.com/azurecognitive-services/textanalytics/language

To use the latest version of the container, you can use the latest tag. You can also find a full
list of tags on the MCR
Use the docker pull

.

command to download a container image from the Microsoft Container

Registry.

docker pull mcr.microsoft.com/azure-cognitive-services/textanalytics/language:latest

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container with docker run

Once the container is on the host computer, use the docker run

command to run the

containers. The container will continue to run until you stop it.
ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove this based on your host operating
system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container won't start. For more information, see Billing.
To run the Language Detection container, execute the following docker run command. Replace
the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Language resource.
You can find it on your resource's Key

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

and endpoint page, on the Azure
portal.
{ENDPOINT_URI}

The endpoint for accessing the

https://<your-custom-

language detection API. You can find
it on your resource's Key and
endpoint page, on the Azure portal.

subdomain>.cognitiveservices.azure.com

Bash
docker run --rm -it -p 5000:5000 --memory 4g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/textanalytics/language \
Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY}

This command:
Runs a Language Detection container from the container image
Allocates one CPU core and 4 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Expand table

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

Run the container disconnected from the internet
To use this container disconnected from the internet, you must first request access by filling
out an application, and purchasing a commitment plan. See Use Docker containers in
disconnected environments for more information.
If you have been approved to run the container disconnected from the internet, use the
following example shows the formatting of the docker run command you'll use, with
placeholder values. Replace these placeholder values with your own values.
The DownloadLicense=True parameter in your docker run command will download a license file
that will enable your Docker container to run when it isn't connected to the internet. It also
contains an expiration date, after which the license file will be invalid to run the container. You
can only use a license file with the appropriate container that you've been approved for. For
example, you can't use a license file for a speech to text container with a Document
Intelligence container.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitive-

{LICENSE_MOUNT}

The path where the
license will be

services/form-recognizer/invoice
/host/license:/path/to/license/directory

Placeholder

Value

Format or example

downloaded, and
mounted.
{ENDPOINT_URI}

The endpoint for
authenticating your
service request. You can

https://<your-customsubdomain>.cognitiveservices.azure.com

find it on your
resource's Key and
endpoint page, on the
Azure portal.
{API_KEY}

The key for your Text
Analytics resource. You
can find it on your

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

resource's Key and
endpoint page, on the
Azure portal.
{CONTAINER_LICENSE_DIRECTORY}

Location of the license
folder on the
container's local

/path/to/license/directory

filesystem.

Bash
docker run --rm -it -p 5000:5000 \
-v {LICENSE_MOUNT} \
{IMAGE} \
eula=accept \
billing={ENDPOINT_URI} \
apikey={API_KEY} \
DownloadLicense=True \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}

Once the license file has been downloaded, you can run the container in a disconnected
environment. The following example shows the formatting of the docker run command you'll
use, with placeholder values. Replace these placeholder values with your own values.
Wherever the container is run, the license file must be mounted to the container and the
location of the license folder on the container's local filesystem must be specified with
Mounts:License= . An output mount must also be specified so that billing usage records can be

written.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitiveservices/form-recognizer/invoice

{MEMORY_SIZE}

The appropriate size
of memory to
allocate for your
container.

4g

{NUMBER_CPUS}

The appropriate

4

number of CPUs to
allocate for your
container.
{LICENSE_MOUNT}

The path where the
license will be
located and

/host/license:/path/to/license/directory

mounted.
{OUTPUT_PATH}

The output path for
logging usage
records.

/host/output:/path/to/output/directory

{CONTAINER_LICENSE_DIRECTORY}

Location of the
license folder on the
container's local

/path/to/license/directory

filesystem.
{CONTAINER_OUTPUT_DIRECTORY}

Location of the
output folder on the
container's local
filesystem.

/path/to/output/directory

Bash
docker run --rm -it -p 5000:5000 --memory {MEMORY_SIZE} --cpus {NUMBER_CPUS} \
-v {LICENSE_MOUNT} \
-v {OUTPUT_PATH} \
{IMAGE} \
eula=accept \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}
Mounts:Output={CONTAINER_OUTPUT_DIRECTORY}

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
The language detection containers send billing information to Azure, using a Language
resource on your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure
The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If
the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

For more information about these options, see Configure containers.

Summary
In this article, you learned concepts and workflow for downloading, installing, and running
language detection containers. In summary:
Language detection provides Linux containers for Docker
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You must specify billing information when instantiating a container.
ï¼‰ Important
This container is not licensed to run without being connected to Azure for metering.
Customers need to enable the containers to communicate billing information with the
metering service at all times. Azure AI containers do not send customer data (e.g. text that
is being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

Use Docker containers in disconnected
environments
Containers enable you to run Foundry Tools APIs in your own environment, and are great for
your specific security and data governance requirements. Disconnected containers enable you
to use several of these APIs disconnected from the internet. Currently, the following containers
can be run in this manner:
Speech to text
Custom Speech to text
Neural Text to speech
Text Translation (Standard)
Azure Language in Foundry Tools
Sentiment Analysis
Key Phrase Extraction
Language Detection
Summarization
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)
Azure Vision in Foundry Tools - Read
Document Intelligence
Before attempting to run a Docker container in an offline environment, make sure you know
the steps to successfully download and use the container. For example:
Host computer requirements and recommendations.
The Docker pull command you use to download the container.
How to validate that a container is running.
How to send queries to the container's endpoint, once it's running.

Request access to use containers in disconnected
environments
Fill out and submit the request form

to request access to the containers disconnected from

the internet.
The form requests information about you, your company, and the user scenario for which you'll
use the container. After you submit the form, the Foundry Tools team reviews it and emails you
with a decision within 10 business days.

ï¼‰ Important
On the form, you must use an email address associated with an Azure subscription
ID.
The Azure resource you use to run the container must have been created with the
approved Azure subscription ID.
Check your email (both inbox and junk folders) for updates on the status of your
application from Microsoft.
After you're approved, you'll be able to run the container after you download it from the
Microsoft Container Registry (MCR), described later in the article.
You won't be able to run the container if your Azure subscription hasn't been approved.
Access is limited to customers that meet the following requirements:
Your organization should be identified as strategic customer or partner with Microsoft.
Disconnected containers are expected to run fully offline, hence your use cases must
meet one of these or similar requirements:
Environments or devices with zero connectivity to internet.
Remote location that occasionally has internet access.
Organization under strict regulation of not sending any kind of data back to cloud.
Application completed as instructed - Pay close attention to guidance provided
throughout the application to ensure you provide all the necessary information required
for approval.

Purchase a commitment tier pricing plan for
disconnected containers
Create a new resource
1. Sign in to the Azure portal

and select Create a new resource for one of the applicable

Foundry Tools listed.
2. Enter the applicable information to create your resource. Be sure to select Commitment
tier disconnected containers as your pricing tier.
ï¼— Note

You only see the option to purchase a commitment tier if you have been
approved by Microsoft.
Pricing details are only examples.
3. Select Review + Create at the bottom of the page. Review the information, and select
Create.

Configure container for disconnected usage
See the following documentation for steps on downloading and configuring the container for
disconnected usage:
Vision - Read
Language Understanding (LUIS)
Text Translation (Standard)
Document Intelligence
Speech service
Speech to text
Custom Speech to text
Neural Text to speech
Language service
Sentiment Analysis
Key Phrase Extraction
Language Detection
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)

Environment variable names in Kubernetes
deployments
Some Azure AI Containers, for example Translator, require users to pass environmental variable
names that include colons ( : ) when running the container. This works fine when using Docker,
but Kubernetes doesn't accept colons in environmental variable names. To resolve this, you can
replace colons with double underscore characters ( __ ) when deploying to Kubernetes. See the
following example of an acceptable format for environment variable names:

Kubernetes
env:
- name: Mounts__License
value: "/license"
- name: Mounts__Output
value: "/output"

This example replaces the default format for the Mounts:License and Mounts:Output
environment variable names in the docker run command.

Container image and license updates
Container license files are used as keys to decrypt certain files within each container image. If
these encrypted files happen to be updated within a new container image, the license file you
have may fail to start the container even if it worked with the previous version of the container
image. To avoid this issue, we recommend that you download a new license file from the
resource endpoint for your container provided in Azure portal after you pull new image
versions from mcr.microsoft.com.
To download a new license file, you can add DownloadLicense=True to your docker run
command along with a license mount, your API Key, and your billing endpoint. Refer to your
container's documentation for detailed instructions.

Usage records
When operating Docker containers in a disconnected environment, the container writes usage
records to a volume where they're collected over time. You can also call a REST endpoint to
generate a report about service usage.

Arguments for storing logs
When run in a disconnected environment, an output mount must be available to the container
to store usage logs. For example, you would include -v /host/output:{OUTPUT_PATH} and
Mounts:Output={OUTPUT_PATH} in the example below, replacing {OUTPUT_PATH} with the path

where the logs are stored:
Docker
docker run -v /host/output:{OUTPUT_PATH} ... <image> ... Mounts:Output={OUTPUT_PATH}

Get records using the container endpoints

The container provides two endpoints for returning records about its usage.

Get all records
The following endpoint provides a report summarizing all of the usage collected in the
mounted billing record directory.
HTTP
https://<service>/records/usage-logs/

It returns JSON similar to the example below.
JSON
{
"apiType": "noop",
"serviceName": "noop",
"meters": [
{
"name": "Sample.Meter",
"quantity": 253
}
]
}

Get records for a specific month
The following endpoint provides a report summarizing usage over a specific month and year.
HTTP
https://<service>/records/usage-logs/{MONTH}/{YEAR}

It returns a JSON response similar to the example below:
JSON
{
"apiType": "string",
"serviceName": "string",
"meters": [
{
"name": "string",
"quantity": 253
}

]
}

Purchase a commitment plan to use containers in
disconnected environments
Commitment plans for disconnected containers have a calendar year commitment period.
When you purchase a plan, you are charged the full price immediately. During the commitment
period, you can't change your commitment plan, however you can purchase more units at a
pro-rated price for the remaining days in the year. You have until midnight (UTC) on the last
day of your commitment, to end a commitment plan.
You can choose a different commitment plan in the Commitment Tier pricing settings of your
resource.

End a commitment plan
If you decide that you don't want to continue purchasing a commitment plan, you can set your
resource's auto-renewal to Do not auto-renew. Your commitment plan expires on the
displayed commitment end date. After this date, you won't be charged for the commitment
plan. You are able to continue using the Azure resource to make API calls, charged at Standard
pricing. You have until midnight (UTC) on the last day of the year to end a commitment plan for
disconnected containers, and not be charged for the following year.

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Disconnected containers
Frequently asked questions (FAQ).

Next steps
Azure AI containers overview

Last updated on 10/02/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

What is key phrase extraction in Azure
Language in Foundry Tools?
Key phrase extraction is one of the features offered by Azure Language in Foundry Tools. This
capability is part of a suite of cloud-based machine learning and AI tools designed for building
intelligent applications that process written language. Use key phrase extraction to quickly
identify the main concepts in text. For example, in the text "The food was delicious and the staff
were wonderful.", key phrase extraction returns the main topics: "food" and "wonderful staff."
This documentation contains the following types of articles:
Quickstarts are getting-started instructions to guide you through making requests to the
service.
How-to guides contain instructions for using the service in more specific or customized
ways.

Typical workflow
To use this feature, you submit data for analysis and handle the API output in your application.
Analysis is performed as-is, with no added customization to the model used on your data.
1. Create an Azure Language in Foundry Tools resource, which grants you access to the
features offered by Language. It generates a password (called a key) and an endpoint URL
that you use to authenticate API requests.
2. Create a request using either the REST API or the client library for C#, Java, JavaScript, and
Python. You can also send asynchronous calls with a batch request to combine API
requests for multiple features into a single call.
3. Send the request containing your text data. Your key and endpoint are used for
authentication.
4. Stream or store the response locally.

Get started with Key phrase extraction
To use key phrase extraction, you submit raw unstructured text for analysis and handle the API
output in your application. Analysis is performed as-is, with no additional customization to the
model used on your data. There are two ways to use key phrase extraction:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use entity linking with text
examples with your own data when you sign up. For more information, see the
Foundry website or Foundry documentation.

REST API or Client
library (Azure SDK)

Integrate key phrase extraction into your applications using the REST API, or the
client library available in a variety of languages. For more information, see the
key phrase extraction quickstart.

Docker container

Use the available Docker container to deploy this feature on-premises. These
docker containers enable you to bring the service closer to your data for
compliance, security, or other operational reasons.

Reference documentation and code samples
As you use this feature in your applications, see the following reference documentation and
samples for Azure Language in Foundry Tools:
ï¾‰

Expand table

Development option / language

Reference documentation

Samples

REST API

REST API documentation

C#

C# documentation

C# samples

Java

Java documentation

Java Samples

JavaScript

JavaScript documentation

JavaScript samples

Python

Python documentation

Python samples

Responsible AI
An AI system includes the technology, the individuals who operate the system, the people who
experience its effects, and the broader environment where the system functions all play a role.
Read the transparency note for key phrase extraction to learn about responsible AI use and
deployment in your systems. For more information, see the following articles:
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Next steps
There are two ways to get started using the entity linking feature:
Microsoft Foundry is a web-based platform that lets you use several Language features
without needing to write code.
The quickstart article for instructions on making requests to the service using the REST
API and client library SDK.

Last updated on 11/18/2025

Quickstart: using the Key Phrase Extraction
client library and REST API
Prerequisites
Create a Project in Foundry in the Microsoft Foundry portal

Navigate to the Foundry Playground
Using the left side pane, select Playgrounds. Then select the Try Azure Language Playground
button.

ï Š

Use Key Phrase Extraction in the Foundry
Playground
The Language Playground consists of four sections:
Top banner: You can select any of the currently available Languages here.
Right pane: This pane is where you can find the Configuration options for the service,
such as the API and model version, along with features specific to the service.
Center pane: This pane is where you enter your text for processing. After the operation is
run, some results are shown here.
Right pane: This pane is where Details of the run operation are shown.
Here you can select the Key Phrase Extraction capability by choosing the top banner tile,
Extract key phrases.

Use Extract key phrases
Extract key phrases is designed to extract key phrases from text.
In Configuration there are the following options:
ï¾‰

Option

Description

Select API version

Select which version of the API to use.

Select model version

Select which version of the model to use.

Select text language

Select the language of the input text.

Expand table

After your operation is completed, each entity is underlined in the center pane and the Details
section contains the following fields for the overall sentiment and the sentiment of each
sentence:
ï¾‰

Field

Description

Extracted key phrases

A list of the extracted key phrases.

Expand table

ï Š

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
Key phrase extraction overview

Last updated on 11/18/2025

Language support for Key Phrase
Extraction
Use this article to find the natural languages supported by Key Phrase Extraction. Both the
cloud-based API and Docker containers support the same languages.

Supported languages
Total supported language codes: 94
ï¾‰

Language

Language code

Afrikaans

af

Albanian

sq

Amharic

am

Arabic

ar

Armenian

hy

Assamese

as

Azerbaijani

az

Basque

eu

Belarusian

be

Bengali

bn

Bosnian

bs

Breton

br

Bulgarian

bg

Burmese

my

Catalan

ca

Chinese-Simplified

zh-hans

Chinese-Traditional

zh-hant

Notes

Expand table

Language

Language code

Croatian

hr

Czech
Danish

cs
da

Dutch

nl

English

en

Esperanto

eo

Estonian

et

Filipino

fil

Finnish

fi

French

fr

Galician

gl

Georgian

ka

German

de

Greek

el

Gujarati

gu

Hausa

ha

Hebrew

he

Hindi

hi

Hungarian

hu

Indonesian

id

Irish

ga

Italian

it

Japanese

ja

Javanese

jv

Kannada

kn

Kazakh

kk

Notes

Language

Language code

Khmer

km

Korean

ko

Kurdish (Kurmanji)

ku

Kyrgyz

ky

Lao

lo

Latin

la

Latvian

lv

Lithuanian

lt

Macedonian

mk

Malagasy

mg

Malay

ms

Malayalam

ml

Marathi

mr

Mongolian

mn

Nepali

ne

Norwegian (BokmÃ¥l)

no

Odia

or

Oromo

om

Pashto

ps

Persian

fa

Polish

pl

Portuguese (Brazil)

pt-BR

Portuguese (Portugal)

pt-PT

Punjabi

pa

Romanian

ro

Russian

ru

Notes

nb also accepted

pt also accepted

Language

Language code

Sanskrit

sa

Scottish Gaelic

gd

Serbian

sr

Sindhi

sd

Sinhala

si

Slovak

sk

Slovenian

sl

Somali

so

Spanish

es

Sudanese

su

Swahili

sw

Swedish

sv

Tamil

ta

Telugu

te

Thai

th

Turkish

tr

Ukrainian

uk

Urdu

ur

Uyghur

ug

Uzbek

uz

Vietnamese

vi

Welsh

cy

Western Frisian

fy

Xhosa

xh

Yiddish

yi

Notes

Next steps
How to call the API for more information.
Quickstart: Use the key phrase extraction client library and REST API

Last updated on 11/18/2025

Transparency note for Key Phrase
Extraction
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
ï¼‰ Important
This article assumes that you're familiar with guidelines and best practices for Azure
Language in Foundry Tools. For more information, see Transparency note for Language.
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance. Microsoft's Transparency
Notes are intended to help you understand how our AI technology works, the choices system
owners can make that influence system performance and behavior, and the importance of
thinking about the whole system, including the technology, the people, and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who will use or be affected by your system.
Microsoft's Transparency notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Responsible AI principles from Microsoft.

Introduction to key phrase extraction
Language Key Phrase Extraction feature allows you to quickly identify the main concepts in
text. For example, in the text "The food was delicious and there were wonderful staff", Key
Phrase Extraction will return the main talking points: "food" and "wonderful staff". Nonessential words are discarded single terms or phrases that appear to be the subject or object of
a sentence are returned.
Note that no confidence score is returned for this feature, unlike some other Language
features.

Example use cases
Key Phrase Extraction is used in multiple scenarios across a variety of industries. Some
examples include:
Enhancing search. Key phrases can be used to create a search index that can enhance
search results. For example, customers can provide thousands of documents and then run
Key Phrase Extraction on top of it using the built-in Azure Search skill. The outcome of
this are key phrases from the input dataset, which can then be used to create an index.
This index can be updated by running the skill again whenever there is a new document
set available.
View aggregate trends in text data. For example, a word cloud can be generated with
key phrases to help visualize key concepts in text comments or feedback. For example, a
hotel could generate a word cloud based on key phrases identified in their comments and
might see that people are commenting most frequently about the location, cleanliness
and helpful staff.

Considerations when choosing a use case
Do not use
Do not use for automatic actions without human intervention for high risk scenarios. A
person should always review source data when another person's economic situation,
health or safety is affected.
Legal and regulatory considerations: Organizations need to evaluate potential specific legal
and regulatory obligations when using any AI services and solutions, which may not be
appropriate for use in every industry or scenario. Additionally, AI services or solutions are not
designed for and may not be used in ways prohibited in applicable terms of service and
relevant codes of conduct.

Characteristics and limitations
Depending on your scenario and input data, you could experience different levels of
performance. The following information is designed to help you understand key concepts
about performance as they apply to using the Language key phrase extraction feature.

System limitations and best practices for enhancing
performance

Unlike other Language features' models, the key phrase extraction model is an unsupervised
model that is not trained on human labeled ground truth data. All of the noun phrases in the
text sent to the service are detected and then ranked based on frequency and co-occurrence.
Therefore, what is returned by the model may not agree with what a human would choose as
the most important phrases. In some cases the model may appear partially correct, in that a
noun is returned without the adjective that modifies it.
Longer text will perform better. Do not break your source text up into pieces like
sentences or paragraphs. Send the entire text, for example, a complete customer review
or paper abstract.
If your text includes some boilerplate or other text that has no topical relevance to the
actual content you're trying to analyze, the words in this text will affect your results. For
example, emails might have "Subject:", "Body:", "Sender:", etc. included in the text. We
recommend removing any known text that is not part of the actual content you are trying
to analyze before sending it to the service.

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

How to use key phrase extraction
The key phrase extraction feature can evaluate unstructured text, and for each document,
return a list of key phrases.
This feature is useful if you need to quickly identify the main points in a collection of
documents. For example, given input text "The food was delicious and the staff was wonderful",
the service returns the main topics: "food" and "wonderful staff".
îª€ Tip
If you want to start using this feature, you can follow the quickstart article to get started.
You can also make requests using Microsoft Foundry without needing to write code.

Development options
To use key phrase extraction, you submit raw unstructured text for analysis and handle the API
output in your application. Analysis is performed as-is, with no additional customization to the
model used on your data. There are two ways to use key phrase extraction:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use entity linking with text
examples with your own data when you sign up. For more information, see the
Foundry website or Foundry documentation.

REST API or Client
library (Azure SDK)

Integrate key phrase extraction into your applications using the REST API, or the
client library available in a variety of languages. For more information, see the
key phrase extraction quickstart.

Docker container

Use the available Docker container to deploy this feature on-premises. These
docker containers enable you to bring the service closer to your data for
compliance, security, or other operational reasons.

Determine how to process the data (optional)
Specify the key phrase extraction model
By default, key phrase extraction uses the latest available AI model on your text. You can also
configure your API requests to use a specific model version.

Input languages
When you submit documents to be processed by key phrase extraction, you can specify which
of the supported languages they're written in. if you don't specify a language, key phrase
extraction defaults to English. The API may return offsets in the response to support different
multilingual and emoji encodings.

Submitting data
Key phrase extraction works best when you give it bigger amounts of text to work on. This is
opposite from sentiment analysis, which performs better on smaller amounts of text. To get the
best results from both operations, consider restructuring the inputs accordingly.
To send an API request, You need your Language resource endpoint and key.
ï¼— Note
You can find the key and endpoint for your Language resource on the Azure portal. They
are located on the resource's Key and endpoint page, under resource management.
Analysis is performed upon receipt of the request. Using the key phrase extraction feature
synchronously is stateless. No data is stored in your account, and results are returned
immediately in the response.
When using this feature asynchronously, the API results are available for 24 hours from the
time the request was ingested, and is indicated in the response. After this time period, the
results are purged and are no longer available for retrieval.

Getting key phrase extraction results
When you receive results from the API, the order of the returned key phrases is determined
internally, by the model. You can stream the results to an application, or save the output to a
file on the local system.

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

Next steps
Key Phrase Extraction overview

Last updated on 11/18/2025

Install and run Key Phrase Extraction
containers
Containers enable you to host the Key Phrase Extraction API on your own infrastructure. If you
have security or data governance requirements that can't be fulfilled by calling Key Phrase
Extraction remotely, then containers might be a good option.
ï¼— Note
The free account is limited to 5,000 text records per month and only the Free and
Standard pricing tiers

are valid for containers. For more information on

transaction request rates, see Data and service limits.
Containers enable you to run the Key Phrase Extraction APIs in your own environment and are
great for your specific security and data governance requirements. The Key Phrase Extraction
containers provide advanced natural language processing over raw text, and include three
main functions: sentiment analysis, Key Phrase Extraction, and language detection.

Prerequisites
If you don't have an Azure subscription, create a free account
Docker

.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts
A Language resource

.

with the free (F0) or standard (S) pricing tier

.

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:

Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the available
Key Phrase Extraction containers. Each CPU core must be at least 2.6 gigahertz (GHz) or faster.
The allowable Transactions Per Second (TPS) are also listed.
ï¾‰

Expand table

Minimum host

Recommended host

Minimum

Maximum

specs

specs

TPS

TPS

Key Phrase

1 core, 2GB

1 core, 4GB memory

15

30

Extraction

memory

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Get the container image with docker pull
The key phrase extraction container image can be found on the mcr.microsoft.com container
registry syndicate. It resides within the azure-cognitive-services/textanalytics/ repository
and is named keyphrase . The fully qualified container image name is,
mcr.microsoft.com/azure-cognitive-services/textanalytics/keyphrase .

To use the latest version of the container, you can use the latest tag. You can also find a full
list of tags on the MCR
Use the docker pull

.

command to download a container image from Microsoft Container

Registry.

docker pull mcr.microsoft.com/azure-cognitiveservices/textanalytics/keyphrase:latest

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container with docker run
Once the container is on the host computer, use the docker run

command to run the

containers. The container will continue to run until you stop it.
ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove this based on your host operating
system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container won't start. For more information, see Billing.
The sentiment analysis and language detection containers use v3 of the API, and are
generally available. The Key Phrase Extraction container uses v2 of the API, and is in
preview.
To run the Key Phrase Extraction container, execute the following docker run command.
Replace the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Key Phrase

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Extraction resource. You can find it on
your resource's Key and endpoint
page, on the Azure portal.
{ENDPOINT_URI}

Bash

The endpoint for accessing the Key

https://<your-custom-

Phrase Extraction API. You can find it
on your resource's Key and endpoint
page, on the Azure portal.

subdomain>.cognitiveservices.azure.com

docker run --rm -it -p 5000:5000 --memory 4g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/textanalytics/keyphrase \
Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY}

This command:
Runs a Key Phrase Extraction container from the container image
Allocates one CPU core and 4 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

Expand table

Request URL

Purpose

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

Run the container disconnected from the internet
To use this container disconnected from the internet, you must first request access by filling
out an application, and purchasing a commitment plan. See Use Docker containers in
disconnected environments for more information.
If you have been approved to run the container disconnected from the internet, use the
following example shows the formatting of the docker run command you'll use, with
placeholder values. Replace these placeholder values with your own values.
The DownloadLicense=True parameter in your docker run command will download a license file
that will enable your Docker container to run when it isn't connected to the internet. It also

contains an expiration date, after which the license file will be invalid to run the container. You
can only use a license file with the appropriate container that you've been approved for. For
example, you can't use a license file for a speech to text container with a Document
Intelligence container.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitiveservices/form-recognizer/invoice

{LICENSE_MOUNT}

The path where the
license will be
downloaded, and
mounted.

/host/license:/path/to/license/directory

{ENDPOINT_URI}

The endpoint for
authenticating your
service request. You can
find it on your
resource's Key and
endpoint page, on the

https://<your-customsubdomain>.cognitiveservices.azure.com

Azure portal.
{API_KEY}

The key for your Text
Analytics resource. You
can find it on your
resource's Key and
endpoint page, on the

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Azure portal.
{CONTAINER_LICENSE_DIRECTORY}

Location of the license
folder on the
container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 \
-v {LICENSE_MOUNT} \
{IMAGE} \
eula=accept \
billing={ENDPOINT_URI} \
apikey={API_KEY} \
DownloadLicense=True \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}

/path/to/license/directory

Once the license file has been downloaded, you can run the container in a disconnected
environment. The following example shows the formatting of the docker run command you'll
use, with placeholder values. Replace these placeholder values with your own values.
Wherever the container is run, the license file must be mounted to the container and the
location of the license folder on the container's local filesystem must be specified with
Mounts:License= . An output mount must also be specified so that billing usage records can be

written.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitive-

{MEMORY_SIZE}

The appropriate size
of memory to
allocate for your

services/form-recognizer/invoice
4g

container.
{NUMBER_CPUS}

The appropriate
number of CPUs to
allocate for your
container.

4

{LICENSE_MOUNT}

The path where the
license will be

/host/license:/path/to/license/directory

located and
mounted.
{OUTPUT_PATH}

The output path for
logging usage
records.

/host/output:/path/to/output/directory

{CONTAINER_LICENSE_DIRECTORY}

Location of the
license folder on the

/path/to/license/directory

container's local
filesystem.
{CONTAINER_OUTPUT_DIRECTORY}

Bash

Location of the
output folder on the
container's local
filesystem.

/path/to/output/directory

docker run --rm -it -p 5000:5000 --memory {MEMORY_SIZE} --cpus {NUMBER_CPUS} \
-v {LICENSE_MOUNT} \
-v {OUTPUT_PATH} \
{IMAGE} \
eula=accept \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}
Mounts:Output={CONTAINER_OUTPUT_DIRECTORY}

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
The Key Phrase Extraction containers send billing information to Azure, using a Key Phrase
Extraction resource on your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure
The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If

the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

\

Summary
In this article, you learned concepts and workflow for downloading, installing, and running Key
Phrase Extraction containers. In summary:
Key Phrase Extraction provides Linux containers for Docker.
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You can use either the REST API or SDK to call operations in Key Phrase Extraction
containers by specifying the host URI of the container.
You must specify billing information when instantiating a container.
ï¼‰ Important
Azure AI containers are not licensed to run without being connected to Azure for
metering. Customers need to enable the containers to communicate billing information

with the metering service at all times. Azure AI containers do not send customer data (e.g.
text that is being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

Use Docker containers in disconnected
environments
Containers enable you to run Foundry Tools APIs in your own environment, and are great for
your specific security and data governance requirements. Disconnected containers enable you
to use several of these APIs disconnected from the internet. Currently, the following containers
can be run in this manner:
Speech to text
Custom Speech to text
Neural Text to speech
Text Translation (Standard)
Azure Language in Foundry Tools
Sentiment Analysis
Key Phrase Extraction
Language Detection
Summarization
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)
Azure Vision in Foundry Tools - Read
Document Intelligence
Before attempting to run a Docker container in an offline environment, make sure you know
the steps to successfully download and use the container. For example:
Host computer requirements and recommendations.
The Docker pull command you use to download the container.
How to validate that a container is running.
How to send queries to the container's endpoint, once it's running.

Request access to use containers in disconnected
environments
Fill out and submit the request form

to request access to the containers disconnected from

the internet.
The form requests information about you, your company, and the user scenario for which you'll
use the container. After you submit the form, the Foundry Tools team reviews it and emails you
with a decision within 10 business days.

ï¼‰ Important
On the form, you must use an email address associated with an Azure subscription
ID.
The Azure resource you use to run the container must have been created with the
approved Azure subscription ID.
Check your email (both inbox and junk folders) for updates on the status of your
application from Microsoft.
After you're approved, you'll be able to run the container after you download it from the
Microsoft Container Registry (MCR), described later in the article.
You won't be able to run the container if your Azure subscription hasn't been approved.
Access is limited to customers that meet the following requirements:
Your organization should be identified as strategic customer or partner with Microsoft.
Disconnected containers are expected to run fully offline, hence your use cases must
meet one of these or similar requirements:
Environments or devices with zero connectivity to internet.
Remote location that occasionally has internet access.
Organization under strict regulation of not sending any kind of data back to cloud.
Application completed as instructed - Pay close attention to guidance provided
throughout the application to ensure you provide all the necessary information required
for approval.

Purchase a commitment tier pricing plan for
disconnected containers
Create a new resource
1. Sign in to the Azure portal

and select Create a new resource for one of the applicable

Foundry Tools listed.
2. Enter the applicable information to create your resource. Be sure to select Commitment
tier disconnected containers as your pricing tier.
ï¼— Note

You only see the option to purchase a commitment tier if you have been
approved by Microsoft.
Pricing details are only examples.
3. Select Review + Create at the bottom of the page. Review the information, and select
Create.

Configure container for disconnected usage
See the following documentation for steps on downloading and configuring the container for
disconnected usage:
Vision - Read
Language Understanding (LUIS)
Text Translation (Standard)
Document Intelligence
Speech service
Speech to text
Custom Speech to text
Neural Text to speech
Language service
Sentiment Analysis
Key Phrase Extraction
Language Detection
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)

Environment variable names in Kubernetes
deployments
Some Azure AI Containers, for example Translator, require users to pass environmental variable
names that include colons ( : ) when running the container. This works fine when using Docker,
but Kubernetes doesn't accept colons in environmental variable names. To resolve this, you can
replace colons with double underscore characters ( __ ) when deploying to Kubernetes. See the
following example of an acceptable format for environment variable names:

Kubernetes
env:
- name: Mounts__License
value: "/license"
- name: Mounts__Output
value: "/output"

This example replaces the default format for the Mounts:License and Mounts:Output
environment variable names in the docker run command.

Container image and license updates
Container license files are used as keys to decrypt certain files within each container image. If
these encrypted files happen to be updated within a new container image, the license file you
have may fail to start the container even if it worked with the previous version of the container
image. To avoid this issue, we recommend that you download a new license file from the
resource endpoint for your container provided in Azure portal after you pull new image
versions from mcr.microsoft.com.
To download a new license file, you can add DownloadLicense=True to your docker run
command along with a license mount, your API Key, and your billing endpoint. Refer to your
container's documentation for detailed instructions.

Usage records
When operating Docker containers in a disconnected environment, the container writes usage
records to a volume where they're collected over time. You can also call a REST endpoint to
generate a report about service usage.

Arguments for storing logs
When run in a disconnected environment, an output mount must be available to the container
to store usage logs. For example, you would include -v /host/output:{OUTPUT_PATH} and
Mounts:Output={OUTPUT_PATH} in the example below, replacing {OUTPUT_PATH} with the path

where the logs are stored:
Docker
docker run -v /host/output:{OUTPUT_PATH} ... <image> ... Mounts:Output={OUTPUT_PATH}

Get records using the container endpoints

The container provides two endpoints for returning records about its usage.

Get all records
The following endpoint provides a report summarizing all of the usage collected in the
mounted billing record directory.
HTTP
https://<service>/records/usage-logs/

It returns JSON similar to the example below.
JSON
{
"apiType": "noop",
"serviceName": "noop",
"meters": [
{
"name": "Sample.Meter",
"quantity": 253
}
]
}

Get records for a specific month
The following endpoint provides a report summarizing usage over a specific month and year.
HTTP
https://<service>/records/usage-logs/{MONTH}/{YEAR}

It returns a JSON response similar to the example below:
JSON
{
"apiType": "string",
"serviceName": "string",
"meters": [
{
"name": "string",
"quantity": 253
}

]
}

Purchase a commitment plan to use containers in
disconnected environments
Commitment plans for disconnected containers have a calendar year commitment period.
When you purchase a plan, you are charged the full price immediately. During the commitment
period, you can't change your commitment plan, however you can purchase more units at a
pro-rated price for the remaining days in the year. You have until midnight (UTC) on the last
day of your commitment, to end a commitment plan.
You can choose a different commitment plan in the Commitment Tier pricing settings of your
resource.

End a commitment plan
If you decide that you don't want to continue purchasing a commitment plan, you can set your
resource's auto-renewal to Do not auto-renew. Your commitment plan expires on the
displayed commitment end date. After this date, you won't be charged for the commitment
plan. You are able to continue using the Azure resource to make API calls, charged at Standard
pricing. You have until midnight (UTC) on the last day of the year to end a commitment plan for
disconnected containers, and not be charged for the following year.

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Disconnected containers
Frequently asked questions (FAQ).

Next steps
Azure AI containers overview

Last updated on 10/02/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

Tutorial: Extract key phrases from text
stored in Power BI
Microsoft Power BI Desktop is a free application that lets you connect to, transform, and
visualize your data. Key phrase extraction, one of the features of Azure Language in Foundry
Tools, provides natural language processing. Given raw unstructured text, it can extract the
most important phrases, analyze sentiment, and identify well-known entities such as brands.
Together, these tools can help you quickly see what your customers are talking about and how
they feel about it.
In this tutorial, you learn how to:
ï¼‚ Use Power BI Desktop to import and transform data
ï¼‚ Create a custom function in Power BI Desktop
ï¼‚ Integrate Power BI Desktop with the Key Phrase Extraction feature of Language
ï¼‚ Use Key Phrase Extraction to get the most important phrases from customer feedback
ï¼‚ Create a word cloud from customer feedback

Prerequisites
Microsoft Power BI Desktop. Download at no charge
A Microsoft Azure account. Create a free account

.

or sign in

.

A Language resource. If you don't have one, you can create one

.

The Language resource key that was generated for you when you created the resource.
Customer comments. You can use our example data

or your own data. This tutorial

assumes you're using our example data.

Load customer data
To get started, open Power BI Desktop and load the comma-separated value (CSV) file that you
downloaded as part of the prerequisites. This file represents a day's worth of hypothetical
activity in a fictional small company's support forum.
ï¼— Note
Power BI can use data from a wide variety of web-based sources, such as SQL databases.
For more information, see Power Query documentation.
In the main Power BI Desktop window, select the Home ribbon. In the External data group of
the ribbon, open the Get Data drop-down menu and select Text/CSV.

The Open dialog appears. Navigate to your Downloads folder, or to the folder where you
downloaded the CSV file. Select the name of the file, then the Open button. The CSV import
dialog appears.

The CSV import dialog lets you verify that Power BI Desktop correctly detected the character
set, delimiter, header rows, and column types. This information is all correct, so select Load.
To see the loaded data, select the Data View button on the left edge of the Power BI
workspace. A table opens that contains the data, like in Microsoft Excel.

Prepare the data
You might need to transform your data in Power BI Desktop before Key Phrase Extraction
processing.
The sample data contains a subject column and a comment column. With the Merge Columns
function in Power BI Desktop, you can extract key phrases from the data in both these columns,
rather than just the comment column.
In Power BI Desktop, select the Home ribbon. In the External data group, select Edit Queries.

Select FabrikamComments in the Queries list at the left side of the window if it isn't already
selected.
Now select both the subject and comment columns in the table. You might need to scroll
horizontally to see these columns. First select the subject column header, then hold down the
Control key and select the comment column header.

Selecting fields to be merged
Select the Transform ribbon. In the Text Columns group of the ribbon, select Merge Columns.
The Merge Columns dialog appears.

In the Merge Columns dialog, choose Tab as the separator, then select OK.
You might also consider filtering out blank messages using the Remove Empty filter, or
removing unprintable characters using the Clean transformation. If your data contains a
column like the spamscore column in the sample file, you can skip "spam" comments using a
Number Filter.

Understand the API
Key Phrase Extraction can process up to a thousand text documents per HTTP request. Power
BI prefers to deal with records one at a time, so in this tutorial your calls to the API include only
a single document each. The Key Phrases API requires the following fields for each document
being processed.
ï¾‰

Expand table

Field

Description

id

A unique identifier for this document within the request. The response also contains this field.
That way, if you process more than one document, you can easily associate the extracted key
phrases with the document they came from. In this tutorial, because you're processing only
one document per request, you can hard-code the value of id to be the same for each
request.

text

The text to be processed. The value of this field comes from the Merged column you created
in the previous section, which contains the combined subject line and comment text. The Key
Phrases API requires this data be no longer than about 5,120 characters.

Field

Description

language

The code for the natural language the document is written in. All the messages in the sample
data are in English, so you can hard-code the value en for this field.

Create a custom function
Now you're ready to create the custom function that integrates Power BI and Key Phrase
Extraction. The function receives the text to be processed as a parameter. It converts data to
and from the required JSON format and makes the HTTP request to the Key Phrases API. The
function then parses the response from the API and returns a string that contains a commaseparated list of the extracted key phrases.
ï¼— Note
Power BI Desktop custom functions are written in the Power Query M formula language,
or just "M" for short. M is a functional programming language based on F#. You don't
need to be a programmer to finish this tutorial, though; the required code is included.
In Power BI Desktop, make sure you're still in the Query Editor window. If you aren't, select the
Home ribbon, and in the External data group, select Edit Queries.
Now, in the Home ribbon, in the New Query group, open the New Source drop-down menu
and select Blank Query.
A new query, initially named Query1 , appears in the Queries list. Double-click this entry and
name it KeyPhrases .
Now, in the Home ribbon, in the Query group, select Advanced Editor to open the Advanced
Editor window. Delete the code that's already in that window and paste in the following code.
ï¼— Note
Replace the following example endpoint (containing <your-custom-subdomain> ) with the
endpoint generated for your Language resource. You can find this endpoint by signing in
to the Azure portal

, navigating to your resource, and selecting Key and endpoint.

F#
// Returns key phrases from the text in a comma-separated list
(text) => let
apikey
= "YOUR_API_KEY_HERE",

endpoint
= "https://<your-customsubdomain>.cognitiveservices.azure.com/text/analytics" & "/v3.0/keyPhrases",
jsontext
= Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text),
5000))),
jsonbody
= "{ documents: [ { language: ""en"", id: ""0"", text: " & jsontext
& " } ] }",
bytesbody
= Text.ToBinary(jsonbody),
headers
= [#"Ocp-Apim-Subscription-Key" = apikey],
bytesresp
= Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),
jsonresp
= Json.Document(bytesresp),
keyphrases = Text.Lower(Text.Combine(jsonresp[documents]{0}[keyPhrases], ", "))
in keyphrases

Replace YOUR_API_KEY_HERE with your Language resource key. You can also find this key by
signing in to the Azure portal

, navigating to your Language resource, and selecting the Key

and endpoint page. Be sure to leave the quotation marks before and after the key. Then select
Done.

Use the custom function
Now you can use the custom function to extract the key phrases from each of the customer
comments and store them in a new column in the table.
In Power BI Desktop, in the Query Editor window, switch back to the FabrikamComments query.
Select the Add Column ribbon. In the General group, select Invoke Custom Function.

The Invoke Custom Function dialog appears. In New column name, enter keyphrases . In
Function query, select the custom function you created, KeyPhrases .
A new field appears in the dialog, text (optional). This field is asking which column we want to
use to provide values for the text parameter of the Key Phrases API. (Remember that you
already hard-coded the values for the language and id parameters.) Select Merged (the

column you created previously by merging the subject and message fields) from the dropdown menu.

Finally, select OK.
If everything is ready, Power BI calls your custom function once for each row in the table. It
sends the queries to the Key Phrases API and adds a new column to the table to store the
results. But before that happens, you might need to specify authentication and privacy settings.

Authentication and privacy
After you close the Invoke Custom Function dialog, a banner may appear asking you to specify
how to connect to the Key Phrases API.

Select Edit Credentials, make sure Anonymous is selected in the dialog, then select Connect.
ï¼— Note
You select Anonymous because Key Phrase Extraction authenticates requests using your
access key, so Power BI doesn't need to provide credentials for the HTTP request itself.

If you see the Edit Credentials banner even after choosing anonymous access, you check to see
if you pasted your Language resource key into the code in the KeyPhrases custom function.
Next, a banner may appear asking you to provide information about your data sources' privacy.

Select Continue and choose Public for each of the data sources in the dialog. Then select
Save.

Create the word cloud
Once you address with any banners that appear, select Close & Apply in the Home ribbon to
close the Query Editor.
Power BI Desktop takes a moment to make the necessary HTTP requests. For each row in the
table, the new keyphrases column contains the key phrases detected in the text by the Key
Phrases API.

Now you use this column to generate a word cloud. To get started, select the Report button in
the main Power BI Desktop window, to the left of the workspace.
ï¼— Note
Why use extracted key phrases to generate a word cloud, rather than the full text of every
comment? The key phrases provide us with the important words from our customer
comments, not just the most common words. Also, word sizing in the resulting cloud
doesn't correlate to the frequent use of a word in a relatively small number of comments.
If you don't already have the Word Cloud custom visual installed, install it. In the Visualizations
panel to the right of the workspace, select the three dots (...) and choose Import From Market.
If the word "cloud" isn't among the displayed visualization tools in the list, you can search for
"cloud" and select the Add button next the Word Cloud visual. Power BI installs the Word
Cloud visual and lets you know that it installed successfully.

First, select the Word Cloud icon in the Visualizations panel.

A new report appears in the workspace. Drag the keyphrases field from the Fields panel to the
Category field in the Visualizations panel. The word cloud appears inside the report.
Now switch to the Format page of the Visualizations panel. In the Stop Words category, turn on
Default Stop Words to eliminate short, common words like "of" from the cloud. However,
because we're visualizing key phrases, they might not contain stop words.

Scroll down the panel and turn off Rotate Text and Title.

Select the Focus Mode tool in the report to get a better look at our word cloud. The tool
expands the word cloud to fill the entire workspace.

Using other features
Language also provides sentiment analysis and language detection. The language detection in
particular is useful if your customer feedback isn't all in English.
Both of these other APIs are similar to the Key Phrases API. That means you can integrate them
with Power BI Desktop using custom functions that are nearly identical to the one you created
in this tutorial. Just create a blank query and paste the appropriate code into the Advanced
Editor, as you did earlier. (Don't forget your access key!) Then, as before, use the function to
add a new column to the table.
The Sentiment Analysis function below returns a label indicating how positive the sentiment
expressed in the text is.
F#
// Returns the sentiment label of the text, for example, positive, negative or
mixed.
(text) => let
apikey = "YOUR_API_KEY_HERE",
endpoint = "<your-custom-subdomain>.cognitiveservices.azure.com" &
"/text/analytics/v3.1/sentiment",
jsontext = Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text), 5000))),
jsonbody = "{ documents: [ { language: ""en"", id: ""0"", text: " & jsontext & "
} ] }",
bytesbody = Text.ToBinary(jsonbody),
headers = [#"Ocp-Apim-Subscription-Key" = apikey],
bytesresp = Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),
jsonresp = Json.Document(bytesresp),

sentiment
= jsonresp[documents]{0}[sentiment]
in sentiment

Here are two versions of a Language Detection function. The first returns the ISO language
code (for example, en for English), while the second returns the "friendly" name (for example,
English ). You may notice that only the last line of the body differs between the two versions.

F#
// Returns the two-letter language code (for example, 'en' for English) of the text
(text) => let
apikey
= "YOUR_API_KEY_HERE",
endpoint
= "https://<your-custom-subdomain>.cognitiveservices.azure.com" &
"/text/analytics/v3.1/languages",
jsontext
= Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text),
5000))),
jsonbody
= "{ documents: [ { id: ""0"", text: " & jsontext & " } ] }",
bytesbody
= Text.ToBinary(jsonbody),
headers
= [#"Ocp-Apim-Subscription-Key" = apikey],
bytesresp
= Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),
jsonresp
= Json.Document(bytesresp),
language
= jsonresp [documents]{0}[detectedLanguage] [name] in language

F#
// Returns the name (for example, 'English') of the language in which the text is
written
(text) => let
apikey
= "YOUR_API_KEY_HERE",
endpoint
= "https://<your-custom-subdomain>.cognitiveservices.azure.com" &
"/text/analytics/v3.1/languages",
jsontext
= Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text),
5000))),
jsonbody
= "{ documents: [ { id: ""0"", text: " & jsontext & " } ] }",
bytesbody
= Text.ToBinary(jsonbody),
headers
= [#"Ocp-Apim-Subscription-Key" = apikey],
bytesresp
= Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),
jsonresp
= Json.Document(bytesresp),
language
=jsonresp [documents]{0}[detectedLanguage] [name] in language

Finally, here's a variant of the Key Phrases function already presented that returns the phrases
as a list object, rather than as a single string of comma-separated phrases.
ï¼— Note
Returning a single string simplified our word cloud example. A list, on the other hand, is a
more flexible format for working with the returned phrases in Power BI. You can

manipulate list objects in Power BI Desktop using the Structured Column group in the
Query Editor's Transform ribbon.

F#
// Returns key phrases from the text as a list object
(text) => let
apikey
= "YOUR_API_KEY_HERE",
endpoint
= "https://<your-custom-subdomain>.cognitiveservices.azure.com" &
"/text/analytics/v3.1/keyPhrases",
jsontext
= Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text),
5000))),
jsonbody
= "{ documents: [ { language: ""en"", id: ""0"", text: " & jsontext
& " } ] }",
bytesbody
= Text.ToBinary(jsonbody),
headers
= [#"Ocp-Apim-Subscription-Key" = apikey],
bytesresp
= Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),
jsonresp
= Json.Document(bytesresp),
keyphrases = jsonresp[documents]{0}[keyPhrases]
in keyphrases

Next steps
Learn more about Language, the Power Query M formula language, or Power BI.
Language overview
Power Query M reference
Power BI documentation

Last updated on 11/18/2025

What is Named Entity Recognition (NER) in
Azure Language in Foundry Tools?
Named Entity Recognition (NER) is one of the features offered by Azure Language in Foundry
Tools, a collection of machine learning and AI algorithms in the cloud for developing intelligent
applications that involve written language. The NER feature can identify and categorize entities
in unstructured text. For example: people, places, organizations, and quantities. The prebuilt
NER feature has a preset list of recognized entities. The custom NER feature allows you to train
the model to recognize specialized entities specific to your use case.
Quickstarts are getting-started instructions to guide you through making requests to the
service.
How-to guides contain instructions for using the service in more specific or customized
ways.
The conceptual articles provide in-depth explanations of the service's functionality and
features.

Typical workflow
To use this feature, you submit data for analysis and handle the API output in your application.
Analysis is performed as-is, with no added customization to the model used on your data.
1. Create an Azure Language in Foundry Tools resource, which grants you access to the
features offered by Language. It generates a password (called a key) and an endpoint URL
that you use to authenticate API requests.
2. Create a request using either the REST API or the client library for C#, Java, JavaScript, and
Python. You can also send asynchronous calls with a batch request to combine API
requests for multiple features into a single call.
3. Send the request containing your text data. Your key and endpoint are used for
authentication.
4. Stream or store the response locally.

Get started with named entity recognition
To use named entity recognition, you submit raw unstructured text for analysis and handle the
API output in your application. Analysis is performed as-is, with no additional customization to
the model used on your data. There are two ways to use named entity recognition:

ï¾‰

Development

Expand table

Description

option
Microsoft Foundry

Foundry is a web-based platform that lets you use named entity recognition with
text examples with your own data when you sign up. For more information, see
the Foundry website or Foundry documentation.

REST API or Client
library (Azure SDK)

Integrate named entity recognition into your applications using the REST API, or
the client library available in a variety of languages. For more information, see the
named entity recognition quickstart.

Reference documentation and code samples
As you use this feature in your applications, see the following reference documentation and
samples for Azure Language in Foundry Tools:
ï¾‰

Expand table

Development option / language

Reference documentation

Samples

REST API

REST API documentation

C#

C# documentation

C# samples

Java

Java documentation

Java Samples

JavaScript

JavaScript documentation

JavaScript samples

Python

Python documentation

Python samples

Responsible AI
An AI system consists of more than just its core technology. It also includes the people who
operate it, the people its use affects, and the broader deployment context. All these
interconnected elements shape the effectiveness and outcomes of AI. Read the transparency
note for NER to learn about responsible AI use and deployment in your systems. For more
information, see the following articles:
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Scenarios
Enhance search capabilities and search indexing. Customers can build knowledge graphs
based on entities detected in documents to enhance document search as tags.
Automate business processes - Insurance claims, recognized entities like name and
location can be highlighted to facilitate review. Support tickets can be automatically
generated with customer name and company from an email.
In-depth customer analysis. Determine the most popular information conveyed by
customers in reviews, emails, and calls to determine relevant topics and trends over time.

Next steps
There are two ways to get started using the Named Entity Recognition (NER) feature:
Microsoft Foundry is a web-based platform that lets you use several Language features
without needing to write code.
The quickstart article for instructions on making requests to the service using the REST
API and client library SDK.

Last updated on 11/18/2025

Quickstart: Detecting named entities (NER)
Prerequisites
Create a Project in Foundry in the Microsoft Foundry portal

Navigate to the Foundry Playground
Using the left side pane, select Playgrounds. Then select the Try Azure Language Playground
button.

ï Š

Use NER in the Foundry Playground
The Language Playground consists of four sections:

Top banner: You can select any of the currently available Languages here.
Right pane: This pane is where you can find the Configuration options for the service,
such as the API and model version, along with features specific to the service.
Center pane: This pane is where you enter your text for processing. After the operation is
run, some results are shown here.
Right pane: This pane is where Details of the run operation are shown.
Here you can select the Named Entity Recognition capability by choosing the top banner tile,
Extract Named Entities.

Use Extract Named Entities
Extract Named Entities is designed to identify named entities in text.
In Configuration there are the following options:
ï¾‰

Expand table

Option

Description

Select API version

Select which version of the API to use.

Select model version

Select which version of the model to use.

Select text language

Select which language the language is input in.

Select types to include

Select they types of information you want to extract.

Overlap policy

Select the policy for overlapping entities.

Inference options

Additional options to customize the return of the processed data.

After your operation is completed, the type of entity is displayed beneath each entity in the
center pane and the Details section contains the following fields for each entity:
ï¾‰

Expand table

Field

Description

Entity

The detected entity.

Category

The type of entity that was detected.

Offset

The number of characters that the entity was detected from the beginning of the line.

Length

The character length of the entity.

Field

Description

Confidence

How confident the model is in the correctness of identification of entity's type.

ï Š

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
NER overview

Last updated on 11/18/2025

Named Entity Recognition (NER) language
support
Use this article to learn which natural languages are supported by the NER feature of Azure
Language in Foundry Tools.
ï¼— Note
You can additionally find the language support for the Preview API in the second tab.

NER language support
Generally Available API

ï¾‰

Language

Language Code

Afrikaans

af

Albanian

sq

Amharic

am

Arabic

ar

Armenian

hy

Assamese

as

Azerbaijani

az

Basque

eu

Bengali

bn

Bosnian

bs

Bulgarian

bg

Burmese

my

Catalan

ca

Notes

Expand table

Language

Language Code

Notes

Chinese (Simplified)

zh-Hans

zh also accepted

Chinese (Traditional)

zh-Hant

Croatian

hr

Czech

cs

Danish

da

Dutch

nl

English

en

Estonian

et

Finnish

fi

French

fr

Galician

gl

Georgian

ka

German

de

Greek

el

Gujarati

gu

Hebrew

he

Hindi

hi

Hungarian

hu

Indonesian

id

Irish

ga

Italian

it

Japanese

ji

Kannada

kn

Kazakh

kk

Khmer

km

Korean

ko

Language

Language Code

Kurdish (Kurmanji)

ku

Kyrgyz

ky

Lao

lo

Latvian

lv

Lithuanian

lt

Macedonian

mk

Malagasy

mg

Malay

ms

Malayalam

ml

Marathi

mr

Mongolian

mn

Nepali

ne

Norwegian (Bokmal)

no

Odia

or

Pashto

ps

Persian

fa

Polish

pl

Portuguese (Brazil)

pt-BR

Portuguese (Portugal)

pt-PT

Punjabi

pa

Romanian

ro

Russian

ru

Serbian

sr

Slovak

sk

Slovenian

sl

Somali

so

Notes

nb also accepted

pt also accepted

Language

Language Code

Spanish

es

Swahili

sw

Swazi

ss

Swedish

sv

Tamil

ta

Telugu

te

Thai

th

Turkish

tr

Ukrainian

uk

Urdu

ur

Uyghur

ug

Uzbek

uz

Vietnamese

vi

Welsh

cy

Next steps
NER feature overview

Last updated on 11/18/2025

Notes

Transparency note for Named Entity
Recognition including Personally
Identifiable Information (PII)
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
ï¼‰ Important
This article assumes that you're familiar with guidelines and best practices for Azure
Language in Foundry Tools. For more information, see Transparency note for Language.
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance. Microsoft's Transparency
Notes are intended to help you understand how our AI technology works, the choices system
owners can make that influence system performance and behavior, and the importance of
thinking about the whole system, including the technology, the people, and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who will use or be affected by your system.
Microsoft's Transparency notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Responsible AI principles from Microsoft.

Introduction to Named Entity Recognition and
Personally Identifiable Information (PII)
Language supports named entity recognition to identify and categorize information in your
text. These include general entities such as Product and Event and Personally Identifiable
Information (PII) entities. A wide variety of personal entities such as names, organizations,
addresses, phone numbers, financial account numbers or codes and government and country
or region specific identification numbers can be recognized. A subset of these personal entities

is protected health information (PHI). If you specify domain=phi in your request, you will only
get the PHI entities returned. The full list of PII and PHI entity categories can be found in the
table here. In addition, the PII recognition supports the ability to specify specific entity
categories you want in the response and redact PII entities in the response. The PII entities will
be replaced by asterisks in the redactedText property of the response.
Read example NER request and example response to see how to send text to the service and
what to expect back.

Example use cases
Customers may want to recognize various categories of named entities two main reasons:
Enhance search capabilities - Customers can build knowledge graphs based on entities
detected in documents to enhance document search.
Enhance or automate business processes - For example, when reviewing insurance
claims, recognized entities like name and location could be highlighted to facilitate the
review. Or a support ticket could be generated with a customer's name and company
automatically from an email.
Customers may want to recognize various categories of PII entities specifically for several
reasons:
Apply sensitivity labels - For example, based on the results from the PII service, a public
sensitivity label might be applied to documents where no PII entities are detected. For
documents where US addresses and phone numbers are recognized, a confidential label
might be applied. A highly confidential label might be used for documents where bank
routing numbers are recognized.
Redact some categories of personal information from documents to protect privacy For example, if customer contact records are accessible to first line support
representatives, the company may want to redact unnecessary customer's personal
information from customer history to preserve the customer's privacy.
Redact personal information in order to reduce unconscious bias - For example, during
a company's resume review process, they may want to block name, address and phone
number to help reduce unconscious gender or other biases.
Replace personal information in source data for machine learning to reduce unfairness
â€“ For example, if you want to remove names that might reveal gender when training a
machine learning model, you could use the service to identify them and you could
replace them with generic placeholders for model training.

Considerations when choosing a use case

Do not use
PII only - Do not use for automatic redaction or information classification scenarios â€“ Any
scenario where failures to redact personal information could expose people to the risk of
identity theft and physical or psychological harms should include careful human
oversight.
NER and PII - Do not use for scenarios that use personal information for a purpose that
consent was not obtained for - For example, a company has resumes from past job
applicants. The applicants did not give their consent to be contacted for promotional
events when they submitted their resumes. Based on this scenario, both NER and PII
services should not be used to identify contact information for the purpose of inviting the
past applicants to a trade show.
NER and PII - Customers are prohibited from using of this service to harvest personal
information from publicly available content without consent from person(s) whom are the
subject of the personal information.
NER and PII - Do not use for scenarios that replace personal information in text with the
intent to mislead people.
Legal and regulatory considerations: Organizations need to evaluate potential specific legal
and regulatory obligations when using any AI services and solutions, which may not be
appropriate for use in every industry or scenario. Additionally, AI services or solutions are not
designed for and may not be used in ways prohibited in applicable terms of service and
relevant codes of conduct.

Characteristics and limitations
Depending on your scenario, input data and the entities you wish to extract, you could
experience different levels of performance. The following sections are designed to help you
understand key concepts about performance as they apply to using the Language NER and PII
services.

Understand and measure performance of NER
Since both false positive and false negative errors can occur, it is important to understand how
both types of errors might affect your overall system. With Named Entity Recognition (NER), a
false positive occurs when an entity is not present in the text, but is recognized and returned
by the system. A false negative is when an entity is present in the text, but is not recognized
and returned by the system.

Understanding performance for PII

In redaction scenarios, for example, false negatives could lead to personal information leakage.
For redaction scenarios, consider a process for human review to account for this type of error.
For sensitivity label scenarios, both false positives and false negatives could lead to
misclassification of documents. The audience may unnecessarily limited for documents labelled
as confidential where a false positive occurred. PII could be leaked where a false negative
occurred and a public label was applied.
You can adjust the threshold for confidence score your system uses to tune your system. If it is
more important to identify all potential instances of PII, you can use a lower threshold. This
means that you may get more false positives (non- PII data being recognized as PII entities),
but fewer false negatives (PII entities not recognized as PII). If it is more important for your
system to recognize only true PII data, you can use a higher threshold. Threshold values may
not have consistent behavior across individual categories of PII entities. Therefore, it is critical
that you test your system with real data it will process in production.

System limitations and best practices for enhancing
performance
Make sure you understand all the entity categories for NER and PII that can be
recognized by the system. Depending on your scenario, your data may include other
information that could be considered personal but is not covered by the categories the
service currently supports.
Context is important for all entity categories to be correctly recognized by the system, as
it often is for humans to recognize an entity. For example, without context a ten-digit
number is just a number. However, given context like "You can reach me at my office
phone number 2345678901," both the system and a human can recognize the ten-digit
number as a phone number. Always include context when sending text to the system to
obtain the best possible performance.
Person names in particular require linguistic context. Send as much context as possible for
better person name detection.
For conversational data, consider sending more than a single turn in the conversation to
ensure higher likelihood that the required context is included with the actual entities.
In the following conversation, if you send a single row at a time, the passport number will
not have any context associated with it and the EU Passport Number PII category will not
be recognized.
Hi, how can I help you today?
I want to renew my passport

Sure, what is your current passport number?
Its 123456789, thanks.
However, if you send the whole conversation it will be recognized because the context is
included.
Sometimes multiple entity categories can be recognized for the same entity. If we take
the previous example:
Hi, how can I help you today?
I want to renew my passport
Sure, what is your current passport number?
Its 123456789, thanks.
Several different countries have the same format for passport numbers, so several
different specific entity categories may be recognized. In some cases, using the highest
confidence score may not be sufficient to choose the right entity class. If your scenario
depends on the specific entity category being recognized, you may need to disambiguate
the result elsewhere in your system either through a human review or additional
validation code. Thorough testing on real life data can help you identify if you're likely to
see multiple entity categories for recognized for your scenario.
Not all entity categories are supported in all languages for both NER and PII. Be sure to
check the entity type article for the entities in the language you want to detect.
Many international PII entities are supported. By default, the entity categories returned
are those that match the language code sent with the API call. If you expect entities from
locales other than the one specified, you will need to specify them with the
piiCategories parameter. Learn more about how to specify what your response will

include in the API reference

. Learn more about the categories supported for each locale

in the named entity types documentation.
In PII redaction scenarios, if you are using the version of the API that includes the optional
parameter piiCategories , it is important that you consider all the PII categories that
could be present in your text. If you are redacting only specific entity categories or the
default entity categories for a specific locale, other PII entity categories that unexpectedly
appear in your text will be leaked. For example, if you have sent the EN-US locale and not
specified any optional PII categories and a German Driver's License Number is present in
your text, it will be leaked. To prevent this you would need to specify the German Driver's
License Number category in the piiCategories parameter. In addition, if you have
specified one or more categories using the piiCategories parameter for the specified

locale, be aware that those are the only categories that would be redacted. For example, if
you have sent the EN-US locale and have specified U.S. Social Security Number (SSN) as
the PII category for redaction, then any other EN-US categories such as U.S. Driver's
License Number or U.S. Passport Number would be leaked if they appear in the input text.
Since the PII service returns PII categories that match the language code in the call,
consider verifying the language the input text is in if you're not sure what language or
locale it will be. You can use the Language Detection feature to do this.
The PII service only takes text as an input. If you are redacting information from
documents in other formats, make sure to carefully test your redaction code to ensure
identified entities are not accidentally leaked.

See also
Transparency note for Language
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language
Guidance for integration and responsible use with Language

Last updated on 08/17/2025

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

How to use Named Entity Recognition
(NER)
The NER feature can evaluate unstructured text, and extract named entities from text in several
predefined categories, for example: person, location, event, product, and organization.

Development options
To use named entity recognition, you submit raw unstructured text for analysis and handle the
API output in your application. Analysis is performed as-is, with no additional customization to
the model used on your data. There are two ways to use named entity recognition:
ï¾‰

Expand table

Development
option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use named entity recognition with
text examples with your own data when you sign up. For more information, see
the Foundry website or Foundry documentation.

REST API or Client
library (Azure SDK)

Integrate named entity recognition into your applications using the REST API, or
the client library available in a variety of languages. For more information, see the
named entity recognition quickstart.

Determine how to process the data (optional)
Input languages
When you submit input text to be processed, you can specify which of the supported
languages they're written in. If you don't specify a language, key phrase extraction defaults to
English. The API may return offsets in the response to support different multilingual and emoji
encodings.

Submitting data
Analysis is performed upon receipt of the request. Using the NER feature synchronously is
stateless. No data is stored in your account, and results are returned immediately in the
response.

When using this feature asynchronously, the API results are available for 24 hours from the
time the request was ingested, and is indicated in the response. After this time period, the
results are purged and are no longer available for retrieval.
The API attempts to detect the defined entity categories for a given input text language.

Getting NER results
When you get results from NER, you can stream the results to an application or save the output
to a file on the local system. The API response includes recognized entities, including their
categories and subcategories, and confidence scores.

Select which entities to be returned
The API attempts to detect the defined entity types and tags for a given input text language.
The entity types and tags replace the categories and subcategories structure the older models
use to define entities for more flexibility. You can also specify which entities are detected and
returned, use the optional inclusionList and exclusionList parameters with the appropriate
entity types. The following example would detect only Location . You can specify one or more
entity types to be returned. Given the types and tags hierarchy introduced for this version, you
have the flexibility to filter on different granularity levels as so:
Input:
ï¼— Note
In this example, it returns only the "Location" entity type.

Bash
{
"kind": "EntityRecognition",
"parameters":
{
"inclusionList" :
[
"Location"
]
},
"analysisInput":
{
"documents":
[
{

"id":"1",
"language": "en",
"text": "We went to Contoso foodplace located at downtown Seattle
last week for a dinner party, and we adore the spot! They provide marvelous food and
they have a great menu. The chief cook happens to be the owner (I think his name is
John Doe) and he is super nice, coming out of the kitchen and greeted us all. We
enjoyed very much dining in the place! The pasta I ordered was tender and juicy, and
the place was impeccably clean. You can even pre-order from their online menu at
www.contosofoodplace.com, call 112-555-0176 or send email to
order@contosofoodplace.com! The only complaint I have is the food didn't come fast
enough. Overall I highly recommend it!"
}
]
}
}

The above examples would return entities falling under the Location entity type such as the
GPE , Structural , and Geological tagged entities as outlined by entity types and tags. We

could also further filter the returned entities by filtering using one of the entity tags for the
Location entity type such as filtering over GPE tag only as outlined:

Bash
"parameters":
{
"inclusionList" :
[
"GPE"
]
}

This method returns all Location entities only falling under the GPE tag and ignore any other
entity falling under the Location type that is tagged with any other entity tag such as
Structural or Geological tagged Location entities. We can also further analyze our results by

using the exclusionList parameter. GPE tagged entities could be tagged with the following
tags: City , State , CountryRegion , Continent . We could, for example, exclude Continent and
CountryRegion tags for our example:

Bash
"parameters":
{
"inclusionList" :
[
"GPE"

],
"exclusionList": :
[
"Continent",
"CountryRegion"
]
}

Using these parameters we can successfully filter on only Location entity types, since the GPE
entity tag included in the inclusionList parameter, falls under the Location type. We then
filter on only Geopolitical entities and exclude any entities tagged with Continent or
CountryRegion tags.

Supported output attributes
In order to provide users with more insight into an entity's types and provide increased
usability, NER supports these attributes in the output:
ï¾‰

Name of the
attribute

Type

Definition

type

String

The most specific type of detected entity.

Expand table

For example, "Seattle" is a City , a GPE (Geo Political Entity) and a Location .
The most granular classification for "Seattle" is City . The type is City for the
text "Seattle."
tags

List
(tags)

A list of tag objects that expresses the affinity of the detected entity to a
hierarchy or any other grouping.
A tag contains two fields:
- name : A unique name for the tag.
- confidenceScore : The associated confidence score for a tag ranging from 0
to 1.
This unique tagName is used to filter in the inclusionList and exclusionList
parameters.

metadata

Object

Metadata is an object containing more data about the entity type detected. It
changes based on the field metadataKind .

Sample output

This sample output includes an example of output attributes.
Bash
{
"kind": "EntityRecognitionResults",
"results": {
"documents": [
{
"id": "1",
"entities": [
{
"text": "Microsoft",
"category": "Organization",
"type": "Organization",
"offset": 0,
"length": 9,
"confidenceScore": 0.97,
"tags": [
{
"name": "Organization",
"confidenceScore": 0.97
}
]
},
{
"text": "One",
"category": "Quantity",
"type": "Number",
"subcategory": "Number",
"offset": 21,
"length": 3,
"confidenceScore": 0.9,
"tags": [
{
"name": "Number",
"confidenceScore": 0.8
},
{
"name": "Quantity",
"confidenceScore": 0.8
},
{
"name": "Numeric",
"confidenceScore": 0.8
}
],
"metadata": {
"metadataKind": "NumberMetadata",
"numberKind": "Integer",
"value": 1.0
}
}
],

"warnings": []
}
],
"errors": [],
"modelVersion": "2023-09-01"
}
}

Specify the NER model
By default, this feature uses the latest available AI model on your text. You can also configure
your API requests to use a specific model version.

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

Next steps
Named Entity Recognition overview

Last updated on 11/18/2025

Using named entity recognition skill
parameters
Use this article to get an overview of the different API parameters used to adjust the input to a
Named Entity Recognition (NER) API call. The Generally Available NER service now supports the
ability to specify a list of entity tags to be included into the response or excluded from the
response. If a piece of text is classified as more than one entity type, the overlapPolicy
parameter allows customers to specify how the service handles the overlap. The
inferenceOptions parameter allows for users to adjust the inference, such as excluding the

detected entity values from being normalized and included in the metadata.

InclusionList parameter
The inclusionList parameter allows for you to specify which of the NER entity tags, you would
like included in the entity list output in your inference JSON listing out all words and
categorizations recognized by the NER service. By default, all recognized entities are listed.

ExclusionList parameter
The exclusionList parameter allows for you to specify which of the NER entity tags, you would
like excluded in the entity list output in your inference JSON listing out all words and
categorizations recognized by the NER service. By default, all recognized entities are listed.

overlapPolicy parameter
The overlapPolicy parameter allows for you to specify how you like the NER service to
respond to recognized words/phrases that fall into more than one category.
By default, the overlapPolicy parameter is set to matchLongest . This option categorizes the
extracted word/phrase under the entity category that can encompass the longest span of the
extracted word/phrase (longest defined by the most number of characters included).
The alternative option for this parameter is allowOverlap , where all possible entity categories
are listed. Parameters by supported API version

inferenceOptions parameter
Defines a selection of options available for adjusting the inference. Currently we have only one
property called excludeNormalizedValues that excludes the detected entity values to be

normalized and included in the metadata. The numeric and temporal entity types support value
normalization.

Sample
This bit of sample code explains how to use skill parameters.
Bash
{
"analysisInput": {
"documents": [
{
"id": "1",
"text": "My name is John Doe",
"language": "en"
}
]
},
"kind": "EntityRecognition",
"parameters": {
"overlapPolicy": {
"policyKind": "AllowOverlap" //AllowOverlap|MatchLongest(default)
},
"inferenceOptions": {
"excludeNormalizedValues": true //(Default: false)
},
"inclusionList": [
"DateAndTime" // A list of entity tags to be used to allow into the
response.
],
"exclusionList": ["Date"] // A list of entity tags to be used to filter out
from the response.
}
}

Next steps
See Configure containers for configuration settings.

Last updated on 12/05/2025

Install and run Named Entity Recognition
containers
Containers enable you to host the Named Entity Recognition API on your own infrastructure. If
you have security or data governance requirements that can't be fulfilled by calling Named
Entity Recognition remotely, then containers might be a good option.
If you don't have an Azure subscription, create a free account

before you begin.

Prerequisites
You must meet the following prerequisites before using Named Entity Recognition containers.
If you don't have an Azure subscription, create a free account
Docker

.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts

.

A Language resource

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:

Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the available
container. Each CPU core must be at least 2.6 gigahertz (GHz) or faster.
It is recommended to have a CPU with AVX-512 instruction set, for the best experience
(performance and accuracy).
ï¾‰

Named Entity Recognition

Expand table

Minimum host specs

Recommended host specs

1 core, 2GB memory

4 cores, 8GB memory

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Get the container image with docker pull
The Named Entity Recognition container image can be found on the mcr.microsoft.com
container registry syndicate. It resides within the azure-cognitive-services/textanalytics/
repository and is named ner . The fully qualified container image name is,
mcr.microsoft.com/azure-cognitive-services/textanalytics/ner

To use the latest version of the container, you can use the latest tag, which is for English. You
can also find a full list of containers for supported languages using the tags on the MCR

.

The latest Named Entity Recognition container is available in several languages. To download
the container for the English container, use the command below.

docker pull mcr.microsoft.com/azure-cognitive-services/textanalytics/ner:latest

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container with docker run
Once the container is on the host computer, use the docker run

command to run the

containers. The container will continue to run until you stop it. Replace the placeholders with
your own values:
ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove this based on your host operating
system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container won't start. For more information, see Billing.
To run the Named Entity Recognition container, execute the following docker run command.
Replace the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Language resource.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

You can find it on your resource's Key
and endpoint page, on the Azure
portal.
{ENDPOINT_URI}

The endpoint for accessing the API.
You can find it on your resource's Key
and endpoint page, on the Azure

https://<your-customsubdomain>.cognitiveservices.azure.com

portal.
{IMAGE_TAG}

The image tag representing the
language of the container you want
to run. Make sure this matches the
docker pull command you used.

latest

Bash
docker run --rm -it -p 5000:5000 --memory 8g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/textanalytics/ner:{IMAGE_TAG} \
Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY}

This command:
Runs a Named Entity Recognition container from the container image
Allocates one CPU core and 8 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Expand table

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

For information on how to call NER see our guide.

Run the container disconnected from the internet
To use this container disconnected from the internet, you must first request access by filling
out an application, and purchasing a commitment plan. See Use Docker containers in
disconnected environments for more information.
If you have been approved to run the container disconnected from the internet, use the
following example shows the formatting of the docker run command you'll use, with

placeholder values. Replace these placeholder values with your own values.
The DownloadLicense=True parameter in your docker run command will download a license file
that will enable your Docker container to run when it isn't connected to the internet. It also
contains an expiration date, after which the license file will be invalid to run the container. You
can only use a license file with the appropriate container that you've been approved for. For
example, you can't use a license file for a speech to text container with a Document
Intelligence container.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitiveservices/form-recognizer/invoice

{LICENSE_MOUNT}

The path where the
license will be
downloaded, and
mounted.

/host/license:/path/to/license/directory

{ENDPOINT_URI}

The endpoint for

https://<your-custom-

authenticating your
service request. You can
find it on your
resource's Key and
endpoint page, on the
Azure portal.

subdomain>.cognitiveservices.azure.com

{API_KEY}

The key for your Text
Analytics resource. You
can find it on your
resource's Key and
endpoint page, on the
Azure portal.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

{CONTAINER_LICENSE_DIRECTORY}

Location of the license

/path/to/license/directory

folder on the
container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 \
-v {LICENSE_MOUNT} \
{IMAGE} \
eula=accept \
billing={ENDPOINT_URI} \
apikey={API_KEY} \

DownloadLicense=True \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}

Once the license file has been downloaded, you can run the container in a disconnected
environment. The following example shows the formatting of the docker run command you'll
use, with placeholder values. Replace these placeholder values with your own values.
Wherever the container is run, the license file must be mounted to the container and the
location of the license folder on the container's local filesystem must be specified with
Mounts:License= . An output mount must also be specified so that billing usage records can be

written.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image

mcr.microsoft.com/azure-cognitive-

you want to use.

services/form-recognizer/invoice

The appropriate size

4g

{MEMORY_SIZE}

of memory to
allocate for your
container.
{NUMBER_CPUS}

The appropriate
number of CPUs to
allocate for your

4

container.
{LICENSE_MOUNT}

The path where the
license will be
located and
mounted.

/host/license:/path/to/license/directory

{OUTPUT_PATH}

The output path for
logging usage

/host/output:/path/to/output/directory

records.
{CONTAINER_LICENSE_DIRECTORY}

Location of the
license folder on the
container's local
filesystem.

/path/to/license/directory

{CONTAINER_OUTPUT_DIRECTORY}

Location of the
output folder on the

/path/to/output/directory

container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 --memory {MEMORY_SIZE} --cpus {NUMBER_CPUS} \
-v {LICENSE_MOUNT} \
-v {OUTPUT_PATH} \
{IMAGE} \
eula=accept \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}
Mounts:Output={CONTAINER_OUTPUT_DIRECTORY}

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
The Named Entity Recognition containers send billing information to Azure, using a Language
resource on your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure

The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If
the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

For more information about these options, see Configure containers.

Summary
In this article, you learned concepts and workflow for downloading, installing, and running
Named Entity Recognition containers. In summary:
Named Entity Recognition provides Linux containers for Docker
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You must specify billing information when instantiating a container.
ï¼‰ Important
Azure AI containers are not licensed to run without being connected to Azure for
metering. Customers need to enable the containers to communicate billing information

with the metering service at all times. Azure AI containers do not send customer data (e.g.
text that is being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

Use Docker containers in disconnected
environments
Containers enable you to run Foundry Tools APIs in your own environment, and are great for
your specific security and data governance requirements. Disconnected containers enable you
to use several of these APIs disconnected from the internet. Currently, the following containers
can be run in this manner:
Speech to text
Custom Speech to text
Neural Text to speech
Text Translation (Standard)
Azure Language in Foundry Tools
Sentiment Analysis
Key Phrase Extraction
Language Detection
Summarization
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)
Azure Vision in Foundry Tools - Read
Document Intelligence
Before attempting to run a Docker container in an offline environment, make sure you know
the steps to successfully download and use the container. For example:
Host computer requirements and recommendations.
The Docker pull command you use to download the container.
How to validate that a container is running.
How to send queries to the container's endpoint, once it's running.

Request access to use containers in disconnected
environments
Fill out and submit the request form

to request access to the containers disconnected from

the internet.
The form requests information about you, your company, and the user scenario for which you'll
use the container. After you submit the form, the Foundry Tools team reviews it and emails you
with a decision within 10 business days.

ï¼‰ Important
On the form, you must use an email address associated with an Azure subscription
ID.
The Azure resource you use to run the container must have been created with the
approved Azure subscription ID.
Check your email (both inbox and junk folders) for updates on the status of your
application from Microsoft.
After you're approved, you'll be able to run the container after you download it from the
Microsoft Container Registry (MCR), described later in the article.
You won't be able to run the container if your Azure subscription hasn't been approved.
Access is limited to customers that meet the following requirements:
Your organization should be identified as strategic customer or partner with Microsoft.
Disconnected containers are expected to run fully offline, hence your use cases must
meet one of these or similar requirements:
Environments or devices with zero connectivity to internet.
Remote location that occasionally has internet access.
Organization under strict regulation of not sending any kind of data back to cloud.
Application completed as instructed - Pay close attention to guidance provided
throughout the application to ensure you provide all the necessary information required
for approval.

Purchase a commitment tier pricing plan for
disconnected containers
Create a new resource
1. Sign in to the Azure portal

and select Create a new resource for one of the applicable

Foundry Tools listed.
2. Enter the applicable information to create your resource. Be sure to select Commitment
tier disconnected containers as your pricing tier.
ï¼— Note

You only see the option to purchase a commitment tier if you have been
approved by Microsoft.
Pricing details are only examples.
3. Select Review + Create at the bottom of the page. Review the information, and select
Create.

Configure container for disconnected usage
See the following documentation for steps on downloading and configuring the container for
disconnected usage:
Vision - Read
Language Understanding (LUIS)
Text Translation (Standard)
Document Intelligence
Speech service
Speech to text
Custom Speech to text
Neural Text to speech
Language service
Sentiment Analysis
Key Phrase Extraction
Language Detection
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)

Environment variable names in Kubernetes
deployments
Some Azure AI Containers, for example Translator, require users to pass environmental variable
names that include colons ( : ) when running the container. This works fine when using Docker,
but Kubernetes doesn't accept colons in environmental variable names. To resolve this, you can
replace colons with double underscore characters ( __ ) when deploying to Kubernetes. See the
following example of an acceptable format for environment variable names:

Kubernetes
env:
- name: Mounts__License
value: "/license"
- name: Mounts__Output
value: "/output"

This example replaces the default format for the Mounts:License and Mounts:Output
environment variable names in the docker run command.

Container image and license updates
Container license files are used as keys to decrypt certain files within each container image. If
these encrypted files happen to be updated within a new container image, the license file you
have may fail to start the container even if it worked with the previous version of the container
image. To avoid this issue, we recommend that you download a new license file from the
resource endpoint for your container provided in Azure portal after you pull new image
versions from mcr.microsoft.com.
To download a new license file, you can add DownloadLicense=True to your docker run
command along with a license mount, your API Key, and your billing endpoint. Refer to your
container's documentation for detailed instructions.

Usage records
When operating Docker containers in a disconnected environment, the container writes usage
records to a volume where they're collected over time. You can also call a REST endpoint to
generate a report about service usage.

Arguments for storing logs
When run in a disconnected environment, an output mount must be available to the container
to store usage logs. For example, you would include -v /host/output:{OUTPUT_PATH} and
Mounts:Output={OUTPUT_PATH} in the example below, replacing {OUTPUT_PATH} with the path

where the logs are stored:
Docker
docker run -v /host/output:{OUTPUT_PATH} ... <image> ... Mounts:Output={OUTPUT_PATH}

Get records using the container endpoints

The container provides two endpoints for returning records about its usage.

Get all records
The following endpoint provides a report summarizing all of the usage collected in the
mounted billing record directory.
HTTP
https://<service>/records/usage-logs/

It returns JSON similar to the example below.
JSON
{
"apiType": "noop",
"serviceName": "noop",
"meters": [
{
"name": "Sample.Meter",
"quantity": 253
}
]
}

Get records for a specific month
The following endpoint provides a report summarizing usage over a specific month and year.
HTTP
https://<service>/records/usage-logs/{MONTH}/{YEAR}

It returns a JSON response similar to the example below:
JSON
{
"apiType": "string",
"serviceName": "string",
"meters": [
{
"name": "string",
"quantity": 253
}

]
}

Purchase a commitment plan to use containers in
disconnected environments
Commitment plans for disconnected containers have a calendar year commitment period.
When you purchase a plan, you are charged the full price immediately. During the commitment
period, you can't change your commitment plan, however you can purchase more units at a
pro-rated price for the remaining days in the year. You have until midnight (UTC) on the last
day of your commitment, to end a commitment plan.
You can choose a different commitment plan in the Commitment Tier pricing settings of your
resource.

End a commitment plan
If you decide that you don't want to continue purchasing a commitment plan, you can set your
resource's auto-renewal to Do not auto-renew. Your commitment plan expires on the
displayed commitment end date. After this date, you won't be charged for the commitment
plan. You are able to continue using the Azure resource to make API calls, charged at Standard
pricing. You have until midnight (UTC) on the last day of the year to end a commitment plan for
disconnected containers, and not be charged for the following year.

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Disconnected containers
Frequently asked questions (FAQ).

Next steps
Azure AI containers overview

Last updated on 10/02/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

Named entity categories and types
Named Entity Recognition (NER) is a computational linguistic process within natural language processing
(NLP) that uses predictive models to detect and identify entities within unstructured text. After entities are
detected, each entity receives a semantic label and is organized into predefined categories and types:
Entity Categories refer to main classifications of named entities such as Location, Organization, Date,
or Quantity.
Entity Types provide more detailed distinctions within the broader categories, allowing for more
granularity and flexibility.
This article provides a list of entity categories identified and returned by the Named Entity Recognition
(NER) process.

Language Support
The NER language support page lists all languages available for the named entities in this article. Any
exceptions are noted for specific named entities.

Supported API versions:
Stable: Generally Available (GA)
Preview: 2025-05-15-preview
ï¼— Note
Beginning with the GA API (released 2024-11-01), the Subcategory field is no longer supported. All
entity classifications now use the type field.

NER named entity types
ï¾‰

Expand table

Entities

Entities

Entities

Entities

Entities

Entities

Address

Age

Airport

Area

City

ComputingProduct

Continent

CountryRegion

CulturalEvent

Currency

Date

DateRange

DateTime

DateTimeRange

Dimension

Duration

Email

Event

Geological

GPE

Height

Information

IpAddress

Length

Location

NaturalEvent

Number

NumberRange

Ordinal

Organization

OrganizationMedical

OrganizationSports

OrganizationStockExchange

Percentage

Person

PersonType

Entities

Entities

Entities

Entities

Entities

Entities

PhoneNumber

Product

SetTemporal

Skill

Speed

SportsEvent

State

Structural

Temporal

Temperature

Time

TimeRange

URL

Volume

Weight

Type: Address
Category: Address
ï¾‰

Expand table

Entity

Tags

Detail

Address

Address

A distinct identifier assigned to a physical or geographic location, utilized for navigation, delivery
services, and formal administrative purposes.

Type: Age
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

Metadata

Age

Numeric,
Quantity, Age

A quantitative measure representing the length of time from an individual's
birth to a specific reference date.

Age
metadata

Type: Airport
Category: Location
ï¾‰

Expand table

Entity

Tags

Detail

Airport

Location,
Airport

A designated location equipped with facilities for the landing, takeoff, and maintenance of
aircraft.

Type: Area
Category: Quantity

ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Area

Numeric, Quantity, Dimension,
Area

The measurement of a surface or region expressed in
square units.

Area
metadata

Type: City
Category: Location
ï¾‰

Entity

Tags

Detail

City

Location,GPE,City

A settlement characterized by a dense population and infrastructure.

Expand table

Type: ComputingProduct
Category: Product
ï¾‰

Expand table

Entity

Tags

Detail

ComputingProduct

Product,
ComputingProduct

A hardware or software item designed for computational tasks or
digital processing.

Type: Continent
Category: Location

Entity

Tags

Detail

Continent

Location,GPE,Continent

A vast, continuous landmass on the Earth's surface.

ï¾‰

Expand table

ï¾‰

Expand table

Type: CountryRegion
Category: Location

Entity

Tags

Detail

CountryRegion

Location,GPE,CountryRegion

A distinct territorial entity recognized as a nation or administrative area.

Type: CulturalEvent
Category: Event
Expand table

ï¾‰

Entity

Tags

Detail

Language
support

CulturalEvent

Event,
EventCultural

An organized activity or gathering that reflects or celebrates
cultural practices or traditions

en

Type: Currency
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Currency

Numeric, Quantity,
Currency

A system of money in common use, typically issued by a
government and used as a medium of exchange.

Currency
metadata

Type: Date
Category: DateTime
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Date

DateTime, Date

A specific calendar day expressed in terms of day, month, and year.

Date metadata

Type: DateRange
Category: DateTime
ï¾‰

Expand table

Entity

Tags

Detail

DateRange

DateTime, DateRange

A span of time defined by a start and end date.

Type: DateTime
Category: DateTime
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

DateTime

DateTime

A data type encompassing date and time components used in scheduling
or logging events.

DateTime
metadata

Type: DateTimeRange
Category: DateTime
ï¾‰

Expand table

Entity

Tags

Detail

DateTimeRange

DateTime, DateTimeRange

A period defined by a starting and ending date and time.

Type: Dimension
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

Dimension

Numeric, Quantity,
Dimension

The measurable size or extent of an object or area, commonly expressed in
terms of length, width, height, or depth.

Type: Duration
Category: DateTime
ï¾‰

Entity

Tags

Detail

Duration

DateTime, Duration

The total time interval during which an event occurs or continues.

Expand table

Type: Email
Category: Email

Entity

Tags

Detail

Email

Email

An electronic message sent and received via digital mail systems.

ï¾‰

Expand table

ï¾‰

Expand table

ï¾‰

Expand table

Type: Event
Category: Event

Entity

Tags

Detail

Event

Event

A specific or noteworthy instance, or activity occurring within a defined context.

Type: Geographical
Category: Location

Entity

Tags

Detail

Geographical

Location,

Earth's physical geography and natural features, including landforms like rivers,

Geographical

mountains, and valleys.

Type: GPE
Category: Location
ï¾‰

Expand table

Entity

Tags

Detail

GPE

Location, GPE

Geo political entity (GPE) is a region or area defined by political boundaries or governance.

Type: Height
Category: Quantity

ï¾‰

Entity

Tags

Detail

Height

Numeric, Quantity, Dimension, Height

The measurement of vertical distance.

Expand table

Type: Information
Category: Information
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Information

Information

Structured data or processed knowledge transmitted or acquired
about a specific entity, event, or condition.

Information
metadata

Type: IpAddress
Category:IpAddress
ï¾‰

Expand table

Entity

Tags

Detail

IpAddress

IpAddress

A unique numerical label assigned a device connected to a computer network using Internet
Protocol.

Type: Length
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Length

Numeric, Quantity, Dimension,

The measurement of an object or distance between

Length

Length

two points.

metadata

Type: Location
Category: Location
ï¾‰

Expand table

Entity

Tags

Detail

Location

Location

A specific point or area in physical or virtual space defined by exact coordinates, metadata, or
unique identifiers that can be referenced, queried, or accessed.

Type: NaturalEvent
Category: Event
ï¾‰

Entity

Tags

Detail

Expand table
Language
support

NaturalEvent

Event,

An occurrence or phenomenon that takes place in a physical

EventNatural

environment as a result of natural processes, without direct human
intervention.

en

Type: Number
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Number

Numeric, Quantity,

A mathematical value used for counting, measuring, or

Number

Number

labeling.

metadata

Type: NumberRange
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

NumberRange

Numeric, Quantity,
NumberRange

A set of numbers that includes all values between a
specified minimum and maximum boundary.

NumberRange
metadata

Type: Ordinal
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Ordinal

Numeric,

A number indicating position or order in a sequence, such as first,

Ordinal

Ordinal

second, or third.

metadata

Type: Organization
Category: Organization

Entity

Tags

Detail

Organization

Organization

A company, institution, or group formed for a specific purpose.

ï¾‰

Expand table

ï¾‰

Expand table

Type: OrganizationMedical
Category: Organization

Entity

Tags

Detail

Language
support

OrganizationMedical

Organization,

An entity that delivers or facilitates

OrganizationMedical

healthcare or medical services.

en

Type: OrganizationSports
ï¾‰

Expand table

Entity

Tags

Detail

Language
support

OrganizationSports

Organization,
OrganizationSports

An entity that manages or promotes sports
activities or teams (Organization).

en

Type: OrganizationStockExchange
Category: Organization
ï¾‰

Expand table

Entity

Tags

Detail

Language
support

OrganizationStockExchange

Organization,

An institution that manages or

en

OrganizationStockExchange

facilitates the trading of stocks and
securities.

Type: Percentage
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

Percentage

Numeric, Quantity,
Percentage

A value expressed as a fraction of 100, representing a proportion or
share.

Type: Person
Category: Person

Entity

Tags

Detail

Person

Person

An individual human being or a legal entity with rights and responsibilities.

ï¾‰

Expand table

ï¾‰

Expand table

Type: PersonType
Category: PersonType

Entity

Tags

Detail

PersonType

PersonType

A classification describing the role or category of a person, such as employee or customer.

Type: PhoneNumber
Category: PhoneNumber
ï¾‰

Expand table

Entity

Tags

Detail

PhoneNumber

PhoneNumber

A unique sequence of digits assigned to a telephone line or mobile device that serves
as an identifier within a communication network.

Type: Product
Category: Product

Entity

Tags

Detail

Product

Product

An item or service offering value and created for sale or use.

ï¾‰

Expand table

ï¾‰

Expand table

Type: SetTemporal
Category: DateTime

Entity

Tags

Detail

MetaData

Set

DateTime, Set

A sequence of sets, where each individual set is associated with a timestamp.

Set metadata

Type: Skill
Category: Skill

Entity

Tags

Detail

Skill

Skill

The ability to perform a task or activity, acquired through training or experience.

ï¾‰

Expand table

ï¾‰

Expand table

Type: Speed
Category: Quantity

Entity

Tags

Detail

MetaData

Speed

Numeric, Quantity,

The rate at which something moves or operates, typically

Speed

Dimension, Speed

measured in units per time.

metadata

Type: SportsEvent
Category: Event
Expand table

ï¾‰

Entity

Tags

Detail

Language
support

SportsEvent

Event,

An organized competition or exhibition that involves skill or strategy

en

EventSports

typically governed by a set of rules.

Type: State
Category: Location
ï¾‰

Expand table

Entity

Tags

Detail

State

Location,GPE,State

The institutional framework and governing apparatus for a defined geographical area or
political entity.

Type: Structural
Category: Location
ï¾‰

Expand table

Entity

Tags

Detail

Structural

Location,

The configuration or organizational schema of components within a system or object

Structural

that define the overall architecture.

Type: Temporal
Category: DateTime
ï¾‰

Entity

Tags

Temporal

Related to time or time-based changes, such as data, events, or processes that vary over time.

Type: Temperature

Expand table
Detail

Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Temperature

Numeric, Quantity,

A quantitative expression that indicates the measure of heat or

Temperature

Temperature

cold present in an object or environment, commonly expressed
in units such as degrees.

metadata

Type: Time
Category: DateTime
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Time

DateTime,
Time

A quantifiable interval during which an event occurs, a process unfolds, or a
condition persists.

Time
metadata

Type: TimeRange
Category: DateTime
ï¾‰

Expand table

Entity

Tags

Detail

TimeRange

DateTime, TimeRange

An interval period defined by specific start and designated end times.

Type: URL
Category: URL
ï¾‰

Expand table

Entity

Tags

Detail

Skill

URL

A Uniform Resource Identifier is a string of characters that uniquely identifies a resource on the internet.

Type: Volume
Category: Quantity

ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Volume

Numeric, Quantity,

The measure of three-dimensional space taken up by a

Volume

Dimension, Volume

substance or object, typically expressed in cubic units.

metadata

Type: Weight
Category: Quantity
ï¾‰

Expand table

Entity

Tags

Detail

MetaData

Weight

Numeric, Quantity,
Dimension, Weight

The measure of the force exerted on an object due to gravity
typically expressed in units like kilograms or pounds.

Weight
metadata

Next steps
NER overview

Last updated on 11/18/2025

Entity Metadata
The entity metadata object stores optional supplementary details about detected entities,
specifically providing standardized resolutions for numeric and temporal data.
This attribute is only populated when extra information is available and may be empty or
missing for some entities.
Metadata resolutions convert various entity forms into consistent formatsâ€”for example, both
"eighty" and "80" resolve to the integer 80. These NER resolutions enable downstream actions,
such as extracting date and time entities for integration with a meeting scheduling system.
ï¼— Note
Support for Entity Metadata is available with API 2023-04-15-preview and later versions.
For older API versions, see Entity Resolutions.

Entities with metadata attributes
ï¾‰

Expand table

Entities

Entities

Entities

Entities

Entities

Entities

Age

Area

Currency

Date

Datetime

Information

Length

Number

NumericRange

Ordinal

Set

Speed

Temperature

Time

Volume

Weight

Age
ï¾‰

Metadata

Type

Description

unit

string

Unit of measurement for age.

value

number

Numeric value for age.

JSON
"metadata": {
"unit": "Year",

Expand table

"value": 10
}

Possible values for unit:
Day
Month
Week
Year
Unspecified

Area
ï¾‰

Metadata

Type

Description

unit

string

Unit of measurement for area.

value

number

Numeric value for area.

JSON
"metadata": {
"unit": "Acre",
"value": 30
}

Possible values for unit:
Acre
SquareCentimeter
SquareDecameter
SquareDecimeter
SquareFoot
SquareHectometer
SquareInch
SquareKilometer
SquareMeter
SquareMile
SquareMillimeter
SquareYard
Unspecified

Expand table

Currency
ï¾‰

Expand table

Metadata

Type

Description

unit

string

Name of currency.

value

number

Numeric value for currency.

ISO4217

string

The ISO 4217 three-letter currency code uses the first two letters from the
country's ISO 3166 code, and, when possible, the third letter is the first letter of
the currency name.

JSON
"metadata": {
"unit": "Egyptian pound",
"value": 30,
"ISO4217": "EGP"
}

Possible values for ISO4217:
ISO 4217 reference

.

Date
ï¾‰

Metadata

Type

Description

timex

string

The ISO 8601 formatted date: YYYY-MM-DD (year, month, day).

value

string

The actual denoted date.

Expand table

Whenever an ambiguous date is provided, you're offered different options for your resolution.
For example, "12 April" could refer to any year. Resolution provides this year and the next as
options. The timex value XXXX indicates no year was specified in the query.
JSON
"metadata": {
"dateValues": [
{
"timex": "XXXX-04-12",
"value": "2022-04-12"

},
{
"timex": "XXXX-04-12",
"value": "2023-04-12"
}
]
}

Ambiguity can occur even for a given day of the week. For example, saying "Monday" could
refer to last Monday or this Monday. Once again the timex value indicates no year or month
was specified, and uses a day of the week identifier (W) to indicate the first day of the week.
JSON
"metadata" :{
"dateValues": [
{
"timex": "XXXX-WXX-1",
"value": "2022-10-03"
},
{
"timex": "XXXX-WXX-1",
"value": "2022-10-10"
}
]
}

Datetime
ï¾‰

Expand table

Metadata

Type

Description

timex

string

The ISO 8601 formatted date and time:
YYYY-MM-DDTHH:MM:SS (year, month, day, hour, minutes, seconds, milliseconds) with a
T separator.

value

string

The actual denoted date and time.

Similar to dates, you can have ambiguous datetime entities. Resolution provides this year and
the next as options. The timex value XXXX indicates no year was specified.
JSON
"metadata": {
"dateValues": [
{
"timex": "XXXX-05-03T12",

"value": "2022-05-03 12:00:00"
},
{
"timex": "XXXX-05-03T12",
"value": "2023-05-03 12:00:00"
}
]
}

Information
ï¾‰

Metadata

Type

Description

unit

string

Unit of measurement for information (data).

value

number

Numeric value for information.

JSON
"metadata": {
"unit": "Kilobit",
"value": 30
}

Possible values for unit:
Bit
Byte
Gigabit
Gigabyte
Kilobit
Kilobyte
Megabit
Megabyte
Petabit
Petabyte
Terabit
Terabyte
Unspecified

Length

Expand table

Metadata

Type

Description

unit

string

Unit of measurement for length

value

number

Numeric value.

ï¾‰

Expand table

ï¾‰

Expand table

JSON
"metadata": {
"unit": "Kilobit",
"value": 30
}

Possible values for unit:
Centimeter
Decameter
Decimeter
Foot
Hectometer
Inch
Kilometer
LightYear
Meter
Micrometer
Mile
Millimeter
Nanometer
Picometer
Point
Yard
Unspecified

Number

Metadata

Type

Description

numberKind

string

Number type.

Metadata

Type

Description

value

number

Numeric value for number.

JSON
"metadata": {
"numberKind": "Integer",
"value": 30
}

Possible values for numberKind:
Decimal
Fraction
Integer
Percent
Power
Unspecified

NumericRange
ï¾‰

Metadata

Type

Description

rangeKind

string

A supported numeric range.

minimum

number

The beginning value of the interval.

maximum

number

The ending value of the interval.

JSON
"metadata": {
"rangeKind": "length",
"minimum": 30,
"maximum": 100
}

Possible values for rangeKind:
Age

Expand table

Area
Currency
Information
Length
Number
Speed
Temperature
Volume
Weight

Ordinal
ï¾‰

Expand table

Metadata

Type

Description

offset

string

The offset with respect to the reference (for example,
offset = -1 indicates the second to last)

relativeTo

The reference point that the
ordinal number denotes.

value

number

Numeric value for ordinal position.

JSON
"metadata": {
"offset": -1,
"relativeTo":"Current",
"value": "first"
}

Possible values for relativeTo:
Current
End
Start

Set
A recurring datetime period (example: "every Monday at 6:00 PM.")

ï¾‰

Expand table

Metadata

Type

Description

timex

string

The ISO 8601 formatted date and time:
YYYY-MM-DDTHH:MM:SS (year, month, day, hour, minutes, seconds, milliseconds) with a
T separator.

value

string

Sets don't resolve to exact values, as they don't indicate an exact datetime.

JSON
"metadata": {
"timex": "XXXX-WXX-1T18",
"value": "not resolved"
}

Possible values for type:
begin
end
duration
modifier (example: before , after )
timex

Speed
ï¾‰

Metadata

Type

Description

unit

string

Unit of measurement for speed.

value

number

Numeric value for speed.

JSON
"metadata": {
"unit": "Knots",
"value": 50
}

Possible values for unit:

Expand table

CentimetersPerMillisecond
FeetPerMinute
FeetPerSecond
KilometersPerHour
KilometersPerMillisecond
KilometersPerMinute
KilometersPerSecond
Knots
MetersPerMillisecond
MetersPerSecond
MilesPerHour
YardsPerMinute
YardsPerSecond
Unspecified

Temperature
ï¾‰

Metadata

Type

Description

unit

string

Unit of measurement for temperature.

value

number

Numeric value.

JSON
"metadata": {
"unit" "Kelvin",
"value": 310
}

Possible values for unit:
Celsius
Fahrenheit
Kelvin
Rankine
Unspecified

Time

Expand table

Metadata

Type

Description

timex

string

The ISO 8601 formatted date time:
[hh]:[mm]:[ss] (hour, minutes, seconds).

value

number

Numeric value.

ï¾‰

Expand table

ï¾‰

Expand table

JSON
"metadata": {
"timex":"T14:30:15",
"value": "14:30:15"
}

Volume

Metadata

Type

Description

unit

string

Unit of measurement for volume.

value

number

Numeric value for volume.

JSON
"metadata": {
"unit": "Quart",
"value": 4
}

Possible values for unit:
Barrel
Bushel
Centiliter
Cord
CubicCentimeter
CubicFoot
CubicInch

CubicMeter
CubicMile
CubicMillimeter
CubicYard
Cup
Decaliter
FluidDram
FluidOunce
Gill
Hectoliter
Hogshead
Liter
Milliliter
Minim
Peck
Pinch
Pint
Quart
Tablespoon
Teaspoon
Unspecified

Weight
ï¾‰

Metadata

Type

Description

unit

string

Unit of measurement for weight.

value

number

Numeric value for weight.

JSON
"metadata": {
"unit": "Ounce",
"value": 16
}

Possible values for unit:
Dram

Expand table

Gallon
Grain
Gram
Kilogram
LongTonBritish
MetricTon
Milligram
Ounce
PennyWeight
Pound
ShortHundredWeightUS
ShortTonUS
Stone
Ton
Unspecified

Next steps
Learn how to use NER

Last updated on 11/18/2025

Entity types and tags
Use this article to get an overview of the new API changes starting from version 2024-11-01 .
This API change mainly introduces two new concepts ( entity types and entity tags )
replacing the category and subcategory fields in the current Generally Available API. A detailed
overview of each API parameter and the corresponding supported API versions are found on
the [Skill Parameters][../how-to/skill-parameters.md] page.
Since an entity like "Seattle" could be classified as a City, GPE (Geo Political Entity), and a
Location, the type attribute is used to define the most granular classification, in this case City.
The tags attribute in the service output is a list all possible classifications (City, GPE, and
Location) and their respective confidence score. A full mapping of possible tags for each are
listed. The metadata attributes in the service output contain additional information about the
entity, such as the integer value associated with the entity.

Entity types
Entity types represent the lowest (or finest) granularity at which the entity is detected. Types
are considered to be the base class detected.

Entity tags
Entity tags are used to further identify an entity where a detected entity is tagged to indicate
the entity type. The entity tags list can include categories, subcategories, and subsubcategories.

Changes from versions 2022-05-01 and 2023-04-01
to version 2024-11-01 API
The changes introduce better flexibility for the named entity recognition service, including:
Updates to the structure of input formats:
InclusionList
ExclusionList
Overlap policy
Updates to the handling of output formats:

More granular entity recognition outputs through introducing the tags list where an
entity is tagged with more than one entity tag.
Overlapping entities where entities could be recognized as more than one entity type and
if so, this entity would be returned twice. If an entity was recognized to belong to two
entity tags under the same entity type, both entity tags are returned in the tags list.
Filtering entities using entity tags: to learn more, see Select returned entities.
Metadata Objects that contain additional information about the entity but currently only
act as a wrapper for the existing entity resolution feature. You can learn more about this
new feature here.

Versions 2022-05-01 and 2023-04-01 to current
version API entity mappings
You can see a comparison between the structure of the entity categories/types in the
Supported Named Entity Recognition (NER) entity categories and entity types article. The
following table presents the mappings between the results you would expect to see from
versions 2022-05-01 and 2023-04-01 and the current version API.
ï¾‰

Type

Tags

Date

Temporal, Date

DateRange

Temporal, DateRange

DateTime

Temporal, DateTime

DateTimeRange

Temporal, DateTimeRange

Duration

Temporal, Duration

SetTemporal

Temporal, SetTemporal

Time

Temporal, Time

TimeRange

Temporal, TimeRange

City

GPE, Location, City

State

GPE, Location, State

CountryRegion

GPE, Location, CountryRegion

Continent

GPE, Location, Continent

Expand table

Type

Tags

GPE

Location, GPE

Location

Location

Airport

Structural, Location

Structural

Location, Structural

Geological

Location, Geological

Age

Numeric, Age

Currency

Numeric, Currency

Number

Numeric, Number

PhoneNumber

PhoneNumber

NumberRange

Numeric, NumberRange

Percentage

Numeric, Percentage

Ordinal

Numeric, Ordinal

Temperature

Numeric, Dimension, Temperature

Speed

Numeric, Dimension, Speed

Weight

Numeric, Dimension, Weight

Height

Numeric, Dimension, Height

Length

Numeric, Dimension, Length

Volume

Numeric, Dimension, Volume

Area

Numeric, Dimension, Area

Information

Numeric, Dimension, Information

Address

Address

Person

Person

PersonType

PersonType

Organization

Organization

Product

Product

ComputingProduct

Product, ComputingProduct

Type

Tags

IP

IP

Email

Email

URL

URL

Skill

Skill

Event

Event

CulturalEvent

Event, CulturalEvent

SportsEvent

Event, SportsEvent

NaturalEvent

Event, NaturalEvent

Last updated on 11/20/2025

Extract information in Excel using Named
Entity Recognition(NER) and Power
Automate
In this tutorial, you create a Power Automate flow to extract text in an Excel spreadsheet
without having to write code.
This flow uses a spreadsheet consisting of issues reported about an apartment complex, and
classifies them into two categories: plumbing and other. It also extracts the names and phone
numbers of the tenants who sent them. Lastly, the flow appends this information to the Excel
sheet.
In this tutorial, you learn how to:
ï¼‚ Use Power Automate to create a flow
ï¼‚ Upload Excel data from OneDrive
ï¼‚ Extract text from Excel, and send it for Named Entity Recognition(NER)
ï¼‚ Use the information from the API to update an Excel sheet.

Prerequisites
A Microsoft Azure account. Create a free account

or sign in

.

A Language resource. If you don't have one, you can create one in the Azure portal

and

use the free tier to complete this tutorial.
The key and endpoint that was generated for you when you created the resource.
A spreadsheet containing tenant issues. Example data for this tutorial is available on
GitHub .
Microsoft 365, with OneDrive .

Add the Excel file to OneDrive
Download the example Excel file from GitHub . This file must be stored in your OneDrive
account.

ï Š

The issues are reported in raw text. We use the Named Entity Recognition (NER) feature to
extract the person name and phone number. Then the flow looks for the word "plumbing" in

the description to categorize the issues.

Create a new Power Automate workflow
Go to the Power Automate site

, and sign in. Then select Create and Scheduled flow.

ï Š

On the Build a scheduled cloud flow page, initialize your flow with the following fields:
ï¾‰

Field

Value

Flow name

Scheduled Review or another name.

Starting

Enter the current date and time.

Repeat every

1 hour

Add variables to the flow

Expand table

Create variables representing the information added to the Excel file. Select New Step and
search for Initialize variable. Do this four times and create four variables.

ï Š

Add the following information to the variables you created. They represent the columns of the
Excel file. If any variables are collapsed, you can select them to expand them.
ï¾‰

Expand table

Action

Name

Type

Value

Initialize variable

var_person

String

Person

Initialize variable 2

var_phone

String

Phone Number

Initialize variable 3

var_plumbing

String

plumbing

Initialize variable 4

var_other

String

other

ï Š

Read the excel file
Select New Step and type Excel, then select List rows present in a table from the list of actions.

ï Š

Add the Excel file to the flow by filling in the fields in this action. This tutorial requires that you
upload the file to OneDrive.

ï Š

Select New Step and add an Apply to each action.

ï Š

Select Select an output from previous step. In the Dynamic content box that appears, select
value.

ï Š

Send a request for entity recognition

If you haven't already, you need to create a Language resource

in the Azure portal.

Create a Language connection
In the Apply to each, select Add an action. Go to your Language resource's key and endpoint
page in the Azure portal, and get the key and endpoint for your Language resource.
In your flow, enter the following information to create a new Language connection.
ï¼— Note
If you already created a Language connection and want to change your connection
details, Select the ellipsis on the top right corner, and select + Add new connection.

ï¾‰

Field

Value

Connection

A name for the connection to your Language resource. For example,

Name

TAforPowerAutomate .

Account key

The key for your Language resource.

Site URL

The endpoint for your Language resource.

Expand table

ï Š

Extract the Excel content
After the connection is created, search for Text Analytics and select Named Entity Recognition.
This extracts information from the description column of the issue.

ï Š

Select in the Text field and select Description from the Dynamic content windows that appears.
Enter en for Language, and a unique name as the document ID (you might need to select
Show advanced options).

ï Š

Within the Apply to each, select Add an action and create another Apply to each action. Select
inside the text box and select documents in the Dynamic Content window that appears.

ï Š

Extract the person name
Next, we find the person entity type in the NER output. Within the Apply to each 2, select Add
an action, and create another Apply to each action. Select inside the text box and select
Entities in the Dynamic Content window that appears.

ï Š

Within the newly created Apply to each 3 action, select Add an action, and add a Condition
control.

ï Š

In the Condition window, select the first text box. In the Dynamic content window, search for
Category and select it.

ï Š

Make sure the second box is set to is equal to. Then select the third box, and search for
var_person in the Dynamic content window.

ï Š

In the If yes condition, type in Excel then select Update a Row.

ï Š

Enter the Excel information, and update the Key Column, Key Value, and PersonName fields.
This step appends the name detected by the API to the Excel sheet.

ï Š

Get the phone number
Minimize the Apply to each 3 action by selecting the name. Then add another Apply to each
action to Apply to each 2, like before, action is named Apply to each 4. Select the text box,
and add entities as the output for this action.

ï Š

Within Apply to each 4, add a Condition control. This control is named Condition 2. In the first
text box, search for, and add categories from the Dynamic content window. Be sure the center
box is set to is equal to. Then, in the right text box, enter var_phone .

ï Š

In the If yes condition, add an Update a row action. Then enter the information like we did
before, for the phone numbers column of the Excel sheet. This step appends the phone
number detected by the API to the Excel sheet.

ï Š

Get the plumbing issues
Minimize Apply to each 4 by selecting the name. Then create another Apply to each in the
parent action. Select the text box, and add Entities as the output for this action from the
Dynamic content window.

ï Š

Next, the flow checks if the issue description from the Excel table row contains the word
"plumbing." If yes, it adds "plumbing" in the IssueType column. If not, we enter "other."
Inside the Apply to each 4 action, add a Condition Control. Its named Condition 3. In the first
text box, search for, and add Description from the Excel file, using the Dynamic content
window. Be sure the center box says contains. Then, in the right text box, find and select
var_plumbing .

ï Š

In the If yes condition, select Add an action, and select Update a row. Then enter the
information like before. In the IssueType column, select var_plumbing . This step applies a
"plumbing" label to the row.
In the If no condition, select Add an action, and select Update a row. Then enter the
information like before. In the IssueType column, select var_other . This step applies an "other"
label to the row.

ï Š

Test the workflow
In the top-right corner of the screen, select Save, then Test. Under Test Flow, select manually.
Then select Test, and Run flow.
The Excel file gets updated in your OneDrive account. It looks like the following example:

ï Š

Next steps
Last updated on 11/18/2025

What is custom named entity recognition?
Custom named entity recognition (NER) is a cloud-based API service that uses machine
learning to help you build models designed for your unique entity recognition requirements.
It's one of the specialized features available through Azure Language in Foundry Tools. With
custom NER, you can create AI models that extract domain-specific entities from unstructured
text, such as contracts or financial documents. When you start a Custom NER project, you can
repeatedly label data, train and evaluate your model, and improve its performance before
deploying it. The quality of your labeled data is essential, as it directly impacts the model's
accuracy.
To simplify building and customizing your model, the service offers a custom web platform that
can be accessed through the Microsoft Foundry

. You can easily get started with the service

by following the steps in this quickstart.
This documentation contains the following article types:
Quickstarts are getting-started instructions to guide you through making requests to the
service.
Concepts provide explanations of the service functionality and features.
How-to guides contain instructions for using the service in more specific or customized
ways.

Example usage scenarios
Custom named entity recognition can be used in multiple scenarios across various industries:

Information extraction
Many financial and legal organizations extract and normalize data from thousands of complex,
unstructured text sources on a daily basis. Such sources include bank statements, legal
agreements, or bank forms. For example, mortgage application data extraction done manually
by human reviewers may take several days to extract. Automating these steps by building a
custom NER model simplifies the process and saves cost, time, and effort.

Knowledge mining to enhance/enrich semantic search
Search is foundational to any app that surfaces text content to users. Common scenarios
include catalog or document search, retail product search, or knowledge mining for data
science. Many enterprises across various industries want to build a rich search experience over
private, heterogeneous content, which includes both structured and unstructured documents.

As a part of their pipeline, developers can use custom NER for extracting entities from the text
that are relevant to their industry. These entities can be used to enrich the indexing of the file
for a more customized search experience.

Audit and compliance
Instead of manually reviewing long text files to audit and apply policies, IT departments
in financial or legal enterprises can use custom NER to build automated solutions. These
solutions can be helpful to enforce compliance policies, and set up necessary business
rules based on knowledge mining pipelines that process structured and unstructured content.

Project development lifecycle
Using custom NER typically involves several different steps.

ï Š

1. Define your schema: Know your data and identify the entities you want extracted. Avoid
ambiguity.
2. Label your data: Labeling data is a key factor in determining model performance. Label
precisely, consistently and completely.
Label precisely: Label each entity to its right type always. Only include what you
want extracted and avoid unnecessary data in your labels.
Label consistently: The same entity should have the same label across all the files.
Label completely: Label all the instances of the entity in all your files.
3. Train the model: Your model starts learning from your labeled data.
4. View the model's performance: After training, review evaluation results and analyze
performance for improvement.
5. Deploy the model: Deploying a model makes it available for use via the Analyze API

.

6. Extract entities: Use your custom models for entity extraction tasks.

Reference documentation and code samples
As you use custom NER, see the following reference documentation and samples for Azure
Language in Foundry Tools:
ï¾‰

Expand table

Development option / language

Reference documentation

Samples

REST APIs (Authoring)

REST API documentation

REST APIs (Runtime)

REST API documentation

C# (Runtime)

C# documentation

C# samples

Java (Runtime)

Java documentation

Java Samples

JavaScript (Runtime)

JavaScript documentation

JavaScript samples

Python (Runtime)

Python documentation

Python samples

Responsible AI
An AI system includes not only the technology, but also the people who use it, the people
affected by it, and the deployment environment. Read the transparency note to learn about
responsible AI use and deployment in your systems. For more information, see the following
articles:
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Next steps
Use the quickstart article to start using custom named entity recognition.
As you go through the project development lifecycle, review the glossary to learn more
about the terms used throughout the documentation for this feature.
Remember to view the service limits for information such as regional availability.

Last updated on 11/18/2025

Quickstart: Custom named entity
recognition
This guide provides step-by-step instructions for using custom named entity recognition (NER)
with Microsoft Foundry or the REST API. NER lets you detect and categorize entities in
unstructured textâ€”like people, places, organizations, and numbers. Custom NER enables the
training of models to identify entities that are specific to a business and allows for ongoing
adaptation as requirements change.
To get start, a sample loan agreement

is provided as a dataset to build a custom NER model

and extract these key entities:
Date of the agreement
Borrower's name, address, city, and state
Lender's name, address, city, and state
Loan and interest amounts
ï¼— Note
If you already have an Azure Language in Foundry Tools or multi-service resourceâ€”
whether used on its own or through Language Studioâ€”you can continue to use
those existing Language resources within the Microsoft Foundry portal. For more
information, see How to use Foundry Tools in the Foundry portal.

Prerequisites
An Azure subscription. If you don't have one, you can create one for free

.

The Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
An Language resource with a storage account

. On the select additional features page,

select the Custom text classification, Custom named entity recognition, Custom
sentiment analysis & Custom Text Analytics for health box to link a required storage
account to this resource:

ï¼— Note
You need to have an owner role assigned on the resource group to create a
Language resource.
If you're connecting a preexisting storage account, you should have an owner role
assigned to it.
Don't move the storage account to a different resource group or subscription once
linked with Azure Language resource.
A Foundry project created in the Foundry. For more information, see Create a Foundry
project.
A custom NER dataset uploaded to your storage container. A custom named entity
recognition (NER) dataset is the collection of labeled text documents used to train your
custom NER model. You can download our sample dataset

for this quickstart. The

source language is English.

Step 1: Configure required roles, permissions, and
settings
Let's begin by configuring your resources.

Enable custom named entity recognition feature
Make sure the Custom text classification / Custom Named Entity Recognition feature is
enabled in the Azure portal

.

1. Navigate to your Language resource in the Azure portal

.

2. From the left side menu, under Resource Management section, select Features.
3. Make sure the Custom text classification / Custom Named Entity Recognition feature is
enabled.
4. If your storage account isn't assigned, select and connect your storage account.
5. Select Apply.

Add required roles for your Language resource
1. From the Language resource page in the Azure portal , select Access Control (IAM) in
the left pane.
2. Select Add to Add Role Assignments, and add Cognitive Services Language Owner or
Cognitive Services Contributor role assignment for your Language resource.
3. Within Assign access to, select User, group, or service principal.
4. Select Select members.
5. Select your user name. You can search for user names in the Select field. Repeat this step
for all roles.
6. Repeat these steps for all the user accounts that need access to this resource.

Add required roles for your storage account
1. Go to your storage account page in the Azure portal

.

2. Select Access Control (IAM) in the left pane.
3. Select Add to Add Role Assignments, and choose the Storage blob data contributor role
on the storage account.
4. Within Assign access to, select Managed identity.
5. Select Select members.
6. Select your subscription, and Language as the managed identity. You can search for your
language resource in the Select field.

Add required user roles
ï¼‰ Important

If you skip this step, you get a 403 error when you try to connect to your custom project.
It's important that your current user has this role to access storage account blob data,
even if you're the owner of the storage account.
1. Go to your storage account page in the Azure portal

.

2. Select Access Control (IAM) in the left pane.
3. Select Add to Add Role Assignments, and choose the Storage blob data contributor role
on the storage account.
4. Within Assign access to, select User, group, or service principal.
5. Select Select members.
6. Select your User. You can search for user names in the Select field.
ï¼‰ Important
If you have a Firewall or virtual network or private endpoint, be sure to select Allow Azure
services on the trusted services list to access this storage account under the Networking
tab in the Azure portal.

Step 2: Upload your dataset to your storage
container
Next, let's add a container and upload your dataset files directly to the root directory of your
storage container. These documents are used to train your model.
1. Add a container to the storage account associated with your language resource. For more
information, see create a container.
2. Download the sample dataset
loan agreements:

from GitHub. The provided sample dataset contains 20

Each agreement includes two parties: a lender and a borrower.
You extract relevant information for: both parties, agreement date, loan amount, and
interest rate.
3. Open the .zip file, and extract the folder containing the documents.
4. Navigate to the Foundry.
5. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
6. Once signed in, access your existing Foundry project for this quickstart.
7. Select Management center from the left navigation menu.
8. Select Connected resources from the Hub section of the Management center menu.
9. Next choose the workspace blob storage that was set up for you as a connected resource.
10. On the workspace blob storage, select View in Azure portal.
11. On the AzurePortal page for your blob storage, select Upload from the top menu. Next,
choose the .txt and .json files you downloaded earlier. Finally, select the Upload
button to add the file to your container.

Now that the required Azure resources are provisioned and configured within the Azure portal,
let's use these resources in the Foundry to create a fine-tuned custom Named Entity
Recognition (NER) model.

Step 3: Connect your Language resource
Next we create a connection to your Language resource so Foundry can access it securely. This
connection provides secure identity management and authentication, as well as controlled and
isolated access to data.
1. Return to the Foundry

.

2. Access your existing Foundry project for this quickstart.
3. Select Management center from the left navigation menu.
4. Select Connected resources from the Hub section of the Management center menu.
5. In the main window, select the + New connection button.
6. Select Language from the Add a connection to external assets window.
7. Select Add connection, then select Close.

Step 4: Fine tune your custom NER model
Now, we're ready to create a custom NER fine-tune model.
1. From the Project section of the Management center menu, select Go to project.
2. From the Overview menu, select Fine-tuning.
3. From the main window, select the AI Service fine-tuning tab and then the + Fine-tune
button.
4. From the Create service fine-tuning window, choose the Custom named entity
recognition tab, and then select Next.

ï Š

5. In the Create service fine-tuning task window, complete the fields as follows:
Connected service. The name of your Language resource should already appear in
this field by default. if not, add it from the drop-down menu.
Name. Give your fine-tuning task project a name.
Language. English is set as the default and already appears in the field.
Description. You can optionally provide a description or leave this field empty.
Blob store container. Select the workspace blob storage container from Step 2 and
choose the Connect button.
6. Finally, select the Create button. It can take a few minutes for the creating operation to
complete.

Step 5: Train your model

1. From the Getting Started menu, choose Manage data. In the Add data for training and
testing window, you see the sample data that you previously uploaded to your Azure
Blob Storage container.
2. Next, from the Getting Started menu, select Train model.

3. Select the + Train model button. When the Train a new model window appears, enter a
name for your new model and keep the default values. Select the Next button.
4. In the Train a new model window, keep the default Automatically split the testing set
from training data enabled with the recommended percentage set at 80% for training
data and 20% for testing data.
5. Review your model configuration then select the Create button.
6. After training a model, you can select Evaluate model from the Getting started menu.
You can select your model from the Evaluate you model window and make
improvements if necessary.

Step 6: Deploy your model
Typically, after training a model, you review its evaluation details. For this quickstart, you can
just deploy your model and make it available to test in Azure Language playground, or by
calling the prediction API

. However, if you wish, you can take a moment to select Evaluate

your model from the left-side menu and explore the in-depth telemetry for your model.
Complete the following steps to deploy your model within Foundry.
1. Select Deploy model from the left-side menu.
2. Next, select âž•Deploy a trained model from the Deploy your model window.

3. Make sure the Create a new deployment button is selected.

4. Complete the Deploy a trained model window fields:
Deployment name. Name your model.
Assign a model. Select your trained model from the drop-down menu.
Region. Select a region from the drop-down menu.
5. Finally, select the Create button. It may take a few minutes for your model to deploy.
6. After successful deployment, you can view your model's deployment status on the
Deploy your model page. The expiration date that appears marks the date when your
deployed model becomes unavailable for prediction tasks. This date is usually 18 months
after a training configuration is deployed.

Step 7: Try Azure Language playground
The Language playground provides a sandbox to test and configure your fine-tuned model
before deploying it to production, all without writing code.
1. From the top menu bar, select Try in playground.
2. In Azure Language Playground window, select the Custom named entity recognition tile.
3. In the Configuration section, select your Project name and Deployment name from the
drop-down menus.
4. Enter an entity and select Run.
5. You can evaluate the results in the Details window.
That's it, congratulations!
In this quickstart, you created a fine-tuned custom NER model, deployed it in Foundry, and
tested your model in Azure Language playground.

Clean up resources
If you no longer need your project, you can delete it from the Foundry.

1. Navigate to the Foundry

home page. Initiate the authentication process by signing in,

unless you already completed this step and your session is active.
2. Select the project that you want to delete from the Keep building with Foundry.
3. Select Management center.
4. Select Delete project.
To delete the hub along with all its projects:
1. Navigate to the Overview tab in the Hub section.
2. On the right, select Delete hub.
3. The link opens the Azure portal for you to delete the hub there.

Related content
After you create your entity extraction model, you can use the runtime API to extract entities.
As you create your own custom NER projects, use our how-to articles to learn more about
tagging, training, and consuming your model in greater detail:
Data selection and schema design
Tag data
Train a model
Model evaluation

Last updated on 01/11/2026

Language support for custom named entity
recognition
Use this article to learn about the languages currently supported by custom named entity
recognition feature.

Multi-lingual option
With custom named entity recognition (NER), you can train a model in one language and use
to extract entities from documents in another language. This feature is powerful because it
helps save time and effort. Instead of building separate projects for every language, you can
handle multi-lingual dataset in one project. Your dataset doesn't have to be entirely in the
same language but you should enable the multi-lingual option for your project while creating
or later in project settings. If you notice your model performing poorly in certain languages
during the evaluation process, consider adding more data in these languages to your training
set.
You can train your project entirely with English documents, and query it in: French, German,
Mandarin, Japanese, Korean, and others. Custom named entity recognition makes it easy for
you to scale your projects to multiple languages by using multilingual technology to train your
models.
Whenever you identify that a particular language isn't performing as well as other languages,
you can add more documents for that language in your project. For data labeling in Microsoft
Foundry

, you can select the language of the document you're adding. When you introduce

more documents for that language to the model, the model is introduced to more of the
syntax of that language, and learns to predict it better.
You aren't expected to add the same number of documents for every language. You should
build most your project in one language, and only add a few documents in languages you
observe aren't performing well. If you develop a project mainly in English, and then begin
testing it in French, German, and Spanish, you may notice some differences. Specifically,
German may underperform compared to the other two languages. While French and Spanish
might yield better results, German could present more challenges or produce less favorable
outcomes during testing. In that case, consider adding 5% of your original English documents
in German, train a new model and test in German again. You should see better results for
German queries. The more labeled documents you add, the more likely the results are going to
get better.
When you add data in another language, you shouldn't expect it to negatively affect other
languages.

Language support
Custom NER supports .txt files in the following languages:
ï¾‰

Language

Language code

Afrikaans

af

Amharic

am

Arabic

ar

Assamese

as

Azerbaijani

az

Belarusian

be

Bulgarian

bg

Bengali

bn

Breton

br

Bosnian

bs

Catalan

ca

Czech

cs

Welsh

cy

Danish

da

German

de

Greek

el

English (US)

en-us

Esperanto

eo

Spanish

es

Estonian

et

Basque

eu

Persian

fa

Expand table

Language

Language code

Finnish

fi

French

fr

Western Frisian

fy

Irish

ga

Scottish Gaelic

gd

Galician

gl

Gujarati

gu

Hausa

ha

Hebrew

he

Hindi

hi

Croatian

hr

Hungarian

hu

Armenian

hy

Indonesian

id

Italian

it

Japanese

ja

Javanese

jv

Georgian

ka

Kazakh

kk

Khmer

km

Kannada

kn

Korean

ko

Kurdish (Kurmanji)

ku

Kyrgyz

ky

Latin

la

Lao

lo

Language

Language code

Lithuanian

lt

Latvian

lv

Malagasy

mg

Macedonian

mk

Malayalam

ml

Mongolian

mn

Marathi

mr

Malay

ms

Burmese

my

Nepali

ne

Dutch

nl

Norwegian (Bokmal)

nb

Odia

or

Punjabi

pa

Polish

pl

Pashto

ps

Portuguese (Brazil)

pt-br

Portuguese (Portugal)

pt-pt

Romanian

ro

Russian

ru

Sanskrit

sa

Sindhi

sd

Sinhala

si

Slovak

sk

Slovenian

sl

Somali

so

Language

Language code

Albanian

sq

Serbian

sr

Sundanese

su

Swedish

sv

Swahili

sw

Tamil

ta

Telugu

te

Thai

th

Filipino

tl

Turkish

tr

Uyghur

ug

Ukrainian

uk

Urdu

ur

Uzbek

uz

Vietnamese

vi

Xhosa

xh

Yiddish

yi

Chinese (Simplified)

zh-hans

Zulu

zu

Next steps
Custom NER overview
Service limits

Last updated on 11/18/2025

Frequently asked questions for Custom
Named Entity Recognition
Find answers to commonly asked questions about concepts, and scenarios related to custom
NER in Azure Language in Foundry Tools.

How do I get started with the service?
For more information, see our quickstart or how to create projects.

What are the service limits?
For more information, see service limits.

How many tagged files are needed?
Generally, diverse and representative tagged data leads to better results, given that the
tagging is done precisely, consistently and completely. There's no set number of tagged
instances for a model to perform well. Performance highly dependent on your schema, and the
ambiguity of your schema. Ambiguous entity types need more tags. Performance also depends
on the quality of your tagging. The recommended number of tagged instances per entity is 50.

How long should it take to train a model?
The training process can take a long time. As a rough estimate, the expected training time for
files with a combined length of 12,800,000 chars is 6 hours.

How do I build my custom model
programmatically?
ï¼— Note
Currently you can only build a model using the REST API or Language Studio.
You can use the REST APIs

to build your custom models. Follow this quickstart to get started

with creating a project and creating a model through APIs for examples of how to call the
Authoring API.

When you're ready to start using your model to make predictions, you can use the REST API, or
the client library.

What is the recommended CI/CD process?
Here's a list of actions you take within Microsoft Foundry

:

Train multiple models on the same dataset within a single project.
View your model's performance.
Deploy and test your model and add or remove labels from your data.
Choose how your dataset is split into training and testing sets.

Your data can be split randomly into training and testing sets, but this means model evaluation
may not be based on the same test set, making results noncomparable. We recommended that
you develop your own test set and use it to evaluate both models to accurately measure
improvements.

Make sure to review service limits to understand the maximum number of trained models
allowed per project.

Does a low or high model score guarantee bad or
good performance in production?
Model evaluation may not always be comprehensive. The scope depends on the following
factors:
The size of the test set. If the test set is too small, the good/bad scores aren't as
representative of model's actual performance. Also if a specific entity type is missing or
under-represented in your test set it affects model performance.
The diversity of your data. If your data only includes a limited number of scenarios or
examples of the text you anticipate in production, your model may not encounter every
possible situation. As a result, the model could perform poorly when faced with unfamiliar
scenarios.
The representation within your data. If the dataset used to train the model isn't
representative of the data that would be introduced to the model in production, model
performance is affected greatly.
For more information, see data selection and schema design.

How do I improve model performance?
View the model confusion matrix. If you notice that a certain entity type is frequently not
predicted correctly, consider adding more tagged instances for this class.
When two different entity types are often being predicted as one another, it indicates that the
schema lacks clarity. To improve performance, you should think about combining these two
entity types into a single, unified type. If two entity types are consistently mistaken for each
other during prediction, this result suggests ambiguity in your schema. Merging them into one
entity type can help enhance overall model accuracy.
Review test set predictions. If one of the entity types has a lot more tagged instances than
the others, your model may be biased towards this type. Add more data to the other
entity types or remove examples from the dominating type.
Learn more about data selection and schema design.
Review your test set. Review the predicted entities alongside the tagged entities and gain
a clearer understanding of your model's accuracy. This comparison can help you
determine whether adjustments to the schema or tag set are needed.

Why do I get different results when I retrain my
model?
When you train your model, you can determine if you want your data to be split randomly
into train and test sets. If you choose to proceed, there's no assurance that the model
evaluation is performed on the same test set, which means the results may not be directly
comparable. By doing so, you risk evaluating the model on a different test set, making it
impossible to reliably compare the outcomes.
If you're retraining the same model, your test set is the same, but you might notice a
slight change in predictions made by the model. The issue arises because the trained
model lacks sufficient robustness. This outcome is dependent on how well your data
represents different scenarios, how distinct the data points are, and the overall quality of
your data tagging. Several factors influence the model's performance. The model's
robustness, the distinctiveness and diversity of the dataset, and the precision and
uniformity of the tags assigned to the data all play important roles. To achieve optimal
results, you must ensure your dataset not only accurately represents the target domain
but also offers unique examples, and that all tags are applied with both consistency and
accuracy throughout the data.

How do I get predictions in different languages?
First, you need to enable the multilingual option when creating your project or you can enable
it later from the project settings page. After you train and deploy your model, you can start
querying it in multiple languages. You may get varied results for different languages. To
improve the accuracy of any language, add more tagged instances to your project in that
language to introduce the trained model to more syntax of that language.

I trained my model, but I can't test it
You need to deploy your model before you can test it.

How do I use my trained model for predictions?
After deploying your model, you call the prediction API, using either the REST API or client
libraries.

Data privacy and security
Your data is only stored in your Azure Storage account. Custom NER only has access to read
from it during training. Custom NER users have full control to view, export, or delete any user
content either through the Foundry

or programmatically by using REST APIs

. For more

information, see Data, privacy, and security for Language

How to clone my project?
To clone your project, you need to use the export API to export the project assets, and then
import them into a new project. See the REST API

Next steps
Custom NER overview
Quickstart

Last updated on 11/18/2025

reference for both operations.

Custom named entity recognition
definitions and terms
Use this article to learn about some of the definitions and terms you may encounter when
using custom NER.

Entity
An entity is a span of text that indicates a certain type of information. The text span can consist
of one or more words. In the scope of custom NER, entities represent the information that the
user wants to extract from the text. Developers tag entities within their data with the needed
entities before passing it to the model for training. For example "Invoice number," "Start date,"
"Shipment number," "Birthplace," "Origin city," "Supplier name" or "Client address."
For example, in the sentence "John borrowed 25,000 USD from Fred." the entities might be:
ï¾‰

Entity name/type

Entity

Borrower Name

John

Lender Name

Fred

Loan Amount

25,000 USD

Expand table

F1 score
The F1 score is needed when you seek a balance between precision and recall.

Model
A model is an object that is trained to do a certain task, in this case custom entity recognition.
Models are trained by providing labeled data to learn from so they can later be used for
recognition tasks.
Model training is the process of teaching your model what to extract based on your
labeled data.
Model evaluation is the process that happens right after training to know how well does
your model perform.

Deployment is the process of assigning your model to a deployment to make it available
for use via the prediction API

.

Precision
Measures how precise/accurate your model is. It's the ratio between the correctly identified
positives (true positives) and all identified positives. The precision metric reveals how many of
the predicted classes are correctly labeled.

Project
A project is a work area for building your custom ML models based on your data. Your project
is only accessible by you and others who have access to the Azure resource being used. As a
prerequisite to creating a custom entity extraction project, you have to connect your resource
to a storage account with your dataset when you create a new project. Your project
automatically includes all the .txt files available in your container.
Here's a list of actions you can take:
Label your data: The process of labeling your data so that when you train your model it
learns what you want to extract.
Build and train your model: The core step of your project, where your model starts
learning from your labeled data.
View model evaluation details: Review your model performance to decide if there's room
for improvement, or you're satisfied with the results.
Deployment: After you review the model's performance and decided it can be used in
your environment, you need to assign it to a deployment to use it. Assigning the model
to a deployment makes it available for use through the prediction API

.

Test model: After deploying your model, test your deployment in Microsoft Foundry
see how it would perform in production.

Recall
Measures the model's ability to predict actual positive classes. It's the ratio between the
predicted true positives and what was tagged. The recall metric reveals how many of the
predicted classes are correct.

Next steps
Data and service limits.

to

Custom NER overview.

Last updated on 11/18/2025

How to create custom named entity
recognition (NER) project
Use this article to learn how to set up the requirements for starting with custom NER and
create a project.

Prerequisites
Before you start using custom NER, you need:
An Azure subscription - Create one for free

.

Create a Language resource
Before you start using custom NER, you need an Azure Language in Foundry Tools resource.
We recommend that you create your Language resource and connect a storage account to it in
the Azure portal. Creating a resource in the Azure portal lets you create an Azure storage
account at the same time, with all of the required permissions preconfigured. You can also read
further in the article to learn how to use a preexisting resource, and configure it to work with
custom named entity recognition.
You also need an Azure storage account where you upload your .txt documents that are used
to train a model to extract entities.
ï¼— Note
You need to have an owner role assigned on the resource group to create a
Language resource.
If you connect a preexisting storage account, you should have an owner role
assigned to it.

Create Language resource and connect storage
account
You can create a resource in the following ways:
The Azure portal
PowerShell

ï¼— Note
You shouldn't move the storage account to a different resource group or subscription
once it's linked with Azure Language resource.

Create a new resource from the Azure portal
1. Sign in to the Azure portal

to create a new Azure Language in Foundry Tools resource.

2. In the window that appears, select Custom text classification & custom named entity
recognition from the custom features. Select Continue to create your resource at the
bottom of the screen.

ï Š

3. Create a Language resource with following details.
ï¾‰

Expand table

Name

Description

Subscription

Your Azure subscription.

Resource

A resource group that contains your resource. You can use an existing one, or

group

create a new one.

Region

The region for your Language resource. For example, "West US 2."

Name

A name for your resource.

Name

Description

Pricing tier

The pricing tier for your Language resource. You can use the Free (F0) tier to try
the service.

ï¼— Note
If you get a message saying "your sign in account isn't an owner of the selected
storage account's resource group," your account needs to have an owner role
assigned on the resource group before you can create a Language resource. Contact
your Azure subscription owner for assistance.
4. In the Custom text classification & custom named entity recognition section, select an
existing storage account or select New storage account. These values are to help you get
started, and not necessarily the storage account values you want to use in production
environments. To avoid latency during building your project, connect to storage accounts
in the same region as your Language resource.
ï¾‰

Storage account value

Recommended value

Storage account name

Any name

Storage account type

Standard locally redundant storage (LRS)

Expand table

5. Make sure the Responsible AI Notice is checked. Select Review + create at the bottom of
the page, then select Create.

Create a new Language resource using PowerShell
You can create a new resource and a storage account using the following CLI template
parameters

and

files, which are hosted on GitHub.

Edit the following values in the parameters file:
ï¾‰

Expand table

Parameter name

Value description

name

Name of your Language resource

location

Region in which your resource is hosted. for more information, see Service

Parameter name

Value description
limits.

sku

Pricing tier of your resource.

storageResourceName

Name of your storage account

storageLocation

Region in which your storage account is hosted.

storageSkuType

SKU of your storage account.

storageResourceGroupName

Resource group of your storage account

Use the following PowerShell command to deploy the Azure Resource Manager (ARM)
template with the files you edited.
PowerShell
New-AzResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName
ExampleResourceGroup `
-TemplateFile <path-to-arm-template> `
-TemplateParameterFile <path-to-parameters-file>

See the ARM template documentation for information on deploying templates and parameter
files.
ï¼— Note
The process of connecting a storage account to your Language resource is
irreversible. It can't be disconnected later.
You can only connect your language resource to one storage account.

Using a preexisting Language resource
You can use an existing Language resource to get started with custom NER as long as this
resource meets the below requirements:
ï¾‰

Expand table

Requirement

Description

Regions

Make sure your existing resource is provisioned in one of the supported regions. If not,
you need to create a new resource in one of these regions.

Requirement

Description

Pricing tier

Learn more about supported pricing tiers.

Managed

Make sure that the resource's managed identity setting is enabled. Otherwise, read the

identity

next section.

To use custom named entity recognition, you need to create an Azure storage account if you
don't have one already.

Enable identity management for your resource
Your Language resource must have identity management, to enable it using the Azure portal :
1. Go to your Language resource
2. From left hand menu, under Resource Management section, select Identity
3. From System assigned tab, make sure to set Status to On

Enable custom named entity recognition feature
Make sure to enable Custom text classification / Custom Named Entity Recognition feature
from Azure portal.
1. Go to your Language resource in the Azure portal

.

2. From the left side menu, under Resource Management section, select Features.
3. Enable Custom text classification / Custom Named Entity Recognition feature.
4. Connect your storage account.
5. Select Apply.
ï¼‰ Important
Make sure that the user making changes the storage blob data contributor role assigned
for them.

Add required roles
Use the following steps to set the required roles for your Language resource and storage
account.

ï Š

Roles for your Azure Language in Foundry Tools resource
1. Go to your storage account or Language resource in the Azure portal

.

2. Select Access Control (IAM) in the left pane.
3. Select Add to Add Role Assignments, and choose the appropriate role for your account.
You should have the owner or contributor role assigned on your Language resource.
4. Within Assign access to, select User, group, or service principal
5. Select Select members
6. Select your user name. You can search for user names in the Select field. Repeat this for
all roles.
7. Repeat these steps for all the user accounts that need access to this resource.

Roles for your storage account
1. Go to your storage account page in the Azure portal

.

2. Select Access Control (IAM) in the left pane.
3. Select Add to Add Role Assignments, and choose the Storage blob data contributor role
on the storage account.

4. Within Assign access to, select Managed identity.
5. Select Select members
6. Select your subscription, and Language as the managed identity. You can search for user
names in the Select field.

Roles for your user
ï¼‰ Important
If you skip this step, you'll have a 403 error when trying to connect to your custom project.
It's important that your current user has this role to access storage account blob data,
even if you're the owner of the storage account.
1. Go to your storage account page in the Azure portal

.

2. Select Access Control (IAM) in the left pane.
3. Select Add to Add Role Assignments, and choose the Storage blob data contributor role
on the storage account.
4. Within Assign access to, select User, group, or service principal.
5. Select Select members
6. Select your User. You can search for user names in the Select field.
ï¼‰ Important
If you have a virtual network or private endpoint, be sure to select Allow Azure services
on the trusted services list to access this storage account in the Azure portal.

Enable CORS for your storage account
Make sure to allow (GET, PUT, DELETE) methods when enabling Cross-Origin Resource Sharing
(CORS). Set allowed origins field to https://language.cognitive.azure.com . Allow all header by
adding * to the allowed header values, and set the maximum age to 500 .

ï Š

Create a custom named entity recognition project
(REST API)
Once your resource and storage container are configured, create a new custom NER project. A
project is a work area for building your custom AI models based on your data. Only you can
access your project along with others who have access to the Azure resource being used. If you
labeled data, you can use it to get started by importing a project.
To start creating a custom named entity recognition model, you need to create a project.
Creating a project lets you label data, train, evaluate, improve, and deploy your models.
ï¼— Note
The project name is case-sensitive for all operations.
Create a PATCH request using the following URL, headers, and JSON body to create your
project.

Request URL
Use the following URL to create a project. Replace the following placeholders with your own
values.
rest

{Endpoint}/language/authoring/analyze-text/projects/{projectName}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

myProject

NAME}

case-sensitive.

{API-

The version of the API you're calling. The
value referenced is for the latest released

VERSION}

2022-05-01

version. For more information, for more
information, see Model lifecycle.

Request headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Required

Type

Value

Ocp-Apim-Subscription-

True

string

The key to your resource. Used for authenticating your
API requests.

True

string

application/merge-patch+json

Key
Content-Type

Request body
Use the following JSON in your request. Replace the following placeholders with your own
values.
JSON
{
"projectName": "{PROJECT-NAME}",
"language": "{LANGUAGE-CODE}",
"projectKind": "CustomEntityRecognition",
"description": "Project description",
"multilingual": "True",
"storageInputContainerName": "{CONTAINER-NAME}"

}

ï¾‰

Key

Placeholder

Value

Example

projectName

{PROJECT-NAME}

The name of your
project. This value
is case-sensitive.

myProject

language

{LANGUAGE-CODE}

A string specifying

en-us

Expand table

the language code
for the documents
used in your
project. If your
project is a
multilingual
project, select the
code for the
language most
frequently
represented in the
documents. See
language support
to learn more
about supported
language codes.
projectKind

CustomEntityRecognition

Your project kind.

CustomEntityRecognition

multilingual

true

A boolean value
that enables you to

true

have documents in
multiple languages
in your dataset and
when your model
is deployed you
can query the
model in any
supported
language (not
necessarily
included in your
training
documents). See
language support
to learn more

Key

Placeholder

Value

Example

about multilingual
support.
storageInputContainerName

{CONTAINER-NAME

The name of your
Azure storage
container your
documents were
uploaded.

myContainer

This request returns a 201 response, which means that the project is created.
This request returns an error if:
The selected resource doesn't have proper permission for the storage account.

Import project (REST API)
If you already labeled data, you can use it to get started with the service. Make sure that your
labeled data follows the accepted data formats.
Submit a POST request using the following URL, headers, and JSON body to import your labels
file. Make sure that your labels file follow the accepted format.
If a project with the same name already exists, the data of that project is replaced.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}/:import?apiversion={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

myProject

NAME}

case-sensitive.

{API-

The version of the API you're calling. The

VERSION}

value referenced here's for the latest
version released. For more information,
see Model lifecycle.

2022-05-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request. Replace the placeholder values with your own values.
JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectName": "{PROJECT-NAME}",
"projectKind": "CustomEntityRecognition",
"description": "Trying out custom NER",
"language": "{LANGUAGE-CODE}",
"multilingual": true,
"storageInputContainerName": "{CONTAINER-NAME}",
"settings": {}
},
"assets": {
"projectKind": "CustomEntityRecognition",
"entities": [
{
"category": "Entity1"
},
{
"category": "Entity2"
}
],
"documents": [
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"entities": [
{
"regionOffset": 0,
"regionLength": 500,
"labels": [
{
"category": "Entity1",

"offset": 25,
"length": 10
},
{
"category": "Entity2",
"offset": 120,
"length": 8
}
]
}
]
},
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"entities": [
{
"regionOffset": 0,
"regionLength": 100,
"labels": [
{
"category": "Entity2",
"offset": 20,
"length": 5
}
]
}
]
}
]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

api-version

{API-VERSION}

The version of the
API you're calling.

2022-03-01-preview

The version used
here must be the
same API version in
the URL. Learn more
about other
available API
versions
projectName

{PROJECT-NAME}

The name of your
project. This value is
case-sensitive.

myProject

Key

Placeholder

Value

Example

projectKind

CustomEntityRecognition

Your project kind.

CustomEntityRecognition

language

{LANGUAGE-CODE}

A string specifying
the language code

en-us

for the documents
used in your project.
If your project is a
multilingual project,
choose the
language code of
most the
documents.
multilingual

true

A boolean value
that enables you to

true

have documents in
multiple languages
in your dataset and
when your model is
deployed you can
query the model in
any supported
language (not
necessarily included
in your training
documents. See
language support
for information on
multilingual
support.
storageInputContainerName

{CONTAINER-NAME}

The name of your
Azure storage

myContainer

container containing
your uploaded
documents.
entities

Array containing all
the entity types you
have in the project
and extracted from
your documents.

documents

Array containing all
the documents in
your project and list
of the entities

[]

Key

Placeholder

Value

Example

labeled within each
document.
location

{DOCUMENT-NAME}

The location of the

doc1.txt

documents in the
storage container.
dataset

{DATASET}

The test set to
which this file goes

Train

to when split before
training. For more
information, see
How to train a
model. Possible
values for this field
are Train and Test .

Once you send your API request, you receive a 202 response indicating that the job was
submitted correctly. In the response headers, extract the operation-location value. Here's an
example of the format:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOBID}?api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You use this

URL to get the import job status.
Possible error scenarios for this request:
The selected resource doesn't have proper permissions for the storage account.
The storageInputContainerName specified doesn't exist.
Invalid language code is used, or if the language code type isn't string.
multilingual value is a string and not a boolean.

Get project details (REST API)
Use the following GET request to get your project details. Replace the placeholder values with
your own values.
rest

{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value

myProject

NAME}

is case-sensitive.

{API-

The version of the API you're calling.
For more information, see Model

VERSION}

2022-05-01

lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response body
JSON
{
"createdDateTime": "2021-10-19T23:24:41.572Z",
"lastModifiedDateTime": "2021-10-19T23:24:41.572Z",
"lastTrainedDateTime": "2021-10-19T23:24:41.572Z",
"lastDeployedDateTime": "2021-10-19T23:24:41.572Z",
"projectKind": "CustomEntityRecognition",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectName": "{PROJECT-NAME}",
"multilingual": false,
"description": "Project description",
"language": "{LANGUAGE-CODE}"
}

ï¾‰

Expand table

Value

Placeholder

Description

Example

projectKind

CustomEntityRecognition

Your project kind.

CustomEntityRecognition

storageInputContainerName

{CONTAINER-NAME}

The name of your
Azure storage

myContainer

container for your
uploaded
documents.
projectName

{PROJECT-NAME}

The name of your
project. This value is

myProject

case-sensitive.
multilingual

true

A boolean value

true

that enables you to
have documents in
multiple languages
in your dataset and
when your model is
deployed you can
query the model in
any supported
language (not
necessarily included
in your training
documents. For
more information
about multilingual
support, see
Language support.
language

{LANGUAGE-CODE}

A string specifying

en-us

the language code
for the documents
used in your project.
If your project is a
multilingual project,
choose the
language code for
most of the
documents.

Once you send your API request, you receive a 200 response indicating success and JSON
response body with your project details.

Delete project (REST API)

When you no longer need your project, you can delete it with the following DELETE request.
Replace the placeholder values with your own values.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

myProject

NAME}

case-sensitive.

{API-

The version of the API you're calling. The

VERSION}

value referenced is for the latest version
released. For more information, see

2022-05-01

Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success, which means
your project is deleted. A successful call results with an Operation-Location header used to
check the status of the job.

Next steps
You should have an idea of the project schema you use to label your data.
After your project is created, you can start labeling your data. This process informs your
entity extraction model how to interpret text, and is used for training and evaluation.

Last updated on 12/17/2025

How to prepare data and define a schema
for custom NER
In order to create a custom NER model, you need quality data to train it. This article covers how
you should select and prepare your data, along with defining a schema. Defining the schema is
the first step in project development lifecycle, and it defines the entity types/categories that
you need your model to extract from the text at runtime.

Schema design
The schema defines the entity types/categories that you need your model to extract from text
at runtime.
Review documents in your dataset to be familiar with their format and structure.
Identify the entities you want to extract from the data.
For example, if you're extracting entities from support emails, you might need to extract
"Customer name," "Product name," "Request date," and "Contact information."
Avoid entity types ambiguity.
Ambiguity happens when entity types you select are similar to each other. The more
ambiguous your schema the more labeled data you need to differentiate between
different entity types.
For example, if you're extracting data from a legal contract, to extract "Name of first
party" and "Name of second party" you need to add more examples to overcome
ambiguity since the names of both parties look similar. Avoid ambiguity as it saves time,
effort, and yields better results.
Avoid complex entities. Complex entities can be difficult to pick out precisely from text.
Consider breaking it down into multiple entities.
For example, extracting "Address" would be challenging if not broken down into smaller
entities. There are so many variations of how addresses appear, it would take large
number of labeled entities to teach the model to extract an address, as a whole, without
breaking it down. However, if you replace "Address" with "Street Name," "PO Box," "City,"
"State" and "Zip," the model requires fewer labels per entity.

Data selection

The quality of data you train your model with affects model performance greatly.
Use real-life data that reflects your domain's problem space to effectively train your
model. You can use synthetic data to accelerate the initial model training process, but it
differs from your real-life data and make your model less effective when used.
Balance your data distribution as much as possible without deviating far from the
distribution in real-life.
Use diverse data whenever possible to avoid overfitting your model. Less diversity in
training data may lead to your model learning spurious correlations that may not exist in
real-life data.
Avoid duplicate documents in your data. Duplicate data has a negative effect on the
training process, model metrics, and model performance.
Consider where your data comes from. If you're collecting data from one person,
department, or part of your scenario, you're likely missing diversity that may be important
for your model to learn about.
ï¼— Note
If your documents are in multiple languages, select the enable multi-lingual option
during project creation and set the language option to the language of most your
documents.

Data preparation
As a prerequisite for creating a project, your training data needs to be uploaded to a blob
container in your storage account. You can create and upload training documents from Azure
directly, or through using the Azure Storage Explorer tool. Using the Azure Storage Explorer
tool allows you to upload more data quickly.
Create and upload documents from Azure
Create and upload documents using Azure Storage Explorer
You can only use .txt documents. If your data is in other format, you can use CLUtils parse
command

to change your document format.

You can upload an annotated dataset, or you can upload an unannotated one and label your
data.

Test set
When defining the testing set, make sure to include example documents that aren't present in
the training set. Defining the testing set is an important step to calculate the model
performance. Also, make sure that the testing set include documents that represent all entities
used in your project.

Next steps
If you haven't already, create a custom NER project. If it's your first time using custom NER,
consider following the quickstart to create an example project. For more information, see the
how-to article.

Last updated on 11/18/2025

Label your data in Azure Language in
Foundry Tools Studio
Before training your model, you need to label your documents with the custom entities you
want to extract. Data labeling is a crucial step in development lifecycle. You can create the
entity types you want to extract from your data and label these entities within your documents.
This data is used in the next step when training your model so that your model can learn from
the labeled data. If you already labeled data, you can directly import it into your project, but
you need to make sure that your data follows the accepted data format. See create project to
learn more about importing labeled data into your project.
Before creating a custom NER model, you first need to label your data. If your data isn't labeled
already, you can label it in the Language Studio

. Labeled data informs the model how to

interpret text, and is used for training and evaluation.

Prerequisites
Before you can label your data, you need:
A successfully created project with a configured Azure blob storage account
Text data is uploaded to your storage account.
For more information, see the project development lifecycle.

Data labeling guidelines
After preparing your data, designing your schema, and creating your project, you need to label
your data. Labeling your data is important so your model knows which words are associated
with the entity types you need to extract. When you label your data in Language Studio

(or

import labeled data), these labels are stored in the JSON document in your storage container
that you connected to this project.
As you label your data, keep in mind:
In general, more labeled data leads to better results, provided the data is labeled
accurately.
The precision, consistency, and completeness of your labeled data are key factors to
determining model performance.
Label precisely: Label each entity to its right type always. Only include what you want
extracted. Avoid unnecessary data in your labels.

Label consistently: The same entity should have the same label across all the
documents.
Label completely: Label all the instances of the entity in all your documents. You can
use the autolabeling feature to ensure complete labeling.
ï¼— Note
There's no fixed number of labels that can guarantee your model performs the best.
Model performance is dependent on possible ambiguity in your schema, and the
quality of your labeled data. Nevertheless, we recommend having around 50 labeled
instances per entity type.

Label your data
Use the following steps to label your data:
1. Go to your project page in Language Studio

.

2. From the left side menu, select Data labeling. You can find a list of all documents in your
storage container.
îª€ Tip
You can use the filters in top menu to view the unlabeled documents so that you can
start labeling them. You can also use the filters to view the documents that are
labeled with a specific entity type.
3. Change to a single document view from the left side in the top menu or select a specific
document to start labeling. You can find a list of all .txt documents available in your
project to the left. You can use the Back and Next button from the bottom of the page to
navigate through your documents.
ï¼— Note
If you enabled multiple languages for your project, you find a Language dropdown
in the top menu, which lets you select the language of each document.
4. In the right side pane, Add entity type to your project so you can start labeling your data
with them.

5. You have two options to label your document:
ï¾‰

Expand table

Option

Description

Label using a

Select the brush icon next to an entity type in the right pane, then highlight the

brush

text in the document you want to annotate with this entity type.

Label using a

Highlight the word you want to label as an entity, and a menu appears. Select the

menu

entity type you want to assign for this entity.

The following screenshot shows labeling using a brush.

ï Š

6. In the right side pane under the Labels pivot you can find all the entity types in your
project and the count of labeled instances per each.
7. In the bottom section of the right side pane, you can add the current document you're
viewing to the training set or the testing set. By default all the documents are added to
your training set. Learn more about training and testing sets and how they're used for
model training and evaluation.
îª€ Tip
If you're planning on using Automatic data splitting, use the default option of
assigning all the documents into your training set.
8. Under the Distribution pivot, you can view the distribution across training and testing
sets. You have two options for viewing:
Total instances where you can view count of all labeled instances of a specific entity
type.
documents with at least one label where each document is counted if it contains at
least one labeled instance of this entity.

9. When you're labeling, your changes are synced periodically, if they aren't saved yet you
get a warning at the top of your page. If you want to save manually, select Save labels
button at the bottom of the page.

Remove labels
To remove a label
1. Select the entity you want to remove a label from.
2. Scroll through the menu that appears, and select Remove label.

Delete entities
To delete an entity, select the delete icon next to the entity you want to remove. Deleting an
entity removes all its labeled instances from your dataset.

Next steps
After you label your data, you can begin training a model that will learn based on your data.

Last updated on 11/18/2025

How to use autolabeling for Custom
Named Entity Recognition
Labeling process is an important part of preparing your dataset. Since this process requires
both time and effort, you can use the autolabeling feature to automatically label your entities.
You can start autolabeling jobs based on a model you previously trained or using GPT models.
With autolabeling based on a model you previously trained, you can start labeling a few of
your documents, train a model, then create an autolabeling job to produce entity labels for
other documents based on that model. With autolabeling with GPT , you may immediately
trigger an autolabeling job without any prior model training. This feature can save you the time
and effort of manually labeling your entities.

Prerequisites
Autolabel based on a model you trained

Before you can use autolabeling based on a model you trained, you need:
A successfully created project with a configured Azure blob storage account.
Text data uploaded to your storage account.
Labeled data
A successfully trained model

Trigger an autolabeling job
Autolabel based on a model you trained

When you trigger an autolabeling job based on a model you trained, there's a monthly
limit of 5,000 text records per month, per resource. The same limit applies on all projects
within the same resource.
îª€ Tip
A text record is calculated as the ceiling of (Number of characters in a document /
1,000). For example, if a document has 8,921 characters, the number of text records is:
ceil(8921/1000) = ceil(8.921) , which is nine text records.

1. From the left pane, select Data labeling.
2. Select the Autolabel button under the Activity pane to the right of the page.

ï Š

3. Choose autolabel based on a model you trained and select Next.

ï Š

A screenshot showing model choice for auto labeling.
4. Choose a trained model. We recommend that you check the model performance
before using it for autolabeling.

ï Š

5. Choose the entities you want to be included in the autolabeling job. By default, all
entities are selected. You can see the total labels, precision, and recall of each entity.
We recommend that you include entities that perform well to ensure the quality of
the automatically labeled entities.

ï Š

6. Choose the documents you want to be automatically labeled. The number of text
records of each document is displayed. When you select one or more documents,
you should see the number of texts records selected. We recommend that you
choose the unlabeled documents from the filter.
ï¼— Note
If an entity was automatically labeled, but has a user defined label, only the
user defined label is used and visible.
You can view the documents by selecting the document name.

ï Š
A screenshot showing which documents to be included in the autotag job.
7. Select Autolabel to trigger the autolabeling job. You should see the model used,
number of documents included in the autolabeling job, number of text records and
entities to be automatically labeled. Autolabeling jobs can take anywhere from a few
seconds to a few minutes, depending on the number of documents you included.

ï Š

Review the auto labeled documents
When the autolabeling job is complete, you can see the output documents in the Data
labeling page of Language Studio. Select Review documents with autolabels to view the
documents with the Auto labeled filter applied.

ï Š

A screenshot showing the autolabeled documents
Entities that are automatically labeled appear with a dotted line. These entities have two
selectors (a checkmark and an "X") that allow you to accept or reject the automatic label.
Once an entity is accepted, the dotted line changes to a solid one, and the label is included in
any further model training becoming a user defined label.
Alternatively, you can accept or reject all automatically labeled entities within the document,
using Accept all or Reject all in the top right corner of the screen.
After you accept or reject the labeled entities, select Save labels to apply the changes.
ï¼— Note
We recommend validating automatically labeled entities before accepting them.
All labels that aren't accepted are deleted when you train your model.

ï Š

Next steps
Learn more about labeling your data.

Last updated on 11/18/2025

How to use autolabeling for Custom Text
Classification
Labeling process is an important part of preparing your dataset. Since this process requires
much time and effort, you can use the autolabeling feature to automatically label your
documents with the classes you want to categorize them into. You can currently start
autolabeling jobs based on a model using GPT models where you may immediately trigger an
autolabeling job without any prior model training. This feature can save you the time and effort
of manually labeling your documents.

Prerequisites
Before you can use autolabeling with GPT, you need:
A successfully created project with a configured Azure blob storage account.
Text data uploaded to your storage account.
Class names that are meaningful. The GPT models label documents based on the names
of the classes you've provided.
Labeled data isn't required.
An Azure OpenAI resource and deployment.

Trigger an autolabeling job
When you trigger an autolabeling job with GPT, you're charged to your Azure OpenAI resource
as per your consumption. You're charged an estimate of the number of tokens in each
document being autolabeled. Refer to the Azure OpenAI pricing page

for a detailed

breakdown of pricing per token of different models.
1. From the left pane, select Data labeling.
2. Select the Autolabel button under the Activity pane to the right of the page.

ï Š

3. Choose Autolabel with GPT and select Next.

ï Š

4. Choose your Azure OpenAI resource and deployment. You must create an Azure OpenAI
resource and deploy a model in order to proceed.

ï Š

A screenshot showing how to choose OpenAI resource and deployments
5. Select the classes you want to be included in the autolabeling job. By default, all classes
are selected. Having descriptive names for classes, and including examples for each class
is recommended to achieve good quality labeling with GPT.

ï Š

6. Choose the documents you want to be automatically labeled. We recommend choosing
the unlabeled documents from the filter.
ï¼— Note
If a document was automatically labeled, but this label was already user
defined, only the user defined label is used.
You can view the documents by selecting the document name.

ï Š

7. Select Start job to trigger the autolabeling job. You should be directed to the
autolabeling page displaying the autolabeling jobs initiated. Autolabeling jobs can take

anywhere from a few seconds to a few minutes, depending on the number of documents
you included.

ï Š

Review the auto labeled documents
When the autolabeling job is complete, you can see the output documents in the Data
labeling page of Language Studio. Select Review documents with autolabels to view the
documents with the Auto labeled filter applied.

ï Š

Documents that have been automatically classified have suggested labels in the activity pane
highlighted in purple. Each suggested label has two selectors (a checkmark and a cancel icon)
that allow you to accept or reject the automatic label.
Once a label is accepted, the purple color changes to the default blue one, and the label is
included in any further model training becoming a user defined label.
After you accept or reject the labels for the autolabeled documents, select Save labels to apply
the changes.

ï¼— Note
We recommend validating automatically labeled documents before accepting them.
All labels that were not accepted are deleted when you train your model.

ï Š

Next steps
Learn more about labeling your data.

Last updated on 11/18/2025

Train your custom named entity
recognition model
Training is the process where the model learns from your labeled data. After training is
completed, you'll be able to view the model's performance to determine if you need to
improve your model.
To train a model, you start a training job and only successfully completed jobs create a model.
Training jobs expire after seven days, which means you won't be able to retrieve the job details
after this time. If your training job completed successfully and a model was created, the model
isn't affected. You can only have one training job running at a time, and you can't start other
jobs in the same project.
The training times vary. Training can be anywhere from a few minutes, when dealing with few
documents, or several hours, depending on the dataset size and the complexity of your
schema.

Prerequisites
A successfully created project with a configured Azure blob storage account
Text data uploaded to your storage account.
Labeled data
See the project development lifecycle.

Data splitting
Before you start the training process, labeled documents in your project are divided into a
training set and a testing set. Each one of them serves a different function. The training set is
used in training the model. It's the set from which the model learns the labeled entities and
what spans of text are to be extracted as entities. The testing set is a blind set that isn't
introduced to the model during training but only during evaluation. After model training is
completed successfully, the model is used to make predictions from the test documents and
evaluation metrics are calculated. We recommend that you make sure that all your entities are
adequately represented in both the training and testing set.
Custom NER supports two methods for data splitting:
Automatically splitting the testing set from training data: The system splits your labeled
data between the training and testing sets, according to the percentages you choose. The
recommended percentage split is 80% for training and 20% for testing.

ï¼— Note
If you choose the Automatically splitting the testing set from training data option, only
the data assigned to a training set is split according to the percentages provided.
Use a manual split of training and testing data: This method enables users to define
which labeled documents should belong to which set. This step is only enabled if you
added documents to your testing set during data labeling.

Train model (REST API)
Once you have labeled your data and configured your data split settings, you can start training
your custom NER model using the REST API. The training process involves submitting a training
job request and monitoring its progress until completion. This section provides the API calls
needed to initiate training and check the status of your training job.

Start training job
Submit a POST request using the following URL, headers, and JSON body to submit a training
job. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:train?apiversion={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is

myProject

NAME}

case-sensitive.

{API-

The version of the API you're calling. The

VERSION}

value referenced is for the latest version
released. For more information, see
Model lifecycle.

Headers

2022-05-01

Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in your request body. The model is given as the {MODEL-NAME} once
training is complete. Only successful training jobs produce models.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 80,
"testingSplitPercentage": 20
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-NAME}

The model name that is assigned to your model
once trained successfully.

myModel

trainingConfigVersion

{CONFIG-

This is the model version used to train the
model.

2022-05-01

Option to split your data across training and
testing sets.

{}

Split methods. Possible values are percentage or

percentage

VERSION}

evaluationOptions

kind

percentage

manual . For more information, see How to train a

model.
trainingSplitPercentage

80

Percentage of your tagged data to be included
in the training set. Recommended value is 80 .

80

testingSplitPercentage

20

Percentage of your tagged data to be included
in the testing set. Recommended value is 20 .

20

ï¼— Note
The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set
to percentage and the sum of both percentages should be equal to 100.
Once you send your API request, you receive a 202 response indicating that the job was
submitted correctly. In the response headers, extract the location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}?api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this

URL to get the training status.

Get training job status (REST API)
Training can take some time, depending on the size of your training data and complexity of
your schema. You can use the following request to keep polling the status of the training job
until successful completion.
Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is

myProject

NAME}

case-sensitive.

Placeholder

Value

Example

{JOB-ID}

The ID for locating your model's training
status. This value is in the location

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

header value you received in the previous
step.
{APIVERSION}

The version of the API you're calling. The
value referenced is for the latest version

2022-05-01

released. For more information, see
Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"result": {
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},
"jobId": "{JOB-ID}",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"

}

Cancel training job (REST API)
If you need to stop a training job that's currently in progress, you can cancel it using the REST
API. Canceling a training job is useful when you discover an issue with your data or
configuration and want to make corrections before restarting the training process.
Create a POST request by using the following URL, headers, and JSON body to cancel a
training job.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{Endpoint}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}/:cancel?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com
EmailApp

NAME}

The name for your project. This value is
case-sensitive.

{JOB-ID}

This value is the training job ID.

XXXXX-XXXXX-XXXX-XX

{API-

The version of the API you're calling.
The value referenced is for the latest
released model version.

2022-05-01

{PROJECT-

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

After you send your API request, you receive a 202 response with an Operation-Location
header used to check the status of the job.

Next steps
After training is completed, you'll be able to view model performance to optionally improve
your model if needed. Once you're satisfied with your model, you can deploy it, making it
available to use for extracting entities from text.

Last updated on 12/17/2025

View the custom NER model's evaluation
and details
After your model finishes training, you can view the model performance and see the extracted
entities for the documents in the test set.
ï¼— Note
Using the Automatically split the testing set from training data option may result in
different model evaluation result every time you train a new model, as the test set is
selected randomly from the data. To make sure that the evaluation is calculated on the
same test set every time you train a model, make sure to use the Use a manual split of
training and testing data option when starting a training job and define your Test
documents when labeling data.

Prerequisites
Before viewing model evaluation, you need:
A successfully created project with a configured Azure blob storage account.
Text data uploaded to your storage account.
Labeled data
A successfully trained model
For more information, see the project development lifecycle.

Model details (REST API)
Submit a GET request using the following URL, headers, and JSON body to get trained model
evaluation summary.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/models/{trainedModelLabel}/evaluation/summary-result?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

The name for your project. This

myProject

{PROJECT-NAME}

value is case-sensitive.
{trainedModelLabel}

The name for your trained

Model1

model. This value is casesensitive.
{API-VERSION}

The version of the API you're
calling. For more information,
see Model lifecycle.

2022-05-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"projectKind": "CustomEntityRecognition",
"customEntityRecognitionEvaluation": {
"confusionMatrix": {
"additionalProp1": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0

}
},
"additionalProp2": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp3": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
}
},
"entities": {
"additionalProp1": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp2": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp3": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,

"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
}
},
"microF1": 0,
"microPrecision": 0,
"microRecall": 0,
"macroF1": 0,
"macroPrecision": 0,
"macroRecall": 0
},
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 0,
"testingSplitPercentage": 0
}
}

Load or export model data (REST API)
Load model data
Create a POST request using the following URL, headers, and JSON body to load your model
data to your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/models/{MODELNAME}:load-snapshot?stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}

The name for your project. This
value is case-sensitive.

Expand table

subdomain>.cognitiveservices.azure.com
EmailApp

Placeholder

Value

Example

{API-

The version of the API you're
calling.

2022-10-01-preview

The name of your model. This
value is case-sensitive.

v1

VERSION}
{MODEL-NAME}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/models/{MODELNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the status of your model data loading, using the same authentication method.

Export model data
Create a POST request using the following URL, headers, and JSON body to export your model
data.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}&trainedModelLabel={MODEL-

NAME}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}
{MODEL-NAME}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

EmailApp

The version of the API you're
calling.

2022-10-01-preview

The name of your model. This
value is case-sensitive.

v1

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/jobs/{JOB-ID}?
api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the exported project JSON, using the same authentication method.

Delete model (REST API)
Create a DELETE request using the following URL, headers, and JSON body to delete a trained
model.

Request URL
rest
{Endpoint}/language/authoring/analyze-text/projects/{PROJECTNAME}/models/{trainedModelLabel}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This
value is case-sensitive.

myProject

{trainedModelLabel}

The name for your model name.
This value is case-sensitive.

model1

{API-VERSION}

The version of the API you're
calling. The value referenced is for
the latest version released. For
more information, see Model
lifecycle.

2022-05-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 204 response indicating success, which means
your trained model is deleted.

Next steps
Deploy your model
Learn about the metrics used in evaluation.

Last updated on 12/17/2025

Deploy a model and extract entities from
text using the runtime API
Once you're satisfied with how your model performs, it's ready to be deployed and used to
recognize entities in text. Deploying a model makes it available for use through the prediction
API

.

Prerequisites
A successfully created project with a configured Azure storage account.
Text data that is uploaded to your storage account.
Labeled data and successfully trained model
Reviewed the model evaluation details to determine how your model is performing.
For more information, see project development lifecycle.

Deploy model
After you review your model's performance and decided it can be used in your environment,
you need to assign it to a deployment. Assigning the model to a deployment makes it available
for use through the prediction API . We recommend that you create a deployment named
production to which you assign the best model you built so far and use it in your system. You
can create another deployment called staging to which you can assign the model you're
currently working on to be able to test it. You can have a maximum of 10 deployments in your
project.
Microsoft Foundry

For information on how to deploy your custom model in the Foundry, see Deploy your
fine-tuned model .

Swap deployments
After you're done testing a model assigned to one deployment and you want to assign this
model to another deployment, you can swap these two deployments. Swapping deployments
involves taking the model assigned to the first deployment, and assigning it to the second
deployment. Then taking the model assigned to second deployment, and assigning it to the
first deployment. You can use this process to swap your production and staging deployments
when you want to take the model assigned to staging and assign it to production.

Foundry

To replace a deployed model, you can exchange the deployed model with a different
model in the same region:
1. Select the model name under Name then select Deploy model.
2. Select Swap model.
The redeployment takes several minutes to complete. In the meantime, deployed
model continues to be available for use with the Translator API until this process is
complete.

Delete deployment
Foundry

If you no longer need your project, you can delete it from the Foundry.
1. Navigate to the Foundry

home page. Initiate the authentication process by signing

in, unless you already completed this step and your session is active.
2. Select the project that you want to delete from the Keep building with Foundry
3. Select Management center.
4. Select Delete project.
To delete the hub along with all its projects:
1. Navigate to the Overview tab inn the Hub section.
2. On the right, select Delete hub.
3. The link opens the Azure portal for you to delete the hub.

Assign deployment resources
You can deploy your project to multiple regions by assigning different Language resources that
exist in different regions.
Foundry

For more information on how to deploy you custom model, see Deploy your fine-tuned
model

Unassign deployment resources
To unassign or remove a deployment resource from a project, you also delete all the
deployments for to that resource region.
Foundry

If you no longer need your project, you can delete it from the Foundry.
1. Navigate to the Foundry

home page. Initiate the authentication process by signing

in, unless you already completed this step and your session is active.
2. Select the project that you want to delete from the Keep building with Foundry
3. Select Management center.
4. Select Delete project.
To delete the hub along with all its projects:
1. Navigate to the Overview tab inn the Hub section.
2. On the right, select Delete hub.
3. The link opens the Azure portal for you to delete the hub.

Next steps
After you have a deployment, you can use it to extract entities from text.

Last updated on 11/18/2025

Query your custom model
After the deployment is added successfully, you can query the deployment to extract entities
from your text based on the model you assigned to the deployment.
You can query the deployment programmatically using the Prediction API or through the client
libraries (Azure SDK).

Test deployed model
You can retrieve up-to-date information about your projects, make any necessary changes, and
oversee project management tasks efficiently through the Microsoft Foundry.
To test your deployed models from within the Language Studio

:

1. Select Testing deployments from the left side menu.
2. Select the deployment you want to test. You can only test models that are assigned to
deployments.
3. For multilingual projects, from the language dropdown, select the language of the text
you're testing.
4. Select the deployment you want to query/test from the dropdown.
5. You can enter the text you want to submit to the request or upload a .txt file to use.
6. Select Run the test from the top menu.
7. In the Result tab, you can see the extracted entities from your text and their types. You
can also view the JSON response under the JSON tab.

ï Š

A screenshot showing the model test results.

Submit a custom NER task
Use this POST request to start a text classification task.
rest
{ENDPOINT}/language/analyze-text/jobs?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{API-

The version of the API you're calling. The

2022-05-01

VERSION}

value referenced is for the latest version
released. For more information, see
Model lifecycle.

Headers
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

Your key that provides access to this API.

Body
JSON
{
"displayName": "Extracting entities",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "{LANGUAGE-CODE}",
"text": "Text1"
},
{
"id": "2",
"language": "{LANGUAGE-CODE}",
"text": "Text2"
}
]
},
"tasks": [
{
"kind": "CustomEntityRecognition",
"taskName": "Entity Recognition",
"parameters": {
"projectName": "{PROJECT-NAME}",
"deploymentName": "{DEPLOYMENT-NAME}"
}
}

Expand table

]
}

ï¾‰

Key

Placeholder

Value

Example

displayName

{JOB-NAME}

Your job name.

MyJobName

documents

[{},{}]

List of documents to run tasks on.

[{},{}]

id

{DOC-ID}

Document name or ID.

doc1

language

{LANGUAGE-CODE}

A string specifying the language

en-us

Expand table

code for the document. If this key
isn't specified, the service assumes
the default language of the
project that was selected during
project creation. See language
support for a list of supported
language codes.
text

{DOC-TEXT}

tasks
taskName

CustomEntityRecognition

Document task to run the tasks

Lorem ipsum dolor sit

on.

amet

List of tasks we want to perform.

[]

The task name

CustomEntityRecognition

List of parameters to pass to the

parameters

task.
project-

{PROJECT-NAME}

name

myProject

value is case-sensitive.

name
deployment-

The name for your project. This

{DEPLOYMENT-NAME}

The name of your deployment.
This value is case-sensitive.

prod

Response
You receive a 202 response indicating that your task has been submitted successfully. In the
response headers, extract operation-location . operation-location is formatted like this:
rest
{ENDPOINT}/language/analyze-text/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to query the task completion status and get the results when task is
completed.

Get task results
Use the following GET request to query the status/results of the custom entity recognition task.
rest
{ENDPOINT}/language/analyze-text/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{API-

The version of the API you're calling. The

2022-05-01

VERSION}

value referenced is for the latest version
released. For more information, see
Model lifecycle.

Headers
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

Your key that provides access to this API.

Response Body
The response will be a JSON document with the following parameters
JSON
{
"createdDateTime": "2021-05-19T14:32:25.578Z",
"displayName": "MyJobName",
"expirationDateTime": "2021-05-19T14:32:25.578Z",
"jobId": "xxxx-xxxx-xxxxx-xxxxx",
"lastUpdateDateTime": "2021-05-19T14:32:25.578Z",
"status": "succeeded",
"tasks": {

Expand table

"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "EntityRecognitionLROResults",
"taskName": "Recognize Entities",
"lastUpdateDateTime": "2020-10-01T15:01:03Z",
"status": "succeeded",
"results": {
"documents": [
{
"entities": [
{
"category": "Event",
"confidenceScore": 0.61,
"length": 4,
"offset": 18,
"text": "trip"
},
{
"category": "Location",
"confidenceScore": 0.82,
"length": 7,
"offset": 26,
"subcategory": "GPE",
"text": "Seattle"
},
{
"category": "DateTime",
"confidenceScore": 0.8,
"length": 9,
"offset": 34,
"subcategory": "DateRange",
"text": "last week"
}
],
"id": "1",
"warnings": []
}
],
"errors": [],
"modelVersion": "2020-04-01"
}
}
]
}
}

Client libraries (Azure SDK)

First you need to get your resource key and endpoint:

Get your key and endpoint
Next you will need the key and endpoint from the resource to connect your application to
the API. You'll paste your key and endpoint into the code later in the quickstart.
1. After Azure Language resource deploys successfully, click the Go to Resource button
under Next Steps.

ï Š

2. On the screen for your resource, select Keys and endpoint on the left pane. You will
use one of your keys and your endpoint in the steps below.

ï Š

3. Download and install the client library package for your language of choice:
ï¾‰

Expand table

Language

Package version

.NET

5.2.0-beta.3

Java

5.2.0-beta.3

JavaScript

6.0.0-beta.1

Python

5.2.0b4

4. After you install the client library, use the following samples on GitHub to start calling
the API.
C#
Java
JavaScript
Python
5. For more information, see the following reference documentation:
C#
Java
JavaScript
Python

Next steps
Frequently asked questions

Last updated on 11/18/2025

Back up and recover your custom NER
models
When you create a Language resource, you specify a region for it to be created in. From then
on, your resource and all of the operations related to it take place in the specified Azure server
region. It's rare, but not impossible, to encounter a network issue that affects an entire region.
If your solution needs to always be available, then you should design it to fail over into another
region. This process requires two Azure Language in Foundry Tools resources in different
regions and synchronizing custom models across them.
If your app or business depends on the use of a custom NER model, we recommend that you
create a replica of your project in another supported region. If a regional outage occurs, you
can then access your model in the other fail-over region where you replicated your project.
Replicating a project means that you export your project metadata and assets, and import
them into a new project. This step only makes a copy of your project settings and tagged data.
You still need to train and deploy the models to be available for use with prediction APIs

.

In this article, you learn to how to use the export and import APIs to replicate your project from
one resource to another existing in different supported geographical regions.

Prerequisites
Two Language resources in different Azure regions. Create your resources and connect
them to an Azure storage account. We recommend that you connect each of your
Language resources to different storage accounts. Each storage account should be
located in the same respective regions that your separate Language resources are in. You
can follow the quickstart to create another Language resource and storage account.

Get your resource keys endpoint
Use the following steps to get the keys and endpoint of your primary and secondary resources.
1. Go to your resource overview page in the Azure portal
2. From the menu on the left side, select Keys and Endpoint. The endpoint and key are used
for API requests.

ï Š

îª€ Tip
Keep a note of keys and endpoints for both primary and secondary resources as well as
the primary and secondary container names. Use these values to replace the following
placeholders: {PRIMARY-ENDPOINT} , {PRIMARY-RESOURCE-KEY} , {PRIMARY-CONTAINER-NAME} ,
{SECONDARY-ENDPOINT} , {SECONDARY-RESOURCE-KEY} , and {SECONDARY-CONTAINER-NAME} . Also

take note of your project name, your model name, and your deployment name. Use these
values to replace the following placeholders: {PROJECT-NAME} , {MODEL-NAME} , and
{DEPLOYMENT-NAME} .

Export your primary project assets
Start by exporting the project assets from the project in your primary resource.

Submit export job
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Create a POST request using the following URL, headers, and JSON body to export your
project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.

rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your
API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

MyProject

NAME}

case-sensitive.

{API-

The version of the API you're calling.
The value referenced is the latest

VERSION}

2022-05-01

model version released.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request body specifying that you want to export all the assets.
JSON
{
"assetsToExport": ["*"]
}

Once you send your API request, you receive a 202 response indicating that the job was
submitted correctly. In the response headers, extract the operation-location value formatted
like this:
rest

{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/export/jobs/{JOBID}?api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You use this

URL to get the export job status.

Get export job status
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of exporting your project assets. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/export/jobs/{JOBID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

The name of your project. This value is
case-sensitive.

myProject

The ID for locating your model's training
status. It's in the location header value

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECTNAME}
{JOB-ID}

you received in the previous step.
{APIVERSION}

The version of the API you're calling. The
value referenced is for the latest version
released. For more information, see

2022-05-01

Model lifecycle.

Headers
Use the following header to authenticate your request.

ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response body
JSON
{
"resultUrl": "{RESULT-URL}",
"jobId": "string",
"createdDateTime": "2021-10-19T23:24:41.572Z",
"lastUpdatedDateTime": "2021-10-19T23:24:41.572Z",
"expirationDateTime": "2021-10-19T23:24:41.572Z",
"status": "unknown",
"errors": [
{
"code": "unknown",
"message": "string"
}
]
}

Use the URL from the resultUrl key in the body and view the exported assets from this job.

Get export results
Submit a GET request using the {RESULT-URL} you received from the previous step to view the
results of the export job.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Copy the response body to use it as the body for the next import job.

Import to a new project
Now go ahead and import the exported project assets in your new project in the secondary
region so you can replicate it.

Submit import job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , {SECONDARYRESOURCE-KEY} , and {SECONDARY-CONTAINER-NAME} that you obtained in the first step.

Submit a POST request using the following URL, headers, and JSON body to import your labels
file. Make sure that your labels file follow the accepted format.
If a project with the same name already exists, the data of that project is replaced.
rest
{Endpoint}/language/authoring/analyze-text/projects/{projectName}/:import?apiversion={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This value is
case-sensitive.

myProject

The version of the API you're calling. The
value referenced here's for the latest
version released. For more information,
see Model lifecycle.

2022-05-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following JSON in your request. Replace the placeholder values with your own values.
JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectName": "{PROJECT-NAME}",
"projectKind": "CustomEntityRecognition",
"description": "Trying out custom NER",
"language": "{LANGUAGE-CODE}",
"multilingual": true,
"storageInputContainerName": "{CONTAINER-NAME}",
"settings": {}
},
"assets": {
"projectKind": "CustomEntityRecognition",
"entities": [
{
"category": "Entity1"
},
{
"category": "Entity2"
}
],
"documents": [
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"entities": [
{
"regionOffset": 0,
"regionLength": 500,
"labels": [
{
"category": "Entity1",
"offset": 25,
"length": 10
},
{
"category": "Entity2",
"offset": 120,
"length": 8
}
]
}
]
},
{

"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"entities": [
{
"regionOffset": 0,
"regionLength": 100,
"labels": [
{
"category": "Entity2",
"offset": 20,
"length": 5
}
]
}
]
}
]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

api-version

{API-VERSION}

The version of the

2022-03-01-preview

API you're calling.
The version used
here must be the
same API version in
the URL. Learn more
about other
available API
versions
projectName

{PROJECT-NAME}

The name of your
project. This value is
case-sensitive.

myProject

projectKind

CustomEntityRecognition

Your project kind.

CustomEntityRecognition

language

{LANGUAGE-CODE}

A string specifying
the language code

en-us

for the documents
used in your project.
If your project is a
multilingual project,
choose the
language code of
most the
documents.

Key

Placeholder

Value

Example

multilingual

true

A boolean value
that enables you to
have documents in
multiple languages
in your dataset and
when your model is

true

deployed you can
query the model in
any supported
language (not
necessarily included
in your training
documents. See
language support
for information on
multilingual
support.
storageInputContainerName

{CONTAINER-NAME}

The name of your

myContainer

Azure storage
container containing
your uploaded
documents.
Array containing all
the entity types you

entities

have in the project
and extracted from
your documents.
Array containing all
the documents in
your project and list
of the entities

documents

[]

labeled within each
document.
location

{DOCUMENT-NAME}

The location of the
documents in the
storage container.

doc1.txt

dataset

{DATASET}

The test set to

Train

which this file goes
to when split before
training. For more
information, see
How to train a
model. Possible

Key

Placeholder

Value

Example

values for this field
are Train and Test .

Once you send your API request, you receive a 202 response indicating that the job was
submitted correctly. In the response headers, extract the operation-location value. Here's an
example of the format:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOBID}?api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You use this

URL to get the import job status.
Possible error scenarios for this request:
The selected resource doesn't have proper permissions for the storage account.
The storageInputContainerName specified doesn't exist.
Invalid language code is used, or if the language code type isn't string.
multilingual value is a string and not a boolean.

Get import job status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of your importing your project. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/import/jobs/{JOBID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}
{JOB-ID}

subdomain>.cognitiveservices.azure.com

The name of your project. This value is
case-sensitive.

myProject

The ID for locating your model's training
status. This value is in the location

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

header value you received in the previous
step.
{APIVERSION}

The version of the API you're calling. The
value referenced is for the latest version

2022-05-01

released. For more information, see
Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Train your model
After importing your project, you only copied the project's assets and metadata and assets. You
still need to train your model, which incurs usage on your account.

Submit training job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Submit a POST request using the following URL, headers, and JSON body to submit a training
job. Replace the placeholder values with your own values.
rest

{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/:train?apiversion={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name of your project. This value is
case-sensitive.

myProject

The version of the API you're calling. The
value referenced is for the latest version

2022-05-01

released. For more information, see
Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in your request body. The model is given as the {MODEL-NAME} once
training is complete. Only successful training jobs produce models.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 80,
"testingSplitPercentage": 20
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-NAME}

The model name that is assigned to your model

myModel

once trained successfully.
trainingConfigVersion

{CONFIG-

This is the model version used to train the

VERSION}

model.

evaluationOptions

kind

percentage

2022-05-01

Option to split your data across training and
testing sets.

{}

Split methods. Possible values are percentage or

percentage

manual . For more information, see How to train a

model.
trainingSplitPercentage

80

Percentage of your tagged data to be included

80

in the training set. Recommended value is 80 .
testingSplitPercentage

20

Percentage of your tagged data to be included
in the testing set. Recommended value is 20 .

20

ï¼— Note
The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set
to percentage and the sum of both percentages should be equal to 100.
Once you send your API request, you receive a 202 response indicating that the job was
submitted correctly. In the response headers, extract the location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}?api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this

URL to get the training status.

Get training status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}/train/jobs/{JOBID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is

myProject

NAME}

case-sensitive.

{JOB-ID}

The ID for locating your model's training

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

status. This value is in the location
header value you received in the previous
step.
{APIVERSION}

The version of the API you're calling. The
value referenced is for the latest version

2022-05-01

released. For more information, see
Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.

JSON
{
"result": {
"modelLabel": "{MODEL-NAME}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},
"jobId": "{JOB-ID}",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

Deploy your model
In this step, you make your trained model available form consumption via the runtime
prediction API .
îª€ Tip
Use the same deployment name as your primary project for easier maintenance and
minimal changes to your system to handle redirecting your traffic.

Submit deployment job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Submit a PUT request using the following URL, headers, and JSON body to submit a
deployment job. Replace the placeholder values with your own values.
rest
{Endpoint}/language/authoring/analyze-

text/projects/{projectName}/deployments/{deploymentName}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is

myProject

NAME}

case-sensitive.

{DEPLOYMENT-

The name of your deployment. This

NAME}

value is case-sensitive.

{API-

The version of the API you're calling. The

VERSION}

value referenced is for the latest version
released. For more information, see

staging

2022-05-01

Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following JSON in the body of your request. Use the name of the model you to assign
to the deployment.
JSON
{
"trainedModelLabel": "{MODEL-NAME}"
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

trainedModelLabel

{MODEL-

The model name that is assigned to your deployment.
You can only assign successfully trained models. This

myModel

NAME}

value is case-sensitive.

Once you send your API request, you receive a 202 response indicating that the job was
submitted correctly. In the response headers, extract the operation-location value formatted
like this:
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}
{JOB-ID} is used to identify your request, since this operation is asynchronous. You can use this

URL to get the deployment status.

Get the deployment status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Use the following GET request to query the status of the deployment job. You can use the URL
you received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name of your project. This value is

myProject

NAME}

case-sensitive.

{DEPLOYMENT-

The name of your deployment. This

NAME}

value is case-sensitive.

{JOB-ID}

The ID for locating your model's training
status. It's in the location header value

staging

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

Placeholder

Value

Example

you received in the previous step.
{API-

The version of the API you're calling. The

VERSION}

value referenced is for the latest version

2022-05-01

released. For more information, see
Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded". You should get a 200 code to indicate the
success of the request.
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Changes in calling the runtime
Within your system, at the step where you call runtime prediction API

check for the response

code returned from the submitted task API. If you observe a consistent failure in submitting
the request, it could indicate an outage in your primary region. Failure once doesn't mean an
outageâ€”it may be transient issue. Retry submitting the job through the secondary resource
you create. For the second request use your {SECONDARY-ENDPOINT} , and {SECONDARY-RESOURCE-

KEY} , if you followed the previous steps, {PROJECT-NAME} and {DEPLOYMENT-NAME} would be the

same so no changes are required to the request body.
In case you revert to using your secondary resource you may observe slight increase in latency
because of the difference in regions where your model is deployed.

Check if your projects are out of sync
Maintaining the freshness of both projects is an important part of the process. You need to
frequently check if any updates were made to your primary project so that you move them
over to your secondary project. This way if your primary region fails and you move into the
secondary region you should expect similar model performance since it already contains the
latest updates. Setting the frequency of checking if your projects are in sync is an important
choice. We recommend that you do this check daily in order to guarantee the freshness of data
in your secondary model.

Get project details
Use the following url to get your project details, one of the keys returned in the body indicates
the last modified date of the project. Repeat the following step twiceâ€”once for your primary
project and again for your secondary project. Compare the timestamp returned for both of
them to check if they're out of sync.
Use the following GET request to get your project details. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-text/projects/{PROJECT-NAME}?api-version={APIVERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API

https://<your-custom-

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

myProject

NAME}

case-sensitive.

{API-

The version of the API you're calling. The
value referenced is for the latest version

VERSION}

2022-05-01

Placeholder

Value

Example

released. For more information, see
Model lifecycle.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response body
JSON
{
"createdDateTime": "2021-10-19T23:24:41.572Z",
"lastModifiedDateTime": "2021-10-19T23:24:41.572Z",
"lastTrainedDateTime": "2021-10-19T23:24:41.572Z",
"lastDeployedDateTime": "2021-10-19T23:24:41.572Z",
"projectKind": "CustomEntityRecognition",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectName": "{PROJECT-NAME}",
"multilingual": false,
"description": "Project description",
"language": "{LANGUAGE-CODE}"
}

Repeat the same steps for your replicated project using {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} . Compare the returned lastModifiedDateTime from both projects. If

your primary project was modified sooner than your secondary one, you need to repeat the
steps of exporting, importing, training, and deploying.

Next steps
In this article, you learned how to use the export and import APIs to replicate your project to a
secondary Language resource in other region. Next, explore the API reference docs to see what
else you can do with authoring APIs.
Authoring REST API reference

Runtime prediction REST API reference

Last updated on 11/18/2025

Install and run Custom Named Entity
Recognition containers
Containers enable you to host the Custom Named Entity Recognition API on your own
infrastructure using your own trained model. If you have security or data governance
requirements that can't be fulfilled by calling Custom Named Entity Recognition remotely, then
containers might be a good option.
ï¼— Note
The free account is limited to 5,000 text records per month and only the Free and
Standard pricing tiers

are valid for containers. For more information on

transaction request rates, see Data and service limits.

Prerequisites
If you don't have an Azure subscription, create a free account
Docker

.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts
A Language resource

.

with the free (F0) or standard (S) pricing tier

.

A trained and deployed Custom Named Entity Recognition model

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:

Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for Custom
Named Entity Recognition containers. Each CPU core must be at least 2.6 gigahertz (GHz) or
faster. The allowable Transactions Per Second (TPS) are also listed.
ï¾‰

Expand table

Minimum host

Recommended host

Minimum

Maximum

specs

specs

TPS

TPS

Custom Named Entity

1 core, 2 GB

1 core, 4 GB memory

15

30

Recognition

memory

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Export your Custom Named Entity Recognition
model
Before you proceed with running the docker image, you need to export your own trained
model to expose it to your container. Use the following command to extract your model and
replace the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Custom
Named Entity Recognition

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

resource. You can find it on
your resource's Key and
endpoint page, on the Azure
portal.
{ENDPOINT_URI}

The endpoint for accessing
the Custom Named Entity
Recognition API. You can find
it on your resource's Key and
endpoint page, on the Azure
portal.

https://<your-customsubdomain>.cognitiveservices.azure.com

Placeholder

Value

Format or example

{PROJECT_NAME}

The name of the project
containing the model that you

myProject

want to export. You can find it
on your projects tab in Azure
Language Studio portal.
{TRAINED_MODEL_NAME}

The name of the trained

myTrainedModel

model you want to export.
You can find your trained
models on your model
evaluation tab under your
project in Azure Language
Studio portal.

Bash
curl --location --request PUT '{ENDPOINT_URI}/language/authoring/analyzetext/projects/{PROJECT_NAME}/exported-models/{TRAINED_MODEL_NAME}?api-version=202304-15-preview' \
--header 'Ocp-Apim-Subscription-Key: {API_KEY}' \
--header 'Content-Type: application/json' \
--data-raw '{
"TrainedmodelLabel": "{TRAINED_MODEL_NAME}"
}'

Get the container image with docker pull
The Custom Named Entity Recognition container image can be found on the
mcr.microsoft.com container registry syndicate. It resides within the azure-cognitiveservices/textanalytics/ repository and is named customner . The fully qualified container

image name is, mcr.microsoft.com/azure-cognitive-services/textanalytics/customner .
To use the latest version of the container, you can use the latest tag. You can also find a full
list of tags on the MCR
Use the docker pull

.

command to download a container image from Microsoft Container

Registry.

docker pull mcr.microsoft.com/azure-cognitiveservices/textanalytics/customner:latest

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container with docker run
Once the container is on the host computer, use the docker run

command to run the

containers. The container will continue to run until you stop it.
ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove this based on your host operating
system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container won't start. For more information, see Billing.
To run the Custom Named Entity Recognition container, execute the following docker run
command. Replace the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Custom
Named Entity Recognition

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

resource. You can find it on
your resource's Key and
endpoint page, on the Azure
portal.

Placeholder

Value

Format or example

{ENDPOINT_URI}

The endpoint for accessing

https://<your-custom-

the Custom Named Entity
Recognition API. You can find
it on your resource's Key and

subdomain>.cognitiveservices.azure.com

endpoint page, on the Azure
portal.
{PROJECT_NAME}

The name of the project

myProject

containing the model that you
want to export. You can find it
on your projects tab in Azure
Language Studio portal.
{LOCAL_PATH}

The path where the exported

C:/custom-ner-model

model in the previous step
will be downloaded in. You
can choose any path of your
liking.
{TRAINED_MODEL_NAME}

The name of the trained
model you want to export.

myTrainedModel

You can find your trained
models on your model
evaluation tab under your
project in Azure Language
Studio portal.

Bash
docker run --rm -it -p5000:5000 --memory 4g --cpus 1 \
-v {LOCAL_PATH}:/modelPath \
mcr.microsoft.com/azure-cognitive-services/textanalytics/customner:latest \
EULA=accept \
BILLING={ENDPOINT_URI} \
APIKEY={API_KEY} \
projectName={PROJECT_NAME}
exportedModelName={TRAINED_MODEL_NAME}

This command:
Runs a Custom Named Entity Recognition container and downloads your exported model
to the local path specified.
Allocates one CPU core and 4 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Expand table

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
The Custom Named Entity Recognition containers send billing information to Azure, using a
Custom Named Entity Recognition resource on your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure
The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If
the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

Summary
In this article, you learned concepts and workflow for downloading, installing, and running
Custom Named Entity Recognition containers. In summary:
Custom Named Entity Recognition provides Linux containers for Docker.
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.

You can use either the REST API or SDK to call operations in Custom Named Entity
Recognition containers by specifying the host URI of the container.
You must specify billing information when instantiating a container.
ï¼‰ Important
Azure AI containers are not licensed to run without being connected to Azure for
metering. Customers need to enable the containers to communicate billing information
with the metering service at all times. Azure AI containers do not send customer data (e.g.
text that is being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

Evaluation metrics for custom named entity
recognition models
Your dataset is split into two parts: a set for training, and a set for testing. The training set is
used to train the model, while the testing set is used as a test for model after training to
calculate the model performance and evaluation. The testing set isn't introduced to the model
through the training process, to make sure that the model is tested on new data.
Model evaluation is triggered automatically after training is completed successfully. The
evaluation process starts by using the trained model to predict user defined entities for
documents in the test set, and compares them with the provided data tags (which establishes a
baseline of truth). The results are returned so you can review the model's performance. For
evaluation, custom NER uses the following metrics:
Precision: Measures how precise/accurate your model is. It's the ratio between the
correctly identified positives (true positives) and all identified positives. The precision
metric reveals how many of the predicted entities are correctly labeled.
Precision = #True_Positive / (#True_Positive + #False_Positive)

Recall: Measures the model's ability to predict actual positive classes. It's the ratio
between the predicted true positives and what was tagged. The recall metric reveals how
many of the predicted entities are correct.
Recall = #True_Positive / (#True_Positive + #False_Negatives)

F1 score: The F1 score is a function used when you seek a balance between Precision and
Recall.
F1 Score = 2 * Precision * Recall / (Precision + Recall)

ï¼— Note
Precision, recall, and F1 score are calculated for each entity separately (entity-level
evaluation) and for the model collectively (model-level evaluation).

Model-level and entity-level evaluation metrics
Precision, recall, and F1 score are calculated for each entity separately (entity-level evaluation)
and for the model collectively (model-level evaluation).

The definitions of precision, recall, and evaluation are the same for both entity-level and
model-level evaluations. However, the counts for True Positives, False Positives, and False
Negatives differ can differ. For example, consider the following text.

Example
The first party of this contract is John Smith, resident of 5678 Main Rd., City of Frederick, state of
Nebraska. And the second party is Forrest Ray, resident of 123-345 Integer Rd., City of Corona,
state of New Mexico. There's also Fannie Thomas resident of 7890 River Road, city of Colorado
Springs, State of Colorado.
The model extracting entities from this text could have the following predictions:
ï¾‰

Entity

Predicted as

Actual type

John Smith

Person

Person

Frederick

Person

City

Forrest

City

Person

Fannie Thomas

Person

Person

Colorado Springs

City

City

Expand table

Entity-level evaluation for the person entity
The model would have the following entity-level evaluation, for the person entity:
ï¾‰

Expand table

Key

Count

Explanation

True Positive

2

John Smith and Fannie Thomas were correctly predicted as person.

False Positive

1

Frederick was incorrectly predicted as person while it should be city.

False Negative

1

Forrest was incorrectly predicted as city while it should be person.

Precision: #True_Positive / (#True_Positive + #False_Positive) = 2 / (2 + 1) = 0.67
Recall: #True_Positive / (#True_Positive + #False_Negatives) = 2 / (2 + 1) = 0.67
F1 Score: 2 * Precision * Recall / (Precision + Recall) = (2 * 0.67 * 0.67) / (0.67 +
0.67) = 0.67

Entity-level evaluation for the city entity
The model would have the following entity-level evaluation, for the city entity:
ï¾‰

Expand table

Key

Count

Explanation

True Positive

1

Colorado Springs was correctly predicted as city.

False Positive

1

Forrest was incorrectly predicted as city while it should be person.

False Negative

1

Frederick was incorrectly predicted as person while it should be city.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 1) = 0.5
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.5 * 0.5) / (0.5 +
0.5) = 0.5

Model-level evaluation for the collective model
The model would have the following evaluation for the model in its entirety:
ï¾‰

Expand table

Key

Count

Explanation

True

3

John Smith and Fannie Thomas were correctly predicted as person. Colorado

Positive

False

Springs was correctly predicted as city. This number is the sum of true positives for
all entities.
2

Positive

False
Negative

Forrest was incorrectly predicted as city while it should be person. Frederick was
incorrectly predicted as person while it should be city. This number is the sum of
false positives for all entities.

2

Forrest was incorrectly predicted as city while it should be person. Frederick was
incorrectly predicted as person while it should be city. This number is the sum of
false negatives for all entities.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 3 / (3 + 2) = 0.6
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 3 / (3 + 2) = 0.6
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.6 * 0.6) / (0.6 +
0.6) = 0.6

Interpreting entity-level evaluation metrics
So what does it actually mean to have high precision or high recall for a certain entity?
ï¾‰

Expand table

Recall

Precision

Interpretation

High

High

The model identified the entity.

Low

High

The model can't always extract this entity, but when it does it is with high
confidence.

High

Low

The model extracts this entity well; however it is with low confidence as it is
sometimes extracted as another type.

Low

Low

The model doesn't identify this entity type because it isn't normally extracted. When
it is, it isn't with high confidence.

Guidance
After you trained your model, you see some guidance and recommendation on how to
improve the model. A model that covers all points in the guidance section is recommended.
Training set has enough data: When an entity type has fewer than 15 labeled examples in
the training data, the model's accuracy drops. This result occurs because it lacks sufficient
exposure to those cases. In this case, consider adding more labeled data in the training
set. You can check the data distribution tab for more guidance.
All entity types are present in test set: When the testing data lacks labeled instances for
an entity type, the model's test performance may become less comprehensive due to
untested scenarios. You can check the test set data distribution tab for more guidance.
Entity types are balanced within training and test sets: When sampling bias causes an
inaccurate representation of an entity type's frequency, it can lead to lower accuracy due
to the model expecting that entity type to occur too often or too little. You can check the
data distribution tab for more guidance.
Entity types are evenly distributed between training and test sets: When the mix of entity
types doesn't match between training and test sets, it can lead to lower testing accuracy
due to the model being trained differently from how it's being tested. You can check the
data distribution tab for more guidance.
Unclear distinction between entity types in training set: When the training data is similar
for multiple entity types, it can lead to lower accuracy because the entity types may be

frequently misclassified as each other. Review the following entity types and consider
merging them if they're similar. Otherwise, add more examples to better distinguish them
from each other. You can check the confusion matrix tab for more guidance.

Confusion matrix
A Confusion matrix is an N x N matrix used for model performance evaluation, where N is the
number of entities. The matrix compares the expected labels with the ones predicted by the
model. This matrix gives a holistic view of how well the model is performing and what kinds of
errors it's making.
You can use the Confusion matrix to identify entities that are too close to each other and often
get mistaken (ambiguity). In this case consider merging these entity types together. If that isn't
possible, consider adding more tagged examples of both entities to help the model
differentiate between them.
The highlighted diagonal in the following image is the correctly predicted entities, where the
predicted tag is the same as the actual tag.

ï Š

You can calculate the entity-level and model-level evaluation metrics from the confusion
matrix:
The values in the diagonal are the True Positive values of each entity.
The sum of the values in the entity rows (excluding the diagonal) is the false positive of
the model.
The sum of the values in the entity columns (excluding the diagonal) is the false Negative
of the model.
Similarly,

The true positive of the model is the sum of true Positives for all entities.
The false positive of the model is the sum of false positives for all entities.
The false Negative of the model is the sum of false negatives for all entities.

Next steps
Train a model

Last updated on 11/18/2025

Accepted custom NER data formats
If you're trying to import your data into custom NER, it has to follow a specific format. If you
don't have data to import, you can create your project and use Microsoft Foundry

to label

your documents.

Labels file format
Your Labels file should be in json format for use in importing your labels into a project.
JSON
{
"projectFileVersion": "2022-05-01",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "CustomEntityRecognition",
"storageInputContainerName": "{CONTAINER-NAME}",
"projectName": "{PROJECT-NAME}",
"multilingual": false,
"description": "Project-description",
"language": "en-us",
"settings": {}
},
"assets": {
"projectKind": "CustomEntityRecognition",
"entities": [
{
"category": "Entity1"
},
{
"category": "Entity2"
}
],
"documents": [
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"entities": [
{
"regionOffset": 0,
"regionLength": 500,
"labels": [
{
"category": "Entity1",
"offset": 25,
"length": 10
},
{
"category": "Entity2",

"offset": 120,
"length": 8
}
]
}
]
},
{
"location": "{DOCUMENT-NAME}",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"entities": [
{
"regionOffset": 0,
"regionLength": 100,
"labels": [
{
"category": "Entity2",
"offset": 20,
"length": 5
}
]
}
]
}
]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

multilingual

true

A boolean value that enables you to have
documents in multiple languages in your

true

dataset and when your model is deployed
you can query the model in any supported
language (not necessarily included in your
training documents). See language support
to learn more about multilingual support.
projectName

{PROJECT-

Project name

myproject

Container name

mycontainer

NAME}

storageInputContainerName

{CONTAINERNAME}

entities

Array containing all the entity types you
have in the project. Entity types extracted
from your documents.

Key

Placeholder

documents

Value

Example

Array containing all the documents in your

[]

project and list of the entities labeled within
each document.
location

{DOCUMENT-

The location of the documents in the

NAME}

storage container. Since all the documents
are in the root of the container, this location

doc1.txt

should be the document name.
dataset

{DATASET}

The test set to which this file goes to when

Train

split before training. Learn more about data
splitting here . Possible values for this field
are Train and Test .
The inclusive character position of the start

regionOffset

0

of the text.
The length of the bounding box in terms of

regionLength

500

UTF16 characters. Training only considers
the data in this region.
category

The type of entity associated with the span
of text specified.

Entity1

offset

The start position for the entity text.

25

length

The length of the entity in terms of UTF16

20

characters.
language

{LANGUAGECODE}

A string specifying the language code for
the document used in your project. If your
project is a multilingual project, choose the

en-us

language code for most of the documents.
For more information, see Language
support.

Next steps
You can import your labeled data into your project directly. Learn how to import project
See the how-to article more information about labeling your data. When you're done
labeling your data, you can train your model.

Last updated on 11/18/2025

Deploy custom language projects to
multiple regions
ï¼— Note
This article applies to the following custom features in Azure Language in Foundry Tools:
Conversational language understanding
Custom text classification
Custom named entity recognition (NER)
Orchestration workflow
Custom Language features enable you to deploy your project to more than one region. This
capability makes it much easier to access your project globally while you manage only one
instance of your project in one place. Beginning in November 2024, custom Language features
allow you to deploy your project to multiple resources within a single region using the API.
Thus, you can access and utilize your custom model wherever needed.
Before you deploy a project, you can assign deployment resources in other regions. Each
deployment resource is a different Language resource from the one that you use to author
your project. You deploy to those resources and then target your prediction requests to that
resource in their respective regions and your queries are served directly from that region.
When you create a deployment, you can select which of your assigned deployment resources
and their corresponding regions you want to deploy to. The model you deploy is then
replicated to each region and accessible with its own endpoint dependent on the deployment
resource's custom subdomain.

Example
Suppose you want to make sure your project, which is used as part of a customer support
chatbot, is accessible by customers across the United States and India. You author a project
with the name ContosoSupport by using a West US 2 Language resource named MyWestUS2 .
Before deployment, you assign two deployment resources to your project: MyEastUS and
MyCentralIndia in East US and Central India, respectively.

When you deploy your project, you select all three regions for deployment: the original West
US 2 region and the assigned ones through East US and Central India.
You now have three different endpoint URLs to access your project in all three regions:

West US 2: https://mywestus2.cognitiveservices.azure.com/language/:analyzeconversations

East US: https://myeastus.cognitiveservices.azure.com/language/:analyzeconversations

Central India: https://mycentralindia.cognitiveservices.azure.com/language/:analyzeconversations

The same request body to each of those different URLs serves the exact same response directly
from that region.

Validations and requirements
Assigning deployment resources requires Microsoft Entra authentication. Microsoft Entra ID is
used to confirm that you have access to the resources that you want to assign to your project
for multiregion deployment. In Language Studio, you can automatically enable Microsoft Entra
authentication

by assigning yourself the Azure Cognitive Services Language Owner role to

your original resource. To programmatically use Microsoft Entra authentication, learn more
from the Foundry Tools documentation.
Your project name and resource are used as its main identifiers. A Language resource can only
have a specific project name in each resource. Any other projects with the same name can't be
deployed to that resource.
For example, if a project ContosoSupport was created via the resource MyWestUS2 in West US 2
and deployed to the resource MyEastUS in East US, the resource MyEastUS can't create a
different project called ContosoSupport and deploy a project to that region. Similarly, your
collaborators can't then create a project ContosoSupport with the resource MyCentralIndia in
Central India and deploy it to either MyWestUS2 or MyEastUS .
You can only swap deployments that are available in the exact same regions. Otherwise,
swapping fails.
If you remove an assigned resource from your project, all of the project deployments to that
resource are deleted.
Some regions are only available for deployment and not for authoring projects.

Related content
Learn how to deploy models for:
Conversational language understanding

Custom text classification
Custom NER
Orchestration workflow

Last updated on 12/05/2025

Project versioning
ï¼— Note
This article applies to the following custom features in Azure Language in Foundry Tools:
Conversational language understanding
Custom text classification
Custom named entity recognition (NER)
Orchestration workflow
Building your project typically happens in increments. You may add, remove, or edit intents,
entities, labels, and data at each stage. Every time you train, a snapshot of your current project
state is taken to produce a model. That model saves the snapshot to be loaded back at any
time. Every model acts as its own version of the project.
For example, if your project has 10 intents and/or entities, with 50 training documents or
utterances, it can be trained to create a model named v1. Afterwards, you might make changes
to the project to alter the numbers of training data. The project can be trained again to create
a new model named v2. If you don't like the changes you made in v2 and would like to
continue from where you left off in model v1, then you would just need to load the model data
from v1 back into the project. Loading a model's data is possible through both Microsoft
Foundry and API. Once complete, the project has the original amount and types of training
data.
If the project data isn't saved in a trained model, it can be lost. For example, if you loaded
model v1, your project now has the data that was used to train it. If you then made changes,
didn't train, and loaded model v2, you would lose those changes as they weren't saved to any
specific snapshot.
If you overwrite a model with a new snapshot of data, you can't revert back to any previous
state of that model.
You always can locally export the data for every model.

Data location
The data for your model versions is saved in different locations, depending on the custom
feature you're using.
Custom NER

In custom named entity recognition, the data being saved to the snapshot is the labels file.

Next steps
Learn how to load or export model data for:
Conversational language understanding
Custom text classification
Custom NER
Orchestration workflow

Last updated on 12/17/2025

What is orchestration workflow?
Orchestration workflow is one of the features offered by Azure Language in Foundry Tools. This
cloud-based API service uses machine learning to facilitate the development of orchestration
models that seamlessly integrate Conversational Language Understanding (CLU) and Custom
question Answering projects. Developers can create an orchestration workflow to iteratively tag
utterances, train models, and evaluate their performance before deployment.
To simplify building and customizing your model, the service offers a custom playground that
can be accessed through the Microsoft Foundry

. You can easily get started with the service

by following the steps in this quickstart.
This documentation contains the following article types:
Quickstarts are getting-started instructions to guide you through making requests to the
service.
Concepts provide explanations of the service functionality and features.
How-to guides contain instructions for using the service in more specific or customized
ways.

Example usage scenarios
Orchestration workflow can be used in multiple scenarios across various industries. Some
examples are:

Enterprise chat bot
In a large corporation, an enterprise chat bot might handle various employee affairs. For
example, it could process frequently asked questions using a custom question answering
knowledge base. Additionally, it might manage calendar-specific operations through
conversational language understanding capabilities. The bot could also handle interview
feedback processing. To support these diverse functions, the bot needs to appropriately route
incoming requests to the correct service. Orchestration workflow allows you to connect those
skills to one project that handles the routing of incoming requests appropriately to power the
enterprise bot.

Project development lifecycle
Creating an orchestration workflow project typically involves several different steps.

ï Š

Follow these steps to get the most out of your model:
1. Define your schema: Know your data and define the actions and relevant information
that needs to be recognized from user's input utterances. Create the intents that you
want to assign to user's utterances and the projects you want to connect to your
orchestration project.
2. Label your data: The quality of data tagging is a key factor in determining model
performance.
3. Train a model: Your model starts learning from your tagged data.
4. View the model's performance: View the evaluation details for your model to determine
how well it performs when introduced to new data.
5. Improve the model: After reviewing the model's performance, you can then learn how
you can improve the model.
6. Deploy the model: Deploying a model makes it available for use via the prediction API.
7. Predict intents: Use your custom model to predict intents from user's utterances.

Reference documentation and code samples
As you use orchestration workflow, see the following reference documentation and samples for
Azure Language in Foundry Tools:
ï¾‰

Development option / language

Reference documentation

REST APIs (Authoring)

REST API documentation

REST APIs (Runtime)

REST API documentation

Samples

Expand table

Development option / language

Reference documentation

Samples

C# (Runtime)

C# documentation

C# samples

Python (Runtime)

Python documentation

Python samples

Responsible AI
An AI system includes not only the technology, but also the people who use it, the people who
it affects, and the deployment environment. Read the transparency note for CLU and
orchestration workflow to learn about responsible AI use and deployment in your systems.
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Next steps
Use the quickstart article to start using orchestration workflow.
As you go through the project development lifecycle, review the glossary to learn more
about the terms used throughout the documentation for this feature.
Remember to view the service limits for information such as regional availability.

Last updated on 01/13/2026

Quickstart: Orchestration workflow
Orchestration workflow, offered by Azure Language in Foundry Tools, enables you to integrate
multiple language modelsâ€”such as Conversational Language Understanding (CLU) and
Custom question answering (CQA)â€”into a single project. This functionality intelligently routes
user requests to the most suitable model via a unified endpoint, providing seamless and
sophisticated conversational experiences across various language services tasks.
This quickstart walks you through the essentials of working with orchestration workflow
projects Following each step builds a strong foundation in the core concepts. Completing this
quickstart provides you with hands-on experience, preparing you to confidently tackle
orchestration workflow projects in your own environment.

Prerequisites
ï¼— Note
If you already have an Azure Language in Foundry Tools or multi-service resource
you can continue to use those existing Language resources within the Microsoft
Foundry portal via a Foundry Hub project.
For more information, see How to use Foundry Tools in the Foundry portal.
We highly recommended that you use a Foundry resource in the Foundry; however,
you can also follow these instructions using a Language resource.
An Azure subscription. If you don't have one, you can create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
A Foundry resource (recommended). For more information, see Configure a Foundry
resource. Alternately, you can use a Language resource

.

A Foundry project created in the Foundry. For more information, see Create a Foundry
project.
A Conversational language understanding (CQA) or Custom question answering (CQA)
project created in the Foundry.

Get started

After you create your Foundry resource, you can initiate an orchestration workflow project in
the Microsoft Foundry

. This project serves as a dedicated workspace for developing custom

machine learning models using your data. Access to the project is restricted to you and others
who have permissions for the associated Foundry resource.
For this quickstart, you can complete the Conversational Language Understanding quickstart
or Custom question answering (CQA) to establish a project for use in our subsequent
orchestration workflow quickstart steps.
Let's begin:
1. Navigate to the Foundry.
2. If you aren't already signed in, the portal prompts you to do so with your Azure
credentials.
3. Once signed in, you can create or access your existing projects within Foundry.
4. If you're not already in your CLU or CQA project, select it now.

Create an orchestration workflow project
1. Select Fine-tuning from the left navigation pane.
2. From the window that appears, select the AI Service fine-tuning tab and then the + Finetune button.

3. From the window that appears, select Conversational Orchestration Workflow as the task
type, then select Next.

4. In the Create service fine-tuning window, you can choose to create a new task or import
an existing one. Complete all required fields, then select Create:
Name: Provide a unique name for your orchestration workflow project.
Language: Select the language for your project.
Description: Optionally, provide a description for your project.
5. After creating the orchestration workflow project, you'll be directed to the project
overview page. Here, you can manage your project settings, monitor training progress,
and access various tools to enhance your model.

Link tasks
1. To add existing CLU or CQA models to your orchestration workflow, navigate to the Link
tasks button within your project. Here, you can add intents and entities from your existing
models to the orchestration workflow.

ï Š

2. Link your tasks by selecting from the Task type and Fine-tuning task name dropdown
menus. The Intent name field automatically populates with the same name as the finetuning task name field. Once everything is set, select Add to continue.

Add training data
1. Navigate to the Manage Data tab and add your utterances file. For this project, you can
download our sample utterances file

which comes preconfigured with labeled

utterances.
2. After uploading your utterances file, select the unlinked intents from the Insights pane.
This action allows you to map these intents to the appropriate linked tasks within your
orchestration workflow.

ï Š

Train your model

1. Navigate to the Train model section and select the Train model button to start training
your orchestration workflow with the linked tasks and uploaded utterances. This process
may take some time depending on the size of your dataset and the complexity of your
model.

2. In the Train a new model window, provide a name for your model, keep the default
standard training mode, and select Next to proceed.

3. In the data splitting window, you can choose to either use the default data split or
customize it according to your needs. After making your selection, select Next to
continue.
4. Review your selections in the summary window, and if everything looks correct, select
Create to initiate the training process for your orchestration workflow model.

5. After initiating the training process, you can monitor the progress and view detailed
metrics on the training dashboard. Once the training is complete, your orchestration
workflow model is ready for deployment and testing.

Deploy your model
Deploy your trained model by navigating to the Deploy model section and selecting the
Deploy button. Follow the prompts to complete the deployment process.

Test your model
After your model successfully deploys, you can test it directly within the Foundry interface.
Navigate to the Test in playground section, input various utterances, and observe how your
orchestration workflow routes requests to the appropriate linked tasks.
That's it, congratulations!

Clean up resources
If you no longer need your project, you can delete it from the Foundry.
1. Navigate to the Foundry

home page. Initiate the authentication process by signing in,

unless you already completed this step and your session is active.
2. Select the project that you want to delete from the Keep building with Foundry
3. Select Management center.
4. Select Delete project.

Next steps
Learn about orchestration workflows

Last updated on 01/12/2026

Frequently asked questions for
orchestration workflows
Use this article to quickly get the answers to common questions about orchestration workflows

How do I create a project?
For more information, see the quickstart to quickly create your first project, or the how-to
article.

How do I connect other service applications in
orchestration workflow projects?
For more information, see How to create projects and build schemas.

Which LUIS applications can I connect to in
orchestration workflow projects?
LUIS applications that use Azure Language resource as their authoring resource are available

for connection. You can only connect to LUIS applications that are owned via the same
resource. This option is only available for resources in West Europe, as it's the only common
available region between LUIS and CLU .

Which question answering project can I connect to
in orchestration workflow projects?
Question answering projects that use Azure Language resource are available for connection.
You can only connect to question answering projects that are in the same Language resource.

Training is taking a long time, is this time period
expected?
For orchestration projects, long training times are expected. Based on the number of examples
you have your training times may vary from 5 minutes to 1 hour or more.

Can I add entities to orchestration workflow
projects?
No. Orchestration projects are only enabled for intents that can be connected to other projects
for routing.

How do I get more accurate results for my project?
For more information, see evaluation metrics.

Can I label the same word as two different entities?
Unlike LUIS , you can't label the same text as two different entities. Learned components across
different entities are mutually exclusive, and only one learned span is predicted for each set of
characters.

Is there any SDK support?
Yes, only for predictions, and samples are available . There's currently no authoring support
for the SDK.

Are there APIs for this feature?
Yes, all the APIs are available.
Authoring APIs
Prediction API

Next steps
Orchestration workflow overview

Last updated on 12/18/2025

Language support for orchestration
workflow projects
Use this article to learn about the languages currently supported by orchestration workflow
projects.

Multilingual options
Orchestration workflow projects do not support the multi-lingual option.

Language support
Orchestration workflow projects support the following languages:
ï¾‰

Language

Language code

German

de

English

en-us

Spanish

es

French

fr

Italian

it

Portuguese (Brazil)

pt-br

Next steps
Orchestration workflow overview
Service limits

Last updated on 11/18/2025

Expand table

How to create projects in orchestration
workflow
Orchestration workflow allows you to create projects that connect your applications to:
Custom Language Understanding
Question Answering
LUIS

Prerequisites
Before you start using orchestration workflow, you will need several things:
An Azure subscription - Create one for free

.

An Azure Language in Foundry Tools resource

Create a Language resource
Before you start using orchestration workflow, you will need a Language resource.
ï¼— Note
You need to have an owner role assigned on the resource group to create a
Language resource.
If you're planning to use question answering, you have to enable question answering
in resource creation

Create a new resource from the Azure portal
1. Go to the Azure portal

to create a new Azure Language in Foundry Tools resource.

2. Select Continue to create your resource
3. Create a Language resource with following details.
ï¾‰

Instance detail

Required value

Region

One of the supported regions.

Expand table

Instance detail

Required value

Name

A name for your Language resource.

Pricing tier

One of the supported pricing tiers.

Create an orchestration workflow project (REST
API)
Once you have a Language resource created, create an orchestration workflow project.
Submit a PATCH request using the following URL, headers, and JSON body to create a new
project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}?apiversion={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

myProject

NAME}

value is case-sensitive.

{API-

The version of the API you're
calling.

VERSION}

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following sample JSON as your body.
JSON
{
"projectName": "{PROJECT-NAME}",
"language": "{LANGUAGE-CODE}",
"projectKind": "Orchestration",
"description": "Project description"
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

projectName

{PROJECT-

The name of your project. This value is case-sensitive.

EmailApp

A string specifying the language code for the utterances used
in your project. If your project is a multilingual project, choose

en-us

NAME}
language

{LANGUAGECODE}

the language code for most of the utterances.

Import an orchestration workflow project (REST
API)
You can import an orchestration workflow JSON into the service
Submit a POST request using the following URL, headers, and JSON body to import your
project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest

{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:import?
api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This
value is case-sensitive.

{API-

The version of the API you're

2023-04-01

VERSION}

calling.

{PROJECT-

Expand table

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
ï¼— Note
Each intent should only be of one type only from (CLU,LUIS and qna)
Use the following sample JSON as your body.
JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "Orchestration",
"settings": {
"confidenceThreshold": 0
},
"projectName": "{PROJECT-NAME}",

"description": "Project description",
"language": "{LANGUAGE-CODE}"
},
"assets": {
"projectKind": "Orchestration",
"intents": [
{
"category": "string",
"orchestration": {
"kind": "luis",
"luisOrchestration": {
"appId": "00001111-aaaa-2222-bbbb-3333cccc4444",
"appVersion": "string",
"slotName": "string"
},
"cluOrchestration": {
"projectName": "string",
"deploymentName": "string"
},
"qnaOrchestration": {
"projectName": "string"
}
}
}
],
"utterances": [
{
"text": "Trying orchestration",
"language": "{LANGUAGE-CODE}",
"intent": "string"
}
]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

api-version

{API-

2022-03-01-

VERSION}

The version of the API you're calling. The version used here
must be the same API version in the URL.

preview

{PROJECT-

The name of your project. This value is case-sensitive.

EmailApp

A string specifying the language code for the utterances
used in your project. If your project is a multilingual

en-us

projectName

NAME}
language

{LANGUAGECODE}

project, choose the language code for most of the
utterances.

Export project (REST API)
You can export an orchestration workflow project as a JSON file at any time.
Create a POST request using the following URL, headers, and JSON body to export your
project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

EmailApp

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest

{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the exported project JSON, using the same authentication method.

Get orchestration project details (REST API)
To get an orchestration workflow project's details, submit a GET request using the following
URL and headers. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}?apiversion={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

myProject

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.

JSON
{
"createdDateTime": "2022-04-18T13:53:03Z",
"lastModifiedDateTime": "2022-04-18T13:53:03Z",
"lastTrainedDateTime": "2022-04-18T14:14:28Z",
"lastDeployedDateTime": "2022-04-18T14:49:01Z",
"projectKind": "Orchestration",
"projectName": "{PROJECT-NAME}",
"description": "This is a sample orchestration project.",
"language": "{LANGUAGE-CODE}"
}

Once you send your API request, you receive a 200 response indicating success and JSON
response body with your project details.

Delete project (REST API)
When you don't need your project anymore, you can delete your project using the APIs.
Create a DELETE request using the following URL, headers, and JSON body to delete a
conversational language understanding project.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}?apiversion={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

myProject

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.

ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success, which means
your project is deleted.

ï Š

Next Steps
Build schema

Last updated on 12/18/2025

How to build your project schema for
orchestration workflow
In orchestration workflow projects, the schema is defined as the combination of intents within
your project. Schema design is a crucial part of your project's success. When creating a schema,
you should think about the intents to include in your project.

Guidelines and recommendations
Consider the following guidelines and recommendations for your project:
Build orchestration projects when you need to manage the NLU for a multi-faceted virtual
assistant or chatbot.
Orchestrate between different domains. A domain is a collection of intents and entities
that serve the same purpose, such as Email commands vs. Restaurant commands.
If there's an overlap of similar intents between domains, create the common intents in a
separate domain and removing them from the others for the best accuracy.
For intents that are general across domains, such as Greeting , Confirm , or Reject , you
can either add them in a separate domain or as direct intents in the Orchestration project.
Orchestrate to Custom question answering knowledge base when a domain has FAQ type
questions with static answers. Ensure that the vocabulary and language used to ask
questions is distinctive from the one used in the other Conversational Language
Understanding projects and LUIS applications.
If an utterance is being misclassified and routed to an incorrect intent, then add similar
utterances to the intent to influence its results. If the intent is connected to a project, then
add utterances to the connected project itself. After you retrain your orchestration
project, the new utterances in the connected project will influence predictions.
Add test data to your orchestration projects to validate there isn't confusion between
linked projects and other intents.

Next steps
Add utterances

Last updated on 12/18/2025

Add utterances
Once you build a schema, you should add training and testing utterances to your project. The
utterances should be similar to what your users use when interacting with the project. When
you add an utterance, you have to assign which intent it belongs to.
Adding utterances is a crucial step in project development lifecycle; this data is used in the next
step when training your model so that your model can learn from the added data. If you
already have utterances, you can directly import it into your project, but you need to make sure
that your data follows the accepted data format. Labeled data informs the model how to
interpret text, and is used for training and evaluation.

Prerequisites
A successfully created project.
For more information, see project development lifecycle.

How to add utterances
Use the following steps to add utterances:
1. Go to your project page in Language Studio

.

2. From the left side menu, select Add utterances.
3. From the top pivots, you can change the view to be training set or testing set. Learn
more about training and testing sets and how they're used for model training and
evaluation.
4. From the Select intent dropdown menu, select one of the intents. Type in your utterance,
and select the enter key in the utterance's text box to add the utterance. You can also
upload your utterances directly by selecting Upload utterance file from the top menu.
Make sure the utterances follow the accepted format.
ï¼— Note
If you're planning on using Automatically split the testing set from training data
splitting, add all your utterances to the training set. You can add training utterances
to nonconnected intents only.

ï Š

5. Under Distribution you can view the distribution across training and testing sets. You can
view utterances per intent:
Utterance per nonconnected intent
Utterances per connected intent

Next Steps
Train Model

Last updated on 12/18/2025

Train your orchestration workflow model
Training is the process where the model learns from your labeled utterances. After training is
completed, you can view model performance.
To train a model, start a training job. Only successfully completed jobs create a model. Training
jobs expire after seven days, after this time you can no longer be able to retrieve the job
details. If your training job completed successfully and a model is created, it isn't affected if the
job expires. You can only have one training job running at a time, and you can't start other jobs
in the same project.
The training times can be anywhere from a few seconds when dealing with simple projects, up
to a couple of hours when you reach the maximum limit of utterances.
Model evaluation is triggered automatically after training is completed successfully. The
evaluation process starts by using the trained model to run predictions on the utterances in the
testing set, and compares the predicted results with the provided labels (which establishes a
baseline of truth). The results are returned so you can review the model's performance.

Prerequisites
A successfully created project with a configured Azure blob storage account
See the project development lifecycle.

Data splitting
Before you start the training process, labeled utterances in your project are divided into a
training set and a testing set. Each one of them serves a different function. The training set is
used in training the model. It's the set from which the model learns the labeled utterances. The
testing set is a blind set that isn't introduced to the model during training but only during
evaluation.
After the model is trained successfully, the model can be used to make predictions from the
utterances in the testing set. These predictions are used to calculate evaluation metrics.
We recommend that all your intents are adequately represented in both the training and
testing set.
Orchestration workflow supports two methods for data splitting:
Automatically splitting the testing set from training data: The system splits your tagged
data between the training and testing sets, according to the percentages you choose. The

recommended percentage split is 80% for training and 20% for testing.
ï¼— Note
If you choose the Automatically splitting the testing set from training data option, only
the data assigned to training set is split according to the percentages provided.
Use a manual split of training and testing data: This method enables users to define
which utterances should belong to which set. This step is only enabled if you added
utterances to your testing set during labeling.
ï¼— Note
You can only add utterances in the training dataset for non-connected intents only.

Train model
Start training job
Create a POST request using the following URL, headers, and JSON body to submit a training
job.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:train?
api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

EmailApp

NAME}

value is case-sensitive.

Placeholder

Value

Example

{API-

The version of the API you're

2023-04-01

VERSION}

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following object in your request. The model will be named MyModel once training is
complete.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingMode": "standard",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"testingSplitPercentage": 20,
"trainingSplitPercentage": 80
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-NAME}

Your Model name.

Model1

trainingMode

standard

Training mode. Only one mode for training is

standard

available in orchestration, which is standard .
trainingConfigVersion

{CONFIG-

The training configuration model version. By

VERSION}

default, the latest model version is used.

2022-05-01

Key

Placeholder

Value

Example

kind

percentage

Split methods. Possible values are percentage

percentage

or manual . See how to train a model for more
information.
trainingSplitPercentage

80

Percentage of your tagged data to be included
in the training set. Recommended value is 80 .

80

testingSplitPercentage

20

Percentage of your tagged data to be included
in the testing set. Recommended value is 20 .

20

ï¼— Note
The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set
to percentage and the sum of both percentages should be equal to 100.
Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to get the training job status.

Get training job status
Training could take sometime depending on the size of your training data and complexity of
your schema. You can use the following request to keep polling the status of the training job
until it successfully completes.
Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{YOUR-

The endpoint for authenticating your API
request.

https://<your-custom-

ENDPOINT}

Expand table

subdomain>.cognitiveservices.azure.com
EmailApp

NAME}

The name for your project. This value is
case-sensitive.

{JOB-ID}

The ID for locating your model's training

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

status. It's in the location header value
you received when submitted your
training job.
{API-

The version of the API you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".
JSON
{
"result": {
"modelLabel": "{MODEL-LABEL}",
"trainingConfigVersion": "{TRAINING-CONFIG-VERSION}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,

"status": "notStarted"
}
},
"jobId": "xxxxxx-xxxxx-xxxxxx-xxxxxx",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

ï¾‰

Key

Value

Example

modelLabel

The model name

Model1

trainingConfigVersion

The training configuration version. By default, the
latest version is used.

2022-05-01

startDateTime

The time training started

2022-04-

Expand table

14T10:23:04.2598544Z
status

The status of the training job

running

estimatedEndDateTime

Estimated time for the training job to finish

2022-0414T10:29:38.2598544Z

jobId

Your training job ID

xxxxx-xxxx-xxxx-xxxxxxxxxxxxx

createdDateTime

Training job creation date and time

2022-04-14T10:22:42Z

lastUpdatedDateTime

Training job last updated date and time

2022-04-14T10:23:45Z

expirationDateTime

Training job expiration date and time

2022-04-14T10:22:42Z

Cancel training job
Create a POST request using the following URL, headers, and JSON body to cancel a training
job.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest

{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}/:cancel?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com
EmailApp

NAME}

The name for your project. This
value is case-sensitive.

{JOB-ID}

This is the training job ID

XXXXX-XXXXX-XXXX-XX

{API-

The version of the API you're
calling.

2023-04-01

{PROJECT-

VERSION}

Expand table

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success, which means
your training job has been canceled. A successful call results with an Operation-Location
header used to check the status of the job.

Next steps
Model evaluation metrics concepts
How to deploy a model

Last updated on 12/18/2025

View orchestration workflow model details
After model training is completed, you can view your model details and see how well it
performs against the test set. Observing how well your model performed is called evaluation.
The test set consists of data that wasn't introduced to the model during the training process.
ï¼— Note
Using the Automatically split the testing set from training data option may result in
different model evaluation result every time you train a new model, as the test set is
selected randomly from your utterances. To make sure that the evaluation is calculated on
the same test set every time you train a model, make sure to use the Use a manual split of
training and testing data option when starting a training job and define your Testing set
when add your utterances.

Prerequisites
Before viewing a model's evaluation, you need:
An orchestration workflow project.
A successfully trained model
See the project development lifecycle for more information.

Model details
Create a GET request using the following URL, headers, and JSON body to get the trained
model evaluation summary.

Request URL
rest
{ENDPOINT}/language/authoring/analyzeconversations/projects/{projectName}/models/{trainedModelLabel}/evaluation/summaryresult?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for

https://<your-custom-

authenticating your API
request.

subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project.
This value is case-sensitive.

myProject

{trainedModelLabel}

The name for your trained
model. This value is casesensitive.

Model1

{API-VERSION}

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response.
JSON
{
"entitiesEvaluation": {
"confusionMatrix": {
"additionalProp1": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}

},
"additionalProp2": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp3": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
}
},
"entities": {
"additionalProp1": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp2": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp3": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,

"falsePositivesCount": 0,
"falseNegativesCount": 0
}
},
"microF1": 0,
"microPrecision": 0,
"microRecall": 0,
"macroF1": 0,
"macroPrecision": 0,
"macroRecall": 0
},
"intentsEvaluation": {
"confusionMatrix": {
"additionalProp1": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp2": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
},
"additionalProp3": {
"additionalProp1": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp2": {
"normalizedValue": 0,
"rawValue": 0
},
"additionalProp3": {
"normalizedValue": 0,
"rawValue": 0
}
}

},
"intents": {
"additionalProp1": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp2": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
},
"additionalProp3": {
"f1": 0,
"precision": 0,
"recall": 0,
"truePositivesCount": 0,
"trueNegativesCount": 0,
"falsePositivesCount": 0,
"falseNegativesCount": 0
}
},
"microF1": 0,
"microPrecision": 0,
"microRecall": 0,
"macroF1": 0,
"macroPrecision": 0,
"macroRecall": 0
},
"evaluationOptions": {
"kind": "percentage",
"trainingSplitPercentage": 0,
"testingSplitPercentage": 0
}
}

Load or export model data
Load model data
Create a POST request using the following URL, headers, and JSON body to load your model
data to your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/models/{MODEL-NAME}:load-snapshot?stringIndexType=Utf16CodeUnit&api-version=
{API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

EmailApp

2022-10-01-preview

VERSION}

The version of the API you're
calling.

{MODEL-NAME}

The name of your model. This

v1

{PROJECTNAME}
{API-

Expand table

value is case-sensitive.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/models/{MODEL-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the status of your model data loading, using the same authentication method.

Export model data
Create a POST request using the following URL, headers, and JSON body to export your model
data.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}&trainedModelLabel={MODELNAME}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com
EmailApp

NAME}

The name for your project. This
value is case-sensitive.

{API-

The version of the API you're

2022-10-01-preview

VERSION}

calling.

{MODEL-NAME}

The name of your model. This
value is case-sensitive.

{PROJECT-

v1

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the exported project JSON, using the same authentication method.

Delete model
Create a DELETE request using the following URL, headers, and JSON body to delete a model.

Request URL
rest
{ENDPOINT}/language/authoring/analyzeconversations/projects/{projectName}/models/{trainedModelLabel}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

Example

{YOUR-ENDPOINT}

The endpoint for
authenticating your API
request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project.
This value is case-sensitive.

myProject

{trainedModelLabel}

The name for your model

model1

name. This value is casesensitive.
{API-VERSION}

Headers

The version of the API you're
calling.

Expand table

2023-04-01

Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 204 response indicating success, which means
your model is deleted.

Next steps
As you review how your model performs, learn about the evaluation metrics that are
used.
If you're happy with your model performance, you can deploy your model

Last updated on 12/18/2025

Deploy an orchestration workflow model
Once you're satisfied with how your model performs, it's ready to be deployed, and query it for predictions from utterances. Deploying a model
makes it available for use through the prediction API

.

Prerequisites
A successfully created project
Labeled utterances and successfully trained model
For more information, see project development lifecycle.

Deploy model
After you reviewed the model's performance and decide it's a good fit for use in your environment, you need to assign it to a deployment to be
able to query it. Assigning the model to a deployment makes it available for use through the prediction API

. We recommended that you create

a deployment named production to which you assign the best model you built so far and use it in your system. You can create another
deployment called staging to which you can assign the model you're currently working on to be able to test it. You can have a maximum on 10
deployments in your project.

Submit deployment job
Create a PUT request using the following URL, headers, and JSON body to start deploying an orchestration workflow model.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{DEPLOYMENT-NAME}

The name for your deployment. This value is case-sensitive.

staging

{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

ï¾‰

Expand table

ï¾‰

Expand table

Request Body
JSON
{
"trainedModelLabel": "{MODEL-NAME}",
}

Key
trainedModelLabel

Placeholder

Value

Example

{MODEL-

The model name that is assigned to your deployment. You can only assign successfully trained models. This value is

myModel

NAME}

case-sensitive.

Once you send your API request, you receive a 202 response indicating success. In the response headers, extract the operation-location value
formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?apiversion={API-VERSION}

You can use this URL to get the deployment job status.

Get deployment job status
Use the following GET request to get the status of your deployment job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?apiversion={API-VERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{DEPLOYMENT-

The name for your deployment. This value is case-sensitive.

staging

The ID for locating your model's training status. It's in the location header value you

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

NAME}
{JOB-ID}

received from the API in response to your model deployment request.
{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Swap deployments
After you're done testing a model assigned to one deployment, you might want to assign it to another deployment. Swapping deployments
involves:
Taking the model assigned to the first deployment, and assigning it to the second deployment.
taking the model assigned to second deployment and assign it to the first deployment.
This step can be used to swap your production and staging deployments when you want to take the model assigned to staging and assign it to
production .

Create a POST request using the following URL, headers, and JSON body to start a swap deployments job.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/:swap?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

ï¾‰

Expand table

ï¾‰

Expand table

Request Body
JSON
{
"firstDeploymentName": "{FIRST-DEPLOYMENT-NAME}",
"secondDeploymentName": "{SECOND-DEPLOYMENT-NAME}"
}

Key

Placeholder

Value

Example

firstDeploymentName

{FIRST-DEPLOYMENT-NAME}

The name for your first deployment. This value is case-sensitive.

production

secondDeploymentName

{SECOND-DEPLOYMENT-NAME}

The name for your second deployment. This value is case-sensitive.

staging

Once you send your API request, you receive a 202 response indicating success.

Delete deployment
Create a DELETE request using the following URL, headers, and JSON body to delete a conversational language understanding deployment.

Request URL

rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{projectName}/deployments/{deploymentName}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{DEPLOYMENT-NAME}

The name for your deployment name. This value is case-sensitive.

staging

{API-VERSION}

The version of the API you're calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Once you send your API request, you receive a 202 response indicating success, which means your deployment is deleted.

Assign deployment resources
You can deploy your project to multiple regions by assigning different Language resources that exist in different regions.
Assigning deployment resources programmatically requires Microsoft Entra authentication**. Microsoft Entra ID is used to confirm you have
access to the resources you're interested in assigning to your project for multi-region deployment. To programmatically use Microsoft Entra
authentication when making REST API calls, see the Foundry Tools authentication documentation.

Assign resource
Submit a POST request using the following URL, headers, and JSON body to assign deployment resources.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/:assign?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2022-10-01-preview

Headers
Use Microsoft Entra authentication to authenticate this API.

Body

Use the following sample JSON as your body.
JSON
{
"resourcesMetadata": [
{
"azureResourceId": "{AZURE-RESOURCE-ID}",
"customDomain": "{CUSTOM-DOMAIN}",
"region": "{REGION-CODE}"
}
]
}

ï¾‰

Key
azureResourceId

Expand table

Placeholder

Value

{AZURE-

The full resource ID path you want to

/subscriptions/aaaa0a0a-bb1b-cc2c-dd3d-

RESOURCE-

assign. Found in the Azure portal under
the Properties tab for the resource,

eeeeee4e4e4e/resourceGroups/ContosoResourceGroup/providers/Microsoft.CognitiveServices

ID}

Example

within the Resource ID field.
customDomain

{CUSTOM-

The custom subdomain of the resource

DOMAIN}

you want to assign. Found in the Azure

contosoresource

portal under the Keys and Endpoint tab
for the resource, part of the Endpoint
field in the URL https://<your-customsubdomain>.cognitiveservices.azure.com/
region

{REGION-

A region code specifying the region of

CODE}

the resource you want to assign. Found in
the Azure portal under the Keys and

eastus

Endpoint tab for the resource, as part of
the Location/Region field.

Get assign resource status
Use the following GET request to get the status of your assign deployment resource job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/assign/jobs/{JOB-ID}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is case-sensitive.

myProject

The job ID for getting your assign deployment status. It's in the operation-location header

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

NAME}
{JOB-ID}

value you received from the API in response to your assign deployment resource request.
{API-

The version of the API you're calling.

2022-10-01-preview

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Unassign deployment resources
When unassigning or removing a deployment resource from a project, you can also delete all the deployments that are deployed to that resource
region.

Unassign resource
Submit a POST request using the following URL, headers, and JSON body to unassign or remove deployment resources from your project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/:unassign?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API request.

https://<your-custom-subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This value is case-sensitive.

myProject

{API-VERSION}

The version of the API you're calling.

2022-10-01-preview

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
Use the following sample JSON as your body.
JSON
{
"assignedResourceIds": [
"{AZURE-RESOURCE-ID}"

Expand table

]
}

ï¾‰

Key
assignedResourceIds

Expand table

Placeholder

Value

Example

{AZURE-

The full
resource

eeeeee4e4e4e/resourceGroups/ContosoResourceGroup/providers/Microsoft.CognitiveServices/accounts/ContosoResource

RESOURCEID}

/subscriptions/aaaa0a0a-bb1b-cc2c-dd3d-

ID path
you want
to
unassign.
Found in
the Azure
portal
under the
Properties
tab for
the
resource
as the
Resource
ID field.

Get unassign resource status
Use the following GET request to get the status of your unassign deployment resources job. Replace the placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/resources/unassign/jobs/{JOB-ID}?api-version={APIVERSION}

ï¾‰

Placeholder

Value

{ENDPOINT}

The endpoint for authenticating your API request.

Expand table

Example
https://<your-customsubdomain>.cognitiveservices.azure.com

The name for your project. This value is case-sensitive.

myProject

{JOB-ID}

The job ID for getting your assign deployment status. It's in the operation-location header
value you received from the API in response to your unassign deployment resource request.

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{API-

The version of the API you're calling.

2022-10-01-preview

{PROJECTNAME}

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Expand table

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until the status parameter changes to "succeeded".
JSON

{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Next steps
Use prediction API to query your model

Last updated on 12/18/2025

Query deployment for intent predictions
After the deployment is added successfully, you can query the deployment for intent and
entities predictions from your utterance based on the model you assigned to the deployment.
You can query the deployment programmatically Prediction API

or through the Client

libraries (Azure SDK).

Test deployed model
First you need to get your resource key and endpoint:
Go to your resource overview page in the Azure portal

. From the menu on the left side,

select Keys and Endpoint. You will use the endpoint and key for API requests.

ï Š

Query your model
Create a POST request using the following URL, headers, and JSON body to start testing an
orchestration workflow model.

Request URL
rest
{ENDPOINT}/language/:analyze-conversations?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

{API-

The version of the API you're

2023-04-01

VERSION}

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request Body
JSON
{
"kind": "Conversation",
"analysisInput": {
"conversationItem": {
"text": "Text1",
"participantId": "1",
"id": "1"
}
},
"parameters": {
"projectName": "{PROJECT-NAME}",
"deploymentName": "{DEPLOYMENT-NAME}",
"directTarget": "qnaProject",
"targetProjectParameters": {
"qnaProject": {
"targetProjectKind": "QuestionAnswering",
"callingOptions": {
"context": {
"previousUserQuery": "Meet Surface Pro 4",
"previousQnaId": 4
},
"top": 1,
"question": "App Service overview"
}
}
}

}
}

Response Body
Once you send the request, you get the following response for the prediction!
JSON
{
"kind": "ConversationResult",
"result": {
"query": "App Service overview",
"prediction": {
"projectKind": "Orchestration",
"topIntent": "qnaTargetApp",
"intents": {
"qnaTargetApp": {
"targetProjectKind": "QuestionAnswering",
"confidenceScore": 1,
"result": {
"answers": [
{
"questions": [
"App Service overview"
],
"answer": "The compute resources you use are determined by the *App
Service plan* that you run your apps on.",
"confidenceScore": 0.7384000000000001,
"id": 1,
"source": "https://learn.microsoft.com/azure/app-service/overview",
"metadata": {},
"dialog": {
"isContextOnly": false,
"prompts": []
}
}
]
}
}
}
}
}
}

Client libraries (Azure SDK)

First you need to get your resource key and endpoint:

Go to your resource overview page in the Azure portal

. From the menu on the left side,

select Keys and Endpoint. You will use the endpoint and key for API requests.

ï Š

Use the client libraries (Azure SDK)
You can also use the client libraries provided by the Azure SDK to send requests to your
model.
ï¼— Note
The client library for conversational language understanding is only available for:
.NET
Python
1. Go to your resource overview page in the Azure portal
2. From the menu on the left side, select Keys and Endpoint. Use endpoint for the API
requests and you will need the key for Ocp-Apim-Subscription-Key header.

ï Š

3. Download and install the client library package for your language of choice:
ï¾‰

Language

Package version

.NET

1.0.0

Python

1.0.0

Expand table

4. After you've installed the client library, use the following samples on GitHub to start
calling the API.
C#
Python
5. See the following reference documentation for more information:
C#
Python

Next steps
Orchestration workflow overview

Last updated on 12/18/2025

Back up and recover your conversational
language understanding models
When you create a Language resource in the Azure portal, you specify a region for it to be
created in. From then on, your resource and all of the operations related to it take place in the
specified Azure server region. It's rare, but not impossible, to encounter a network issue that
hits an entire region. If your solution needs to always be available, then you should design it to
either fail-over into another region. This process requires two Azure Language in Foundry Tools
resources in different regions and the ability to sync your CLU models across regions.
If your app or business depends on the use of a CLU model, we recommend that you create a
replica of your project into another supported region. So that if a regional outage occurs, you
can then access your model in the other fail-over region where you replicated your project.
Replicating a project means that you export your project metadata and assets and import them
into a new project. This action only makes a copy of your project settings, intents, entities, and
utterances. You still need to train and deploy the models to be available for use with runtime
APIs

.

In this article, you learn to use the export and import APIs to replicate your project from one
resource to another existing in different supported geographical regions. We also provide
guidance for keeping your projects in sync and the changes needed to your runtime
consumption.

Prerequisites
Two Language resources in different Azure regions, each of them in a different region.

Get your resource keys endpoint
Use the following steps to get the keys and endpoint for your primary and secondary
resources.
Go to your resource overview page in the Azure portal

. From the menu on the left side,

select Keys and Endpoint. You use the endpoint and key for the API requests

ï Š

îª€ Tip
Keep a note of keys and endpoints for both primary and secondary resources. Use these
values to replace the following placeholders: {PRIMARY-ENDPOINT} , {PRIMARY-RESOURCEKEY} , {SECONDARY-ENDPOINT} , and {SECONDARY-RESOURCE-KEY} . Also take note of your project

name, your model name, and your deployment name. Use these values to replace the
following placeholders: {PROJECT-NAME} , {MODEL-NAME} , and {DEPLOYMENT-NAME} .

Export your primary project assets
Start by exporting the project assets from the project in your primary resource.

Submit export job
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Create a POST request by using the following URL, headers, and JSON body to export your
project.

Request URL
Use the following URL when you create your API request. Replace the placeholder values with
your own values.

rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

EmailApp

NAME}

value is case sensitive.

{API-

The version of the API that you're
calling.

VERSION}

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

After you send your API request, you receive a 202 response that indicates success. In the
response headers, extract the operation-location value. The value is formatted like this
example:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request because this operation is asynchronous. Use this URL to

get the exported project JSON by using the same authentication method.

Get export job status
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to query the status of your export job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/export/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This value is
case-sensitive.

{JOB-ID}

The ID for locating your export job

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

status. It's in the location header value
you received in the previous step.
{API-

The version of the API you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating

{YOUR-PRIMARY-

Subscription-Key

your API requests.

RESOURCE-KEY}

Response body
JSON
{
"resultUrl": "{Endpoint}/language/authoring/analyzeconversations/projects/{PROJECT-NAME}/export/jobs/xxxxxx-xxxxx-xxxxx-xx/result?apiversion={API-VERSION}",
"jobId": "xxxx-xxxxx-xxxxx-xxx",
"createdDateTime": "2022-04-18T15:23:07Z",

"lastUpdatedDateTime": "2022-04-18T15:23:08Z",
"expirationDateTime": "2022-04-25T15:23:07Z",
"status": "succeeded"
}

Use the url from the resultUrl key in the body and view the exported assets from this job.

Get export results
Submit a GET request using the {RESULT-URL} you received from the previous step to view the
results of the export job.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-Subscription-

The key to your resource. Used for authenticating your
API requests.

{PRIMARY-RESOURCE-

Key

KEY}

Copy the response body to use as the body for the next import job.

Import to a new project
Now go ahead and import the exported project assets in your new project in the secondary
region so you can replicate it.

Submit import job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Submit a POST request by using the following URL, headers, and JSON body to import your
project.

Request URL
Use the following URL when you create your API request. Replace the placeholder values with
your own values.

rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:import?
api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

{PROJECTNAME}

{API-

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This value is
case sensitive and must match the
project name in the JSON file that you're
importing.

EmailAppDemo

The version of the API that you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
The JSON body you send is similar to the following example. For more information about the
JSON object, see the reference documentation.
JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "Conversation",
"settings": {
"confidenceThreshold": 0.7
},
"projectName": "{PROJECT-NAME}",
"multilingual": true,

"description": "Trying out CLU",
"language": "{LANGUAGE-CODE}"
},
"assets": {
"projectKind": "Conversation",
"intents": [
{
"category": "intent1"
},
{
"category": "intent2"
}
],
"entities": [
{
"category": "entity1"
}
],
"utterances": [
{
"text": "text1",
"dataset": "{DATASET}",
"intent": "intent1",
"entities": [
{
"category": "entity1",
"offset": 5,
"length": 5
}
]
},
{
"text": "text2",
"language": "{LANGUAGE-CODE}",
"dataset": "{DATASET}",
"intent": "intent2",
"entities": []
}
]
}
}

ï¾‰

Key

Placeholder

Value

{API-

2023-04-01

VERSION}

The version of
the API that
you're calling.

projectName

{PROJECT-NAME}

The name of your project. This value is case sensitive.

Expand table
Example

EmailAppDemo

Key

Placeholder

Value

Example

language

{LANGUAGE-CODE}

A string that specifies the language code for the
utterances used in your project. If your project is a
multilingual project, choose the language code of
most of the utterances.

en-us

multilingual

true

A Boolean value that enables you to have documents
in multiple languages in your dataset. When your
model is deployed, you can query the model in any
supported language, including languages that aren't
included in your training documents.

true

dataset

{DATASET}

For information on how to split your data between a

Train

testing and training set, see Label your utterances in
Foundry. Possible values for this field are Train and
Test .

After a successful request, the API response contains an operation-location header with a URL
that you can use to check the status of the import job. The header is formatted like this
example:
HTTP
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/import/jobs/{JOB-ID}?api-version={API-VERSION}

Get import job status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

When you send a successful project import request, the full request URL for checking the
import job's status (including your endpoint, project name, and job ID) is contained in the
response's operation-location header.
Use the following GET request to query the status of your import job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/import/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

myProject

NAME}

value is case-sensitive.

{JOB-ID}

The ID for locating your import
job status.

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{API-

The version of the API you're
calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating
your API requests.

{YOUR-PRIMARY-

Subscription-Key

RESOURCE-KEY}

Response body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded."
JSON
{
"jobId": "xxxxx-xxxxx-xxxx-xxxxx",
"createdDateTime": "2022-04-18T15:17:20Z",
"lastUpdatedDateTime": "2022-04-18T15:17:22Z",
"expirationDateTime": "2022-04-25T15:17:20Z",
"status": "succeeded"
}

Train your model
After importing your project, you only have copies of the project's assets and metadata. You
still need to train your model, which incurs usage on your account.

Submit training job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Create a POST request using the following URL, headers, and JSON body to submit a training
job.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:train?
api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

EmailApp

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body

Use the following object in your request. The model will be named after the value you use for
the modelLabel parameter once training is complete.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingMode": "{TRAINING-MODE}",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"testingSplitPercentage": 20,
"trainingSplitPercentage": 80
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-

Your Model name.

Model1

{CONFIG-

The training configuration model version. By

2022-05-01

VERSION}

default, the latest model version is used.

{TRAINING-

The training mode to be used for training.

MODE}

Supported modes are Standard training, faster
training, but only available for English and

NAME}
trainingConfigVersion

trainingMode

standard

Advanced training supported for other
languages and multilingual projects, but
involves longer training times. Learn more about
training modes.
kind

percentage

Split methods. Possible Values are percentage

percentage

or manual . See how to train a model for more
information.
trainingSplitPercentage

80

Percentage of your tagged data to be included
in the training set. Recommended value is 80 .

80

testingSplitPercentage

20

Percentage of your tagged data to be included

20

in the testing set. Recommended value is 20 .

ï¼— Note

The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set
to percentage and the sum of both percentages should be equal to 100.
Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to get the training job status.

Get Training Status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

When you send a successful training request, the full request URL for checking the job's status
(including your endpoint, project name, and job ID) is contained in the response's operationlocation header.

Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{YOUR-

The endpoint for authenticating
your API request.

https://<your-custom-

ENDPOINT}

Expand table

subdomain>.cognitiveservices.azure.com
EmailApp

NAME}

The name for your project. This
value is case-sensitive.

{JOB-ID}

The ID for locating your model's

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

Placeholder

Value

Example

training status.
{API-VERSION}

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".
JSON
{
"result": {
"modelLabel": "{MODEL-LABEL}",
"trainingConfigVersion": "{TRAINING-CONFIG-VERSION}",
"trainingMode": "{TRAINING-MODE}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},
"jobId": "xxxxx-xxxxx-xxxx-xxxxx-xxxx",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

ï¾‰

Key

Value

Example

modelLabel

The model name

Model1

trainingConfigVersion

The training configuration version. By default, the
latest version is used.

2022-05-01

trainingMode

Your selected training mode.

standard

startDateTime

The time training started

2022-04-

Expand table

14T10:23:04.2598544Z
status

The status of the training job

running

estimatedEndDateTime

Estimated time for the training job to finish

2022-0414T10:29:38.2598544Z

jobId

Your training job ID

xxxxx-xxxx-xxxx-xxxxxxxxxxxxx

createdDateTime

Training job creation date and time

2022-04-14T10:22:42Z

lastUpdatedDateTime

Training job last updated date and time

2022-04-14T10:23:45Z

expirationDateTime

Training job expiration date and time

2022-04-14T10:22:42Z

Deploy your model
This step is where you make your trained model available form consumption via the runtime
prediction API .
îª€ Tip
Use the same deployment name as your primary project for easier maintenance and
minimal changes to your system to handle redirecting your traffic.

Submit deployment job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Create a PUT request using the following URL, headers, and JSON body to start deploying a
conversational language understanding model.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This
value is case-sensitive.

myProject

{DEPLOYMENT-

staging

NAME}

The name for your deployment.
This value is case-sensitive.

{API-VERSION}

The version of the API you're

2023-04-01

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request Body
JSON
{
"trainedModelLabel": "{MODEL-NAME}",
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

trainedModelLabel

{MODEL-

The model name that is assigned to your deployment.
You can only assign successfully trained models. This

myModel

NAME}

value is case-sensitive.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to get the deployment job status.

Get the deployment status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

When you send a successful deployment request, the full request URL for checking the job's
status (including your endpoint, project name, and job ID) is contained in the response's
operation-location header.

Use the following GET request to get the status of your deployment job. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

The name for your project. This

myProject

{PROJECT-NAME}

value is case-sensitive.

Placeholder

Value

Example

{DEPLOYMENT-

The name for your deployment.
This value is case-sensitive.

staging

{JOB-ID}

The ID for locating your model's
training status.

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{API-VERSION}

The version of the API you're

2023-04-01

NAME}

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you'll get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Changes in calling the runtime
Within your system, at the step where you call runtime API

check for the response code

returned from the submitted task API. If you observe a consistent failure in submitting the
request, it could indicate an outage in your primary region. Failure once doesn't mean an
outage. It may also be a transient issue. Retry submitting the job through the secondary
resource you created. For the second request use your {YOUR-SECONDARY-ENDPOINT} and

secondary key, if you followed the previous steps, {PROJECT-NAME} and {DEPLOYMENT-NAME} are
the same, so no changes are required to the request body.
In case you revert to using your secondary resource, you may observe a slight increase in
latency because of the difference in regions where your model is deployed.

Check if your projects are out of sync
Maintaining the freshness of both projects is an important part of process. You need to
frequently check if any updates were made to your primary project so that you move them
over to your secondary project. This way if your primary region fail and you move into the
secondary region you should expect similar model performance since it already contains the
latest updates. Setting the frequency of checking if your projects are in sync is an important
choice, we recommend that you do this check daily in order to guarantee the freshness of data
in your secondary model.

Get project details
Use the following url to get your project details, one of the keys returned in the body indicates
the last modified date of the project. Repeat the following step twice, one for your primary
project and another for your secondary project. Them compare the timestamp returned for
both to check if they're in or out of sync.
Use the following GET request to get your project details. You can use the URL you received
from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}?apiversion={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This
value is case-sensitive.

{API-

The version of the API you're

2023-04-01

VERSION}

calling.

{PROJECT-

Expand table

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating

{YOUR-PRIMARY-

Subscription-Key

your API requests.

RESOURCE-KEY}

Response body
JSON
{
"createdDateTime": "2022-04-18T13:53:03Z",
"lastModifiedDateTime": "2022-04-18T13:53:03Z",
"lastTrainedDateTime": "2022-04-18T14:14:28Z",
"lastDeployedDateTime": "2022-04-18T14:49:01Z",
"projectKind": "Conversation",
"projectName": "{PROJECT-NAME}",
"multilingual": true,
"description": "This is a sample conversation project.",
"language": "{LANGUAGE-CODE}"
}

Repeat the same steps for your replicated project using {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} . Compare the returned lastModifiedDateTime from both projects. If

your primary project was modified sooner than your secondary one, you need to repeat the
steps of exporting, importing, training, and deploying your model.

Next steps
In this article, you learned how to use the export and import APIs to replicate your project to a
secondary Language resource in other region. Next, explore the API reference docs to see what
else you can do with authoring APIs.
Authoring REST API reference
Runtime prediction REST API reference

Last updated on 11/18/2025

Evaluation metrics for orchestration
workflow models
Your dataset is split into two parts: a set for training, and a set for testing. The training set is
used to train the model, while the testing set is used as a test for model after training to
calculate the model performance and evaluation. The testing set isn't introduced to the model
through the training process, to make sure that the model is tested on new data.
Model evaluation is triggered automatically after training is completed successfully. The
evaluation process starts by using the trained model to predict user defined intents for
utterances in the test set, and compares them with the provided tags (which establishes a
baseline of truth). The results are returned so you can review the modelâ€™s performance. For
evaluation, orchestration workflow uses the following metrics:
Precision: Measures how precise/accurate your model is. It's the ratio between the
correctly identified positives (true positives) and all identified positives. The precision
metric reveals how many of the predicted classes are correctly labeled.
Precision = #True_Positive / (#True_Positive + #False_Positive)

Recall: Measures the model's ability to predict actual positive classes. It's the ratio
between the predicted true positives and what was actually tagged. The recall metric
reveals how many of the predicted classes are correct.
Recall = #True_Positive / (#True_Positive + #False_Negatives)

F1 score: The F1 score is a function of Precision and Recall. It's needed when you seek a
balance between Precision and Recall.
F1 Score = 2 * Precision * Recall / (Precision + Recall)

Precision, recall, and F1 score are calculated for:
Each intent separately (intent-level evaluation)
For the model collectively (model-level evaluation).
The definitions of precision, recall, and evaluation are the same for intent-level and model-level
evaluations. However, the counts for True Positives, False Positives, and False Negatives can
differ. For example, consider the following text.

Example
Make a response with thank you very much

Call my friend
Hello
Good morning
These are the intents used: CLUEmail and Greeting
The model could make the following predictions:
ï¾‰

Expand table

Utterance

Predicted intent

Actual intent

Make a response with thank you very much

CLUEmail

CLUEmail

Call my friend

Greeting

CLUEmail

Hello

CLUEmail

Greeting

Goodmorning

Greeting

Greeting

Intent level evaluation for CLUEmail intent
ï¾‰

Key

Count

Explanation

True Positive

1

Utterance 1 was correctly predicted as CLUEmail.

False Positive

1

Utterance 3 was mistakenly predicted as CLUEmail.

False Negative

1

Utterance 2 was mistakenly predicted as Greeting.

Expand table

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 1) = 0.5
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.5 * 0.5) / (0.5 + 0.5)
= 0.5

Intent level evaluation for Greeting intent
ï¾‰

Expand table

Key

Count

Explanation

True Positive

1

Utterance 4 was correctly predicted as Greeting.

False Positive

1

Utterance 2 was mistakenly predicted as Greeting.

False Negative

1

Utterance 3 was mistakenly predicted as CLUEmail.

Precision = #True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 1) = 0.5
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.5 * 0.5) / (0.5 + 0.5)
= 0.5

Model-level evaluation for the collective model
ï¾‰

Key

Count

Explanation

True Positive

2

Sum of TP for all intents

False Positive

2

Sum of FP for all intents

False Negative

2

Sum of FN for all intents

Expand table

Precision = #True_Positive / (#True_Positive + #False_Positive) = 2 / (2 + 2) = 0.5
Recall = #True_Positive / (#True_Positive + #False_Negatives) = 2 / (2 + 2) = 0.5
F1 Score = 2 * Precision * Recall / (Precision + Recall) = (2 * 0.5 * 0.5) / (0.5 + 0.5)
= 0.5

Confusion matrix
A Confusion matrix is an N x N matrix used for model performance evaluation, where N is the
number of intents. The matrix compares the actual tags with the tags predicted by the model.
This gives a holistic view of how well the model is performing and what kinds of errors it is
making.
You can use the Confusion matrix to identify intents that are too close to each other and often
get mistaken (ambiguity). In this case consider merging these intents together. If that isn't

possible, consider adding more tagged examples of both intents to help the model
differentiate between them.
You can calculate the model-level evaluation metrics from the confusion matrix:
The true positive of the model is the sum of true Positives for all intents.
The false positive of the model is the sum of false positives for all intents.
The false Negative of the model is the sum of false negatives for all intents.

Next steps
Train a model in Language Studio

Last updated on 11/18/2025

Back up and recover your orchestration
workflow models
When you create a Language resource in the Azure portal, you specify a region for it to be
created in. From then on, your resource and all of the operations related to it take place in the
specified Azure server region. It's rare, but not impossible, to encounter a network issue that
hits an entire region. If your solution needs to always be available, then you should design it to
either fail-over into another region. This process requires two Azure Language in Foundry Tools
resources in different regions and the ability to sync your orchestration workflow models
across regions.
If your app or business depends on the use of an orchestration workflow model, we
recommend that you create a replica of your project into another supported region. So that if a
regional outage occurs, you can then access your model in the other fail-over region where
you replicated your project.
Replicating a project means that you export your project metadata and assets and import them
into a new project. This step only makes a copy of your project settings, intents, and
utterances. You still need to train and deploy the models before you can query them with the
runtime APIs

.

In this article, you learn to how to use the export and import APIs to replicate your project from
one resource to another existing in different supported geographical regions.

Prerequisites
Two Language resources in different Azure regions, each of them in a different region.

Get your resource keys endpoint
Use the following steps to get the keys and endpoint of your primary and secondary resources.
Go to your resource overview page in the Azure portal

. From the menu on the left side,

select Keys and Endpoint. You will use the endpoint and key for API requests.

ï Š

îª€ Tip
Keep a note of keys and endpoints for both primary and secondary resources. Use these
values to replace the following placeholders: {PRIMARY-ENDPOINT} , {PRIMARY-RESOURCEKEY} , {SECONDARY-ENDPOINT} , and {SECONDARY-RESOURCE-KEY} . Also take note of your project

name, your model name, and your deployment name. Use these values to replace the
following placeholders: {PROJECT-NAME} , {MODEL-NAME} , and {DEPLOYMENT-NAME} .

Export your primary project assets
Start by exporting the project assets from the project in your primary resource.

Submit export job
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Create a POST request using the following URL, headers, and JSON body to export your
project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.

rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:export?
stringIndexType=Utf16CodeUnit&api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

EmailApp

NAME}

value is case-sensitive.

{API-

The version of the API you're
calling.

VERSION}

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/jobs/{JOB-ID}?api-version={API-VERSION}
JOB-ID is used to identify your request, since this operation is asynchronous. Use this URL to

get the exported project JSON, using the same authentication method.

Get export job status
Replace the placeholders in the following request with your {PRIMARY-ENDPOINT} and {PRIMARYRESOURCE-KEY} that you obtained in the first step.

Use the following GET request to query the status of your export job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/export/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com
myProject

NAME}

The name for your project. This value is
case-sensitive.

{JOB-ID}

The ID for locating your export job

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

{PROJECT-

status. It's in the location header value
you received in the previous step.
{API-

The version of the API you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating

{YOUR-PRIMARY-

Subscription-Key

your API requests.

RESOURCE-KEY}

Response body
JSON
{
"resultUrl": "{Endpoint}/language/authoring/analyzeconversations/projects/{PROJECT-NAME}/export/jobs/xxxxxx-xxxxx-xxxxx-xx/result?apiversion={API-VERSION}",
"jobId": "xxxx-xxxxx-xxxxx-xxx",
"createdDateTime": "2022-04-18T15:23:07Z",

"lastUpdatedDateTime": "2022-04-18T15:23:08Z",
"expirationDateTime": "2022-04-25T15:23:07Z",
"status": "succeeded"
}

Use the URL from the resultUrl key in the body and view the exported assets from this job.

Get export results
Submit a GET request using the {RESULT-URL} you received from the previous step to view the
results of the export job.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-Subscription-

The key to your resource. Used for authenticating your
API requests.

{PRIMARY-RESOURCE-

Key

KEY}

Copy the response body to use as the body for the next import job.

Import to a new project
Now go ahead and import the exported project assets in your new project in the secondary
region so you can replicate it.

Submit import job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Submit a POST request using the following URL, headers, and JSON body to import your
project.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.

rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:import?
api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

{PROJECTNAME}
{APIVERSION}

Expand table

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

myProject

The version of the API you're
calling.

2023-04-01

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Body
ï¼— Note
Each intent should only be of one type only from (CLU,LUIS and qna)
Use the following sample JSON as your body.
JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "Orchestration",
"settings": {
"confidenceThreshold": 0

},
"projectName": "{PROJECT-NAME}",
"description": "Project description",
"language": "{LANGUAGE-CODE}"
},
"assets": {
"projectKind": "Orchestration",
"intents": [
{
"category": "string",
"orchestration": {
"kind": "luis",
"luisOrchestration": {
"appId": "00001111-aaaa-2222-bbbb-3333cccc4444",
"appVersion": "string",
"slotName": "string"
},
"cluOrchestration": {
"projectName": "string",
"deploymentName": "string"
},
"qnaOrchestration": {
"projectName": "string"
}
}
}
],
"utterances": [
{
"text": "Trying orchestration",
"language": "{LANGUAGE-CODE}",
"intent": "string"
}
]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

api-version

{API-

2022-03-01-

VERSION}

The version of the API you're calling. The version used here
must be the same API version in the URL.

preview

{PROJECT-

The name of your project. This value is case-sensitive.

EmailApp

A string specifying the language code for the utterances
used in your project. If your project is a multilingual
project, choose the language code for most of the
utterances.

en-us

projectName

NAME}
language

{LANGUAGECODE}

Get import job status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Use the following GET request to query the status of your import job. You can use the URL you
received from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/import/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your

https://<your-custom-

API request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

myProject

NAME}

case-sensitive.

{JOB-ID}

The ID for locating your export job
status. It's in the location header value

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

you received in the previous step.
{API-

The version of the API you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating
your API requests.

{YOUR-PRIMARY-

Subscription-Key

RESOURCE-KEY}

Response body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".

JSON
{
"jobId": "xxxxx-xxxxx-xxxx-xxxxx",
"createdDateTime": "2022-04-18T15:17:20Z",
"lastUpdatedDateTime": "2022-04-18T15:17:22Z",
"expirationDateTime": "2022-04-25T15:17:20Z",
"status": "succeeded"
}

Train your model
After importing your project, you only copied the project's assets and metadata and assets. You
still need to train your model, which incurs usage on your account.

Submit training job
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Create a POST request using the following URL, headers, and JSON body to submit a training
job.

Request URL
Use the following URL when creating your API request. Replace the placeholder values with
your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/:train?
api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating

https://<your-custom-

your API request.

subdomain>.cognitiveservices.azure.com

The name for your project. This
value is case-sensitive.

EmailApp

The version of the API you're
calling.

2023-04-01

{PROJECTNAME}
{API-

Placeholder

Value

Example

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request body
Use the following object in your request. The model will be named MyModel once training is
complete.
JSON
{
"modelLabel": "{MODEL-NAME}",
"trainingMode": "standard",
"trainingConfigVersion": "{CONFIG-VERSION}",
"evaluationOptions": {
"kind": "percentage",
"testingSplitPercentage": 20,
"trainingSplitPercentage": 80
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

modelLabel

{MODEL-NAME}

Your Model name.

Model1

trainingMode

standard

Training mode. Only one mode for training is
available in orchestration, which is standard .

standard

trainingConfigVersion

{CONFIG-

2022-05-01

VERSION}

The training configuration model version. By
default, the latest model version is used.

percentage

Split methods. Possible values are percentage

percentage

kind

or manual . See how to train a model for more

Key

Placeholder

Value

Example

information.
trainingSplitPercentage

80

Percentage of your tagged data to be included

80

in the training set. Recommended value is 80 .
testingSplitPercentage

20

Percentage of your tagged data to be included
in the testing set. Recommended value is 20 .

20

ï¼— Note
The trainingSplitPercentage and testingSplitPercentage are only required if Kind is set
to percentage and the sum of both percentages should be equal to 100.
Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to get the training job status.

Get Training Status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of your model's training progress. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/train/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{YOUR-

The endpoint for authenticating your API

https://<your-custom-

ENDPOINT}

request.

subdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This value is

EmailApp

NAME}

case-sensitive.

{JOB-ID}

The ID for locating your model's training
status. It's in the location header value

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

you received when submitted your
training job.
{API-

The version of the API you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".
JSON
{
"result": {
"modelLabel": "{MODEL-LABEL}",
"trainingConfigVersion": "{TRAINING-CONFIG-VERSION}",
"estimatedEndDateTime": "2022-04-18T15:47:58.8190649Z",
"trainingStatus": {
"percentComplete": 3,
"startDateTime": "2022-04-18T15:45:06.8190649Z",
"status": "running"
},
"evaluationStatus": {
"percentComplete": 0,
"status": "notStarted"
}
},

"jobId": "xxxxxx-xxxxx-xxxxxx-xxxxxx",
"createdDateTime": "2022-04-18T15:44:44Z",
"lastUpdatedDateTime": "2022-04-18T15:45:48Z",
"expirationDateTime": "2022-04-25T15:44:44Z",
"status": "running"
}

ï¾‰

Key

Value

Example

modelLabel

The model name

Model1

trainingConfigVersion

The training configuration version. By default, the

2022-05-01

Expand table

latest version is used.
startDateTime

The time training started

2022-0414T10:23:04.2598544Z

status

The status of the training job

running

estimatedEndDateTime

Estimated time for the training job to finish

2022-0414T10:29:38.2598544Z

jobId

Your training job ID

xxxxx-xxxx-xxxx-xxxxxxxxxxxxx

createdDateTime

Training job creation date and time

2022-04-14T10:22:42Z

lastUpdatedDateTime

Training job last updated date and time

2022-04-14T10:23:45Z

expirationDateTime

Training job expiration date and time

2022-04-14T10:22:42Z

Deploy your model
This step makes your trained model available form consumption via the runtime prediction
API

.

îª€ Tip
Use the same deployment name as your primary project for easier maintenance and
minimal changes to your system to handle redirecting your traffic.

Submit deployment job

Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Create a PUT request using the following URL, headers, and JSON body to start deploying an
orchestration workflow model.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

{PROJECT-NAME}

The name for your project. This
value is case-sensitive.

myProject

{DEPLOYMENT-

The name for your deployment.

staging

NAME}

This value is case-sensitive.

{API-VERSION}

The version of the API you're

2023-04-01

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Request Body
JSON

{
"trainedModelLabel": "{MODEL-NAME}",
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

trainedModelLabel

{MODEL-

The model name that is assigned to your deployment.

myModel

NAME}

You can only assign successfully trained models. This
value is case-sensitive.

Once you send your API request, you receive a 202 response indicating success. In the
response headers, extract the operation-location value formatted like this:
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

You can use this URL to get the deployment job status.

Get the deployment status
Replace the placeholders in the following request with your {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} that you obtained in the first step.

Use the following GET request to get the status of your deployment job. Replace the
placeholder values with your own values.

Request URL
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECTNAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}

ï¾‰

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating your API
request.

https://<your-custom-

Expand table

subdomain>.cognitiveservices.azure.com

Placeholder

Value

Example

{PROJECT-

myProject

NAME}

The name for your project. This value is
case-sensitive.

{DEPLOYMENT-

The name for your deployment. This

staging

NAME}

value is case-sensitive.

{JOB-ID}

The ID for locating your model's training

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx

status. It's in the location header value
you received from the API in response to
your model deployment request.
{API-

The version of the API you're calling.

2023-04-01

VERSION}

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Value

Ocp-Apim-Subscription-Key

The key to your resource. Used for authenticating your API requests.

Response Body
Once you send the request, you get the following response. Keep polling this endpoint until
the status parameter changes to "succeeded".
JSON
{
"jobId":"{JOB-ID}",
"createdDateTime":"{CREATED-TIME}",
"lastUpdatedDateTime":"{UPDATED-TIME}",
"expirationDateTime":"{EXPIRATION-TIME}",
"status":"running"
}

Changes in calling the runtime
Within your system, at the step where you call runtime API

check for the response code

returned from the submitted task API. If you observe a consistent failure in submitting the

request, it could indicate an outage in your primary region. Failure once doesn't mean an
outageâ€”it may be transient issue. Retry submitting the job through the secondary resource
you create. For the second request use your {YOUR-SECONDARY-ENDPOINT} and secondary key, if
you followed the steps, {PROJECT-NAME} and {DEPLOYMENT-NAME} would be the same so no
changes are required to the request body.
In case you revert to using your secondary resource you may observe slight increase in latency
because of the difference in regions where your model is deployed.

Check if your projects are out of sync
Maintaining the freshness of both projects is an important part of process. You need to
frequently check if any updates were made to your primary project so that you move them
over to your secondary project. This way if your primary region fail and you move into the
secondary region you should expect similar model performance since it already contains the
latest updates. Setting the frequency of checking if your projects are in sync is an important
choice, we recommend that you do this check daily in order to guarantee the freshness of data
in your secondary model.

Get project details
Use the following url to get your project details, one of the keys returned in the body indicates
the last modified date of the project. Repeat the following step twiceâ€”once for your primary
project and again for your secondary project. Compare the timestamp returned for both of
them to check if they're out of sync.
Use the following GET request to get your project details. You can use the URL you received
from the previous step, or replace the placeholder values with your own values.
rest
{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}?apiversion={API-VERSION}

ï¾‰

Expand table

Placeholder

Value

Example

{ENDPOINT}

The endpoint for authenticating
your API request.

https://<your-customsubdomain>.cognitiveservices.azure.com

{PROJECT-

The name for your project. This

myProject

NAME}

value is case-sensitive.

Placeholder

Value

Example

{API-

The version of the API you're

2023-04-01

VERSION}

calling.

Headers
Use the following header to authenticate your request.
ï¾‰

Expand table

Key

Description

Value

Ocp-Apim-

The key to your resource. Used for authenticating

{YOUR-PRIMARY-

Subscription-Key

your API requests.

RESOURCE-KEY}

Response body
JSON
{
"createdDateTime": "2022-04-18T13:53:03Z",
"lastModifiedDateTime": "2022-04-18T13:53:03Z",
"lastTrainedDateTime": "2022-04-18T14:14:28Z",
"lastDeployedDateTime": "2022-04-18T14:49:01Z",
"projectKind": "Orchestration",
"projectName": "{PROJECT-NAME}",
"description": "This is a sample Orchestration project.",
"language": "{LANGUAGE-CODE}"
}

Repeat the same steps for your replicated project using {SECONDARY-ENDPOINT} , and
{SECONDARY-RESOURCE-KEY} . Compare the returned lastModifiedDateTime from both projects. If

your primary project was modified sooner than your secondary one, you need to repeat the
steps of exporting, importing, training, and deploying your model.

Next steps
In this article, you learned how to use the export and import APIs to replicate your project to a
secondary Language resource in other region. Next, explore the API reference docs to see what
else you can do with authoring APIs.
Authoring REST API reference

Runtime prediction REST API reference

Last updated on 11/18/2025

The "None" intent in orchestration
workflow
Every project in orchestration workflow includes a default None intent. The None intent is a
required intent and can't be deleted or renamed. The intent is meant to categorize any
utterances that do not belong to any of your other custom intents.
An utterance can be predicted as the None intent if the top scoring intent's score is lower than
the None score threshold. It can also be predicted if the utterance is similar to examples added
to the None intent.

None score threshold
You can go to the project settings of any project and set the None score threshold. The
threshold is a decimal score from 0.0 to 1.0.
For any query and utterance, the highest scoring intent ends up lower than the threshold
score, the top intent will be automatically replaced with the None intent. The scores of all the
other intents remain unchanged.
The score should be set according to your own observations of prediction scores, as they may
vary by project. A higher threshold score forces the utterances to be more similar to the
examples you have in your training data.
When you export a project's JSON file, the None score threshold is defined in the "settings"
parameter of the JSON as the "confidenceThreshold", which accepts a decimal value between
0.0 and 1.0.
The default score for Orchestration Workflow projects is set at 0.5 when creating a new project.
ï¼— Note
During model evaluation of your test set, the None score threshold is not applied.

Adding examples to the None intent
The None intent is also treated like any other intent in your project. If there are utterances that
you want predicted as None, consider adding similar examples to them in your training data.
For example, if you would like to categorize utterances that are not important to your project
as None, then add those utterances to your intent.

Next steps
Orchestration workflow overview

Last updated on 12/18/2025

Data formats accepted by orchestration
workflow
When data is used by your model for learning, it expects the data to be in a specific format.
When you tag your data, it gets converted to the JSON format described in this article. You can
also manually tag your files.

JSON file format
If you upload a tags file, it should follow this format.
JSON
{
"projectFileVersion": "{API-VERSION}",
"stringIndexType": "Utf16CodeUnit",
"metadata": {
"projectKind": "Orchestration",
"projectName": "{PROJECT-NAME}",
"multilingual": false,
"description": "This is a description",
"language": "{LANGUAGE-CODE}"
},
"assets": {
"projectKind": "Orchestration",
"intents": [
{
"category": "{INTENT1}",
"orchestration": {
"targetProjectKind": "Luis|Conversation|QuestionAnswering",
"luisOrchestration": {
"appId": "{APP-ID}",
"appVersion": "0.1",
"slotName": "production"
},
"conversationOrchestration": {
"projectName": "{PROJECT-NAME}",
"deploymentName": "{DEPLOYMENT-NAME}"
},
"questionAnsweringOrchestration": {
"projectName": "{PROJECT-NAME}"
}
}
}
],
"utterances": [
{
"text": "utterance 1",
"language": "{LANGUAGE-CODE}",

"dataset": "{DATASET}",
"intent": "intent1"
}
]
}
}

ï¾‰

Expand table

Key

Placeholder

Value

Example

api-version

{API-VERSION}

The version of the API you're calling. The value
referenced is for the latest released model
version released.

2022-0301-preview

{CONFIDENCE-

This is the threshold score below which the intent

0.7

THRESHOLD}

will be predicted as none intent

projectName

{PROJECT-NAME}

The name of your project. This value is casesensitive.

EmailApp

multilingual

false

Orchestration doesn't support the multilingual

false

confidenceThreshold

feature
language

{LANGUAGE-

A string specifying the language code for the

CODE}

utterances used in your project. See Language

en-us

support for more information about supported
language codes.
intents

[]

Array containing all the intent types you have in
the project. These are the intents used in the
orchestration project.

Utterance format
JSON
[
{
"intent": "intent1",
"language": "{LANGUAGE-CODE}",
"text": "{Utterance-Text}",
},
{
"intent": "intent2",
"language": "{LANGUAGE-CODE}",
"text": "{Utterance-Text}",
}

[]

]

Next steps
You can import your labeled data into your project directly. Learn how to import project
See the how-to article more information about labeling your data. When you're done
labeling your data, you can train your model.

Last updated on 12/18/2025

Deploy custom language projects to
multiple regions
ï¼— Note
This article applies to the following custom features in Azure Language in Foundry Tools:
Conversational language understanding
Custom text classification
Custom named entity recognition (NER)
Orchestration workflow
Custom Language features enable you to deploy your project to more than one region. This
capability makes it much easier to access your project globally while you manage only one
instance of your project in one place. Beginning in November 2024, custom Language features
allow you to deploy your project to multiple resources within a single region using the API.
Thus, you can access and utilize your custom model wherever needed.
Before you deploy a project, you can assign deployment resources in other regions. Each
deployment resource is a different Language resource from the one that you use to author
your project. You deploy to those resources and then target your prediction requests to that
resource in their respective regions and your queries are served directly from that region.
When you create a deployment, you can select which of your assigned deployment resources
and their corresponding regions you want to deploy to. The model you deploy is then
replicated to each region and accessible with its own endpoint dependent on the deployment
resource's custom subdomain.

Example
Suppose you want to make sure your project, which is used as part of a customer support
chatbot, is accessible by customers across the United States and India. You author a project
with the name ContosoSupport by using a West US 2 Language resource named MyWestUS2 .
Before deployment, you assign two deployment resources to your project: MyEastUS and
MyCentralIndia in East US and Central India, respectively.

When you deploy your project, you select all three regions for deployment: the original West
US 2 region and the assigned ones through East US and Central India.
You now have three different endpoint URLs to access your project in all three regions:

West US 2: https://mywestus2.cognitiveservices.azure.com/language/:analyzeconversations

East US: https://myeastus.cognitiveservices.azure.com/language/:analyzeconversations

Central India: https://mycentralindia.cognitiveservices.azure.com/language/:analyzeconversations

The same request body to each of those different URLs serves the exact same response directly
from that region.

Validations and requirements
Assigning deployment resources requires Microsoft Entra authentication. Microsoft Entra ID is
used to confirm that you have access to the resources that you want to assign to your project
for multiregion deployment. In Language Studio, you can automatically enable Microsoft Entra
authentication

by assigning yourself the Azure Cognitive Services Language Owner role to

your original resource. To programmatically use Microsoft Entra authentication, learn more
from the Foundry Tools documentation.
Your project name and resource are used as its main identifiers. A Language resource can only
have a specific project name in each resource. Any other projects with the same name can't be
deployed to that resource.
For example, if a project ContosoSupport was created via the resource MyWestUS2 in West US 2
and deployed to the resource MyEastUS in East US, the resource MyEastUS can't create a
different project called ContosoSupport and deploy a project to that region. Similarly, your
collaborators can't then create a project ContosoSupport with the resource MyCentralIndia in
Central India and deploy it to either MyWestUS2 or MyEastUS .
You can only swap deployments that are available in the exact same regions. Otherwise,
swapping fails.
If you remove an assigned resource from your project, all of the project deployments to that
resource are deleted.
Some regions are only available for deployment and not for authoring projects.

Related content
Learn how to deploy models for:
Conversational language understanding

Custom text classification
Custom NER
Orchestration workflow

Last updated on 12/05/2025

Project versioning
ï¼— Note
This article applies to the following custom features in Azure Language in Foundry Tools:
Conversational language understanding
Custom text classification
Custom named entity recognition (NER)
Orchestration workflow
Building your project typically happens in increments. You may add, remove, or edit intents,
entities, labels, and data at each stage. Every time you train, a snapshot of your current project
state is taken to produce a model. That model saves the snapshot to be loaded back at any
time. Every model acts as its own version of the project.
For example, if your project has 10 intents and/or entities, with 50 training documents or
utterances, it can be trained to create a model named v1. Afterwards, you might make changes
to the project to alter the numbers of training data. The project can be trained again to create
a new model named v2. If you don't like the changes you made in v2 and would like to
continue from where you left off in model v1, then you would just need to load the model data
from v1 back into the project. Loading a model's data is possible through both Microsoft
Foundry and API. Once complete, the project has the original amount and types of training
data.
If the project data isn't saved in a trained model, it can be lost. For example, if you loaded
model v1, your project now has the data that was used to train it. If you then made changes,
didn't train, and loaded model v2, you would lose those changes as they weren't saved to any
specific snapshot.
If you overwrite a model with a new snapshot of data, you can't revert back to any previous
state of that model.
You always can locally export the data for every model.

Data location
The data for your model versions is saved in different locations, depending on the custom
feature you're using.
Custom NER

In custom named entity recognition, the data being saved to the snapshot is the labels file.

Next steps
Learn how to load or export model data for:
Conversational language understanding
Custom text classification
Custom NER
Orchestration workflow

Last updated on 12/17/2025

When to use conversational language
understanding or orchestration workflow
apps
When you create large applications, you should consider whether your use case is best served
by a single conversational app (flat architecture) or by multiple apps that are orchestrated.

Orchestration overview
Orchestration workflow is a feature that allows you to connect different projects from LUIS,
conversational language understanding, and custom question answering in one project. You
can then use this project for predictions by using one endpoint. The orchestration project
makes a prediction on which child project should be called, automatically routes the request,
and returns with its response.
Orchestration involves two steps:
1. Predicting which child project to call.
2. Routing the utterance to the destination child app and returning the child app's response.

Orchestration advantages
Clear decomposition and faster development:
If your overall schema has a substantial number of domains, the orchestration
approach can help decompose your application into several child apps (each serving a
specific domain). For example, an automotive conversational app might have a
navigation domain or a media domain.
Developing each domain app in parallel is easier. People and teams with specific
domain expertise can work on individual apps collaboratively and in parallel.
Because each domain app is smaller, the development cycle becomes faster. Smallersized domain apps take much less time to train than a single large app.
More flexible confidence score thresholds:
Because separate child apps serve each domain, it's easy to set separate thresholds for
different child apps.
AI-quality improvements where appropriate:
Some applications require that certain entities must be domain restricted.
Orchestration makes this task easy to achieve. After the orchestration project predicts
which child app should be called, the other child apps aren't called.

For example, if your app contains a Person.Name prebuilt entity, consider the utterance
"How do I use a jack?" in the context of a vehicle question. In this context, jack is an
automotive tool and shouldn't be recognized as a person's name. When you use
orchestration, this utterance can be redirected to a child app created to answer such a
question, which doesn't have a Person.Name entity.

Orchestration disadvantages
Redundant entities in child apps:
If you need a particular prebuilt entity being returned in all utterances irrespective of
the domain, for example Quantity.Number or Geography.Location , there's no way of
adding an entity to the orchestration app (it's an intent-only model). You would need
to add it to all individual child apps.
Efficiency:
Orchestration apps take two model inferences. One for predicting which child app to
call, and another for the prediction in the child app. Inference times are typically slower
than single apps with a flat architecture.
Train/test split for orchestrator:
Training an orchestration app doesn't allow you to granularly split data between the
testing and training sets. For example, you can't train a 90-10 split for child app A, and
then train an 80-20 split for child app B. This limitation might be minor, but it's worth
keeping in mind.

Flat architecture overview
Flat architecture is the other method of developing conversational apps. Instead of using an
orchestration app to send utterances to one of multiple child apps, you develop a singular (or
flat) app to handle utterances.

Flat architecture advantages
Simplicity:
For small-sized apps or domains, the orchestrator approach can be overly complex.
Because all intents and entities are at the same app level, it might be easier to make
changes to all of them together.
It's easier to add entities that should always be returned:
If you want certain prebuilt or list entities to be returned for all utterances, you only
need to add them alongside other entities in a single app. If you use orchestration, as

mentioned, you need to add it to every child app.

Flat architecture disadvantages
Unwieldy for large apps:
For large apps (say, more than 50 intents or entities), it can become difficult to keep
track of evolving schemas and datasets. This difficulty is evident in cases where the app
has to serve several domains. For example, an automotive conversational app might
have a navigation domain or a media domain.
Limited control over entity matches:
In a flat architecture, there's no way to restrict entities to be returned only in certain
cases. When you use orchestration, you can assign those specific entities to particular
child apps.

Related content
Orchestration workflow overview
Conversational language understanding overview

Last updated on 11/18/2025

Connect different services with
Orchestration workflow
Orchestration workflow is a feature that allows you to connect different projects from
conversational language understanding and custom question answering in one project. You
can then use this project for predictions under one endpoint. The orchestration project makes
a prediction on which project should be called and automatically routes the request to that
project, and returns with its response.
In this tutorial, you learn how to connect a custom question answering knowledge base with a
conversational language understanding project. You then call the project using the .NET SDK
sample for orchestration.
This tutorial includes creating a chit chat knowledge base and email commands project. Chit
chat deals with common niceties and greetings with static responses. Email commands predict
among a few simple actions for an email assistant. The tutorial then teaches you to call the
Orchestrator using the SDK in a .NET environment using a sample solution.

Prerequisites
Create a Language resource

and select the custom question answering feature in the

Azure portal to get your key and endpoint. After it deploys, select Go to resource.
You need the key and endpoint from the resource you create to connect your bot to
the API. You paste your key and endpoint into the code later in the tutorial. Copy them
from the Keys and Endpoint tab in your resource.
When you enable custom question answering, you must select an Azure search
resource to connect to.
Make sure the region of your resource supports conversational language
understanding.
Download the OrchestrationWorkflowSample sample

.

Create a custom question answering knowledge
base
1. Sign into the Language Studio

and select your Language resource.

2. Find and select the Custom question answering

tile in the homepage.

3. Select Create new project and add the name chitchat with the language English before
selecting Create project.

4. When the project loads, select Add source and select Chit chat. Select the professional
personality for chit chat before

ï Š

5. Go to Deploy knowledge base from the left pane and select Deploy and confirm the
popup that shows up.
You're now done with deploying your knowledge base for chit chat. You can explore the type
of questions and answers to expect in the Edit knowledge base page.

Create a conversational language understanding
project
1. In Language Studio, go to the Conversational language understanding
2. Download the EmailProject.json sample file here

service.

.

3. Select the Import button. Browse to the `EmailProject.json`` file you downloaded and
press Done.

ï Š

4. Once the project is loaded, select Training jobs. Press on Start a training job, provide the
model name v1 and press Train.

ï Š

5. Once training is complete, select to Deploying a model on the left. Select Add
Deployment and create a new deployment with the name Testing, and assign model v1
to the deployment.

ï Š

You're now done with deploying a conversational language understanding project for email
commands. You can explore the different commands in the Data labeling page.

Create an Orchestration workflow project
1. In Language Studio, go to the Orchestration workflow

service.

2. Select Create new project. Use the name Orchestrator and the language English before
clicking next then done.
3. Once the project is created, select Add in the Schema definition page.
4. Select Yes, I want to connect it to an existing project. Add the intent name EmailIntent and
select Conversational Language Understanding as the connected service. Select the
recently created EmailProject project for the project name before selecting Add Intent.

ï Š

A screenshot of the connect intent popup in orchestration workflow.
5. Add another intent but now select Question Answering as the service and select chitchat
as the project name.
6. Similar to conversational language understanding, go to Training jobs and start a new
training job with the name v1 and press Train.
7. Once training is complete, select to Deploying a model on the left. Select Add
deployment and create a new deployment with the name Testing, and assign model v1 to
the deployment and press Next.
8. On the next page, select the deployment name Testing for the EmailIntent. This
command tells the orchestrator to call the Testing deployment in EmailProject when it
routes to it. Custom question answering projects only have one deployment by default.

ï Š

A screenshot of the deployment popup for orchestration workflow.
Now your orchestration project is ready to be used. Any incoming request is routed to either
EmailIntent and the EmailProject in conversational language understanding or ChitChatIntent
and the chitchat knowledge base.

Call the orchestration project with the
Conversations SDK
1. In the downloaded sample, open OrchestrationWorkflowSample.sln in Visual Studio.
2. In the OrchestrationWorkflowSample solution, make sure to install all the required
packages. In Visual Studio, go to Tools, NuGet Package Manager and select Package
Manager Console and run the following command.
PowerShell
dotnet add package Azure.AI.Language.Conversations

Alternatively, you can search for "Azure.AI.Language.Conversations" in the NuGet package
manager and install the latest release.
3. In Program.cs , replace {api-key} and the {endpoint} variables. Use the key and endpoint
for Azure Language resource you created earlier. You can find them in the Keys and
Endpoint tab in your Language resource in Azure.
C#
Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");

4. Replace the project and deployment parameters to Orchestrator and Testing if they
aren't set already.
C#
string projectName = "Orchestrator";
string deploymentName = "Testing";

5. Run the project or press F5 in Visual Studio.
6. Input a query such as "read the email from matt" or "hello how are you." You now observe
different responses for each, a conversational language understanding EmailProject
response from the first query, and the answer from the chitchat knowledge base for the
second query.
Conversational Language Understanding:

ï Š

Custom Question Answering:

ï Š

You can now connect other projects to your orchestrator and begin building complex
architectures with various different projects.

Next steps
Learn more about conversational language understanding.
Learn more about custom question answering.

Last updated on 11/18/2025

Orchestration workflow limits
Use this article to learn about the data and service limits when using orchestration workflow.

Language resource limits
Your Language resource has to be created in one of the supported regions.
Pricing tiers
ï¾‰

Expand table

Tier

Description

Limit

F0

Free tier

You're only allowed one Language resource with the F0 tier per subscription.

S

Paid tier

You can have up to 100 Language resources in the S tier per region.

See pricing

for more information.

You can have up to 500 projects per resource.
Project names have to be unique within the same resource across all custom features.

Regional availability
See Language regional availability.

API limits
ï¾‰

Item

Request type

Maximum limit

Authoring API

POST

10 per minute

Authoring API

GET

100 per minute

Prediction API

GET/POST

1,000 per minute

Quota limits

Expand table

Pricing tier

Item

Limit

F

Training time

1 hour per month

S

Training time

Unlimited, Standard

F

Prediction Calls

5,000 request per month

S

Prediction Calls

Unlimited, Standard

ï¾‰

Expand table

ï¾‰

Expand table

Data limits
The following limits are observed for orchestration workflow.

Item

Lower Limit

Upper Limit

Count of utterances per project

1

15,000

Utterance length in characters

1

500

Count of intents per project

1

500

Count of trained models per project

0

10

Count of deployments per project

0

10

Naming limits
ï¾‰

Expand table

Attribute

Limits

Project name

You can only use letters (a-z, A-Z) , and numbers (0-9) , symbols _ . - , with no
spaces. Maximum allowed length is 50 characters.

Model name

You can only use letters (a-z, A-Z) , numbers (0-9) and symbols _ . - . Maximum
allowed length is 50 characters.

Deployment

You can only use letters (a-z, A-Z) , numbers (0-9) and symbols _ . - . Maximum

name

allowed length is 50 characters.

Intent name

You can only use letters (a-z, A-Z) , numbers (0-9) and all symbols except ":", $ & % *
( ) + ~ # / ? . Maximum allowed length is 50 characters.

Next steps
Orchestration workflow overview

Last updated on 11/18/2025

Terms and definitions used in orchestration
workflow
Use this article to learn about some of the definitions and terms you may encounter when
using orchestration workflow.

F1 score
The F1 score is a function of Precision and Recall. The score is needed when you seek a balance
between precision and recall.

Intent
An intent represents a task or action the user wants to perform. It's a purpose or goal
expressed in a user's input, such as booking a flight, or paying a bill.

Model
A model is an object trained to do a certain task, in this case conversation understanding tasks.
Models are trained by providing labeled data to learn from so they can later be used to
understand utterances.
Model evaluation is the process that happens right after training to know how well does
your model perform.
Deployment is the process of assigning your model to a deployment to make it available
for use via the prediction API

.

Overfitting
Overfitting happens when the model is fixated on the specific examples and isn't able to
generalize well.

Precision
Measures how precise/accurate your model is. It's the ratio between the correctly identified
positives (true positives) and all identified positives. The precision metric reveals how many of
the predicted classes are correctly labeled.

Project
A project is a work area for building your custom ML models based on your data. Your project
is only accessible for you and others who have access to the Azure resource being used.

Recall
Measures the model's ability to predict actual positive classes. It's the ratio between the
predicted true positives and what was tagged. The recall metric reveals how many of the
predicted classes are correct.

Schema
Schema is defined as the combination of intents within your project. Schema design is a crucial
part of your project's success. When creating a schema, you want to think about which intents
should be included in your project

Training data
Training data is the set of information that is needed to train a model.

Utterance
An utterance is a short text representative of a sentence in a conversation. It's a natural
language phrase such as "book two tickets to Seattle next Tuesday." Example utterances are
added to train the model and the model predicts on new utterance at runtime

Next steps
Data and service limits.
Orchestration workflow overview.

Last updated on 12/18/2025

What is Azure Language PII detection?
ï¼‰ Important
The Azure Language in Foundry Tools Text Personally Identifiable Information (PII)
detection anonymization feature (synthetic replacement) is currently available in preview
and licensed to you as part of your Azure subscription. Your use of this feature is subject
to the terms applicable to Previews as described in the Supplemental Terms of Use for
Microsoft Azure Previews
Addendum (DPA)

and the Microsoft Products and Services Data Protection

.

Azure Language in Foundry Tools Personally Identifiable Information (PII) detection is a feature
offered by Azure Language. The PII detection service is a cloud-based API that utilizes machine
learning and AI algorithms to help you develop intelligent applications with advanced natural
language understanding. Azure Language PII detection uses Named Entity Recognition (NER)
to identify and redact sensitive information from input data. The service classifies sensitive
personal data into predefined categories. These categories include phone numbers, email
addresses, and identification documents. This classification helps to efficiently detect and
eliminate such information.
îª€ Tip
Try PII detection in Microsoft Foundry portal . There you can utilize a currently existing
Language Studio resource or create a new Foundry resource.

What's new
The 2025-11-15-preview version introduces the following new PII task parameters:
Multiple redaction policies offer the ability to apply various redaction approaches within
a single request:
SyntheticReplacementPolicyType ðŸ†•
CharacterMaskPolicyType (default)
NoMaskPolicyType
EntityMaskPolicyType
Configurable confidence threshold enables you to set a minimum confidence score.
Entities are only included in the output if their confidence score meets or exceeds the
specified threshold.

Disable type validation enforcement enables you to bypass the entity type validation. By
default, the service enforces validation across multiple entity types to ensure data
integrity and minimize false positives. Disabling this enforcement can enhance
operational efficiency in cases where strict validation isn't required.
The following entities are available in preview:
Airport
DateOfBirth
BankAccountNumber
CASocialIdentificationNumber
CVV (Card Verification Value )
City
PassportNumber
DriversLicenseNumber
ExpirationDate
Geopolitical Entity
KRDriversLicenseNumber
KRPassportNumber
KRSocialSecurityNumber
LicensePlate
Location
Password
SortCode
State
USMedicareBeneficiaryId
VIN (vehicle identification number)
ZipCode
Conversational PII detection models (both version 2024-11-01-preview and GA ) are
updated to provide enhanced AI quality and accuracy. The numeric identifier entity type
now also includes Drivers License and Medicare Beneficiary Identifier.
ï¼‚ As of June 2024, we now provide General Availability support for the Conversational
PII service (English-language only).
ï¼‚ Customers can now redact transcripts, chats, and other text written in a
conversational style.
ï¼‚ These capabilities provide better confidence in AI quality. They also offer Azure SLA
support, production environment support, and enterprise-grade security.

Capabilities
Currently, PII support is available for the following capabilities:

General text PII detection for processing sensitive information (PII) and health information
(PHI) in unstructured text across several predefined categories.
Conversation PII detection, a specialized model designed to handle speech transcriptions
and the informal, conversational tone found in meeting and call transcripts.
Native Document PII detection for processing structured document files.
Text PII

Language is a cloud-based service that applies Natural Language Processing (NLP)
features to detect categories of personal information (PII) in text-based data. This
documentation contains the following types:
Quickstarts are getting-started instructions to guide you through making requests to
the service.
How-to guides contain instructions for using the service in more specific or
customized ways.

Typical workflow
To use this feature, you submit data for analysis and handle the API output in your
application. Analysis is performed as-is, with no added customization to the model used
on your data.
1. Create an Azure Language in Foundry Tools resource, which grants you access to the
features offered by Language. It generates a password (called a key) and an endpoint
URL that you use to authenticate API requests.
2. Create a request using either the REST API or the client library for C#, Java, JavaScript,
and Python. You can also send asynchronous calls with a batch request to combine
API requests for multiple features into a single call.
3. Send the request containing your text data. Your key and endpoint are used for
authentication.
4. Stream or store the response locally.

Key features for text PII
Language offers named entity recognition to identify and categorize information within
your text. The feature detects PII categories including names, organizations, addresses,
phone numbers, financial account numbers or codes, and government identification

numbers. A subset of this PII is protected health information (PHI). By specifying
domain=phi in your request, only PHI entities are returned.

Get started with PII detection
To use PII detection, you submit text for analysis and handle the API output in your application.
Analysis is performed as-is, with no customization to the model used on your data. There are
two ways to use PII detection:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry
(new) portal

Foundry (new) is a cloud-based AI platform that provides streamlined access to
Foundry models, agents, and tools through Foundry projects.

Foundry (classic)
portal

Foundry (classic) is a cloud-based platform that supports hub-based projects
and other resource types. When you sign up, you can use your own data to
detect personally identifying information within text examples.

REST API or Client

Integrate PII detection into your applications using the REST API, or the client

library (Azure SDK)

library available in various languages.

Reference documentation and code samples
As you use this feature in your applications, see the following reference documentation and
samples for Azure Language in Foundry Tools:
ï¾‰

Expand table

Development option / language

Reference documentation

REST API

REST API documentation

C#

C# documentation

C# samples

Java

Java documentation

Java Samples

JavaScript

JavaScript documentation

JavaScript samples

Python

Python documentation

Python samples

Input requirements and service limits

Samples

Text PII

Text PII takes text for analysis. For more information, see Data and service limits in
the how-to guide.
PII works with various written languages. For more information, see language
support. You can specify in which supported languages your source text is written. If
you don't specify a language, the extraction defaults to English. The API may return
offsets in the response to support different multilingual and emoji encodings.

Responsible AI
An AI system includes not only the technology, but also the people who use it, the people
affected by it, and the deployment environment. Read the transparency note for PII to learn
about responsible AI use and deployment in your systems. For more information, see the
following articles:
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Example scenarios
Apply sensitivity labels - For example, based on the results from the PII service, a public
sensitivity label might be applied to documents where no PII entities are detected. For
documents where US addresses and phone numbers are recognized, a confidential label
might be applied. A highly confidential label might be used for documents where bank
routing numbers are recognized.
Redact some categories of personal information from documents that get wider
circulation - For example, if customer contact records are accessible to frontline support
representatives, the company can redact the customer's personal information besides
their name from the version of the customer history to preserve the customer's privacy.
Redact personal information in order to reduce unconscious bias - For example, during
a company's resume review process, they can block name, address, and phone number to
help reduce unconscious gender or other biases.
Replace personal information in source data for machine learning to reduce unfairness
â€“ For example, if you want to remove names that might reveal gender when training a
machine learning model, you could use the service to identify them and you could
replace them with generic placeholders for model training.

Remove personal information from call center transcription â€“ For example, if you want
to remove names or other PII data that happen between the agent and the customer in a
call center scenario. You could use the service to identify and remove them.
Data cleaning for data science - PII can be used to make the data ready for data
scientists and engineers to be able to use these data to train their machine learning
models. Redacting the data to make sure that customer data isn't exposed.

Next steps
There are two ways to get started using the entity linking feature:
Foundry is a web-based platform that lets you use several Language features without
needing to write code.
The quickstart article for instructions on making requests to the service using the REST
API and client library SDK.

Last updated on 01/18/2026

Quickstart: Detect Personally Identifiable
Information (PII)
ï¼— Note
This quickstart guides you through the process of identifying personally identifiable
information (PII) in documents. To learn about detecting PII in conversations, see How to
detect and redact PII in conversations. To learn about detecting PII in text, see How to
detect and redact PII in text.

Prerequisites
îª€ Tip
If you already have an Azure Language in Foundry Tools or multi-service resourceâ€”
whether used on its own or through Language Studioâ€”you can continue to use
those existing Language resources within the Microsoft Foundry portal.
For more information, see How to use Foundry Tools in the Foundry portal.
We highly recommended that you use a Foundry resource in the Foundry; however,
you can also follow these instructions using a Language resource.
Azure subscription. If you don't have one, you can create one for free

.

Requisite permissions. Make sure the person establishing the account and project is
assigned as the Azure AI Account Owner role at the subscription level. Alternatively,
having either the Contributor or Cognitive Services Contributor role at the subscription
scope also meets this requirement. For more information, see Role based access control
(RBAC).
Foundry resource. For more information, see Configure a Foundry resource. Alternately,
you can use a Language resource .
A Foundry project created in the Foundry. For more information, see Create a Foundry
project.
Foundry (classic)

ï¼— Note

This content refers to the Foundry (classic)

portal, which supports hub-based

projects and other resource types. To confirm that you're using Foundry (classic),
make sure the version toggle in the portal banner is in the off position.

You can use Foundry (classic)

to:

ï¼‚ Create a project
ï¼‚ Deploy a model
ï¼‚ Run a chat completion
ï¼‚ Create and run an agent
ï¼‚ Upload files to your agent

Navigate to the Foundry (classic)

Playground

Using the left side pane, select Playgrounds. Then select the Try Azure Language
Playground button.

ï Š

Use PII in the Foundry Playground
The Language Playground consists of four sections:
Top banner: You can select any of the currently available Languages here.
Right pane: This pane is where you can find the Configuration options for the
service, such as the API and model version, along with features specific to the service.
Center pane: This pane is where you enter your text for processing. After the
operation is run, some results will be shown here.
Right pane: This pane is where Details of the run operation are shown.
Here you can select from two Personally Identifying Information (PII) detection capabilities
by choosing the top banner tiles, Extract PII from conversation or Extract PII from text.
Each is for a different scenario.

Extract PII from conversation
Extract PII from conversation is designed to identify and mask personally identifying
information in conversational text.
In Configuration there are the following options:
ï¾‰

Option

Description

Select API version

Select which version of the API to use.

Select model version

Select which version of the model to use.

Select text language

Select which language the language is input in.

Select types to
include

Select they types of information you want to redact.

Specify redaction

Select the method of redaction.

Expand table

policy
Specify redaction
character

Select which character is used for redaction. Only available with the
CharacterMask redaction policy.

After your operation is completed, the type of entity is displayed beneath each entity in
the center pane. The Details section contains the following fields for each entity:
ï¾‰

Expand table

Field

Description

Entity

The detected entity.

Category

The entity type that was detected.

Offset

The number of characters that the entity was detected from the beginning of the line.

Length

The character length of the entity.

Confidence

How confident the model is in the correctness of identification of entity's type.

ï Š

Extract PII from text
Extract PII from text is designed to identify and mask personally identifying information in
text.
In Configuration you can select from the following options:
ï¾‰

Option

Description

Select API version

Select which version of the API to use.

Select model version

Select which version of the model to use.

Select text language

Select which language the language is input in.

Select types to
include

Select the types of information you want to redact.

Expand table

Option

Description

Specify redaction
policy

Select the method of redaction.

Specify redaction
character

Select which character is used for redaction. Only available with the
CharacterMask redaction policy.

After your operation is completed, the type of entity is displayed beneath each entity in
the center pane. The Details section contains the following fields for each entity:
ï¾‰

Expand table

Field

Description

Entity

The detected entity.

Category

The entity type that was detected.

Offset

The number of characters that the entity was detected from the beginning of the line.

Length

The character length of the entity.

Confidence

How confident the model is in the correctness of identification of entity's type.

Tags

How confident the model is in the correctness for each identified entity type.

ï Š

Clean up resources

To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
Overview

Last updated on 11/18/2025

Personally Identifiable Information (PII)
detection language support
Use this article to learn which natural languages text PII, document PII, and conversation PII
features support.
Text PII

Text PII language support
ï¾‰

Language

Language code

Afrikaans

af

Albanian

sq

Amharic

am

Arabic

ar

Armenian

hy

Assamese

as

Azerbaijani

az

Basque

eu

Bengali

bn

Bosnian

bs

Bulgarian

bg

Burmese

my

Catalan

ca

Chinese-Simplified

zh-hans

Chinese-Traditional

zh-hant

Croatian

hr

Expand table

Notes

zh also accepted

Language

Language code

Czech

cs

Danish

da

Dutch

nl

English

en

Estonian

et

Finnish

fi

French

fr

Galician

gl

Georgian

ka

German

de

Greek

el

Gujarati

gu

Hebrew

he

Hindi

hi

Hungarian

hu

Indonesian

id

Irish

ga

Italian

it

Japanese

ja

Kannada

kn

Kazakh

kk

Khmer

km

Korean

ko

Kurdish(Kurmanji)

ku

Kyrgyz

ky

Lao

lo

Notes

Language

Language code

Latvian

lv

Lithuanian

lt

Macedonian

mk

Malagasy

mg

Malay

ms

Malayalam

ml

Marathi

mr

Mongolian

mn

Nepali

ne

Norwegian (BokmÃ¥l)

no

Odia

or

Pashto

ps

Persian

fa

Polish

pl

Portuguese (Brazil)

pt-BR

Portuguese (Portugal)

pt-PT

Punjabi

pa

Romanian

ro

Russian

ru

Serbian

sr

Slovak

sk

Slovenian

sl

Somali

so

Spanish

es

Swahili

sw

Swazi

ss

Notes

nb also accepted

pt also accepted

Language

Language code

Swedish

sv

Tamil

ta

Telugu

te

Thai

th

Turkish

tr

Ukrainian

uk

Urdu

ur

Uyghur

ug

Uzbek

uz

Vietnamese

vi

Welsh

cy

Next steps
PII feature overview

Last updated on 11/18/2025

Notes

Transparency note for Personally
Identifiable Information (PII)
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
ï¼‰ Important
This article assumes that you're familiar with guidelines and best practices for Azure
Language. For more information, see Transparency note for Azure Language.
An AI system includes not only the technology, but also the people who use it, the people
affected by it, and the environment where it's deployed. Creating a system that is fit for its
intended purpose requires an understanding of how the technology works, its capabilities and
limitations, and how to achieve the best performance.
Microsoft's Transparency Notes are intended to help you understand how our AI technology
works, the choices system owners can make that influence system performance and behavior,
and the importance of thinking about the whole system, including the technology, the people,
and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who use or are affected by your system.
Microsoft's Transparency notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Responsible AI principles from Microsoft.

Introduction to the Personally Identifiable
Information (PII) feature
Azure Language supports named entity recognition to identify and categorize information in
your text. The PII feature supports the detection of personal (PII) categories of entities. A wide
variety of personal entities such as names, organizations, addresses, phone numbers, financial
account numbers, or codes and government and country or region specific identification

numbers can be recognized. A subset of these personal entities is protected health information
(PHI). If you specify domain=phi in your request, you only get the PHI entities returned. The full
list of PII and PHI entity categories can be found in the table here.
Read example NER request and example response to see how to send text to the service and
what to expect back.

Example use cases
Customers may want to recognize various categories of PII for several reasons:
Apply sensitivity labels - For example, based on the results from the PII service, a public
sensitivity label might be applied to documents where no PII entities are detected. For
documents where US addresses and phone numbers are recognized, a confidential label
might be applied. A highly confidential label might be used for documents where bank
routing numbers are recognized.
Redact some categories of personal information from documents that get wider
circulation - For example, if customer contact records are accessible to frontline support
representatives, the company may want to redact the customer's personal information
besides their name from the version of the customer history to preserve the customer's
privacy.
Redact personal information in order to reduce unconscious bias - For example, during
a company's resume review process, they may want to block name, address, and phone
number to help reduce unconscious gender or other biases.
Replace personal information in source data for machine learning to reduce unfairness
â€“ For example, if you want to remove names that might reveal gender when training a
machine learning model, you could use the service to identify them and you could
replace them with generic placeholders for model training.
Remove personal information from call center transcription â€“ For example, if you want
to remove names or other PII data that happen between the agent and the customer in a
call center scenario. You could use the service to identify and remove them.

Considerations when choosing a use case
Avoid high-risk automatic redaction or information classification scenarios â€“ Any
scenario where failures to redact personal information could expose people to the risk of
identity theft and physical or psychological harms should include careful human
oversight.
Avoid scenarios that use personal information for a purpose that consent was not
obtained for - For example, a company has resumes from past job applicants. The
applicants didn't give their consent to be contacted for promotional events when they

submitted their resumes. Based on this scenario, the PII service shouldn't be used to
identify contact information for the purpose of inviting the past applicants to a trade
show.
Avoid scenarios that use the service to harvest personal information from publicly
available content.
Avoid scenarios that replace personal information in text with the intent to mislead
people.
Legal and regulatory considerations: Organizations need to evaluate potential specific
legal and regulatory obligations when using any AI services and solutions, which may not
be appropriate for use in every industry or scenario. Additionally, AI services or solutions
are not designed for and may not be used in ways prohibited in applicable terms of
service and relevant codes of conduct.

Characteristics and limitations
Depending on your scenario, input data and the entities you wish to extract, you could
experience different levels of performance. The following sections are designed to help you
understand key concepts about performance as they apply to using the Azure Language PII
service.

Understand and measure performance
Since both false positive and false negative errors can occur, it's important to understand how
both types of errors might affect your overall system. In redaction scenarios, for example, false
negatives could lead to personal information leakage. For redaction scenarios, consider a
process for human review to account for this type of error. For sensitivity label scenarios, both
false positives and false negatives could lead to misclassification of documents. The audience
may unnecessarily limit documents labeled as confidential where a false positive occurred. PII
could be leaked where a false negative occurred and a public label was applied.
You can adjust the threshold for confidence score your system uses to tune your system. If it's
more important to identify all potential instances of PII, you can use a lower threshold. This
means that you may get more false positives (non- PII data being recognized as PII entities),
but fewer false negatives (PII entities not recognized as PII). If it is more important for your
system to recognize only true PII data, you can use a higher threshold. Threshold values may
not have consistent behavior across individual categories of PII entities. Therefore, it is critical
that you test your system with real data it will process in production.

System limitations and best practices for enhancing
performance

Make sure you understand all the entity categories that can be recognized by the system.
Depending on your scenario, your data may include other information that could be
considered personal but isn't covered by the categories the service currently supports.
Context is important for all entity categories to be correctly recognized by the system, as
it often is for humans to recognize an entity. For example, without context, a ten-digit
number is just a number, not a PII entity. However, given context like You can reach me at
my office number 2345678901, both the system and a human can recognize the ten-digit
number as a phone number. Always include context when sending text to the system to
obtain the best possible performance.
Person names in particular require linguistic context. Send as much context as possible for
better person name detection.
For conversational data, consider sending more than a single turn in the conversation to
ensure higher likelihood that the required context is included with the actual entities. In
the following conversation, if you send a single row at a time, the passport number
doesn't have any context associated with it and the EU Passport Number PII category isn't
recognized.
Hi, how can I help you today?
I want to renew my passport
Sure, what is your current passport number?
Its 123456789, thanks.
However, if you send the whole conversation it will be recognized because the context is
included.
Sometimes multiple entity categories can be recognized for the same entity. If we take
the previous example:
Hi, how can I help you today?
I want to renew my passport
Sure, what is your current passport number?
Its 123456789, thanks.
Several different countries have the same format for passport numbers, so several
different specific entity categories may be recognized. In some cases, using the highest
confidence score may not be sufficient to choose the right entity class. If your scenario
depends on the specific entity category being recognized, you may need to disambiguate
the result elsewhere in your system either through a human review or additional
validation code. Thorough testing on real life data can help you identify if you're likely to

see multiple entity categories for recognized for your scenario.
Although many international entities are supported, currently the service only supports
English text. Consider verifying the language the input text is in if you're not sure it's all in
English.
The PII service only takes text as an input. If you are redacting information from
documents in other formats, make sure to carefully test your redaction code to ensure
identified entities aren't accidentally leaked.
The anonymization feature (2025-11-15-preview) substitutes personally identifiable
information (PII) with randomly chosen values from a predefined list specific to each
entity category. For example, a person's name is replaced with a name selected from a
relevant preset list.
The preset list of names includes both gender-specific and gender-agnostic options,
along with names from diverse cultural backgrounds. However, when a name is
associated with a particular gender or cultural context, those associations are not
preserved during replacement. As a result, this may cause unintended effects in
scenarios where gender or cultural identifiers associated with name-based personal
information are expected.
Using the anonymization feature can also create confusion for the end users of the
redacted text because, once PII values are replaced, end users may not realize that any
PII values were redacted.

See also
Transparency note for Azure Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Azure Language
Guidance for integration and responsible use with Azure Language

Last updated on 11/08/2025

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

Detect and redact Personally Identifying
Information in text
Azure Language in Foundry Tools is a cloud-based service that applies Natural Language
Processing (NLP) features to text-based data. The PII feature can evaluate unstructured text,
extract, and redact sensitive information (PII) and health information (PHI) in text across several
predefined categories.

Development options
To use PII detection, you submit text for analysis and handle the API output in your application.
Analysis is performed as-is, with no customization to the model used on your data. There are
two ways to use PII detection:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry
(new) portal

Foundry (new) is a cloud-based AI platform that provides streamlined access to
Foundry models, agents, and tools through Foundry projects.

Foundry (classic)
portal

Foundry (classic) is a cloud-based platform that supports hub-based projects
and other resource types. When you sign up, you can use your own data to
detect personally identifying information within text examples.

REST API or Client
library (Azure SDK)

Integrate PII detection into your applications using the REST API, or the client
library available in various languages.

Specify the PII detection model
By default, this feature uses the latest available AI model on your text. You can also configure
your API requests to use a specific model version.

Input languages
When you submit input text to be processed, you can specify which of the supported
languages they're written in. If you don't specify a language, extraction defaults to English. The
API may return offsets in the response to support different multilingual and emoji encodings.

Additional configuration parameters (2025-11-15preview)
ï¼‰ Important
Azure Language in Foundry Tools public preview releases provide early access to
features that are in active development.
Features, approaches, and processes may change, before General Availability (GA),
based on user feedback.
Preview features are subject to the terms applicable to Previews as described in the
Supplemental Terms of Use for Microsoft Azure Previews
Products and Services Data Protection Addendum (DPA)

and the Microsoft
.

Redaction policies
Starting with version 2025-11-15-preview and onward, you can specify the redactionPolicies
parameter to define which redaction policies are applied when processing text. You can include
more than one policy in a single request, with one policy specified as the
defaultRedactionPolicy and additional policy overrides for specified entities.

The policy field accepts four policy types:
ï¼‚ SyntheticReplacement ðŸ†•
ï¼‚ CharacterMask (default)
ï¼‚ NoMask
ï¼‚ EntityMask
For more information, see REST API PII task parameters.

syntheticReplacement policy type ðŸ†•
The syntheticReplacement policy type** replaces a detected PII entity with a replacement
value. For instance, an input like "John Doe received a call from 424-878-9193." can be
transformed into "Sam Johnson received a call from 401-255-6901." These substitutes are
randomly selected from a predefined set of alternative values.
Bash
POST {Endpoint}/language/:analyze-text?api-version=2025-11-15-preview

{
"kind": "PiiEntityRecognition",
"parameters": {
"modelVersion": "latest",
"redactionPolicies": [
{
"policyKind": "syntheticReplacement",
"entityTypes": [
"Person",
"PhoneNumber"
]
}
]
}
}

characterMask policy type
The characterMask policy type** enables you to mask redactedText using a specified character
(for example, "") while preserving the length and offset of the original text. For instance, "***
received a call from ************"
Additionally, there's also an optional field named redactionCharacter that allows you to
specify the character used for redaction when applying the characterMask policy.
Sample request
Bash
POST {Endpoint}/language/:analyze-text?api-version=2025-11-15-preview
{
"kind": "PiiEntityRecognition",
"parameters": {
"modelVersion": "latest",
"redactionPolicies": [
{
"policyKind": "characterMask",
"redactionCharacter": "-"
}
]
}
}

noMask policy type

noMask policy type** enables you to return the response without including the redactedText
field. For example, "John Doe received a call from 424-878-919."
Sample request
Bash
POST {Endpoint}/language/:analyze-text?api-version=2025-11-15-preview
{
"kind": "PiiEntityRecognition",
"parameters": {
"modelVersion": "latest",
"redactionPolicies": [
{
"policyKind": "noMask"
}
]
}
}

entityMask policy type
The entityMask policy type** enables you to mask the detected PII entity text its
corresponding entity type. For example, "[PERSON_1] received a call from
[PHONENUMBER_1]."
Bash
POST {Endpoint}/language/:analyze-text?api-version=2025-11-15-preview
{
"kind": "PiiEntityRecognition",
"parameters": {
"modelVersion": "latest",
"redactionPolicies": [
{
"policyKind": "entityMask"
}
]
}
}

To learn more, see Transparency Note for Personally Identifiable Information (PII).

ConfidenceScoreThreshold ðŸ†•
The PII feature currently redacts all detected entities, regardless of their confidence scores.
Thus, entities with low confidence scores are also removed, even if retaining them is preferred.
To enhance flexibility, you can configure a confidence threshold that determines the minimum
confidence score an entity must have to remain in the output.
Sample request
Bash
POST {Endpoint}/language/:analyze-text?api-version=2025-11-15-preview
{
"kind":"PiiEntityRecognition",
"parameters":{
"modelVersion":"latest",
"confidenceScoreThreshold":{
"default":0.9,
"overrides":[
{
"value":0.8,
"entity":"USSocialSecurityNumber"
},
{
"value":0.6,
"entity":"Person",
"language":"en"
}
]
}
}
}

To learn more, see REST API reference: ConfidenceScoreThreshold

DisableEntityValidation
When you use the PII service, it validates multiple entity types to ensure data integrity and
minimize false positives. However, this strict validation can sometimes slow down workflows
where validation isn't necessary. To give you more flexibility, we're introducing a parameter
that lets you disable entity validation if you choose. By default, this parameter is set to false,
which means strict entity validation remains in place. If you want to bypass entity validation for
your requests, you can set the parameter to true.
Sample request

Bash

POST {Endpoint}/language/:analyze-text?api-version=2025-11-15-preview
{
"kind":"PiiEntityRecognition",
"parameters":{
"modelVersion":"latest",
"disableEntityValidation":"true | false"
},
"analysisInput":{
"documents":[
{
"id":"id01",
"text":"blah"
}
]
}
}

To learn more, see REST API reference: PiiTaskParameters

Select which entities to be returned
The API attempts to detect the defined entity categories for a given input text language. If you
want to specify which entities are detected and returned, use the optional piiCategories
parameter with the appropriate entity categories. This parameter can also let you detect
entities that aren't enabled by default for your input text language. The following example
would detect only Person . You can specify one or more entity types to be returned.
îª€ Tip
If you don't include default when specifying entity categories, The API only returns the
entity categories you specify.
Input:
ï¼— Note
In this example, it returns only the person entity type:

https://<your-language-resource-endpoint>/language/:analyze-text?api-version=2022-05-01

Bash
{
"kind": "PiiEntityRecognition",
"parameters": {
"modelVersion": "latest",
"piiCategories": [
"Person"
],
"redactionPolicies": {
"policyKind": "characterMask",
"redactionCharacter": "*"
# MaskWithCharacter|MaskWithEntityType|DoNotRedact
}
},
"analysisInput": {
"documents": [
{
"id": "1",
"language": "en",
"text": "We went to Contoso foodplace located at downtown Seattle last week
for a dinner party, and we adore the spot! They provide marvelous food and they have
a great menu. The chief cook happens to be the owner (I think his name is John Doe)
and he is super nice, coming out of the kitchen and greeted us all. We enjoyed very
much dining in the place! The pasta I ordered was tender and juicy, and the place
was impeccably clean. You can even pre-order from their online menu at
www.contosofoodplace.com, call 112-555-0176 or send email to
order@contosofoodplace.com! The only complaint I have is the food didn't come fast
enough. Overall I highly recommend it!"
}
]
}
}

Output:
Bash
{
"kind": "PiiEntityRecognitionResults",
"results": {
"documents": [
{
"redactedText": "We went to Contoso foodplace located at downtown
Seattle last week for a dinner party, and we adore the spot! They provide marvelous
food and they have a great menu. The chief cook happens to be the owner (I think his
name is ********) and he is super nice, coming out of the kitchen and greeted us
all. We enjoyed very much dining in the place! The pasta I ordered was tender and
juicy, and the place was impeccably clean. You can even pre-order from their online
menu at www.contosofoodplace.com, call 112-555-0176 or send email to
order@contosofoodplace.com! The only complaint I have is the food didn't come fast
enough. Overall I highly recommend it!",
"id": "1",

"entities": [
{
"text": "John Doe",
"category": "Person",
"offset": 226,
"length": 8,
"confidenceScore": 0.98
}
],
"warnings": []
}
],
"errors": [],
"modelVersion": "2021-01-15"
}
}

Adapting PII to your domain
To accommodate and adapt to a customer's custom vocabulary used to identify entities (also
known as the "context"), the entitySynonyms feature allows customers to define their own
synonyms for specific entity types.
This feature is designed to identify entities within contexts that may be unfamiliar to the model,
especially terms specific to the customer's input. By doing so, it ensures that the customer's
unique terminology is accurately recognized and properly linked during the detection process.
The valueExclusionPolicy option allows customers to adapt the PII service for scenarios where
customers prefer certain terms not to be detected and redacted even if those terms fall into a
PII category they're interested in detected. For example, a police department might want
personal identifiers redacted in most cases except for terms like "police officer," "suspect," and
"witness."
Customers can now adapt the PII service's detecting by specifying their own regex using a
regex recognition configuration file. See our container how-to guides for a tutorial on how to
install and run Personally Identifiable Information (PII) Detection containers.
A more detailed tutorial can be found in the "Adapting PII to your domain" how-to guide.

Submitting data
Analysis is performed upon receipt of the request. Using the PII detection feature
synchronously is stateless. No data is stored in your account, and results are returned
immediately in the response.

When using this feature asynchronously, the API results are available for 24 hours from the
time the request was ingested, and is indicated in the response. After this time period, the
results are purged and are no longer available for retrieval.

Getting PII results
When you get results from PII detection, you can stream the results to an application or save
the output to a file on the local system. The API response includes recognized entities,
including their categories and subcategories, and confidence scores. The text string with the PII
entities redacted is also returned.

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

Next steps
Personally Identifying Information (PII) overview

Last updated on 11/18/2025

Detect and redact Personally Identifying
Information in conversations
Azure Language in Foundry Tools conversation PII API analyzes audio discourse to identify and
redact sensitive information (PII) using various predefined categories. This API works on both
transcribed text (referred to as transcripts) and chats. For transcripts, it also facilitates the
redaction of audio segments containing PII by providing the timing information for those
segments.

Determine how to process the data (optional)
Specify the PII detection model
By default, this feature uses the latest available AI model on your input. You can also configure
your API requests to use a specific model version.

Language support
For more information, see the PII Language Support page. Currently the conversational PII GA
model only supports the English language. The preview model and API support the same list
languages as the other Languages.

Region support
The conversational PII API supports all Azure regions supported by Azure Language.

Submitting data
You can submit the input to the API as list of conversation items. Analysis is performed upon
receipt of the request. Because the API is asynchronous, there may be a delay between sending
an API request, and receiving the results. For information on the size and number of requests
you can send per minute and second, see the following data limits.
When you use the async feature, the API results are available for 24 hours from the time the
request was ingested, and is indicated in the response. After this time period, the results are
purged and are no longer available for retrieval.
When you submit data to conversational PII, you can send one conversation (chat or spoken)
per request.

The API attempts to detect all the defined entity categories for a given conversation input. If
you want to specify which entities are detected and returned, use the optional piiCategories
parameter with the appropriate entity categories.
For spoken transcripts, the entities detected are returned on the redactionSource parameter
value provided. Currently, the supported values for redactionSource are text , lexical , itn ,
and maskedItn (which maps to Speech to text REST API's display \ displayText , lexical , itn ,
and maskedItn format respectively). Additionally, for the spoken transcript input, this API also
provides audio timing information to empower audio redaction. For using the audioRedaction
feature, use the optional includeAudioRedaction flag with true value. The audio redaction is
performed based on the lexical input format.
ï¼— Note
Conversation PII now supports 40,000 characters as document size.

Getting PII results
When you get results from PII detection, you can stream the results to an application or save
the output to a file on the local system. The API response includes recognized entities,
including their categories and subcategories, and confidence scores. The text string with the PII
entities redacted is also returned.

Examples
Client libraries (Azure SDK)

1. Go to your resource overview page in the Azure portal
2. From the menu on the left side, select Keys and Endpoint. You need one of the keys
and the endpoint to authenticate your API requests.
3. Download and install the client library package for your language of choice:
ï¾‰

Language

Package version

.NET

1.0.0

Expand table

Language

Package version

Python

1.0.0

4. For more information on the client and return object, see the following reference
documentation:
C#
Python

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

Last updated on 11/18/2025

Detect and redact Personally Identifying
Information in native documents (preview)
ï¼‰ Important
Azure Language in Foundry Tools public preview releases provide early access to
features that are in active development.
Features, approaches, and processes may change, before General Availability (GA),
based on user feedback.
Language is a cloud-based service that applies Natural Language Processing (NLP) features to
text-based data. The native document support capability enables you to send API requests
asynchronously, using an HTTP POST request body to send your data and HTTP GET request
query string to retrieve the status results. Your processed documents are located in your Azure
Blob Storage target container.
A native document refers to the file format used to create the original document such as
Microsoft Word (docx) or a portable document file (pdf). Native document support eliminates
the need for text preprocessing before using Language resource capabilities. Currently, native
document support is available for the following capabilities:
Personally Identifiable Information (PII). The PII detection feature can identify, categorize,
and redact sensitive information in unstructured text. The PiiEntityRecognition API
supports native document processing.
Document summarization. Document summarization uses natural language processing to
generate extractive (salient sentence extraction) or abstractive (contextual word
extraction) summaries for documents. Both AbstractiveSummarization and
ExtractiveSummarization APIs support native document processing.

Supported document formats
Applications use native file formats to create, save, or open native documents. Currently PII and
Document summarization capabilities supports the following native document formats:
ï¾‰

File type

File extension

Description

Text

.txt

An unformatted text document.

Expand table

File type

File extension

Description

PDF

.pdf

A portable document file formatted document and scanned PDFs.

Microsoft Word

.docx

A Microsoft Word document file.

Input guidelines
Supported file formats

Type

support and limitations

Text within images

Digital images with embedded text aren't supported.

Digital tables

Tables in scanned documents aren't supported.

ï¾‰

Expand table

ï¾‰

Expand table

Document Size

Attribute

Input limit

Total number of documents per request

â‰¤ 40

Total content size per request

â‰¤ 10 MB

Include native documents with an HTTP request
Let's get started:
For this project, we use the cURL command-line tool to make REST API calls.
ï¼— Note
The cURL package is preinstalled on most Windows 10 and Windows 11 and most
macOS and Linux distributions. You can check the package version with the following
commands: Windows: curl.exe -V macOS curl -V Linux: curl --version
An active Azure account

. If you don't have one, you can create a free account

.

An Azure Blob Storage account

. You also need to create containers in your Azure Blob

Storage account for your source and target files:
Source container. This container is where you upload your native files for analysis
(required).
Target container. This container is where your analyzed files are stored (required).
A single-service Language resource

(not a multi-service Microsoft Foundry resource):

Complete Azure Language resource project and instance details fields as follows:
1. Subscription. Select one of your available Azure subscriptions.
2. Resource Group. You can create a new resource group or add your resource to a
preexisting resource group that shares the same lifecycle, permissions, and policies.
3. Resource Region. Choose Global unless your business or application requires a
specific region. If you're planning on using a system-assigned managed identity for
authentication, choose a geographic region like West US.
4. Name. Enter the name you chose for your resource. The name you choose must be
unique within Azure.
5. Pricing tier. You can use the free pricing tier ( Free F0 ) to try the service, and
upgrade later to a paid tier for production.
6. Select Review + Create.
7. Review the service terms and select Create to deploy your resource.
8. After your resource successfully deploys, select Go to resource.

Retrieve your key and Language endpoint
Requests to Azure Language require a read-only key and custom endpoint to authenticate
access.
1. If you created a new resource, after it deploys, select Go to resource. If you have an
existing Language resource, navigate directly to your resource page.
2. In the left rail, under Resource Management, select Keys and Endpoint.
3. You can copy and paste your key and your Language instance endpoint into the code
samples to authenticate your request to Azure Language. Only one key is necessary to
make an API call.

Create Azure Blob Storage containers
Create containers in your Azure Blob Storage account

for source and target files.

Source container. This container is where you upload your native files for analysis
(required).
Target container. This container is where your analyzed files are stored (required).

Authentication
Your Language resource needs granted access to your storage account before it can create,
read, or delete blobs. There are two primary methods you can use to grant access to your
storage data:
Shared access signature (SAS) tokens. User delegation SAS tokens are secured with
Microsoft Entra credentials. SAS tokens provide secure, delegated access to resources in
your Azure storage account.
Managed identity role-based access control (RBAC). Managed identities for Azure
resources are service principals that create a Microsoft Entra identity and specific
permissions for Azure managed resources.
For this project, we authenticate access to the source location and target location URLs with
Shared Access Signature (SAS) tokens appended as query strings. Each token is assigned to a
specific blob (file).

Your source container or blob must designate read and list access.
Your target container or blob must designate write and list access.
îª€ Tip
Since we're processing a single file (blob), we recommend that you delegate SAS access
at the blob level.

Request headers and parameters
ï¾‰

Expand table

parameter

Description

-X POST <endpoint>

Specifies your Language resource endpoint for accessing the
API.

--header Content-Type:

The content type for sending JSON data.

application/json
--header "Ocp-Apim-Subscription-Key:

Specifies Azure Language resource key for accessing the API.

<key>
-data

The JSON file containing the data you want to pass with your
request.

The following cURL commands are executed from a BASH shell. Edit these commands with
your own resource name, resource key, and JSON values. Try analyzing native documents by
selecting the Personally Identifiable Information (PII) or Document Summarization code
sample project:

PII Sample document
For this quickstart, you need a source document uploaded to your source container. You can
download our Microsoft Word sample document

or Adobe PDF

for this project. The source

language is English.

Build the POST request
1. Using your preferred editor or IDE, create a new directory for your app named nativedocument .

2. Create a new json file called pii-detection.json in your native-document directory.
3. Copy and paste the following Personally Identifiable Information (PII) request sample into
your pii-detection.json file. Replace {your-source-container-SAS-URL} and {yourtarget-container-SAS-URL} with values from your Azure portal Storage account containers

instance:
Request sample
JSON
{
"displayName": "Document PII Redaction example",
"analysisInput": {
"documents": [

{
"language": "en-US",
"id": "Output-1",
"source": {
"location": "{your-source-blob-with-SAS-URL}"
},
"target": {
"location": "{your-target-container-with-SAS-URL}"
}
}
]
},
"tasks": [
{
"kind": "PiiEntityRecognition",
"taskName": "Redact PII Task 1",
"parameters": {
"redactionPolicy": {
"policyKind": "entityMask" // Optional. Defines
redactionPolicy; changes behavior based on value. Options: noMask, characterMask
(default), and entityMask.
},
"piiCategories": [
"Person",
"Organization"
],
"excludeExtractionData": false // Default is false. If true, only
the redacted document is stored, without extracted entities data.
}
}
]
}

The source location value is the SAS URL for the source document (blob), not the source
container SAS URL.
The redactionPolicy possible values are UseRedactionCharacterWithRefId (default) or
UseEntityTypeName . For more information, see PiiTask Parameters.

Run the POST request
1. Here's the preliminary structure of the POST request:
Bash
POST {your-language-endpoint}/language/analyze-documents/jobs?apiversion=2024-11-15-preview

2. Before you run the POST request, replace {your-language-resource-endpoint} and {yourkey} with the values from your Azure portal Language instance.

ï¼‰ Important
Remember to remove the key from your code when you're done, and never post it
publicly. For production, use a secure way of storing and accessing your credentials
like Azure Key Vault. For more information, see Foundry Tools security.
PowerShell
PowerShell
cmd /c curl "{your-language-resource-endpoint}/language/analyzedocuments/jobs?api-version=2024-11-15-preview" -i -X POST --header "ContentType: application/json" --header "Ocp-Apim-Subscription-Key: {your-key}" --data
"@pii-detection.json"

command prompt / terminal
Bash
curl -v -X POST "{your-language-resource-endpoint}/language/analyzedocuments/jobs?api-version=2024-11-15-preview" --header "Content-Type:
application/json" --header "Ocp-Apim-Subscription-Key: {your-key}" --data
"@pii-detection.json"

3. Here's a sample response:
HTTP
HTTP/1.1 202 Accepted
Content-Length: 0
operation-location: https://{your-language-resource-endpoint}/language/analyzedocuments/jobs/f1cc29ff-9738-42ea-afa5-98d2d3cabf94?api-version=2024-11-15preview
apim-request-id: e7d6fa0c-0efd-416a-8b1e-1cd9287f5f81
x-ms-region: West US 2
Date: Thu, 25 Jan 2024 15:12:32 GMT

POST response ( jobId)
You receive a 202 (Success) response that includes a read-only Operation-Location header. The
value of this header contains a jobId that can be queried to get the status of the asynchronous
operation and retrieve the results using a GET request:

Get analyze results (GET request)
1. After your successful POST request, poll the operation-location header returned in the
POST request to view the processed data.
2. Here's the preliminary structure of the GET request:
Bash
GET {your-language-endpoint}/language/analyze-documents/jobs/{jobId}?apiversion=2024-11-15-preview

3. Before you run the command, make these changes:
Replace {jobId} with the Operation-Location header from the POST response.
Replace {your-language-resource-endpoint} and {your-key} with the values from
your Language instance in the Azure portal.

Get request
PowerShell
cmd /c curl "{your-language-resource-endpoint}/language/analyzedocuments/jobs/{jobId}?api-version=2024-11-15-preview" -i -X GET --header "ContentType: application/json" --header "Ocp-Apim-Subscription-Key: {your-key}"

Bash
curl -v -X GET "{your-language-resource-endpoint}/language/analyzedocuments/jobs/{jobId}?api-version=2024-11-15-preview" --header "Content-Type:
application/json" --header "Ocp-Apim-Subscription-Key: {your-key}"

Examine the response
You receive a 200 (Success) response with JSON output. The status field indicates the result of
the operation. If the operation isn't complete, the value of status is "running" or "notStarted",
and you should call the API again, either manually or through a script. We recommend an
interval of one second or more between calls.

Sample response
JSON

{
"jobId": "f1cc29ff-9738-42ea-afa5-98d2d3cabf94",
"lastUpdatedDateTime": "2024-01-24T13:17:58Z",
"createdDateTime": "2024-01-24T13:17:47Z",
"expirationDateTime": "2024-01-25T13:17:47Z",
"status": "succeeded",
"errors": [],
"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "PiiEntityRecognitionLROResults",
"lastUpdateDateTime": "2024-01-24T13:17:58.33934Z",
"status": "succeeded",
"results": {
"documents": [
{
"id": "doc_0",
"source": {
"kind": "AzureBlob",
"location": "https://myaccount.blob.core.windows.net/sampleinput/input.pdf"
},
"targets": [
{
"kind": "AzureBlob",
"location": "https://myaccount.blob.core.windows.net/sampleoutput/df6611a3-fe74-44f8-b8d4-58ac7491cb13/PiiEntityRecognition0001/input.result.json"
},
{
"kind": "AzureBlob",
"location": "https://myaccount.blob.core.windows.net/sampleoutput/df6611a3-fe74-44f8-b8d4-58ac7491cb13/PiiEntityRecognition-0001/input.docx"
}
],
"warnings": []
}
],
"errors": [],
"modelVersion": "2023-09-01"
}
}
]
}
}

Upon successful completion:
The analyzed documents can be found in your target container.

The successful POST method returns a 202 Accepted response code indicating that the
service created the batch request.
The POST request also returned response headers including Operation-Location that
provides a value used in subsequent GET requests.

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
PII detection overview

Last updated on 11/18/2025

Adapt Personally Identifying Information
(PII) to your domain
To support using your own terminology to identify entities (also known as context), the
entitySynonyms feature enables you to define custom synonyms for specific entity types. This

feature helps the system detect entities that appear in your inputs using terms or vocabulary
that the model doesn't recognize by default. Aligning your specific terms with standard entities
allows the model to accurately recognize and link these terms during entity detection.
This custom vocabulary support enhances the prebuilt PII (Personally Identifiable Information)
detection service, which is originally trained on general language and may not understand
specialized or informal vocabularyâ€”such as using BAN instead of
InternationalBankAccountNumber. As a result, PII detection is capable of recognizing sensitive
information even when it appears in slang, abbreviations, or informal language. This detection
enhancement strengthens the system's ability to safeguard privacy in everyday, real-world
scenarios.
We strongly recommend that you first test the accuracy of the entity detection feature without
adding synonyms. Then only introduce custom synonyms if the model doesn't perform well
with the default settings. For instance, if the model already recognizes Org as organization,
there's no need to add it as a synonym.
Once you test the service with your own data, you can use entitySynonyms to:
Identify specific entities within the prebuilt service that require custom synonym context
words from your input vocabulary.
Provide a list of custom synonyms for context entities.
Specify the language of each synonym.

API Schema for the 'entitySynonyms' parameter
JSON
{
"parameter":
"entitySynonyms": [
{
"entityType": "InternationalBankAccountNumber",
"synonyms": [ {"synonym": "BAN", "language": "en"} ]
}
]
}

Usage guidelines
1. Synonyms must be restricted to phrases that directly refer to the type, and preserve
semantic correctness. For example, for the entity type InternationalBankAccountNumber , a
valid synonym could be "Financial Account Number" or FAN. But, the word deposit
though may be associated with type, as it doesn't directly have a meaning of a bank
account number and therefore shouldn't be used.
2. Synonyms should be country/region agnostic. For example, German passport wouldn't be
helpful to include.
3. Synonyms can't be reused for more than one entity type.
4. This synonym recognition feature only accepts a subset of entity types supported by the
service. The supported entity types and example synonyms include:
ï¾‰

Supported

Expand table

Entity Type

Example synonyms

ABARoutingNumber

Routing transit number (RTN)

Address

Address

My place is

Age

Age

Years old, age in years, current age, person's

entity type
ABA Routing
Number

age, biological age
Bank Account

BankAccountNumber

Number

Bank acct no., savings account number,
checking account number, financial account
number

Credit Card

CreditCardNumber

Number

Cc number, payment card number, credit acct
no.

Date

DateTime

Given date, specified date

Date of Birth

DateOfBirth

Birthday, DOB, birthdate

International

InternationalBankingAccountNumber

IBAN, intl bank acct no.

Organization

company, business, firm, corporation, agency,

Bank Account
Number
Organization

group, institution, entity, legal entity, party,
respondent, plaintiff, defendant, jurisdiction,
partner, provider, facility, practice, network,
institution, enterprise, LLC, Inc, LLP,
incorporated, employer, brand, subsidiary

Supported

Entity Type

Example synonyms

Person

Person

Name, individual, account holder

Person Type

PersonType

Role, title, position

Phone number

PhoneNumber

Landline, cell, mobile

Swift Code

SWIFTCode

SWIFT code, BIC (Bank Identifier Code), SWIFT
Identifier

entity type

Customizing PII output by specifying values to
exclude
The valueExclusionPolicy option allows you to adapt the PII service for scenarios where
certain preferred terms can be undetected and redacted even if those terms fall into a PII
category you're interested in detecting. For example, a police department might want personal
identifiers redacted in most cases except for terms like police officer, suspect, and witness.
In the following example, you can use the valueExclusionPolicy option to specify a list of
values that you wouldn't like to be detected or redacted from the input text. In the next
example, if the user enters the value 1 Microsoft Way, Redmond, WA 98052, US, this value isn't
redacted. It also isn't included in the returned API payload output, even if the Address entity is
enabled.
A subset of the specified excluded value, such as One Microsoft Way isn't excluded.

Input
JSON
{
"kind": "PiiEntityRecognition",
"parameters": {
"modelVersion": "latest",
"redactionPolicy": {
"policyKind": "characterMask",
"redactionCharacter": "-"
},
"valueExclusionPolicy": {
"caseSensitive": false,
"excludedValues": {
"1 Microsoft Way, Redmond, WA 98052",
"1045 La Avenida St, Mountain View, CA 94043"
}

}
},
"analysisInput": {
"documents": [
{
"id": "1",
"text": "The police and John Doe inspected the storage garages located at
123 Main St, 1 Microsoft Way, Redmond, WA 98052, 456 Washington Blvd, Portland, OR,
and 1045 La Avenida St, Mountain View, CA 94043"
}
]
}
}

Output
JSON
{
"kind": "PiiEntityRecognitionResults",
"results": {
"documents": [
{
"redactedText": "The police and John Doe inspected the storage
garages located at **********, 1 Microsoft Way, Redmond, WA 98052,
********************************, and 1045 La Avenida St, Mountain View, CA 94043"
"id": "1",
"entities": [
{
"text": "John Doe",
"category": "Person",
"offset": 16,
"length": 5,
"confidenceScore": 0.98
}
],
"warnings": []
}
],
"errors": [],
"modelVersion": "2021-01-15"
}
}

Customizing PII detection using your own regex
(only available for Text PII container)
You can now adapt the PII service's detecting by specifying your own regex using a regex
recognition configuration file. See our container how-to guides for a tutorial on how to install

and run Personally Identifiable Information (PII) Detection containers.
ï¼— Note
Regex specification is only available for the Text PII container.

Bash
docker run --rm -it -p 5000:5000 --memory 8g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/textanalytics/pii:{IMAGE_TAG} \
Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY} \
UserRegexRuleFilePath={REGEX_RULE_FILE_PATH}
UserRegexRuleFilePath is the file path of the user defined regex rules.

Regex recognition file format
JSON
[
{
"name": "USSocialSecurityNumber", // category, type and tag to be returned.
This name must be unique
"description": "Rule to identify USSocialSecurityNumber in text", // used to
describe the category
"regexPatterns": [ // list of regex patterns to identify the entities
{
"id": "StrongSSNPattern", // id for the regex pattern
"pattern": "(?<!\\d)([0-9]{3}-[0-9]{2}-[0-9]{4}|[0-9]{3} [0-9]{2} [0-9]
{4}|[0-9]{3}.[0-9]{2}.[0-9]{4})(?!\\d)", // regex pattern to provide matches
"matchScore": 0.65, // score to assign if the regex matches
"locales": [ // list of languages valid for this regex
"en"
]
},
{
"id": "WeakSSNPattern",
"pattern": "(?<!\\d)([0-9]{9})(?!\\d)",
"matchScore": 0.55,
"locales": [
"en"
]
}
],
"matchContext": { // patterns to give matches context
"hints": [
{

"hintText": "ssa(\\s*)number", // regex pattern to find to give a match
context.
"boostingScore": 0.2, // score to boost match confidence if hint is
found
"locales": [ // list of languages valid for this context
"en"
]
},
{
"hintText": "social(\\s*)security(\\s*)(#*)",
"boostingScore": 0.2,
"locales": [
"en"
]
}
],
}
}
]

Overview of each regex recognition file parameter
ï¾‰

Parameter

Subparameters and Descriptions

name

Category, type, and tag to return if there's a regex match.

decription

(optional) User-readable rule description.

regexPatterns

List of regex patterns used to find entities.
* id : Identifier of the regex pattern.

Expand table

- matchScore : Confidence score for regex matches.
* locales : Languages valid for the regex pattern.
matchcontext

Regex patterns providing context to matched entities. Context matching is a
bidirectional search from the matched entity that increases confidence score in when
found. If multiple hints support a match, the hint with the highest score is used.
* hints : List of regex patterns giving context to matched entities.
* hintText : Regex pattern providing context to matched entities.
* boostingScore : (optional) Score added to confidence score from a matched entity.
* locales : Language valid for hintText.
* contextLimit : (optional) Distance from the matched entity to search for context.

Logging
To display information about the running regexRules , add the following property to enable
debug logging: Logging:Console:LogLevel:Default=Debug

Bash
docker run --rm -it -p 5000:5000 --memory 8g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/textanalytics/pii:{IMAGE_TAG} \
Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY} \
UserRegexRuleFilePath={REGEX_RULE_FILE_PATH} \
Logging:Console:LogLevel:Default=Debug

Regex rule constraints
Rule names must begin with CE_
Rule names must be unique.
Rule names may only use alphanumeric characters and underscores (_)
Regex patterns follow the .NET regular Expressions format. For more information, see our
documentation on .NET regular expressions.

Last updated on 11/18/2025

Install and run Personally Identifiable
Information (PII) Detection containers
ï¼— Note
The data limits in a single synchronous API call for the PII container are 5,120 characters
per document and up to 10 documents per call.
Containers enable you to host the PII detection API on your own infrastructure. If you have
security or data governance requirements that can't be fulfilled by calling PII detection
remotely, then containers might be a good option.
If you don't have an Azure subscription, create a free account

before you begin.

Prerequisites
You must meet the following prerequisites before using PII detection containers.
If you don't have an Azure subscription, create a free account
Docker

.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts

.

A Language resource

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:

Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the available
container. Each CPU core must be at least 2.6 gigahertz (GHz) or faster.
We recommend that you have a CPU with AVX-512 instruction set, for the best experience
(performance and accuracy).
ï¾‰

PII detection

Minimum host specs

Recommended host specs

1-core, 2-GB memory

4-cores, 8-GB memory

Expand table

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Get the container image with docker pull
The PII detection container image can be found on the mcr.microsoft.com container registry
syndicate. It resides within the azure-cognitive-services/textanalytics/ repository and is
named pii . The fully qualified container image name is, mcr.microsoft.com/azure-cognitiveservices/textanalytics/pii

To use the latest version of the container, you can use the latest tag, which is for English. You
can also find a full list of containers for supported languages using the tags on the MCR

.

The latest PII detection container is available in several languages. To download the container
for the English container, use the following command.

docker pull mcr.microsoft.com/azure-cognitive-services/textanalytics/pii:latest

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container with docker run
Once the container is on the host computer, use the docker run

command to run the

containers. The container continues to run until you stop it. Replace the placeholders with your
own values:
ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove this character based on your host
operating system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container doesn't start. For more information, see Billing.
To run the PII detection container, execute the following docker run command. Replace the
placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Language resource.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

You can find it on your resource's Key
and endpoint page, on the Azure
portal.
{ENDPOINT_URI}

The endpoint for accessing the API.
You can find it on your resource's Key
and endpoint page, on the Azure

https://<your-customsubdomain>.cognitiveservices.azure.com

portal.
{IMAGE_TAG}

The image tag representing the
language of the container you want
to run. Make sure it matches the
docker pull command you used.

latest

Bash
docker run --rm -it -p 5000:5000 --memory 8g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/textanalytics/pii:{IMAGE_TAG} \
Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY}

This command:
Runs a PII detection container from the container image
Allocates one CPU core and 8 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Expand table

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

For information on how to call PII see our guide.

Run the container disconnected from the internet
To use this container disconnected from the internet, you must first request access by filling
out an application, and purchasing a commitment plan. See Use Docker containers in
disconnected environments for more information.
If you have been approved to run the container disconnected from the internet, use the
following example shows the formatting of the docker run command you'll use, with

placeholder values. Replace these placeholder values with your own values.
The DownloadLicense=True parameter in your docker run command will download a license file
that will enable your Docker container to run when it isn't connected to the internet. It also
contains an expiration date, after which the license file will be invalid to run the container. You
can only use a license file with the appropriate container that you've been approved for. For
example, you can't use a license file for a speech to text container with a Document
Intelligence container.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitiveservices/form-recognizer/invoice

{LICENSE_MOUNT}

The path where the
license will be
downloaded, and
mounted.

/host/license:/path/to/license/directory

{ENDPOINT_URI}

The endpoint for

https://<your-custom-

authenticating your
service request. You can
find it on your
resource's Key and
endpoint page, on the
Azure portal.

subdomain>.cognitiveservices.azure.com

{API_KEY}

The key for your Text
Analytics resource. You
can find it on your
resource's Key and
endpoint page, on the
Azure portal.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

{CONTAINER_LICENSE_DIRECTORY}

Location of the license

/path/to/license/directory

folder on the
container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 \
-v {LICENSE_MOUNT} \
{IMAGE} \
eula=accept \
billing={ENDPOINT_URI} \
apikey={API_KEY} \

DownloadLicense=True \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}

Once the license file has been downloaded, you can run the container in a disconnected
environment. The following example shows the formatting of the docker run command you'll
use, with placeholder values. Replace these placeholder values with your own values.
Wherever the container is run, the license file must be mounted to the container and the
location of the license folder on the container's local filesystem must be specified with
Mounts:License= . An output mount must also be specified so that billing usage records can be

written.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image

mcr.microsoft.com/azure-cognitive-

you want to use.

services/form-recognizer/invoice

The appropriate size

4g

{MEMORY_SIZE}

of memory to
allocate for your
container.
{NUMBER_CPUS}

The appropriate
number of CPUs to
allocate for your

4

container.
{LICENSE_MOUNT}

The path where the
license will be
located and
mounted.

/host/license:/path/to/license/directory

{OUTPUT_PATH}

The output path for
logging usage

/host/output:/path/to/output/directory

records.
{CONTAINER_LICENSE_DIRECTORY}

Location of the
license folder on the
container's local
filesystem.

/path/to/license/directory

{CONTAINER_OUTPUT_DIRECTORY}

Location of the
output folder on the

/path/to/output/directory

container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 --memory {MEMORY_SIZE} --cpus {NUMBER_CPUS} \
-v {LICENSE_MOUNT} \
-v {OUTPUT_PATH} \
{IMAGE} \
eula=accept \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}
Mounts:Output={CONTAINER_OUTPUT_DIRECTORY}

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files. These files are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
The PII detection containers send billing information to Azure, using a Language resource on
your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure

The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If
the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

For more information about these options, see Configure containers.

Summary
In this article, you learned concepts and workflow for downloading, installing, and running PII
detection containers. In summary:
PII detection provides Linux containers for Docker
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You must specify billing information when instantiating a container.
ï¼‰ Important
Azure AI containers aren't licensed to run without being connected to Azure for metering.
Customers need to enable the containers to communicate billing information with the

metering service always. Azure AI containers don't send customer data (for example, text
that's being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 12/05/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

Use Docker containers in disconnected
environments
Containers enable you to run Foundry Tools APIs in your own environment, and are great for
your specific security and data governance requirements. Disconnected containers enable you
to use several of these APIs disconnected from the internet. Currently, the following containers
can be run in this manner:
Speech to text
Custom Speech to text
Neural Text to speech
Text Translation (Standard)
Azure Language in Foundry Tools
Sentiment Analysis
Key Phrase Extraction
Language Detection
Summarization
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)
Azure Vision in Foundry Tools - Read
Document Intelligence
Before attempting to run a Docker container in an offline environment, make sure you know
the steps to successfully download and use the container. For example:
Host computer requirements and recommendations.
The Docker pull command you use to download the container.
How to validate that a container is running.
How to send queries to the container's endpoint, once it's running.

Request access to use containers in disconnected
environments
Fill out and submit the request form

to request access to the containers disconnected from

the internet.
The form requests information about you, your company, and the user scenario for which you'll
use the container. After you submit the form, the Foundry Tools team reviews it and emails you
with a decision within 10 business days.

ï¼‰ Important
On the form, you must use an email address associated with an Azure subscription
ID.
The Azure resource you use to run the container must have been created with the
approved Azure subscription ID.
Check your email (both inbox and junk folders) for updates on the status of your
application from Microsoft.
After you're approved, you'll be able to run the container after you download it from the
Microsoft Container Registry (MCR), described later in the article.
You won't be able to run the container if your Azure subscription hasn't been approved.
Access is limited to customers that meet the following requirements:
Your organization should be identified as strategic customer or partner with Microsoft.
Disconnected containers are expected to run fully offline, hence your use cases must
meet one of these or similar requirements:
Environments or devices with zero connectivity to internet.
Remote location that occasionally has internet access.
Organization under strict regulation of not sending any kind of data back to cloud.
Application completed as instructed - Pay close attention to guidance provided
throughout the application to ensure you provide all the necessary information required
for approval.

Purchase a commitment tier pricing plan for
disconnected containers
Create a new resource
1. Sign in to the Azure portal

and select Create a new resource for one of the applicable

Foundry Tools listed.
2. Enter the applicable information to create your resource. Be sure to select Commitment
tier disconnected containers as your pricing tier.
ï¼— Note

You only see the option to purchase a commitment tier if you have been
approved by Microsoft.
Pricing details are only examples.
3. Select Review + Create at the bottom of the page. Review the information, and select
Create.

Configure container for disconnected usage
See the following documentation for steps on downloading and configuring the container for
disconnected usage:
Vision - Read
Language Understanding (LUIS)
Text Translation (Standard)
Document Intelligence
Speech service
Speech to text
Custom Speech to text
Neural Text to speech
Language service
Sentiment Analysis
Key Phrase Extraction
Language Detection
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)

Environment variable names in Kubernetes
deployments
Some Azure AI Containers, for example Translator, require users to pass environmental variable
names that include colons ( : ) when running the container. This works fine when using Docker,
but Kubernetes doesn't accept colons in environmental variable names. To resolve this, you can
replace colons with double underscore characters ( __ ) when deploying to Kubernetes. See the
following example of an acceptable format for environment variable names:

Kubernetes
env:
- name: Mounts__License
value: "/license"
- name: Mounts__Output
value: "/output"

This example replaces the default format for the Mounts:License and Mounts:Output
environment variable names in the docker run command.

Container image and license updates
Container license files are used as keys to decrypt certain files within each container image. If
these encrypted files happen to be updated within a new container image, the license file you
have may fail to start the container even if it worked with the previous version of the container
image. To avoid this issue, we recommend that you download a new license file from the
resource endpoint for your container provided in Azure portal after you pull new image
versions from mcr.microsoft.com.
To download a new license file, you can add DownloadLicense=True to your docker run
command along with a license mount, your API Key, and your billing endpoint. Refer to your
container's documentation for detailed instructions.

Usage records
When operating Docker containers in a disconnected environment, the container writes usage
records to a volume where they're collected over time. You can also call a REST endpoint to
generate a report about service usage.

Arguments for storing logs
When run in a disconnected environment, an output mount must be available to the container
to store usage logs. For example, you would include -v /host/output:{OUTPUT_PATH} and
Mounts:Output={OUTPUT_PATH} in the example below, replacing {OUTPUT_PATH} with the path

where the logs are stored:
Docker
docker run -v /host/output:{OUTPUT_PATH} ... <image> ... Mounts:Output={OUTPUT_PATH}

Get records using the container endpoints

The container provides two endpoints for returning records about its usage.

Get all records
The following endpoint provides a report summarizing all of the usage collected in the
mounted billing record directory.
HTTP
https://<service>/records/usage-logs/

It returns JSON similar to the example below.
JSON
{
"apiType": "noop",
"serviceName": "noop",
"meters": [
{
"name": "Sample.Meter",
"quantity": 253
}
]
}

Get records for a specific month
The following endpoint provides a report summarizing usage over a specific month and year.
HTTP
https://<service>/records/usage-logs/{MONTH}/{YEAR}

It returns a JSON response similar to the example below:
JSON
{
"apiType": "string",
"serviceName": "string",
"meters": [
{
"name": "string",
"quantity": 253
}

]
}

Purchase a commitment plan to use containers in
disconnected environments
Commitment plans for disconnected containers have a calendar year commitment period.
When you purchase a plan, you are charged the full price immediately. During the commitment
period, you can't change your commitment plan, however you can purchase more units at a
pro-rated price for the remaining days in the year. You have until midnight (UTC) on the last
day of your commitment, to end a commitment plan.
You can choose a different commitment plan in the Commitment Tier pricing settings of your
resource.

End a commitment plan
If you decide that you don't want to continue purchasing a commitment plan, you can set your
resource's auto-renewal to Do not auto-renew. Your commitment plan expires on the
displayed commitment end date. After this date, you won't be charged for the commitment
plan. You are able to continue using the Azure resource to make API calls, charged at Standard
pricing. You have until midnight (UTC) on the last day of the year to end a commitment plan for
disconnected containers, and not be charged for the following year.

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Disconnected containers
Frequently asked questions (FAQ).

Next steps
Azure AI containers overview

Last updated on 10/02/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

Recognized PII and PHI entities
The Personally Identifiable Information (PII) and Protected Health Information (PHI) detection
APIs are cloud-based solutions that use artificial intelligence (AI) and machine learning to help
you create smart applications with advanced natural language processing. The PII and PHI APIs
effectively detect and removes sensitive information from input data by categorizing personal
details into specific, predefined entity types. This comprehensive approach not only safeguards
sensitive data to ensure full compliance with privacy regulations, but also enables applications
to process and utilize information with enhanced security, reliability, and efficiency.
îª€ Tip
Try PII detection in text or conversations using the Microsoft Foundry

language

playground.

Language Support
The PII language support page lists all languages available for the PII entities in this article. Any
exceptions are noted for specific named entities.
Supported API versions:
Stable 2025-11-01: Generally Available (GA)
Preview: 2025-11-15-preview.
The following entities are currently in preview:
The following entities are currently in preview:
Airport
DateOfBirth
BankAccountNumber
CASocialIdentificationNumber
CVV (Card Verification Value )
City
PassportNumber
DriversLicenseNumber
ExpirationDate
Geopolitical Entity
KRDriversLicenseNumber
KRPassportNumber
KRSocialSecurityNumber

LicensePlate
Location
Password
SortCode
State
USMedicareBeneficiaryId
VIN (vehicle identification number)
ZipCode
ï¼— Note
Beginning with the GA API (released 2024-11-01 ), the Subtype field is no longer
supported. All entity classifications now use the type field.

Supported PII entity list
To examine a comprehensive list of all the types of Personally Identifiable Information (PII)
entities that are currently supported, see the Supported PII entity list

Supported PII extraction entities
Personally identifiable information (PII) refers to any single piece of data or combination of
data that enables the unique identification, tracking, or differentiation of an individual.
The Azure Language in Foundry Tools PII extraction API uses Natural Language Processing
(NLP) technology to detect, recognize, and extract PII entities from written text or spoken
conversations. The following entities represent specific types of information that can reveal an
individual's identity:

Type: Geolocation
Data that details an individual's physical location that can be used to pinpoint or monitor
where a person is or has been. This data is considered PII when it is linked to a specific person.

Type: Airport (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Airport in the piiCategories request parameter. If detected,

[Airport]

the entity appears in the PII response payloads.

Type: City (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify City in the piiCategories request parameter. If detected, the

[City]

entity appears in the PII response payloads.

Type: Geopolitical Entity GPE (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify GPE in the piiCategories request parameter. If detected, the
entity appears in the PII response payloads.

[GPE]

Type: Location (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Location in the piiCategories request parameter. If

[Location]

detected, the entity appears in the PII response payloads.

Type: State (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify State in the piiCategories request parameter. If detected, the

[State]

entity appears in the PII response payloads.

Personal
Any data, collected or stored, that can be used to identify or contact a specific individual is
considered personal information. This data may include information that identifies someone
directly, such as their name or social security number. It can also refer to data that, when linked
with other information, could lead to identificationâ€”for example, an address or dates of birth.).

Type: Address
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Address in the piiCategories request parameter. If
detected, the entity appears in the PII and PHI response payloads.

[Address]

Type: Age
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Age in the piiCategories request parameter. If detected, the
entity appears in the PII response payload.

[Age]

Type: Date Of Birth (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DateOfBirth in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[DateOfBirth]

Type: Drivers License Number (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DriversLicenseNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[DriversLicenseNumber]

Type: Email
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Email in the piiCategories request parameter. If detected, the
entity appears in the PII and PHI response payloads.

[Email]

Type: IP Address
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify IPAddress in the piiCategories request parameter. If
detected, the entity appears in the PII and PHI response payloads.

[IPAddress]

Type: License Plate (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify LicensePlate in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[LicensePlate]

Type: Passport Number (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PassportNumber in the piiCategories request

[PassportNumber]

parameter. If detected, the entity appears in the PII response payload.

Type: Password (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Password in the piiCategories request parameter. If

[Password]

detected, the entity appears in the PII response payload.

Type: Person
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Person in the piiCategories request parameter. If detected,
the entity appears in the PII response payloads.

[Person]

Type: Phone Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PhoneNumber in the piiCategories request
parameter. If detected, the entity appears in the PII and PHI response payloads.

[PhoneNumber]

Type: URL
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify URL in the piiCategories request parameter. If detected, the
entity appears in the PII and PHI response payloads.

[URL]

Type: VIN (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify VIN (vehicle registration number) in the piiCategories request

[VIN]

parameter. If detected, the entity appears in the PII response payload.

Financial
Any financial information is connected to a particular individual that can, through identifying
details, be traced back to that person.

Type: American Bankers Association Routing Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ABARoutingNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[ABARoutingNumber]

Type: Bank Account Number (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify BankAccountNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[BankAccountNumber]

Type: Credit Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CreditCardNumber in the piiCategories request

[CreditCardNumber]

parameter. If detected, the entity appears in the PII response payload.

Type: International Banking Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify
InternationalBankingAccountNumber in the piiCategories
request parameter. If detected, the entity appears in the PII

[InternationalBankingAccountNumber]

response payload.

Type: Sort Code (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SortCode in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[SortCode]

Type: SWIFT Code
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SWIFTCode in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[SWIFTCode]

Organization
Any data that an organization collects, stores, or processes that can be used to identify a
specific individual, either directly or indirectly.

Type: Organization
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Organization in the piiCategories request parameter.
If detected, the entity appears in the PII and PHI response payloads.

[Organization]

DateTime
Data that can be used to identify, distinguish, or trace an individual. While a date or time on its
own is often not considered PII, it can become highly sensitive when combined with other data
points.

Type: Date

ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify Date in the piiCategories request parameter. If detected, the
entity appears in the PII and PHI response payloads.

[Date]

Type: Expiration Date (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ExpirationDate in the piiCategories request
parameter. If detected, the entity appears in the PII response payloads.

[ExpirationDate]

Azure-related
Any identifiable Azure information like authentication information and connection strings that
can be used to distinguish or trace an individual's identity.

Type: Azure Document DB Auth Key
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzureDocumentDBAuthKey in the

[AzureDocumentDBAuthKey]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Azure IAAS Database Connection And SQL String
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify
AzureIAASDatabaseConnectionAndSQLString in the
piiCategories request parameter. If detected, the entity

[AzureIAASDatabaseConnectionAndSQLString]

appears in the PII response payload.

Type: Azure IoT Connection String
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzureIoTConnectionString in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[AzureIoTConnectionString]

Type: Azure Publish Setting Password
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzurePublishSettingPassword in the
piiCategories request parameter. If detected, the entity appears in the
PII response payload.

[AzurePublishSettingPassword]

Type: Azure Redis Cache String
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzureRedisCacheString in the piiCategories

[AzureRedisCacheString]

request parameter. If detected, the entity appears in the PII response payload.

Type: Azure SAS
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzureSAS in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[AzureSAS]

Type: Azure Service Bus String
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzureServiceBusString in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[AzureServiceBusString]

Type: Azure Storage Account Generic
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzureStorageAccountGeneric in the

[AzureStorageAccountGeneric]

piiCategories request parameter. If detected, the entity appears in the
PII response payload.

Type: Azure Storage Account Key
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AzureStorageAccountKey in the

[AzureStorageAccountKey]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: SQL Server Connection String
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SQLServerConnectionString in the
piiCategories request parameter. If detected, the entity appears in the PII

[AzureStorageAccountKey]

response payload.

Government
Any government-issued identification that can be used along or combined with other data to
trace and reveal a specific person's identity.

Type: Argentina National Identity Number

ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ARNationalIdentityNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[ARNationalIdentityNumber]

response payload.

Type: Australia Bank Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AUBankAccountNumber in the

[AUBankAccountNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Australia Business Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AUBusinessNumber in the piiCategories

[AUBusinessNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Australia Company Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AUCompanyNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[AUCompanyNumber]

Type: Australia Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AUDriversLicenseNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[AUDriversLicenseNumber]

response payload.

Type: Australia Medical Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AUMedicalAccountNumber in the

[AUMedicalAccountNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Australia Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AUPassportNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[AUPassportNumber]

Type: Australia Tax File Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify AUTaxFileNumber in the piiCategories request

[AUTaxFileNumber]

parameter. If detected, the entity appears in the PII response payload.

Type: Austria Identity Card
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ATIdentityCard in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[ATIdentityCard]

Type: Austria Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ATTaxIdentificationNumber in the

[ATTaxIdentificationNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Austria Value Added Tax Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ATValueAddedTaxNumber in the

[ATValueAddedTaxNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Belgium National Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify BENationalNumber in the piiCategories

[BENationalNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Belgium Value Added Tax Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify BEValueAddedTaxNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[BEValueAddedTaxNumber]

response payload.

Type: Brazil CPF Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify BRCPFNumber in the piiCategories request

[BRCPFNumber]

parameter. If detected, the entity appears in the PII response payload.

Type: Brazil Legal Entity Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify BRLegalEntityNumber in the piiCategories

[BRLegalEntityNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Brazil National IDRG
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify BRNationalIDRG in the piiCategories request

[BRNationalIDRG]

parameter. If detected, the entity appears in the PII response payload.

Type: Bulgaria Uniform Civil Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify BGUniformCivilNumber in the

[BGUniformCivilNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Canada Bank Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CABankAccountNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[CABankAccountNumber]

response payload.

Type: Canada Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CADriversLicenseNumber in the

[CADriversLicenseNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Canada Health Service Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CAHealthServiceNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[CAHealthServiceNumber]

response payload.

Type: Canada Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CAPassportNumber in the piiCategories

[CAPassportNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Canada Personal Health Identification
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CAPersonalHealthIdentification in

[CAPersonalHealthIdentification]

the piiCategories request parameter. If detected, the entity appears in
the PII and PHI response payload.

Type: Canada Social Identification Number (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CASocialIdentificationNumber in
the piiCategories request parameter. If detected, the entity appears in

[CASocialIdentificationNumber]

the PII response payload.

Type: Canada Social Insurance Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CASocialInsuranceNumber in the

[CASocialInsuranceNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Card Verification Value CVV (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CVV in the piiCategories request parameter. If detected, the

[CVV]

entity appears in the PII response payload.

Type: Chile Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CLIdentityCardNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[CLIdentityCardNumber]

Type: China Resident Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CNResidentIdentityCardNumber

[CNResidentIdentityCardNumber]

in the piiCategories request parameter. If detected, the entity

Details

Tag

appears in the PII response payload.

Type: Croatia Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify HRIdentityCardNumber in the

[HRIdentityCardNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Croatia National ID Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify HRNationalIDNumber in the piiCategories

[HRNationalIDNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Croatia Personal Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify HRPersonalIdentificationNumber
in the piiCategories request parameter. If detected, the entity

[HRPersonalIdentificationNumber]

appears in the PII response payload.

Type: Cyprus Identity Card
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CYIdentityCard in the piiCategories request

[CYIdentityCard]

parameter. If detected, the entity appears in the PII response payload.

Type: Cyprus Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CYTaxIdentificationNumber in the

[CYTaxIdentificationNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Czech Republic Personal Identity Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CZPersonalIdentityNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[CZPersonalIdentityNumber]

response payload.

Type: Denmark Personal Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DKPersonalIdentificationNumber

[DKPersonalIdentificationNumber]

in the piiCategories request parameter. If detected, the entity
appears in the PII response payload.

Type: Estonia Personal Identification Code
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EEPersonalIdentificationCode in the
piiCategories request parameter. If detected, the entity appears in the

[EEPersonalIdentificationCode]

PII response payload.

Type: European Union Debit Card Number

ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EUDebitCardNumber in the piiCategories

[EUDebitCardNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: European Union Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EUDriversLicenseNumber in the

[EUDriversLicenseNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: European Union GPS Coordinates
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EUGPSCoordinates in the piiCategories request

[EUGPSCoordinates]

parameter. If detected, the entity appears in the PII response payload.

Type: European Union National Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EUNationalIdentificationNumber

[EUNationalIdentificationNumber]

in the piiCategories request parameter. If detected, the entity
appears in the PII response payload.

Type: European Union Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EUPassportNumber in the piiCategories

[EUPassportNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: European Union Social Security Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EUSocialSecurityNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[EUSocialSecurityNumber]

response payload.

Type: European Union Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify EUTaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[EUTaxIdentificationNumber]

response payload.

Type: Expiration Date (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ExpirationDate in the piiCategories request

[ExpirationDate]

parameter. If detected, the entity appears in the PII response payload.

Type: Finland European Health Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FIEuropeanHealthNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[FIEuropeanHealthNumber]

Details

Tag

response payload.

Type: Finland National ID
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FINationalID in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[FINationalID]

Type: Finland Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FIPassportNumber in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[FIPassportNumber]

Type: France Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FRDriversLicenseNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[FRDriversLicenseNumber]

response payload.

Type: France Health Insurance Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FRHealthInsuranceNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[FRHealthInsuranceNumber]

Type: France National ID
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FRNationalID in the piiCategories request parameter.
If detected, the entity appears in the PII response payload.

[FRNationalID]

Type: France Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FRPassportNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[FRPassportNumber]

Type: France Social Security Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FRSocialSecurityNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[FRSocialSecurityNumber]

Type: France Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FRTaxIdentificationNumber in the

[FRTaxIdentificationNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: France Value Added Tax Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify FRValueAddedTaxNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[FRValueAddedTaxNumber]

response payload.

Type: Germany Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DEDriversLicenseNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[DEDriversLicenseNumber]

Type: Germany Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DEIdentityCardNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[DEIdentityCardNumber]

Type: Germany Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DEPassportNumber in the piiCategories

[DEPassportNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Germany Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DETaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[DETaxIdentificationNumber]

Details

Tag

response payload.

Type: Germany Value Added Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify DEValueAddedNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[DEValueAddedNumber]

Type: Greece National ID Card
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify GRNationalIDCard in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[GRNationalIDCard]

Type: Greece Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify GRTaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[GRTaxIdentificationNumber]

response payload.

Type: Hong Kong SAR Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify HKIdentityCardNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[HKIdentityCardNumber]

Type: Hungary Personal Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify HUPersonalIdentificationNumber
in the piiCategories request parameter. If detected, the entity
appears in the PII response payload.

[HUPersonalIdentificationNumber]

Type: Hungary Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify HUTaxIdentificationNumber in the

[HUTaxIdentificationNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Hungary Value Added Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify HUValueAddedNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[HUValueAddedNumber]

response payload.

Type: India Permanent Account
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify INPermanentAccount in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[INPermanentAccount]

Type: India Unique Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify INUniqueIdentificationNumber in
the piiCategories request parameter. If detected, the entity appears in

[INUniqueIdentificationNumber]

the PII response payload.

Type: Indonesia Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify IDIdentityCardNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[IDIdentityCardNumber]

Type: Ireland Personal Public Service Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify IEPersonalPublicServiceNumber in

[IEPersonalPublicServiceNumber]

the piiCategories request parameter. If detected, the entity appears in
the PII response payload.

Type: Israel Bank Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ILBankAccountNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[ILBankAccountNumber]

Type: Israel National ID
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ILNationalID in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[ILNationalID]

Type: Italy Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ITDriversLicenseNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[ITDriversLicenseNumber]

Type: Italy Fiscal Code
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ITFiscalCode in the piiCategories request parameter. If
detected, the entity appears in the PII response payload.

[ITFiscalCode]

Type: Italy Value Added Tax Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ITValueAddedTaxNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[ITValueAddedTaxNumber]

response payload.

Type: Japan Bank Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPBankAccountNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[JPBankAccountNumber]

response payload.

Type: Japan Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPDriversLicenseNumber in the

[JPDriversLicenseNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Japan My Number Corporate
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPMyNumberCorporate in the
piiCategories request parameter. If detected, the entity appears in the PII

[JPMyNumberCorporate]

response payload.

Type: Japan My Number Personal
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPMyNumberPersonal in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[JPMyNumberPersonal]

Type: Japan Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPPassportNumber in the piiCategories request

[JPPassportNumber]

parameter. If detected, the entity appears in the PII response payload.

Type: Japan Residence Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPResidenceCardNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[JPResidenceCardNumber]

Details

Tag

response payload.

Type: Japan Resident Registration Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPResidentRegistrationNumber in

[JPResidentRegistrationNumber]

the piiCategories request parameter. If detected, the entity appears in
the PII response payload.

Type: Japan Social Insurance Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify JPSocialInsuranceNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[JPSocialInsuranceNumber]

response payload.

Type: Latvia Personal Code
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify LVPersonalCode in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[LVPersonalCode]

Type: Lithuania Personal Code
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify LTPersonalCode in the piiCategories request

[LTPersonalCode]

parameter. If detected, the entity appears in the PII response payload.

Type: Luxembourg National Identification Number Natural
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify
LUNationalIdentificationNumberNatural in the piiCategories
request parameter. If detected, the entity appears in the PII

[LUNationalIdentificationNumberNatural]

response payload.

Type: Luxembourg National Identification Number Non
Natural
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify

[LUNationalIdentificationNumberNonNatural]

LUNationalIdentificationNumberNonNatural in the
piiCategories request parameter. If detected, the entity
appears in the PII response payload.

Type: Malaysia Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify MYIdentityCardNumber in the

[MYIdentityCardNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Malta Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify MTIdentityCardNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[MTIdentityCardNumber]

response payload.

Type: Malta Tax ID Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify MTTaxIDNumber in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[MTTaxIDNumber]

Type: Netherlands Citizens Service Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NLCitizensServiceNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[NLCitizensServiceNumber]

response payload.

Type: Netherlands Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NLTaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[NLTaxIdentificationNumber]

Type: Netherlands Value Added Tax Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NLValueAddedTaxNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[NLValueAddedTaxNumber]

Type: New Zealand Bank Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NZBankAccountNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[NZBankAccountNumber]

response payload.

Type: New Zealand Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NZDriversLicenseNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[NZDriversLicenseNumber]

Type: New Zealand Inland Revenue Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NZInlandRevenueNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[NZInlandRevenueNumber]

Type: New Zealand Ministry Of Health Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NZMinistryOfHealthNumber in the

[NZMinistryOfHealthNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: New Zealand Social Welfare Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NZSocialWelfareNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[NZSocialWelfareNumber]

Type: Norway Identity Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify NOIdentityNumber in the piiCategories

[NOIdentityNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Philippines Unified Multi Purpose ID Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify
PHUnifiedMultiPurposeIDNumber in the piiCategories request
parameter. If detected, the entity appears in the PII response

[PHUnifiedMultiPurposeIDNumber]

payload.

Type: Poland Identity Card
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PLIdentityCard in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[PLIdentityCard]

Type: Poland National ID
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PLNationalID in the piiCategories request parameter.

[PLNationalID]

Details

Tag

If detected, the entity appears in the PII response payload.

Type: Poland Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PLPassportNumber in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[PLPassportNumber]

Type: Poland REGON Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PLREGONNumber in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[PLREGONNumber]

Type: Poland Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PLTaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[PLTaxIdentificationNumber]

Type: Portugal Citizen Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PTCitizenCardNumber in the piiCategories

[PTCitizenCardNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: Portugal Tax Identification Number

ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify PTTaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[PTTaxIdentificationNumber]

response payload.

Type: Romania Personal Numerical Code
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ROPersonalNumericalCode in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[ROPersonalNumericalCode]

Type: Russia Passport Number Domestic
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify RUPassportNumberDomestic in the

[RUPassportNumberDomestic]

piiCategories request parameter. If detected, the entity appears in the
PII response payload.

Type: Russia Passport Number International
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify RUPassportNumberInternational

[RUPassportNumberInternational]

in the piiCategories request parameter. If detected, the entity
appears in the PII response payload.

Type: Saudi Arabia National ID
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SANationalID in the piiCategories request parameter.
If detected, the entity appears in the PII response payload.

[SANationalID]

Type: Singapore National Registration Identity Card Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify
SGNationalRegistrationIdentityCardNumber in the

[SGNationalRegistrationIdentityCardNumber]

piiCategories request parameter. If detected, the entity
appears in the PII response payload.

Type: Slovakia Personal Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SKPersonalNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[SKPersonalNumber]

Type: Slovenia Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SITaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[SITaxIdentificationNumber]

Type: Slovenia Unique Master Citizen Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SIUniqueMasterCitizenNumber in

[SIUniqueMasterCitizenNumber]

the piiCategories request parameter. If detected, the entity appears in

Details

Tag

the PII response payload.

Type: South Africa Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ZAIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[ZAIdentificationNumber]

Type: South Korea Drivers License Number (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify KRDriversLicenseNumber in the

[KRDriversLicenseNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: South Korea Passport Number (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify KRPassportNumber in the piiCategories

[KRPassportNumber]

request parameter. If detected, the entity appears in the PII response payload.

Type: South Korea Social Security Number (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify KRSocialSecurityNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[KRSocialSecurityNumber]

Type: South Korea Resident Registration Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify KRResidentRegistrationNumber in
the piiCategories request parameter. If detected, the entity appears in

[KRResidentRegistrationNumber]

the PII response payload.

Type: Spain DNI
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ESDNI in the piiCategories request parameter. If detected,
the entity appears in the PII response payload.

[ESDNI]

Type: Spain Social Security Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ESSocialSecurityNumber in the

[ESSocialSecurityNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Spain Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ESTaxIdentificationNumber in the

[ESTaxIdentificationNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: Sweden National ID
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SENationalID in the piiCategories request parameter.
If detected, the entity appears in the PII response payload.

[SENationalID]

Type: Sweden Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SEPassportNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response

[SEPassportNumber,
PassportNumber]

payload.

Type: Sweden Tax Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify SETaxIdentificationNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[SETaxIdentificationNumber]

Type: Switzerland Social Security Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify CHSocialSecurityNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[CHSocialSecurityNumber]

Type: Taiwanese ID
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify TWNationalID in the piiCategories request

[TWNationalID]

Details

Tag

parameter. If detected, the entity appears in the PII response payload.

Type: Taiwan Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify TWPassportNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[TWPassportNumber]

Type: Taiwan Resident Certificate
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify TWResidentCertificate in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[TWResidentCertificate]

Type: Thailand Population Identification Code
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify THPopulationIdentificationCode
in the piiCategories request parameter. If detected, the entity appears
in the PII response payload.

[THPopulationIdentificationCode]

Type: TÃ¼rkiye National Identification Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify TRNationalIdentificationNumber

[TRNationalIdentificationNumber]

in the piiCategories request parameter. If detected, the entity
appears in the PII response payload.

Type: Ukraine Passport Number Domestic
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify UAPassportNumberDomestic in the
piiCategories request parameter. If detected, the entity appears in the

[UAPassportNumberDomestic]

PII response payload.

Type: Ukraine Passport Number International
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify UAPassportNumberInternational
in the piiCategories request parameter. If detected, the entity

[UAPassportNumberInternational]

appears in the PII response payload.

Type: United Kingdom Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify UKDriversLicenseNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[UKDriversLicenseNumber]

response payload.

Type: United Kingdom Electoral Roll Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify UKElectoralRollNumber in the
piiCategories request parameter. If detected, the entity appears in the PII

[UKElectoralRollNumber]

response payload.

Type: United Kingdom National Health Number

ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify UKNationalHealthNumber in the

[UKNationalHealthNumber]

piiCategories request parameter. If detected, the entity appears in the PII
response payload.

Type: United Kingdom National Insurance Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify UKNationalInsuranceNumber in the
piiCategories request parameter. If detected, the entity appears in the

[UKNationalInsuranceNumber]

PII response payload.

Type: United Kingdom Unique Taxpayer Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify UKUniqueTaxpayerNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[UKUniqueTaxpayerNumber]

Type: United States Bank Account Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify USBankAccountNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[USBankAccountNumber]

Type: United States Drivers License Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify USDriversLicenseNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[USDriversLicenseNumber]

Type: United States Drug Enforcement Agency Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify

["DrugEnforcementAgencyNumber]

"DrugEnforcementAgencyNumber in the piiCategories request
parameter. If detected, the entity appears in the PII response
payload.

Type: United States Individual Taxpayer Identification
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify
USIndividualTaxpayerIdentification in the piiCategories request
parameter. If detected, the entity appears in the PII response
payload.

[USIndividualTaxpayerIdentification]

Type: United States Medicare Beneficiary Identification
(preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify USMedicareBeneficiaryId in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[*USMedicareBeneficiaryId]

Type: United States Social Security Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify USSocialSecurityNumber in the
piiCategories request parameter. If detected, the entity appears in the PII
response payload.

[USSocialSecurityNumber]

Type: United States/United Kingdom Passport Number
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify USUKPassportNumber in the piiCategories
request parameter. If detected, the entity appears in the PII response payload.

[USUKPassportNumber]

Type: ZipCode (preview)
ï¾‰

Expand table

Details

Tag

To retrieve this entity type, specify ZipCode in the piiCategories request
parameter. If detected, the entity appears in the PII response payload.

[USUKPassportNumber]

Related content
PII entity categories list

Last updated on 11/18/2025

Recognized PII entities list
ï¾‰

Entity

Entity

Entity

Address

Age

American Bankers
Association Routing
Number

Airport ðŸ†•

Argentina National
Identity Number

Australia Bank
Account Number

Australia Company
Number

Australia Drivers
License Number

Australia Medical
Account Number

Australia Passport
Number

Australia Tax File
Number

Austria Identity Card

Austria Tax
Identification
Number

Austria Value Added
Tax Number

Azure Document DB
Auth Key

Azure IAAS Database
Connection And SQL
String

Azure IoT Connection
String

Azure Publish Setting
Password

Azure Redis Cache

Azure SAS

Azure Service Bus

Australia Business
Number

String

String

Azure Storage

Azure Storage

Bank Account

Account Generic

Account Key

Number ðŸ†•

Belgium National

Belgium Value Added

Brazil CPF Number

Number

Tax Number

Brazil Legal Entity

Brazil National IDRG

Number

Bulgaria Uniform
Civil Number

Canada Bank

Canada Drivers

Canada Health

Account Number

License Number

Service Number

Canada Passport

Canada Personal

Canada Social

Number

Health Identification

Insurance Number

Canada Social
Identification

Chile Identity Card
Number

China Resident
Identity Card

Number ðŸ†•

Number

Expand table

Entity

Entity

Entity

City ðŸ†•

Credit Card Number

Croatia Identity Card

Croatia

Croatia Personal

Number

National ID
Number

Identification
Number

CVV ðŸ†•

Cyprus Identity Card

Cyprus Tax
Identification
Number

Date

Date Of Birth ðŸ†•

Denmark Personal
Identification
Number

Drivers License
Number ðŸ†•

Email

Estonia Personal
Identification Code

European Union
Debit Card Number

European Union
Drivers License

European Union GPS
Coordinates

Number
European Union

European Union

European Union

National

Passport Number

Social Security

Identification
Number

Number

Expiration Date ðŸ†•

Finland European
Health Number

Finland Passport

France Drivers

Number

License Number

France Passport

France Social

Number

Security Number

France Tax

France Value Added

Germany Drivers

Identification

Tax Number

License Number

Germany Identity

Germany Passport

Germany Tax

Card Number

Number

Identification
Number

Germany Value
Added Number

GPE ðŸ†•

Greece National ID
Card

Greece Tax
Identification

Hong Kong SAR
Identity Card Number

Hungary Personal
Identification

European Union Tax
Identification
Number
Finland National ID

France National ID

Number

Number

Number

Entity

Entity

Entity

Hungary Value

India Permanent

India Unique

Added Number

Account

Identification
Number

Indonesia Identity

International Banking

Card Number

Account Number

Ireland Personal

Israel Bank Account

Public Service
Number

Number

Italy Drivers License
Number

Italy Fiscal Code

Italy Value Added Tax
Number

Japan Bank Account
Number

Japan Drivers License
Number

Japan My Number
Corporate

Japan My Number

Japan Passport

Japan Residence

Personal

Number

Card Number

Japan Resident

Japan Social

Latvia Personal Code

Registration Number

Insurance Number

License Plate ðŸ†•

Lithuania Personal

Luxembourg

Luxembourg National

Malaysia Identity

Malta

National

Identification Number

Card Number

Identity

Identification
Number Natural

Non Natural

Malta Tax ID Number

Netherlands Citizens

Netherlands Tax

Service Number

Identification
Number

Netherlands Value

New Zealand Bank

New Zealand Drivers

Added Tax Number

Account Number

License Number

New Zealand Inland
Revenue Number

New Zealand Ministry
Of Health Number

New Zealand Social
Welfare Number

Norway Identity

Organization

Passport Number ðŸ†•

Password ðŸ†•

Person

Philippines Unified
Multi Purpose ID
Number

Phone Number

Poland Identity Card

Poland National ID

Code

Number

IP Address

Israel National ID

Location ðŸ†•

Card
Number

Entity

Entity

Entity

Poland Passport
Number

Poland REGON
Number

Poland Tax
Identification
Number

Portugal Tax
Identification
Number

Romania Personal
Numerical Code

Russia Passport
Number Domestic

Russia Passport
Number
International

Saudi Arabia National
ID

Singapore National
Registration Identity
Card Number

Slovakia Personal

Slovenia Tax

Slovenia Unique

Number

Identification Number

Master Citizen
Number

Sort Code ðŸ†•

South Africa

South Korea Drivers

Identification Number

License Number ðŸ†•

South Korea Passport
Number ðŸ†•

South Korea Resident
Registration Number

South Korea Social
Security Number ðŸ†•

Spain DNI

Spain Social Security

Spain Tax

Number

Identification
Number

SQL Server
Connection String

State ðŸ†•

Sweden National ID

Sweden Passport
Number

Sweden Tax
Identification Number

SWIFT Code

Taiwanese ID

Taiwan Passport
Number

Taiwan Resident
Certificate

Thailand Population
Identification Code

TÃ¼rkiye National
Identification Number

Ukraine Passport
Number Domestic

Ukraine Passport
Number

United Kingdom
Drivers License

United Kingdom
Electoral Roll

International

Number

Number

United Kingdom
National Health

United Kingdom
National Insurance

United Kingdom
Unique Taxpayer

Number

Number

Number

United States Bank
Account Number

United States Drivers
License Number

United States Drug
Enforcement Agency
Number

Entity

Entity

Entity

United States
Individual Taxpayer

United States
Medicare Beneficiary

United States Social
Security Number

Identification

Id ðŸ†•

United States/United
Kingdom Passport

URL

Number
ZipCode ðŸ†•

Last updated on 11/18/2025

VIN ðŸ†•

Supported customer content (PII) entity
categories in conversations
Use this article to find the entity categories that can be returned by the conversational PII
detection feature. This feature runs a predictive model to identify, categorize, and redact
sensitive information from an input conversation.

Entity categories
The following entity categories are returned when you're sending API requests PII feature.

Category: Person
This category contains the following entity:
Entity
Name
Details
All first, middle, last or full name is considered PII regardless of whether it is the speakerâ€™s
name, the agentâ€™s name, someone elseâ€™s name or a different version of the speakerâ€™s full name
(Chris vs. Christopher).
To get this entity category, add Person to the pii-categories parameter. Person will be
returned in the API response if detected.
ï¼— Note
As of the 2023-04-15-preview API onwards, this category is 'Person' instead of 'Name'.
Supported document languages
en

Category: Phone
This category contains the following entity:

Entity
Phone
Details
All telephone numbers (including toll-free numbers or numbers that may be easily found or
considered public knowledge) are considered PII
To get this entity category, add Phone to the pii-categories parameter. Phone will be returned
in the API response if detected.
Supported document languages
en

Category: Address
This category contains the following entity:
Entity
Address
Details
Complete or partial addresses are considered PII. All addresses regardless of what residence or
institution the address belongs to (such as: personal residence, business, medical center,
government agency, etc.) are covered under this category.
Note:
If information is limited to City & State only, it will not be considered PII.
If information contains street, zip code or house number, all information is considered as
Address PII , including the city and state
To get this entity category, add Address to the pii-categories parameter. Address will be
returned in the API response if detected.
Supported document languages
en

Category: Email

This category contains the following entity:
Entity
Email
Details
All email addresses are considered PII.
To get this entity category, add Email to the pii-categories parameter. Email will be returned
in the API response if detected.
Supported document languages
en

Category: NumericIdentifier
This category contains the following entities:
Entity
NumericIdentifier
Details
Any numeric or alphanumeric identifier that could contain any PII information. Examples:
Case Number
Member Number
Ticket number
Bank account number
Installation ID
IP Addresses
Product Keys
Serial Numbers (1:1 relationship with a specific item/product)
Shipping tracking numbers, etc.
To get this entity category, add NumericIdentifier to the pii-categories parameter.
NumericIdentifier will be returned in the API response if detected.

Supported document languages
en

Category: Credit card
This category contains the following entity:
Entity
Credit card
Details
Any credit card number, any security code on the back, or the expiration date is considered as
PII.
To get this entity category, add CreditCard to the pii-categories parameter. CreditCard will
be returned in the API response if detected.
Supported document languages
en

Next steps
How to detect PII in conversations

Last updated on 11/18/2025

What is sentiment analysis and opinion
mining?
Sentiment analysis and opinion mining are features offered by Azure Language, a collection of
machine learning and AI algorithms in the cloud for developing intelligent applications that
involve written language. These features help you discover what people think about your brand
or topic by analyzing text for signs of positive or negative sentiment. They can also link these
sentiments to specific aspects of the text.
Both sentiment analysis and opinion mining work with various written languages.

Sentiment analysis
The sentiment analysis feature assigns sentiment labels, such as "negative," "neutral," and
"positive." The service determines these labels using the highest confidence score. Sentiment is
evaluated at both the sentence level and the document level. This feature also returns
confidence scores between 0 and 1 for each document & sentences within it for positive,
neutral, and negative sentiment.

Opinion mining
Opinion mining is a feature of sentiment analysis, also known as aspect-based sentiment
analysis in Natural Language Processing (NLP). This feature provides more granular information
about the opinions related to words (such as the attributes of products or services) in text.

Typical workflow
To use this feature, you submit data for analysis and handle the API output in your application.
Analysis is performed as-is, with no added customization to the model used on your data.
1. Create an Azure Language in Foundry Tools resource, which grants you access to the
features offered by Language. It generates a password (called a key) and an endpoint URL
that you use to authenticate API requests.
2. Create a request using either the REST API or the client library for C#, Java, JavaScript, and
Python. You can also send asynchronous calls with a batch request to combine API
requests for multiple features into a single call.
3. Send the request containing your text data. Your key and endpoint are used for
authentication.

4. Stream or store the response locally.

Get started with sentiment analysis
To use sentiment analysis, you submit raw unstructured text for analysis and handle the API
output in your application. Analysis is performed as-is, with no additional customization to the
model used on your data. There are two ways to use sentiment analysis:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use entity linking with text
examples with your own data when you sign up. For more information, see the
Foundry website

or Foundry documentation.

REST API or Client

Integrate sentiment analysis into your applications using the REST API, or the

library (Azure SDK)

client library available in a variety of languages. For more information, see the
sentiment analysis quickstart.

Docker container

Use the available Docker container to deploy this feature on-premises. These
docker containers enable you to bring the service closer to your data for
compliance, security, or other operational reasons.

Reference documentation and code samples
As you use this feature in your applications, see the following reference documentation and
samples for Azure Language in Foundry Tools:
ï¾‰

Expand table

Development option / language

Reference documentation

REST API

REST API documentation

C#

C# documentation

C# samples

Java

Java documentation

Java Samples

JavaScript

JavaScript documentation

JavaScript samples

Python

Python documentation

Python samples

Reference documentation

Samples

As you use sentiment analysis, see the following reference documentation and samples for
Azure Language:
ï¾‰

Development option / language

Reference documentation

REST APIs (Authoring)

REST API documentation

REST APIs (Runtime)

REST API documentation

Expand table
Samples

Responsible AI
An AI system encompasses more than just the technology itself. An AI system includes the
individuals who operate the system, the people who experience its effects, and the broader
environment where the system functions all play a role. Read the transparency note for
sentiment analysis to learn about responsible AI use and deployment in your systems.

Next steps
Get started with our quickstart articles with instructions on using the service for the first time:
Use sentiment analysis and opinion mining

Last updated on 11/18/2025

Quickstart: Sentiment analysis and opinion
mining
Prerequisites
Create a Project in Foundry in the Microsoft Foundry portal

Navigate to the Foundry Playground
Using the left side pane, select Playgrounds. Then select the Try Azure Language Playground
button.

ï Š

Use Sentiment Analysis in the Foundry Playground

The Language Playground consists of four sections:
Top banner: You can select any of the currently available Languages here.
Right pane: This pane is where you can find the Configuration options for the service,
such as the API and model version, along with features specific to the service.
Center pane: This pane is where you enter your text for processing. After the operation is
run, some results are shown here.
Right pane: This pane is where Details of the run operation are shown.
Here you can select the Sentiment Analysis capability by choosing the top banner tile, Analyze
sentiment.

Use Analyze sentiment
Analyze sentiment is designed to identify positive, negative and neutral sentiment in text.
In Configuration there are the following options:
ï¾‰

Option

Description

Select API version

Select which version of the API to use.

Select model version

Select which version of the model to use.

Select text language

Select the language of the input text.

Enable opinion mining

Enables or disables the opinion mining skill.

Expand table

After your operation is completed, in the center pane, each sentence will be numbered and
opinions will be labeled if Enable opinion mining was checked and the Details section contains
the following fields for the overall sentiment and the sentiment of each sentence:
ï¾‰

Expand table

Field

Description

Sentence

The number of the sentence in the order it was typed. This field is not present for

number

Overall sentiment.

Sentiment

The detected overall sentiment for the segment of text.

Scores

The amount of positive, neutral and negative sentiment detected in the text segment.

The following fields are only present if opinion mining is enabled:

ï¾‰

Expand table

Field

Description

Target

The target of the detected opinion.

Assessments

The detected opinion and the detected persuasion (positive, neutral, negative), as well as
the percent of detected persuasion.

ï Š

Last updated on 11/18/2025

Sentiment Analysis and Opinion Mining
language support
Use this article to learn which languages are supported by Sentiment Analysis and Opinion
Mining. Both the cloud-based API and Docker containers support the same languages.

Sentiment Analysis language support
Total supported language codes: 94
ï¾‰

Language

Language code

Afrikaans

af

Albanian

sq

Amharic

am

Arabic

ar

Armenian

hy

Assamese

as

Azerbaijani

az

Basque

eu

Belarusian (new)

be

Bengali

bn

Bosnian

bs

Breton (new)

br

Bulgarian

bg

Burmese

my

Catalan

ca

Chinese (Simplified)

zh-hans

Chinese (Traditional)

zh-hant

Notes

zh also accepted

Expand table

Language

Language code

Croatian

hr

Czech

cs

Danish

da

Dutch

nl

English

en

Esperanto (new)

eo

Estonian

et

Filipino

fil

Finnish

fi

French

fr

Galician

gl

Georgian

ka

German

de

Greek

el

Gujarati

gu

Hausa (new)

ha

Hebrew

he

Hindi

hi

Hungarian

hu

Indonesian

id

Irish

ga

Italian

it

Japanese

ja

Javanese (new)

jv

Kannada

kn

Kazakh

kk

Notes

Language

Language code

Khmer

km

Korean

ko

Kurdish (Kurmanji)

ku

Kyrgyz

ky

Lao

lo

Latin (new)

la

Latvian

lv

Lithuanian

lt

Macedonian

mk

Malagasy

mg

Malay

ms

Malayalam

ml

Marathi

mr

Mongolian

mn

Nepali

ne

Norwegian

no

Odia

or

Oromo (new)

om

Pashto

ps

Persian

fa

Polish

pl

Portuguese (Portugal)

pt-PT

Portuguese (Brazil)

pt-BR

Punjabi

pa

Romanian

ro

Russian

ru

Notes

pt also accepted

Language

Language code

Sanskrit (new)

sa

Scottish Gaelic (new)

gd

Serbian

sr

Sindhi (new)

sd

Sinhala (new)

si

Slovak

sk

Slovenian

sl

Somali

so

Spanish

es

Sundanese (new)

su

Swahili

sw

Swedish

sv

Tamil

ta

Telugu

te

Thai

th

Turkish

tr

Ukrainian

uk

Urdu

ur

Uyghur

ug

Uzbek

uz

Vietnamese

vi

Welsh

cy

Western Frisian (new)

fy

Xhosa (new)

xh

Yiddish (new)

yi

Notes

Opinion Mining language support
Total supported language codes: 94
ï¾‰

Language

Language code

Afrikaans (new)

af

Albanian (new)

sq

Amharic (new)

am

Arabic

ar

Armenian (new)

hy

Assamese (new)

as

Azerbaijani (new)

az

Basque (new)

eu

Belarusian (new)

be

Bengali

bn

Bosnian (new)

bs

Breton (new)

br

Bulgarian (new)

bg

Burmese (new)

my

Catalan (new)

ca

Chinese (Simplified)

zh-hans

Chinese (Traditional) (new)

zh-hant

Croatian (new)

hr

Czech (new)

cs

Danish

da

Dutch

nl

English

en

Expand table

Notes

zh also accepted

Language

Language code

Esperanto (new)

eo

Estonian (new)

et

Filipino (new)

fil

Finnish

fi

French

fr

Galician (new)

gl

Georgian (new)

ka

German

de

Greek

el

Gujarati (new)

gu

Hausa (new)

ha

Hebrew (new)

he

Hindi

hi

Hungarian

hu

Indonesian

id

Irish (new)

ga

Italian

it

Japanese

ja

Javanese (new)

jv

Kannada (new)

kn

Kazakh (new)

kk

Khmer (new)

km

Korean

ko

Kurdish (Kurmanji)

ku

Kyrgyz (new)

ky

Lao (new)

lo

Notes

Language

Language code

Latin (new)

la

Latvian (new)

lv

Lithuanian (new)

lt

Macedonian (new)

mk

Malagasy (new)

mg

Malay (new)

ms

Malayalam (new)

ml

Marathi

mr

Mongolian (new)

mn

Nepali (new)

ne

Norwegian

no

Odia (new)

or

Oromo (new)

om

Pashto (new)

ps

Persian (new)

fa

Polish

pl

Portuguese (Portugal)

pt-PT

Portuguese (Brazil)

pt-BR

Punjabi (new)

pa

Romanian (new)

ro

Russian

ru

Sanskrit (new)

sa

Scottish Gaelic (new)

gd

Serbian (new)

sr

Sindhi (new)

sd

Sinhala (new)

si

Notes

pt also accepted

Language

Language code

Slovak (new)

sk

Slovenian (new)

sl

Somali (new)

so

Spanish

es

Sundanese (new)

su

Swahili (new)

sw

Swedish

sv

Tamil

ta

Telugu

te

Thai (new)

th

Turkish

tr

Ukrainian (new)

uk

Urdu (new)

ur

Uyghur (new)

ug

Uzbek (new)

uz

Vietnamese (new)

vi

Welsh (new)

cy

Western Frisian (new)

fy

Xhosa (new)

xh

Yiddish (new)

yi

Notes

Multi-lingual option (Custom sentiment analysis
only)
With Custom sentiment analysis, you can train a model in one language and use to classify
documents in another language. This feature is useful because it helps save time and effort.
Instead of building separate projects for every language, you can handle multi-lingual dataset
in one project. Your dataset doesn't have to be entirely in the same language but you should

enable the multi-lingual option for your project while creating or later in project settings. If you
notice your model performing poorly in certain languages during the evaluation process,
consider adding more data in these languages to your training set.
You can train your project entirely with English documents, and query it in: French, German,
Mandarin, Japanese, Korean, and others. Custom sentiment analysis makes it easy for you to
scale your projects to multiple languages by using multilingual technology to train your
models.
Whenever you identify that a particular language is not performing as well as other languages,
you can add more documents for that language in your project.
You aren't expected to add the same number of documents for every language. You should
build the majority of your project in one language, and only add a few documents in languages
you observe aren't performing well. If you create a project that is primarily in English, and start
testing it in French, German, and Spanish, you might observe that German doesn't perform as
well as the other two languages. In that case, consider adding 5% of your original English
documents in German, train a new model and test in German again. You should see better
results for German queries. The more labeled documents you add, the more likely the results
are going to get better.
When you add data in another language, you shouldn't expect it to negatively affect other
languages.

Next steps
how to call the API for more information.
Quickstart: Use the Sentiment Analysis client library and REST API

Last updated on 11/18/2025

Transparency note for Sentiment Analysis
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
ï¼‰ Important
This article assumes that you're familiar with guidelines and best practices for Azure
Language in Foundry Tools. For more information, see Transparency note for Language.
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance. Microsoft's Transparency
Notes are intended to help you understand how our AI technology works, the choices system
owners can make that influence system performance and behavior, and the importance of
thinking about the whole system, including the technology, the people, and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who will use or be affected by your system.
Microsoft's Transparency notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Microsoft AI Principles

.

The basics of Sentiment Analysis
Introduction
The Sentiment Analysis feature of Language evaluates text and returns sentiment scores and
labels for each sentence. This is useful for detecting positive, neutral and negative sentiment in
social media, customer reviews, discussion forums and other product and service scenarios.

Capabilities

System behavior
Sentiment analysis provides sentiment labels (such as "negative", "neutral" and "positive")
based on the highest confidence score found by the service at a sentence and document-level.
This feature also returns confidence scores between 0 and 1 for each document and sentence
for positive, neutral and negative sentiment. Scores closer to 1 indicate a higher confidence in
the label's classification, while lower scores indicate lower confidence. By default, the overall
sentiment label is the greatest of the three confidence scores, however, you can define a
threshold for any or all of the individual sentiment confidence scores depending on what works
best for your scenario. For each document or each sentence, the predicted scores associated
with the labels (positive, negative and neutral) add up to 1. Read more details about sentiment
labels and scores.
In addition, the optional opinion mining feature returns aspects (such as the attributes of
products or services) and their associated opinion words. For each aspect an overall sentiment
label is returned along with confidence scores for positive and negative sentiment. For
example, the sentence "The restaurant had great food and our waiter was friendly" has two
aspects, "food" and "waiter," and their corresponding opinion words are "great" and "friendly."
The two aspects therefore receive sentiment classification positive , with confidence scores
between 0 and 1.0. Read more details about opinion mining.
See the JSON response for this example.

Use cases
Sentiment Analysis can be used in multiple scenarios across a variety of industries. Some
examples include:
Monitor for positive and negative feedback trends in aggregate. After introducing a
new product, a retailer can use the sentiment service to monitor multiple social media
outlets for mentions of the product and its associated sentiment. The trending sentiment
can be used in product meetings to make business decisions about the new product.
Run sentiment analysis on raw text results of surveys to gain insights for analysis and
follow-up with participants (customers, employees, consumers, etc.). A store with a
policy to follow up on customers' negative reviews within 24 hours and positive reviews
within a week can use the sentiment service to categorize reviews for easy and timely
follow up.
Help customer service staff improve customer engagement through insights captured
from real-time analysis of interactions. Extract insights from transcribed customer
services calls to better understand customer-agent interactions and trends to improve
customer engagements.

Considerations when choosing a use case
Avoid automatic actions without human intervention for high impact scenarios. For
example, employee bonuses should not be automatically based on sentiment scores from
their customer service interaction text. Source data should always be reviewed when a
person's economic situation, health or safety is affected.
Carefully consider scenarios outside of the product and service review domain. Since
the model is trained on product and service reviews, the system may not accurately
recognize sentiment focused language in other domains. Always make sure to test the
system on operational test datasets to ensure you get the performance you need. Your
operational test dataset should reflect the real data your system will see in production
with all the characteristics and variation you will have when your product is deployed.
Synthetic data and tests that don't reflect your end-to-end scenario likely won't be
sufficient.
Carefully consider scenarios that take automatic action to filter or remove content. You
can add a human review cycle and/or re-rank content (rather than filtering it completely)
if your goal is to ensure content meets your community standards.
Legal and regulatory considerations: Organizations need to evaluate potential specific
legal and regulatory obligations when using any AI services and solutions, which may not
be appropriate for use in every industry or scenario. Additionally, AI services or solutions
are not designed for and may not be used in ways prohibited in applicable terms of
service and relevant codes of conduct.

Limitations
Depending on your scenario and input data, you could experience different levels of
performance. The following information is designed to help you understand system limitations
and key concepts about performance as they apply to Sentiment Analysis.
Key limitations to consider:
The machine learning model that is used to predict sentiment was trained on product and
service reviews. That means the service will perform most accurately for similar scenarios
and less accurately for scenarios outside the scope of the product and service reviews. For
example, personnel reviews may use different language to describe sentiment and thus,
you might not get the results or performance you would expect. A word like "strong" in
the phrase "Shafali was a strong leader" may not obtain a positive sentiment because the
word strong may not have a clear positive sentiment in product and service reviews.
Since the model is trained on product and service reviews, dialects and language that are
less represented in the dataset may have lower accuracy.

The model has no understanding of the relative importance of various sentences that are
sent together. Since the overall sentiment is a simple aggregate score of the sentences,
the overall sentiment score may not agree with a human's interpretation which would
take into account the fact that some sentences may have more importance in
determining the overall sentiment.
The model may not recognize sarcasm. Context, like tone of voice, facial expression, the
author of the text, the audience for the text, or prior conversation are often important to
understanding the sentiment. With sarcasm, additional context is often needed to
recognize if a text input is positive or negative. Given that the service only sees the text
input, classifying sarcastic sentiment may be less accurate. For example, that was
awesome, could be either positive or negative depending on the context, tone of voice,
facial expression, author and the audience.
The confidence score magnitude does not reflect the intensity of the sentiment. It is
based on the confidence of the model for a particular sentiment (positive, neutral,
negative). Therefore, if your system depends on the intensity of the sentiment, consider
using a human reviewer or post processing logic on the individual opinion scores or the
original text to help rank the intensity of the sentiment.
While weâ€™ve made efforts to reduce the bias exhibited by our models, the limitations that
come with language models, including the potential for it to produce inaccurate,
unreliable, and biased output, apply to the Language Sentiment Analysis model. We
expect the model to have some false negatives and positives for now, but we are eager to
collect user feedback to aid our ongoing work to improve this service.

Best practices for improving system performance
Because sentiment is somewhat subjective, it is not possible to provide a universally applicable
estimate of performance for the model. Ultimately, performance depends on a number of
factors such as the subject domain, the characteristics of the text processed, the use case for
the system, and how people interpret the system's output.
You may find confidence scores for positive, negative, and neutral sentiments differ according
to your scenario. Instead of using the overall sentence level sentiment for the full document or
sentence, you can define a threshold for any or all of the individual sentiment confidence
scores that works best for your scenario. For example, if it is more important to identify all
potential instances of negative sentiment, you can use a lower threshold on the negative
sentiment instead of looking at the overall sentiment label. This means that you may get more
false positives (neutral or positive text being recognized as negative sentiment), but fewer false
negatives (negative text not recognized as negative sentiment). For example, you might want
to read all product feedback that has some potential negative sentiment for ideas for product

improvement. In that case, you could use the negative sentiment score only and set a lower
threshold. This may lead to extra work because you'd end up reading some reviews that aren't
negative, but you're more likely to identify opportunities for improvement. If it is more
important for your system to recognize only true negative text, you can use a higher threshold
or use the overall sentiment label. For example, you may want to respond to product reviews
that are negative. If you want to minimize the work to read and respond to negative reviews,
you could only use the overall sentiment prediction and ignore the individual sentiment scores.
While there may be some negative sentiment predicted that you miss, you're likely to get most
of the truly negative reviews. Threshold values may not have consistent behavior across
scenarios. Therefore, it is critical that you test your system with real data that it will process in
production.

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for Health
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Data Privacy and Security for Language
Guidance for integration and responsible use with Language

Last updated on 08/17/2025

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

How to: Use Sentiment analysis and
Opinion Mining
Sentiment analysis and opinion mining are two ways of detecting positive and negative
sentiment. Using sentiment analysis, you can get sentiment labels (such as "negative" "neutral"
and "positive") and confidence scores at the sentence and document-level. Opinion Mining
provides granular information about the opinions related to words (such as the attributes of
products or services) in the text.

Sentiment Analysis
Sentiment Analysis applies sentiment labels to text, which are returned at a sentence and
document level, with a confidence score for each.
The labels are positive, negative, and neutral. At the document level, the mixed sentiment label
also can be returned. The sentiment of the document is determined as follows:
ï¾‰

Expand table

Sentence sentiment

Returned document
label

At least one positive sentence is in the document. The rest of the sentences
are neutral .

positive

At least one negative sentence is in the document. The rest of the sentences

negative

are neutral .
At least one negative sentence and at least one positive sentence are in the

mixed

document.
All sentences in the document are neutral .

neutral

Confidence scores range from 1 to 0. Scores closer to 1 indicate a higher confidence in the
label's classification, while lower scores indicate lower confidence. For each document or each
sentence, the predicted scores associated with the labels (positive, negative, and neutral) add
up to 1. For more information, see the Responsible AI transparency note.

Opinion Mining
Opinion Mining is a feature of Sentiment Analysis. Also called Aspect-based Sentiment Analysis
in Natural Language Processing (NLP), this feature provides more granular information about

the opinions related to attributes of products or services in text. The API surfaces opinions as a
target (noun or verb) and an assessment (adjective).
For example, if a customer leaves feedback about a hotel such as "The room was great, but the
staff was unfriendly.", Opinion Mining locates targets (aspects) in the text, and their associated
assessments (opinions) and sentiments. Sentiment Analysis might only report a negative
sentiment.

ï Š

If you're using the REST API, to get Opinion Mining in your results, you must include the
opinionMining=true flag in a request for sentiment analysis. The Opinion Mining results are

included in the sentiment analysis response. Opinion mining is an extension of Sentiment
Analysis and is included in your current pricing tier

.

Development options
To use sentiment analysis, you submit raw unstructured text for analysis and handle the API
output in your application. Analysis is performed as-is, with no additional customization to the
model used on your data. There are two ways to use sentiment analysis:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use entity linking with text
examples with your own data when you sign up. For more information, see the
Foundry website or Foundry documentation.

REST API or Client
library (Azure SDK)

Integrate sentiment analysis into your applications using the REST API, or the
client library available in a variety of languages. For more information, see the
sentiment analysis quickstart.

Docker container

Use the available Docker container to deploy this feature on-premises. These
docker containers enable you to bring the service closer to your data for
compliance, security, or other operational reasons.

Determine how to process the data (optional)

Specify the sentiment analysis model
By default, sentiment analysis uses the latest available AI model on your text. You can also
configure your API requests to use a specific model version.

Input languages
When you submit documents for processing by sentiment analysis, you can specify which of
the supported languages they're written in. If you don't specify a language, sentiment analysis
defaults to English. The API may return offsets in the response to support different multilingual
and emoji encodings.

Submitting data
Sentiment analysis and opinion mining produce a higher-quality result when you give it smaller
amounts of text to work on. This process is opposite from some features, like key phrase
extraction, that perform better on larger blocks of text.
To send an API request, you need your Language resource endpoint and key.
ï¼— Note
You can find the key and endpoint for your Language resource on the Azure portal.
They're located on the resource's Key and endpoint page, under resource management.
Analysis is performed upon receipt of the request. Using the sentiment analysis and opinion
mining features synchronously is stateless. No data is stored in your account, and results are
returned immediately in the response.
When using this feature asynchronously, the API results are available for 24 hours from the
time the request was ingested, and is indicated in the response. After this time period, the
results are purged and are no longer available for retrieval.

Getting sentiment analysis and opinion mining
results
When you receive results from the API, the order of the returned key phrases is determined
internally, by the model. You can stream the results to an application, or save the output to a
file on the local system.

Sentiment analysis returns a sentiment label and confidence score for the entire document, and
each sentence within it. Scores closer to 1 indicate a higher confidence in the label's
classification, while lower scores indicate lower confidence. A document can have multiple
sentences, and the confidence scores within each document or sentence add up to 1.
Opinion Mining locates targets (nouns or verbs) in the text, and their associated assessment
(adjective). For example, the sentence "The restaurant had great food and our server was
friendly" has two targets: food and server. Each target has an assessment. For example, the
assessment for food would be great, and the assessment for server would be friendly.
The API returns opinions as a target (noun or verb) and an assessment (adjective).

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

See also
Sentiment analysis and opinion mining overview

Last updated on 11/18/2025

Install and run Sentiment Analysis
containers
Containers enable you to host the Sentiment Analysis API on your own infrastructure. If you
have security or data governance requirements that can't be fulfilled by calling Sentiment
Analysis remotely, then containers might be a good option.
If you don't have an Azure subscription, create a free account

before you begin.

Prerequisites
You must meet the following prerequisites before using Sentiment Analysis containers. If you
don't have an Azure subscription, create a free account
Docker

before you begin.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts
A Language resource

.

with the free (F0) or standard (S) pricing tier

.

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:

Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the available
container. Each CPU core must be at least 2.6 gigahertz (GHz) or faster. The allowable
Transactions Per Second (TPS) are also listed.
ï¾‰

Sentiment

Expand table

Minimum host

Recommended host

Minimum

Maximum

specs

specs

TPS

TPS

1 core, 2GB memory

4 cores, 8GB memory

15

30

Analysis

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Get the container image with docker pull
The Sentiment Analysis container image can be found on the mcr.microsoft.com container
registry syndicate. It resides within the azure-cognitive-services/textanalytics/ repository
and is named sentiment . The fully qualified container image name is,
mcr.microsoft.com/azure-cognitive-services/textanalytics/sentiment

To use the latest version of the container, you can use the latest tag, which is for English. You
can also find a full list of containers for supported languages using the tags on the MCR

.

The sentiment analysis container v3 container is available in several languages. To download
the container for the English container, use the command below.

docker pull mcr.microsoft.com/azure-cognitive-services/textanalytics/sentiment:3.0en

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container with docker run
Once the container is on the host computer, use the docker run

command to run the

containers. The container will continue to run until you stop it.
ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove this based on your host operating
system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container won't start. For more information, see Billing.
To run the Sentiment Analysis container, execute the following docker run command. Replace
the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Language resource.
You can find it on your resource's Key

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

and endpoint page, on the Azure
portal.
{ENDPOINT_URI}

The endpoint for accessing the API.
You can find it on your resource's Key

https://<your-customsubdomain>.cognitiveservices.azure.com

and endpoint page, on the Azure
portal.
{IMAGE_TAG}

The image tag representing the
language of the container you want
to run. Make sure this matches the
docker pull command you used.

Bash

3.0-en

docker run --rm -it -p 5000:5000 --memory 8g --cpus 1 \
mcr.microsoft.com/azure-cognitive-services/textanalytics/sentiment:{IMAGE_TAG} \
Eula=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY}

This command:
Runs a Sentiment Analysis container from the container image
Allocates one CPU core and 8 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

Expand table

Request URL

Purpose

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

Run the container disconnected from the internet
To use this container disconnected from the internet, you must first request access by filling
out an application, and purchasing a commitment plan. See Use Docker containers in
disconnected environments for more information.
If you have been approved to run the container disconnected from the internet, use the
following example shows the formatting of the docker run command you'll use, with
placeholder values. Replace these placeholder values with your own values.
The DownloadLicense=True parameter in your docker run command will download a license file
that will enable your Docker container to run when it isn't connected to the internet. It also

contains an expiration date, after which the license file will be invalid to run the container. You
can only use a license file with the appropriate container that you've been approved for. For
example, you can't use a license file for a speech to text container with a Document
Intelligence container.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitiveservices/form-recognizer/invoice

{LICENSE_MOUNT}

The path where the
license will be
downloaded, and
mounted.

/host/license:/path/to/license/directory

{ENDPOINT_URI}

The endpoint for
authenticating your
service request. You can
find it on your
resource's Key and
endpoint page, on the

https://<your-customsubdomain>.cognitiveservices.azure.com

Azure portal.
{API_KEY}

The key for your Text
Analytics resource. You
can find it on your
resource's Key and
endpoint page, on the

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Azure portal.
{CONTAINER_LICENSE_DIRECTORY}

Location of the license
folder on the
container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 \
-v {LICENSE_MOUNT} \
{IMAGE} \
eula=accept \
billing={ENDPOINT_URI} \
apikey={API_KEY} \
DownloadLicense=True \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}

/path/to/license/directory

Once the license file has been downloaded, you can run the container in a disconnected
environment. The following example shows the formatting of the docker run command you'll
use, with placeholder values. Replace these placeholder values with your own values.
Wherever the container is run, the license file must be mounted to the container and the
location of the license folder on the container's local filesystem must be specified with
Mounts:License= . An output mount must also be specified so that billing usage records can be

written.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitive-

{MEMORY_SIZE}

The appropriate size
of memory to
allocate for your

services/form-recognizer/invoice
4g

container.
{NUMBER_CPUS}

The appropriate
number of CPUs to
allocate for your
container.

4

{LICENSE_MOUNT}

The path where the
license will be

/host/license:/path/to/license/directory

located and
mounted.
{OUTPUT_PATH}

The output path for
logging usage
records.

/host/output:/path/to/output/directory

{CONTAINER_LICENSE_DIRECTORY}

Location of the
license folder on the

/path/to/license/directory

container's local
filesystem.
{CONTAINER_OUTPUT_DIRECTORY}

Bash

Location of the
output folder on the
container's local
filesystem.

/path/to/output/directory

docker run --rm -it -p 5000:5000 --memory {MEMORY_SIZE} --cpus {NUMBER_CPUS} \
-v {LICENSE_MOUNT} \
-v {OUTPUT_PATH} \
{IMAGE} \
eula=accept \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}
Mounts:Output={CONTAINER_OUTPUT_DIRECTORY}

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
The Sentiment Analysis containers send billing information to Azure, using a Language
resource on your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure
The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If

the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

For more information about these options, see Configure containers.

Summary
In this article, you learned concepts and workflow for downloading, installing, and running
Sentiment Analysis containers. In summary:
Sentiment Analysis provides Linux containers for Docker
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You must specify billing information when instantiating a container.
ï¼‰ Important
Azure AI containers are not licensed to run without being connected to Azure for
metering. Customers need to enable the containers to communicate billing information

with the metering service at all times. Azure AI containers do not send customer data (for
example, text that is being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

Use Docker containers in disconnected
environments
Containers enable you to run Foundry Tools APIs in your own environment, and are great for
your specific security and data governance requirements. Disconnected containers enable you
to use several of these APIs disconnected from the internet. Currently, the following containers
can be run in this manner:
Speech to text
Custom Speech to text
Neural Text to speech
Text Translation (Standard)
Azure Language in Foundry Tools
Sentiment Analysis
Key Phrase Extraction
Language Detection
Summarization
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)
Azure Vision in Foundry Tools - Read
Document Intelligence
Before attempting to run a Docker container in an offline environment, make sure you know
the steps to successfully download and use the container. For example:
Host computer requirements and recommendations.
The Docker pull command you use to download the container.
How to validate that a container is running.
How to send queries to the container's endpoint, once it's running.

Request access to use containers in disconnected
environments
Fill out and submit the request form

to request access to the containers disconnected from

the internet.
The form requests information about you, your company, and the user scenario for which you'll
use the container. After you submit the form, the Foundry Tools team reviews it and emails you
with a decision within 10 business days.

ï¼‰ Important
On the form, you must use an email address associated with an Azure subscription
ID.
The Azure resource you use to run the container must have been created with the
approved Azure subscription ID.
Check your email (both inbox and junk folders) for updates on the status of your
application from Microsoft.
After you're approved, you'll be able to run the container after you download it from the
Microsoft Container Registry (MCR), described later in the article.
You won't be able to run the container if your Azure subscription hasn't been approved.
Access is limited to customers that meet the following requirements:
Your organization should be identified as strategic customer or partner with Microsoft.
Disconnected containers are expected to run fully offline, hence your use cases must
meet one of these or similar requirements:
Environments or devices with zero connectivity to internet.
Remote location that occasionally has internet access.
Organization under strict regulation of not sending any kind of data back to cloud.
Application completed as instructed - Pay close attention to guidance provided
throughout the application to ensure you provide all the necessary information required
for approval.

Purchase a commitment tier pricing plan for
disconnected containers
Create a new resource
1. Sign in to the Azure portal

and select Create a new resource for one of the applicable

Foundry Tools listed.
2. Enter the applicable information to create your resource. Be sure to select Commitment
tier disconnected containers as your pricing tier.
ï¼— Note

You only see the option to purchase a commitment tier if you have been
approved by Microsoft.
Pricing details are only examples.
3. Select Review + Create at the bottom of the page. Review the information, and select
Create.

Configure container for disconnected usage
See the following documentation for steps on downloading and configuring the container for
disconnected usage:
Vision - Read
Language Understanding (LUIS)
Text Translation (Standard)
Document Intelligence
Speech service
Speech to text
Custom Speech to text
Neural Text to speech
Language service
Sentiment Analysis
Key Phrase Extraction
Language Detection
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)

Environment variable names in Kubernetes
deployments
Some Azure AI Containers, for example Translator, require users to pass environmental variable
names that include colons ( : ) when running the container. This works fine when using Docker,
but Kubernetes doesn't accept colons in environmental variable names. To resolve this, you can
replace colons with double underscore characters ( __ ) when deploying to Kubernetes. See the
following example of an acceptable format for environment variable names:

Kubernetes
env:
- name: Mounts__License
value: "/license"
- name: Mounts__Output
value: "/output"

This example replaces the default format for the Mounts:License and Mounts:Output
environment variable names in the docker run command.

Container image and license updates
Container license files are used as keys to decrypt certain files within each container image. If
these encrypted files happen to be updated within a new container image, the license file you
have may fail to start the container even if it worked with the previous version of the container
image. To avoid this issue, we recommend that you download a new license file from the
resource endpoint for your container provided in Azure portal after you pull new image
versions from mcr.microsoft.com.
To download a new license file, you can add DownloadLicense=True to your docker run
command along with a license mount, your API Key, and your billing endpoint. Refer to your
container's documentation for detailed instructions.

Usage records
When operating Docker containers in a disconnected environment, the container writes usage
records to a volume where they're collected over time. You can also call a REST endpoint to
generate a report about service usage.

Arguments for storing logs
When run in a disconnected environment, an output mount must be available to the container
to store usage logs. For example, you would include -v /host/output:{OUTPUT_PATH} and
Mounts:Output={OUTPUT_PATH} in the example below, replacing {OUTPUT_PATH} with the path

where the logs are stored:
Docker
docker run -v /host/output:{OUTPUT_PATH} ... <image> ... Mounts:Output={OUTPUT_PATH}

Get records using the container endpoints

The container provides two endpoints for returning records about its usage.

Get all records
The following endpoint provides a report summarizing all of the usage collected in the
mounted billing record directory.
HTTP
https://<service>/records/usage-logs/

It returns JSON similar to the example below.
JSON
{
"apiType": "noop",
"serviceName": "noop",
"meters": [
{
"name": "Sample.Meter",
"quantity": 253
}
]
}

Get records for a specific month
The following endpoint provides a report summarizing usage over a specific month and year.
HTTP
https://<service>/records/usage-logs/{MONTH}/{YEAR}

It returns a JSON response similar to the example below:
JSON
{
"apiType": "string",
"serviceName": "string",
"meters": [
{
"name": "string",
"quantity": 253
}

]
}

Purchase a commitment plan to use containers in
disconnected environments
Commitment plans for disconnected containers have a calendar year commitment period.
When you purchase a plan, you are charged the full price immediately. During the commitment
period, you can't change your commitment plan, however you can purchase more units at a
pro-rated price for the remaining days in the year. You have until midnight (UTC) on the last
day of your commitment, to end a commitment plan.
You can choose a different commitment plan in the Commitment Tier pricing settings of your
resource.

End a commitment plan
If you decide that you don't want to continue purchasing a commitment plan, you can set your
resource's auto-renewal to Do not auto-renew. Your commitment plan expires on the
displayed commitment end date. After this date, you won't be charged for the commitment
plan. You are able to continue using the Azure resource to make API calls, charged at Standard
pricing. You have until midnight (UTC) on the last day of the year to end a commitment plan for
disconnected containers, and not be charged for the following year.

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Disconnected containers
Frequently asked questions (FAQ).

Next steps
Azure AI containers overview

Last updated on 10/02/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

What is summarization?
ï¼‰ Important
Our preview region, Sweden Central, showcases our latest and continually evolving LLM
fine tuning techniques based on GPT models. You're welcome to try them out with a
Language resource in the Sweden Central region.
Conversation summarization is only available using:
REST API
Python
C#
Summarization is a feature offered by Azure Language in Foundry Tools, a combination of
generative Large Language models and task-optimized encoder models that offer
summarization solutions with higher quality, cost efficiency, and lower latency. Use this article
to learn more about this feature, and how to use it in your applications.
Out of the box, the service provides summarization solutions for three types of genre, plain
texts, conversations, and native documents. Text summarization only accepts plain text blocks.
Conversation summarization accepts conversational input, including various speech audio
signals. Native document summarization accepts documents in their native formats, such as
Word, PDF, or plain text. For more information, see Supported document formats.
îª€ Tip
Try out Summarization in Microsoft Foundry portal . There you can utilize a currently
existing Language Studio resource or create a new Foundry resource in order to use this
service.

Capabilities
Text summarization

This documentation contains the following article types:
Quickstarts are getting-started instructions to guide you through making requests to
the service.

How-to guides contain instructions for using the service in more specific or
customized ways.

Typical workflow
To use this feature, you submit data for analysis and handle the API output in your
application. Analysis is performed as-is, with no added customization to the model used
on your data.
1. Create an Azure Language in Foundry Tools resource, which grants you access to the
features offered by Language. It generates a password (called a key) and an endpoint
URL that you use to authenticate API requests.
2. Create a request using either the REST API or the client library for C#, Java, JavaScript,
and Python. You can also send asynchronous calls with a batch request to combine
API requests for multiple features into a single call.
3. Send the request containing your text data. Your key and endpoint are used for
authentication.
4. Stream or store the response locally.

Key features for text summarization
Text summarization uses natural language processing techniques to generate a summary
for plain texts, which can be from a document, conversation, or any texts. There are two
approaches of summarization this API provides:
Extractive summarization: Produces a summary by extracting salient sentences
within the source text, together the positioning information of these sentences.
Multiple extracted sentences: These sentences collectively convey the main idea of
the input text. They're original sentences extracted from the input text content.
Rank score: The rank score indicates how relevant a sentence is to the main topic.
Text summarization ranks extracted sentences, and you can determine whether
they're returned in the order they appear, or according to their rank. For example,
if you request a three-sentence summary extractive summarization returns the
three highest scored sentences.
Positional information: The start position and length of extracted sentences.
Abstractive summarization: Generates a summary with concise, coherent sentences
or words that aren't verbatim extract sentences from the original source.

Summary texts: Abstractive summarization returns a summary for each contextual
input range. A long input can be segmented so multiple groups of summary texts
can be returned with their contextual input range.
Contextual input range: The range within the input that was used to generate the
summary text.
As an example, consider the following paragraph of text:
"At Microsoft, we are on a quest to advance AI beyond existing techniques, by taking a more
holistic, human-centric approach to learning and understanding. As Chief Technology Officer
of Foundry Tools, I have been working with a team of amazing scientists and engineers to
turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the
relationship among three attributes of human cognition: monolingual text (X), audio or
visual sensory signals, (Y) and multilingual (Z). At the intersection of all three, there's magic
â€”what we call XYZ-code as illustrated in Figure 1â€”a joint representation to create more
powerful AI that can speak, hear, see, and understand humans better. We believe XYZ-code
enables us to fulfill our long-term vision: cross-domain transfer learning, spanning
modalities and languages. The goal is to have pretrained models that can jointly learn
representations to support a broad range of downstream AI tasks, much in the way humans
do today. Over the past five years, we achieve human performance on benchmarks in
conversational speech recognition, machine translation, conversational question answering,
machine reading comprehension, and image captioning. These five breakthroughs provided
us with strong signals toward our more ambitious aspiration to produce a leap in AI
capabilities, achieving multi-sensory and multilingual learning that's closer in line with how
humans learn and understand. I believe the joint XYZ-code is a foundational component of
this aspiration, if grounded with external knowledge sources in the downstream AI tasks."
The text summarization API request is processed upon receipt of the request by creating a
job for the API backend. If the job succeeded, the output of the API is returned. The output
is available for retrieval for 24 hours. After this time, the output is purged. Due to
multilingual and emoji support, the response can contain text offsets. For more
information, see how to process offsets.
If we use the preceding example, the API might return these summaries:
Extractive summarization:
"At Microsoft, we are on a quest to advance AI beyond existing techniques, by taking
a more holistic, human-centric approach to learning and understanding."
"We believe XYZ-code enables us to fulfill our long-term vision: cross-domain
transfer learning, spanning modalities and languages."
"The goal is to have pretrained models that can jointly learn representations to
support a broad range of downstream AI tasks, much in the way humans do today."

Abstractive summarization:
"Microsoft is taking a more holistic, human-centric approach to learning and
understanding. We believe XYZ-code enables us to fulfill our long-term vision: crossdomain transfer learning, spanning modalities and languages. Over the past five
years, we achieved human performance on benchmarks in conversational speech
recognition."

Get started with summarization
To use summarization, you submit for analysis and handle the API output in your application.
Analysis is performed as-is, with no added customization to the model used on your data.
There are two ways to use summarization:
Text summarization

ï¾‰

Expand table

Development
option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use entity linking with text
examples with your own data when you sign up. For more information, see
the Foundry website or Foundry documentation.

REST API or Client

Integrate text summarization into your applications using the REST API, or

library (Azure SDK)

the client library available in various languages. For more information, see
the summarization quickstart.

Input requirements and service limits
Text summarization

Summarization takes text for analysis. For more information, see Data and service
limits in the how-to guide.
Summarization works with various written languages. For more information, see
language support.

Reference documentation and code samples
As you use text summarization in your applications, see the following reference documentation
and samples for Language:
ï¾‰

Expand table

Development option / language

Reference documentation

Samples

C#

C# documentation

C# samples

Java

Java documentation

Java Samples

JavaScript

JavaScript documentation

JavaScript samples

Python

Python documentation

Python samples

Responsible AI
An AI system includes not only the technology, but also the people who use it, the people
affected by it, and the deployment environment. Read the transparency note for summarization
to learn about responsible AI use and deployment in your systems. For more information, see
the following articles:
Transparency note for Language
Integration and responsible use
Characteristics and limitations of summarization
Data, privacy, and security

Last updated on 11/18/2025

Quickstart: using text, document and
conversation summarization
ï¼‰ Important
Our preview region, Sweden Central, showcases our latest and continually evolving LLM
fine tuning techniques based on GPT models. You're welcome to try them out with a
Language resource in the Sweden Central region.
Conversation summarization is only available using:
REST API
Python
C#

Prerequisites
Create a Project in Foundry in the Microsoft Foundry portal

Navigate to the Foundry Playground
Using the left side pane, select Playgrounds. Then select the Try Azure Language Playground
button.

ï Š

Use Summarization in the Foundry Playground
The Language Playground consists of four sections:
Top banner: You can select any of the currently available Languages here.
Right pane: This pane is where you can find the Configuration options for the service,
such as the API and model version, along with features specific to the service.
Center pane: This pane is where you enter your text for processing. After the operation is
run, some results are shown here.
Right pane: This pane is where Details of the run operation are shown.
Here you can select the Summarization capability that you want to use by choosing one of
these top banner tiles: Summarize conversation, Summarize for call center, or Summarize
text.

Use Summarize conversation

Summarize conversation is designed to recap conversations and segment long meetings into
timestamped chapters.
In Configuration there are the following options:
ï¾‰

Expand table

Option

Description

Select API version

Select which version of the API to use.

Select text language

Select the language of the input text.

Summarization

Different methods of summarization that are returned. At least one must be

Aspects

selected.

After your operation is completed, the Details section contains the following fields for the
selected methods of summarization:
ï¾‰

Field

Expand table

Description

Sentence
Recap

A recap of the processed text. The Recap Summarization aspect must be toggled on for this
to appear.

Chapter

A list of titles for semantically segmented chapters with corresponding timestamps. The

Title

Chapter title Summarization aspect must be toggled on for this to appear.

Narrative

A list of narrative summaries for semantically segmented chapters with corresponding
timestamps. The Narrative Summarization aspect must be toggled on for this to appear.

ï Š

Use Summarize for call center
Summarize for call center is designed to recap calls and summarize them for customer issues
and resolutions.
In Configuration there are the following options:
ï¾‰

Expand table

Option

Description

Select API version

Select which version of the API to use.

Select text language

Select the language of the input text.

Summarization

Different methods of summarization that are returned. At least one must be

Aspects

selected.

After your operation is completed, the Details section contains the following fields for the
selected methods of summarization:
ï¾‰

Field

Expand table

Description

Sentence
Recap

A recap of the processed text. The Recap Summarization aspect must be toggled on for this
to appear.

Field

Description

Issue

A summary of the customer issue in the customer-and-agent conversation. The Issue
Summarization aspect must be toggled on for this to appear.

Resolution

A summary of the solutions tried in the customer-and-agent conversation. The Resolution
Summarization aspect must be toggled on for this to appear.

ï Š

Use Summarize text
Summarize text is designed to summarize and extract key information at scale from text.
In Configuration there are the following options:
ï¾‰

Expand table

Option

Description

Extractive summarization

The service will produce a summary by extracting salient
sentences.

Number of sentences

The number of sentences that Extractive summarization will
extract.

Abstractive summarization

the service will generate a summary with novel sentences.

Summary length

The length of the summary generated by Abstractive
summarization.

Define keywords for summary focus
(preview)

Helps focus summarization on a particular set of keywords.

After your operation is completed, the Details section contains the following fields for the
selected methods of summarization:
ï¾‰

Expand table

Field

Description

Extractive

Extracted sentences from the input text, ranked by detected relevance and prioritized for

summary

words in the Defined keywords for summary focus field, if any. Sentences are sorted by
rank score of detected relevance (default) or order of appearance in the input text.

Abstractive
summary

A summary of the input text of the length chosen in the Summary length field and
prioritized for words in the Defined keywords for summary focus field, if any.

ï Š

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
How to call text summarization
How to call conversation summarization

Last updated on 11/18/2025

Language support for document and
conversation summarization
Use this article to learn which natural languages are supported by document and conversation
summarization.

Text and document summarization
Extractive text and document summarization support the following languages:
ï¾‰

Language

Language code

Notes

Chinese-Simplified

zh-hans

zh also accepted

English

en

French

fr

German

de

Hebrew

he

Italian

it

Japanese

ja

Korean

ko

Polish

pl

Portuguese

pt

Spanish

es

Expand table

pt-br also accepted

Abstractive text and document summarization support the following languages:
ï¾‰

Language

Language code

Arabic

ar

Chinese-Simplified

zh-hans

English

en

Notes

zh also accepted

Expand table

Language

Language code

French

fr

German

de

Hebrew

he

Italian

it

Japanese

ja

Korean

ko

Polish

pl

Portuguese

pt

Spanish

es

Notes

pt-br also accepted

Conversation summarization
Conversation summarization supports the following languages:
ï¾‰

Language

Language code

Notes

Chinese-Simplified

zh-hans

zh also accepted

Chinese-Traditional

zh-hant

English

en

French

fr

German

de

Hebrew

he

Italian

it

Japanese

ja

Korean

ko

Polish

pl

Portuguese

pt

pt-br also accepted

Expand table

Language

Language code

Spanish

es

Dutch

nl

Swedish

sv

Danish

da

Finnish

fi

Russian

ru

Norwegian

no

Turkish

tr

Arabic

ar

Czech

cs

Hungarian

hu

Thai

th

Next steps
Summarization overview

Last updated on 11/18/2025

Notes

How to use conversation summarization
ï¼‰ Important
Our preview region, Sweden Central, showcases our latest and continually evolving LLM
fine tuning techniques based on GPT models. You're welcome to try them out with a
Language resource in the Sweden Central region.
Conversation summarization is only available using:
REST API
Python
C#

Conversation summarization aspects
Chapter title and narrative (general conversation) are designed to summarize a
conversation into chapter titles, and a summarization of the conversation's contents. This
summarization aspect works on conversations with any number of parties.
Issues and resolutions (call center focused) are designed to summarize text chat logs
between customers and customer-service agents. This feature is capable of providing
both issues and resolutions present in these logs, which occur between two parties.
Narrative is designed to summarize the narrative of a conversation.
Recap is designed to condense lengthy meetings or conversations into a concise oneparagraph summary to provide a quick overview.
Follow-up tasks is designed to summarize action items and tasks that arise during a
meeting.

The AI models used by the API are provided by the service. You just have to send content for
analysis.
For easier navigation, here are links to the corresponding sections for each service:
ï¾‰

Aspect

Section

Issue and Resolution

Issue and Resolution

Chapter Title

Chapter Title

Narrative

Narrative

Recap and Follow-up

Recap and follow-up

Expand table

Features
The conversation summarization API uses natural language processing techniques to
summarize conversations into shorter summaries per request. Conversation summarization can
summarize for issues and resolutions discussed in a two-party conversation or summarize a
long conversation into chapters and a short narrative for each chapter.
There's another feature in Azure Language in Foundry Tools named text summarization that is
more suitable to summarize documents into concise summaries. When you're deciding
between text summarization and conversation summarization, consider the following points:

Input format: Conversation summarization can operate on both chat text and speech
transcripts, which have speakers and their utterances. Text summarization operates using
simple text, or Word, PDF, or PowerPoint formats.
Purpose of summarization: for example, conversation issue and resolution
summarization returns a reason and the resolution for a chat between a customer and a

customer service agent.

Submitting data
You submit documents to the API as strings of text. Analysis is performed upon receipt of the
request. Because the API is asynchronous, there might be a delay between sending an API
request and receiving the results. For information on the size and number of requests you can
send per minute and second, see the following data limits.
When you use this feature, the API results are available for 24 hours from the time the request
was ingested, and is indicated in the response. After this time period, the results are purged
and are no longer available for retrieval.
When you submit data to conversation summarization, we recommend sending one chat log
per request, for better latency.

Get summaries from text chats
You can use conversation issue and resolution summarization to get summaries as you need.
To see an example using text chats, see the quickstart article.

Get summaries from speech transcriptions
The conversation issue and resolution summarization also enables you to get summaries
from speech transcripts by using the Speech service's speech to text feature. The following
example shows a short conversation that you might include in your API requests.
JSON
"conversations":[
{
"id":"abcdefgh-1234-1234-1234-1234abcdefgh",
"language":"en",
"modality":"transcript",
"conversationItems":[
{
"modality":"transcript",
"participantId":"speaker",
"id":"12345678-abcd-efgh-1234-abcd123456",

"content":{
"text":"Hi.",
"lexical":"hi",
"itn":"hi",
"maskedItn":"hi",
"audioTimings":[
{
"word":"hi",
"offset":4500000,
"duration":2800000
}
]
}
}
]
}
]

Get chapter titles
Conversation chapter title summarization lets you get chapter titles from input conversations. A
guided example scenario follows:
1. Copy the following command into a text editor. The BASH example uses the \ line
continuation character. If your console or terminal uses a different line continuation
character, use that character.
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzeconversations/jobs?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \
-d \
'
{
"displayName": "Conversation Task Example",
"analysisInput": {
"conversations": [
{
"conversationItems": [
{
"text": "Hello, you're chatting with Rene. How may I help you?",
"id": "1",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "Hi, I tried to set up wifi connection for Smart Brew 300
espresso machine, but it didn't work.",
"id": "2",

"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "I'm sorry to hear that. Let's see what we can do to fix this
issue. Could you please try the following steps for me? First, could you push the
wifi connection button, hold for 3 seconds, then let me know if the power light is
slowly blinking on and off every second?",
"id": "3",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "Yes, I pushed the wifi connection button, and now the power
light is slowly blinking.",
"id": "4",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "Great. Thank you! Now, please check in your Contoso Coffee app.
Does it prompt to ask you to connect with the machine? ",
"id": "5",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "No. Nothing happened.",
"id": "6",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "I'm very sorry to hear that. Let me see if there's another way
to fix the issue. Please hold on for a minute.",
"id": "7",
"role": "Agent",
"participantId": "Agent_1"
}
],
"modality": "text",
"id": "conversation1",
"language": "en"
}
]
},
"tasks": [
{
"taskName": "Conversation Task 1",
"kind": "ConversationalSummarizationTask",
"parameters": {
"summaryAspects": [
"chapterTitle"
]
}

}
]
}
'

2. Make the following changes in the command where needed:
Replace the value your-value-language-key with your key.
Replace the first part of the request URL your-language-resource-endpoint with your
endpoint URL.
3. Open a command prompt window (for example: BASH).
4. Paste the command from the text editor into the command prompt window, then run the
command.
5. Get the operation-location from the response header. The value looks similar to the
following URL:
HTTP
https://<your-language-resource-endpoint>/language/analyzeconversations/jobs/12345678-1234-1234-1234-12345678?api-version=2023-11-15-preview

6. To get the results of the request, use the following cURL command. Be sure to replace
<my-job-id> with the GUID value you received from the previous operation-location

response header:
curl
curl -X GET https://<your-language-resource-endpoint>/language/analyzeconversations/jobs/<my-job-id>?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>"

Example chapter title summarization JSON response:
JSON
{
"jobId": "b01af3b7-1870-460a-9e36-09af28d360a1",
"lastUpdatedDateTime": "2023-11-15T18:24:26Z",
"createdDateTime": "2023-11-15T18:24:23Z",
"expirationDateTime": "2023-11-16T18:24:23Z",
"status": "succeeded",
"errors": [],
"displayName": "Conversation Task Example",
"tasks": {

"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "conversationalSummarizationResults",
"taskName": "Conversation Task 1",
"lastUpdateDateTime": "2023-11-15T18:24:26.3433677Z",
"status": "succeeded",
"results": {
"conversations": [
{
"summaries": [
{
"aspect": "chapterTitle",
"text": "\"Discussing the Problem of Smart Blend
300 Espresso Machine's Wi-Fi Connectivity\"",
"contexts": [
{
"conversationItemId": "1",
"offset": 0,
"length": 53
},
{
"conversationItemId": "2",
"offset": 0,
"length": 94
},
{
"conversationItemId": "3",
"offset": 0,
"length": 266
},
{
"conversationItemId": "4",
"offset": 0,
"length": 85
},
{
"conversationItemId": "5",
"offset": 0,
"length": 119
},
{
"conversationItemId": "6",
"offset": 0,
"length": 21
},
{
"conversationItemId": "7",
"offset": 0,
"length": 109
}
]

}
],
"id": "conversation1",
"warnings": []
}
],
"errors": [],
"modelVersion": "latest"
}
}
]
}
}

For long conversation, the model might segment it into multiple cohesive parts, and
summarize each segment. There's also a lengthy contexts field for each summary, which tells
from which range of the input conversation we generated the summary.

Get narrative summarization
Conversation summarization also lets you get narrative summaries from input conversations. A
guided example scenario is provided:
1. Copy the following command into a text editor. The BASH example uses the \ line
continuation character. If your console or terminal uses a different line continuation
character, use that character.
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzeconversations/jobs?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \
-d \
'
{
"displayName": "Conversation Task Example",
"analysisInput": {
"conversations": [
{
"conversationItems": [
{
"text": "Hello, you're chatting with Rene. How may I help you?",
"id": "1",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "Hi, I tried to set up wifi connection for Smart Brew 300
espresso machine, but it didn't work.",

"id": "2",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "I'm sorry to hear that. Let's see what we can do to fix this
issue. Could you please try the following steps for me? First, could you push the
wifi connection button, hold for 3 seconds, then let me know if the power light is
slowly blinking on and off every second?",
"id": "3",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "Yes, I pushed the wifi connection button, and now the power
light is slowly blinking.",
"id": "4",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "Great. Thank you! Now, please check in your Contoso Coffee app.
Does it prompt to ask you to connect with the machine? ",
"id": "5",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "No. Nothing happened.",
"id": "6",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "I'm very sorry to hear that. Let me see if there's another way
to fix the issue. Please hold on for a minute.",
"id": "7",
"role": "Agent",
"participantId": "Agent_1"
}
],
"modality": "text",
"id": "conversation1",
"language": "en"
}
]
},
"tasks": [
{
"taskName": "Conversation Task 1",
"kind": "ConversationalSummarizationTask",
"parameters": {
"summaryAspects": [
"narrative"
]

}
}
]
}
'

2. Make the following changes in the command where needed:
Replace the value your-language-resource-key with your key.
Replace the first part of the request URL your-language-resource-endpoint with your
endpoint URL.
3. Open a command prompt window (for example: BASH).
4. Paste the command from the text editor into the command prompt window, then run the
command.
5. Get the operation-location from the response header. The value looks similar to the
following URL:
HTTP
https://<your-language-resource-endpoint>/language/analyzeconversations/jobs/12345678-1234-1234-1234-12345678?api-version=2023-11-15-preview

6. To get the results of a request, use the following cURL command. Be sure to replace <myjob-id> with the GUID value you received from the previous operation-location

response header:
curl
curl -X GET https://<your-language-resource-endpoint>/language/analyzeconversations/jobs/<my-job-id>?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>"

Example narrative summarization JSON response:
JSON
{
"jobId": "d874a98c-bf31-4ac5-8b94-5c236f786754",
"lastUpdatedDateTime": "2022-09-29T17:36:42Z",
"createdDateTime": "2022-09-29T17:36:39Z",
"expirationDateTime": "2022-09-30T17:36:39Z",
"status": "succeeded",
"errors": [],
"displayName": "Conversation Task Example",

"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "conversationalSummarizationResults",
"taskName": "Conversation Task 1",
"lastUpdateDateTime": "2022-09-29T17:36:42.895694Z",
"status": "succeeded",
"results": {
"conversations": [
{
"summaries": [
{
"aspect": "narrative",
"text": "Agent_1 helps customer to set up wifi connection for
Smart Brew 300 espresso machine.",
"contexts": [
{ "conversationItemId": "1", "offset": 0, "length": 53 },
{ "conversationItemId": "2", "offset": 0, "length": 94 },
{ "conversationItemId": "3", "offset": 0, "length": 266 },
{ "conversationItemId": "4", "offset": 0, "length": 85 },
{ "conversationItemId": "5", "offset": 0, "length": 119 },
{ "conversationItemId": "6", "offset": 0, "length": 21 },
{ "conversationItemId": "7", "offset": 0, "length": 109 }
]
}
],
"id": "conversation1",
"warnings": []
}
],
"errors": [],
"modelVersion": "latest"
}
}
]
}
}

For long conversation, the model might segment it into multiple cohesive parts, and
summarize each segment. There's also a lengthy contexts field for each summary, which tells
from which range of the input conversation we generated the summary.

Get recap and follow-up task summarization
Conversation summarization also lets you get recaps and follow-up tasks from input
conversations. A guided example scenario is provided:

1. Copy the following command into a text editor. The BASH example uses the \ line
continuation character. If your console or terminal uses a different line continuation
character, use that character.
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzeconversations/jobs?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \
-d \
'
{
"displayName": "Conversation Task Example",
"analysisInput": {
"conversations": [
{
"conversationItems": [
{
"text": "Hello, you're chatting with Rene. How may I help you?",
"id": "1",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "Hi, I tried to set up wifi connection for Smart Brew 300
espresso machine, but it didn't work.",
"id": "2",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "I'm sorry to hear that. Let's see what we can do to fix this
issue. Could you please try the following steps for me? First, could you push the
wifi connection button, hold for 3 seconds, then let me know if the power light is
slowly blinking on and off every second?",
"id": "3",
"role": "Agent",
"participantId": "Agent_1"
},
{
"text": "Yes, I pushed the wifi connection button, and now the power
light is slowly blinking.",
"id": "4",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "Great. Thank you! Now, please check in your Contoso Coffee app.
Does it prompt to ask you to connect with the machine? ",
"id": "5",
"role": "Agent",
"participantId": "Agent_1"

},
{
"text": "No. Nothing happened.",
"id": "6",
"role": "Customer",
"participantId": "Customer_1"
},
{
"text": "I'm very sorry to hear that. Let me see if there's another way
to fix the issue. Please hold on for a minute.",
"id": "7",
"role": "Agent",
"participantId": "Agent_1"
}
],
"modality": "text",
"id": "conversation1",
"language": "en"
}
]
},
"tasks": [
{
"taskName": "Conversation Task 1",
"kind": "ConversationalSummarizationTask",
"parameters": {
"summaryAspects": [
"recap",
"follow-up tasks"
]
}
}
]
}
'

2. Make the following changes in the command where needed:
Replace the value your-language-resource-key with your key.
Replace the first part of the request URL your-language-resource-endpoint with your
endpoint URL.
3. Open a command prompt window (for example: BASH).
4. Paste the command from the text editor into the command prompt window, then run the
command.
5. Get the operation-location from the response header. The value looks similar to the
following URL:
HTTP

https://<your-language-resource-endpoint>/language/analyzeconversations/jobs/12345678-1234-1234-1234-12345678?api-version=2023-11-15-preview

6. To get the results of a request, use the following cURL command. Be sure to replace <myjob-id> with the GUID value you received from the previous operation-location

response header:
curl
curl -X GET https://<your-language-resource-endpoint>/language/analyzeconversations/jobs/<my-job-id>?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>"

Example recap and follow-up summarization JSON response:
JSON
{
"jobId": "e585d097-c19a-466e-8f99-a9646e55b1f5",
"lastUpdatedDateTime": "2023-11-15T18:19:56Z",
"createdDateTime": "2023-11-15T18:19:53Z",
"expirationDateTime": "2023-11-16T18:19:53Z",
"status": "succeeded",
"errors": [],
"displayName": "Conversation Task Example",
"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "conversationalSummarizationResults",
"taskName": "Conversation Task 1",
"lastUpdateDateTime": "2023-11-15T18:19:56.1801785Z",
"status": "succeeded",
"results": {
"conversations": [
{
"summaries": [
{
"aspect": "recap",
"text": "The customer contacted the service
agent, Rene, regarding an issue with setting up a wifi connection for their Smart
Brew 300 espresso machine. The agent guided the customer through several steps,
including pushing the wifi connection button and checking if the power light was
blinking. However, the customer reported that no prompts were received in the
Contoso Coffee app to connect with the machine. The agent then decided to look for
another solution.",
"contexts": [

{
"conversationItemId": "1",
"offset": 0,
"length": 53
},
{
"conversationItemId": "2",
"offset": 0,
"length": 94
},
{
"conversationItemId": "3",
"offset": 0,
"length": 266
},
{
"conversationItemId": "4",
"offset": 0,
"length": 85
},
{
"conversationItemId": "5",
"offset": 0,
"length": 119
},
{
"conversationItemId": "6",
"offset": 0,
"length": 21
},
{
"conversationItemId": "7",
"offset": 0,
"length": 109
}
]
},
{
"aspect": "Follow-Up Tasks",
"text": "@Agent_1 will ask the customer to push
the wifi connection button, hold for 3 seconds, then check if the power light is
slowly blinking on and off every second."
},
{
"aspect": "Follow-Up Tasks",
"text": "@Agent_1 will ask the customer to check
in the Contoso Coffee app if it prompts to connect with the machine."
},
{
"aspect": "Follow-Up Tasks",
"text": "@Agent_1 will investigate another way
to fix the issue."
}
],
"id": "conversation1",

"warnings": []
}
],
"errors": [],
"modelVersion": "latest"
}
}
]
}
}

For long conversation, the model might segment it into multiple cohesive parts, and
summarize each segment. There's also a lengthy contexts field for each summary, which tells
from which range of the input conversation we generated the summary.

Getting conversation issue and resolution
summarization results
The following text is an example of content you might submit for conversation issue and
resolution summarization. It's only an example. The API can accept longer input text. For more
information, see data limits.
Agent: "Hello, how can I help you?"
Customer: "How can I upgrade my Contoso subscription? I've been trying the entire day."
Agent: "Press the upgrade button then sign in and follow the instructions."
Summarization is performed upon receipt of the request by creating a job for the API backend.
If the job succeeded, the output of the API is returned. The output is available for retrieval for
24 hours. After this time, the output is purged. Due to multilingual and emoji support, the
response might contain text offsets. For more information, see how to process offsets.
In the previous example, the API might return this summarized sentences output:
ï¾‰

Expand table

Summarized text

Aspect

"Customer wants to upgrade their subscription. Customer doesn't know how."

issue

"Customer needs to press upgrade button, and sign in."

resolution

See also

Summarization overview

Last updated on 11/18/2025

How to use text summarization
Text summarization is designed to shorten content that users consider too long to read. Both
extractive and abstractive summarization condense articles, papers, or documents to key
sentences.
Extractive summarization: Produces a summary by extracting sentences that collectively
represent the most important or relevant information within the original content.
Abstractive summarization: Produces a summary by generating summarized sentences from
the document that capture the main idea.
Query-focused summarization: Allows you to use a query when summarizing.
Each of these capabilities is able to summarize around specific items of interest when specified.
The AI models used by the API are provided by the service. You just have to send content for
analysis.
For easier navigation, here are links to the corresponding sections for each service:
ï¾‰

Aspect

Section

Extractive

Extractive Summarization

Abstractive

Abstractive Summarization

Query-focused

Query-focused Summarization

Expand table

Features
îª€ Tip
If you want to start using these features, you can follow the quickstart article to get
started. You can also make example requests using Microsoft Foundry

) without needing

to write code.
The extractive summarization API uses natural language processing techniques to locate key
sentences in an unstructured text document. These sentences collectively convey the main idea
of the document.

Extractive summarization returns a rank score as a part of the system response along with
extracted sentences and their position in the original documents. A rank score is an indicator of
how relevant a sentence is determined to be, to the main idea of a document. The model gives
a score between 0 and 1 (inclusive) to each sentence and returns the highest scored sentences
per request. For example, if you request a three-sentence summary, the service returns the
three highest scored sentences.
There's another feature in Azure Language in Foundry Tools, key phrase extraction, that can
extract key information. When deciding between key phrase extraction and extractive
summarization, consider these factors:
Key phrase extraction returns phrases while extractive summarization returns sentences.
Extractive summarization returns sentences together with a rank score, and top ranked
sentences are returned per request.
Extractive summarization also returns the following positional information:
Offset: The start position of each extracted sentence.
Length: The length of each extracted sentence.

Determine how to process the data (optional)
Submitting data
You submit documents to the API as strings of text. Analysis is performed upon receipt of the
request. Because the API is asynchronous, there might be a delay between sending an API
request, and receiving the results.
When you use this feature, the API results are available for 24 hours from the time the request
was ingested, and is indicated in the response. After this time period, the results are purged
and are no longer available for retrieval.

Getting text summarization results
When you get results from language detection, you can stream the results to an application or
save the output to a file on the local system.
To follow is an example of content you might submit for summarization, which is extracted
using the Microsoft blog article A holistic representation toward integrative AI . This article is
only an example. The API can accept longer input text. For more information, see data and
service limits
"At Microsoft, we are on a quest to advance AI beyond existing techniques, by taking a more
holistic, human-centric approach to learning and understanding. As Chief Technology Officer of

Foundry Tools, I have been working with a team of amazing scientists and engineers to turn this
quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among
three attributes of human cognition: monolingual text (X), audio or visual sensory signals, (Y) and
multilingual (Z). At the intersection of all three, there's magicâ€”what we call XYZ-code as
illustrated in Figure 1â€”a joint representation to create more powerful AI that can speak, hear,
see, and understand humans better. We believe XYZ-code enables us to fulfill our long-term
vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have
pretrained models that can jointly learn representations to support a broad range of downstream
AI tasks, much in the way humans do today. Over the past five years, we achieved human
performance on benchmarks in conversational speech recognition, machine translation,
conversational question answering, machine reading comprehension, and image captioning.
These five breakthroughs provided us with strong signals toward our more ambitious aspiration
to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that's
closer in line with how humans learn and understand. I believe the joint XYZ-code is a
foundational component of this aspiration, if grounded with external knowledge sources in the
downstream AI tasks."
The text summarization API request is processed upon receipt of the request by creating a job
for the API backend. If the job succeeded, the output of the API is returned. The output is
available for retrieval for 24 hours. After this time, the output is purged. Due to multilingual
and emoji support, the response might contain text offsets. For more information, see how to
process offsets.
When you use the preceding example, the API might return these summarized sentences:
Extractive summarization:
"At Microsoft, we are on a quest to advance AI beyond existing techniques, by taking a
more holistic, human-centric approach to learning and understanding."
"We believe XYZ-code enables us to fulfill our long-term vision: cross-domain transfer
learning, spanning modalities and languages."
"The goal is to have pretrained models that can jointly learn representations to support a
broad range of downstream AI tasks, much in the way humans do today."
Abstractive summarization:
"Microsoft is taking a more holistic, human-centric approach to learning and
understanding. We believe XYZ-code enables us to fulfill our long-term vision: crossdomain transfer learning, spanning modalities and languages. Over the past five years, we
achieved human performance on key benchmarks."

Try text extractive summarization

You can use text extractive summarization to get summaries of articles, papers, or documents.
To see an example, see the quickstart article.
You can use the sentenceCount parameter to guide how many sentences are returned, with 3
being the default. The range is from 1 to 20.
You can also use the sortby parameter to specify in what order the extracted sentences are
returned - either Offset or Rank , with Offset being the default.
ï¾‰

Expand table

parameter
value

Description

Rank

Order sentences according to their relevance to the input document, as decided by
the service.

Offset

Keeps the original order in which the sentences appear in the input document.

Try text abstractive summarization
The following example gets you started with text abstractive summarization:
1. Copy the following command into a text editor. The BASH example uses the \ line
continuation character. If your console or terminal uses a different line continuation
character, use that character instead.
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzetext/jobs?api-version=2023-04-01 \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \
-d \
'
{
"displayName": "Text Abstractive Summarization Task Example",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "en",
"text": "At Microsoft, we have been on a quest to advance AI beyond existing
techniques, by taking a more holistic, human-centric approach to learning and
understanding. As Chief Technology Officer of Foundry Tools, I have been working
with a team of amazing scientists and engineers to turn this quest into a reality.
In my role, I enjoy a unique perspective in viewing the relationship among three
attributes of human cognition: monolingual text (X), audio or visual sensory

signals, (Y) and multilingual (Z). At the intersection of all three, there's magicâ€”
what we call XYZ-code as illustrated in Figure 1â€”a joint representation to create
more powerful AI that can speak, hear, see, and understand humans better. We believe
XYZ-code enables us to fulfill our long-term vision: cross-domain transfer learning,
spanning modalities and languages. The goal is to have pretrained models that can
jointly learn representations to support a broad range of downstream AI tasks, much
in the way humans do today. Over the past five years, we have achieved human
performance on benchmarks in conversational speech recognition, machine translation,
conversational question answering, machine reading comprehension, and image
captioning. These five breakthroughs provided us with strong signals toward our more
ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory
and multilingual learning that is closer in line with how humans learn and
understand. I believe the joint XYZ-code is a foundational component of this
aspiration, if grounded with external knowledge sources in the downstream AI tasks."
}
]
},
"tasks": [
{
"kind": "AbstractiveSummarization",
"taskName": "Text Abstractive Summarization Task 1",
}
]
}
'

2. Make the following changes in the command where needed:
Replace the value your-language-resource-key with your key.
Replace the first part of the request URL your-language-resource-endpoint with your
endpoint URL.
3. Open a command prompt window (for example: BASH).
4. Paste the command from the text editor into the command prompt window, then run the
command.
5. Get the operation-location from the response header. The value looks similar to the
following URL:
HTTP
https://<your-language-resource-endpoint>/language/analyze-text/jobs/12345678-12341234-1234-12345678?api-version=2022-10-01-preview

6. To get the results of the request, use the following cURL command. Be sure to replace
<my-job-id> with the numerical ID value you received from the previous operationlocation response header:

Bash
curl -X GET https://<your-language-resource-endpoint>/language/analyzetext/jobs/<my-job-id>?api-version=2022-10-01-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>"

Abstractive text summarization example JSON response
JSON
{
"jobId": "cd6418fe-db86-4350-aec1-f0d7c91442a6",
"lastUpdateDateTime": "2022-09-08T16:45:14Z",
"createdDateTime": "2022-09-08T16:44:53Z",
"expirationDateTime": "2022-09-09T16:44:53Z",
"status": "succeeded",
"errors": [],
"displayName": "Text Abstractive Summarization Task Example",
"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "AbstractiveSummarizationLROResults",
"taskName": "Text Abstractive Summarization Task 1",
"lastUpdateDateTime": "2022-09-08T16:45:14.0717206Z",
"status": "succeeded",
"results": {
"documents": [
{
"summaries": [
{
"text": "Microsoft is taking a more holistic,
human-centric approach to AI. We've developed a joint representation to create more
powerful AI that can speak, hear, see, and understand humans better. We've achieved
human performance on benchmarks in conversational speech recognition, machine
translation, ...... and image captions.",
"contexts": [
{
"offset": 0,
"length": 247
}
]
}
],
"id": "1"
}
],
"errors": [],
"modelVersion": "latest"

}
}
]
}
}

ï¾‰

Expand table

parameter

Description

-X POST <endpoint>

Specifies your endpoint for accessing the API.

-H Content-Type: application/json

The content type for sending JSON data.

-H "Ocp-Apim-Subscription-Key:<key>

Specifies the key for accessing the API.

-d <documents>

The JSON containing the documents you want to send.

The following cURL commands are executed from a BASH shell. Edit these commands with
your own resource name, resource key, and JSON values.

Query based summarization
The query-based text summarization API is an extension to the existing text summarization API.
The biggest difference is a new query field in the request body (under tasks > parameters >
query ).

îª€ Tip
Query based summarization has some differentiation in the utilization of length control
based on the type of query based summarization you're using:
Query based extractive summarization supports length control by specifying
sentenceCount.
Query based abstractive summarization doesn't support length control.
Here's an example request:
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzetext/jobs?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \

-d \
'
{
"displayName": "Text Extractive Summarization Task Example",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "en",
"text": "At Microsoft, we have been on a quest to advance AI beyond existing
techniques, by taking a more holistic, human-centric approach to learning and
understanding. As Chief Technology Officer of Foundry Tools, I have been working
with a team of amazing scientists and engineers to turn this quest into a reality.
In my role, I enjoy a unique perspective in viewing the relationship among three
attributes of human cognition: monolingual text (X), audio or visual sensory
signals, (Y) and multilingual (Z). At the intersection of all three, there's magicâ€”
what we call XYZ-code as illustrated in Figure 1â€”a joint representation to create
more powerful AI that can speak, hear, see, and understand humans better. We believe
XYZ-code enables us to fulfill our long-term vision: cross-domain transfer learning,
spanning modalities and languages. The goal is to have pretrained models that can
jointly learn representations to support a broad range of downstream AI tasks, much
in the way humans do today. Over the past five years, we have achieved human
performance on benchmarks in conversational speech recognition, machine translation,
conversational question answering, machine reading comprehension, and image
captioning. These five breakthroughs provided us with strong signals toward our more
ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory
and multilingual learning that is closer in line with how humans learn and
understand. I believe the joint XYZ-code is a foundational component of this
aspiration, if grounded with external knowledge sources in the downstream AI tasks."
}
]
},
"tasks": [
{
"kind": "AbstractiveSummarization",
"taskName": "Query-based Abstractive Summarization",
"parameters": {
"query": "XYZ-code",
"summaryLength": "short"
}
},
{
"kind": "ExtractiveSummarization",
"taskName": "Query_based Extractive Summarization",
"parameters": {
"query": "XYZ-code"
}
}
]
}
'

Summary length control

Using the summaryLength parameter in abstractive summarization
If you don't specify summaryLength , the model determines the summary length.
For the summaryLength parameter, three values are accepted:
oneSentence: Generates a summary of mostly one sentence, with around 80 tokens.
short: Generates a summary of mostly two to three sentences, with around 120 tokens.
medium: Generates a summary of mostly four to six sentences, with around 170 tokens.
long: Generates a summary of mostly over seven sentences, with around 210 tokens.
Here's is an example request:
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzetext/jobs?api-version=2023-04-01 \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \
-d \
'
{
"displayName": "Text Abstractive Summarization Task Example",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "en",
"text": "At Microsoft, we have been on a quest to advance AI beyond existing
techniques, by taking a more holistic, human-centric approach to learning and
understanding. As Chief Technology Officer of Foundry Tools, I have been working
with a team of amazing scientists and engineers to turn this quest into a reality.
In my role, I enjoy a unique perspective in viewing the relationship among three
attributes of human cognition: monolingual text (X), audio or visual sensory
signals, (Y) and multilingual (Z). At the intersection of all three, there's magicâ€”
what we call XYZ-code as illustrated in Figure 1â€”a joint representation to create
more powerful AI that can speak, hear, see, and understand humans better. We believe
XYZ-code enables us to fulfill our long-term vision: cross-domain transfer learning,
spanning modalities and languages. The goal is to have pretrained models that can
jointly learn representations to support a broad range of downstream AI tasks, much
in the way humans do today. Over the past five years, we have achieved human
performance on benchmarks in conversational speech recognition, machine translation,
conversational question answering, machine reading comprehension, and image
captioning. These five breakthroughs provided us with strong signals toward our more
ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory
and multilingual learning that is closer in line with how humans learn and
understand. I believe the joint XYZ-code is a foundational component of this
aspiration, if grounded with external knowledge sources in the downstream AI tasks."
}
]
},
"tasks": [

{
"kind": "AbstractiveSummarization",
"taskName": "Length controlled Abstractive Summarization",
"parameters": {
"summaryLength": "short"
}
}
]
}
'

Using the sentenceCount parameter in extractive summarization
For the sentenceCount parameter, you can input a value 1-20 to indicate the desired number of
output sentences.
Here's is an example request:
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzetext/jobs?api-version=2023-11-15-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \
-d \
'
{
"displayName": "Text Extractive Summarization Task Example",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "en",
"text": "At Microsoft, we have been on a quest to advance AI beyond existing
techniques, by taking a more holistic, human-centric approach to learning and
understanding. As Chief Technology Officer of Foundry Tools, I have been working
with a team of amazing scientists and engineers to turn this quest into a reality.
In my role, I enjoy a unique perspective in viewing the relationship among three
attributes of human cognition: monolingual text (X), audio or visual sensory
signals, (Y) and multilingual (Z). At the intersection of all three, there's magicâ€”
what we call XYZ-code as illustrated in Figure 1â€”a joint representation to create
more powerful AI that can speak, hear, see, and understand humans better. We believe
XYZ-code enables us to fulfill our long-term vision: cross-domain transfer learning,
spanning modalities and languages. The goal is to have pretrained models that can
jointly learn representations to support a broad range of downstream AI tasks, much
in the way humans do today. Over the past five years, we have achieved human
performance on benchmarks in conversational speech recognition, machine translation,
conversational question answering, machine reading comprehension, and image
captioning. These five breakthroughs provided us with strong signals toward our more
ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory
and multilingual learning that is closer in line with how humans learn and
understand. I believe the joint XYZ-code is a foundational component of this

aspiration, if grounded with external knowledge sources in the downstream AI tasks."
}
]
},
"tasks": [
{
"kind": "ExtractiveSummarization",
"taskName": "Length controlled Extractive Summarization",
"parameters": {
"sentenceCount": "5"
}
}
]
}
'

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

See also
Summarization overview

Last updated on 12/15/2025

How to use native document
summarization (preview)
ï¼‰ Important
Azure Language in Foundry Tools public preview releases provide early access to
features that are in active development.
Features, approaches, and processes may change, before General Availability (GA),
based on user feedback.
Language is a cloud-based service that applies Natural Language Processing (NLP) features to
text-based data. Document summarization uses natural language processing to generate
extractive (salient sentence extraction) or abstractive (contextual word extraction) summaries
for documents. Both AbstractiveSummarization and ExtractiveSummarization APIs support
native document processing. A native document refers to the file format used to create the
original document such as Microsoft Word (docx) or a portable document file (pdf). Native
document support eliminates the need for text preprocessing before using Language resource
capabilities. The native document support capability enables you to send API requests
asynchronously, using an HTTP POST request body to send your data and HTTP GET request
query string to retrieve the status results. Your processed documents are located in your Azure
Blob Storage target container.

Supported document formats
Applications use native file formats to create, save, or open native documents. Currently PII and
Document summarization capabilities supports the following native document formats:
ï¾‰

File type

File extension

Description

Text

.txt

An unformatted text document.

Adobe PDF

.pdf

A portable document file formatted document.

Microsoft Word

.docx

A Microsoft Word document file.

Input guidelines

Expand table

Supported file formats

Type

support and limitations

PDFs

Fully scanned PDFs aren't supported.

Text within images

Digital images with embedded text aren't supported.

Digital tables

Tables in scanned documents aren't supported.

ï¾‰

Expand table

ï¾‰

Expand table

Document Size

Attribute

Input limit

Total number of documents per request

â‰¤ 20

Total content size per request

â‰¤ 10 MB

Include native documents with an HTTP request
Let's get started:
For this project, we use the cURL command-line tool to make REST API calls.
ï¼— Note
The cURL package is preinstalled on most Windows 10 and Windows 11 and most
macOS and Linux distributions. You can check the package version with the following
commands: Windows: curl.exe -V macOS curl -V Linux: curl --version
An active Azure account

. If you don't have one, you can create a free account

An Azure Blob Storage account

.

. You also need to create containers in your Azure Blob

Storage account for your source and target files:
Source container. This container is where you upload your native files for analysis
(required).
Target container. This container is where your analyzed files are stored (required).
A single-service Language resource

(not a multi-service Microsoft Foundry resource):

Complete Azure Language resource project and instance details fields as follows:
1. Subscription. Select one of your available Azure subscriptions.
2. Resource Group. You can create a new resource group or add your resource to a
preexisting resource group that shares the same lifecycle, permissions, and policies.
3. Resource Region. Choose Global unless your business or application requires a
specific region. If you're planning on using a system-assigned managed identity for
authentication, choose a geographic region like West US.
4. Name. Enter the name you chose for your resource. The name you choose must be
unique within Azure.
5. Pricing tier. You can use the free pricing tier ( Free F0 ) to try the service, and
upgrade later to a paid tier for production.
6. Select Review + Create.
7. Review the service terms and select Create to deploy your resource.
8. After your resource successfully deploys, select Go to resource.

Retrieve your key and Language endpoint
Requests to Azure Language require a read-only key and custom endpoint to authenticate
access.
1. If you created a new resource, after it deploys, select Go to resource. If you have an
existing Language resource, navigate directly to your resource page.
2. In the left rail, under Resource Management, select Keys and Endpoint.
3. You can copy and paste your key and your Language instance endpoint into the code
samples to authenticate your request to Azure Language. Only one key is necessary to
make an API call.

Create Azure Blob Storage containers
Create containers in your Azure Blob Storage account

for source and target files.

Source container. This container is where you upload your native files for analysis
(required).
Target container. This container is where your analyzed files are stored (required).

Authentication
Your Language resource needs granted access to your storage account before it can create,
read, or delete blobs. There are two primary methods you can use to grant access to your
storage data:
Shared access signature (SAS) tokens. User delegation SAS tokens are secured with
Microsoft Entra credentials. SAS tokens provide secure, delegated access to resources in
your Azure storage account.
Managed identity role-based access control (RBAC). Managed identities for Azure
resources are service principals that create a Microsoft Entra identity and specific
permissions for Azure managed resources.
For this project, we authenticate access to the source location and target location URLs with
Shared Access Signature (SAS) tokens appended as query strings. Each token is assigned to a
specific blob (file).

Your source container or blob must designate read and list access.
Your target container or blob must designate write and list access.
The extractive summarization API uses natural language processing techniques to locate key
sentences in an unstructured text document. These sentences collectively convey the main idea
of the document.
Extractive summarization returns a rank score as a part of the system response along with
extracted sentences and their position in the original documents. A rank score is an indicator of
how relevant a sentence is determined to be, to the main idea of a document. The model gives
a score between 0 and 1 (inclusive) to each sentence and returns the highest scored sentences
per request. For example, if you request a three-sentence summary, the service returns the
three highest scored sentences.
There's another feature in Language, key phrase extraction, that can extract key information. To
decide between key phrase extraction and extractive summarization, here are helpful
considerations:
Key phrase extraction returns phrases while extractive summarization returns sentences.
Extractive summarization returns sentences together with a rank score, and top ranked
sentences are returned per request.
Extractive summarization also returns the following positional information:

Offset: The start position of each extracted sentence.
Length: The length of each extracted sentence.

Determine how to process the data (optional)
Submitting data
You submit documents to the API as strings of text. Analysis is performed upon receipt of the
request. Because the API is asynchronous, there might be a delay between sending an API
request, and receiving the results.
When you use this feature, the API results are available for 24 hours from the time the request
was ingested, and is indicated in the response. After this time period, the results are purged
and are no longer available for retrieval.

Getting text summarization results
When you get results from language detection, you can stream the results to an application or
save the output to a file on the local system.
Here's an example of content you might submit for summarization, which is extracted using
the Microsoft blog article A holistic representation toward integrative AI

. This article is only

an example. The API can accept longer input text. For more information, see data and service
limits.
"At Microsoft, we are on a quest to advance AI beyond existing techniques, by taking a more
holistic, human-centric approach to learning and understanding. As Chief Technology Officer of
Foundry Tools, I have been working with a team of amazing scientists and engineers to turn this
quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among
three attributes of human cognition: monolingual text (X), audio or visual sensory signals, (Y) and
multilingual (Z). At the intersection of all three, there's magicâ€”what we call XYZ-code as
illustrated in Figure 1â€”a joint representation to create more powerful AI that can speak, hear,
see, and understand humans better. We believe XYZ-code enables us to fulfill our long-term
vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have
pretrained models that can jointly learn representations to support a broad range of downstream
AI tasks, much in the way humans do today. Over the past five years, we achieved human
performance on benchmarks in conversational speech recognition, machine translation,
conversational question answering, machine reading comprehension, and image captioning.
These five breakthroughs provided us with strong signals toward our more ambitious aspiration
to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is
closer in line with how humans learn and understand. I believe the joint XYZ-code is a

foundational component of this aspiration, if grounded with external knowledge sources in the
downstream AI tasks."
The text summarization API request is processed upon receipt of the request by creating a job
for the API backend. If the job succeeded, the output of the API is returned. The output is
available for retrieval for 24 hours. After this time, the output is purged. Due to multilingual
and emoji support, the response might contain text offsets. For more information, see how to
process offsets.
When you use the preceding example, the API might return these summarized sentences:
Extractive summarization:
"At Microsoft, we are on a quest to advance AI beyond existing techniques, by taking a
more holistic, human-centric approach to learning and understanding."
"We believe XYZ-code enables us to fulfill our long-term vision: cross-domain transfer
learning, spanning modalities and languages."
"The goal is to have pretrained models that can jointly learn representations to support a
broad range of downstream AI tasks, much in the way humans do today."
Abstractive summarization:
"Microsoft is taking a more holistic, human-centric approach to learning and
understanding. We believe XYZ-code enables us to fulfill our long-term vision: crossdomain transfer learning, spanning modalities and languages. Over the past five years, we
achieved human performance on benchmarks in."

Try text extractive summarization
You can use text extractive summarization to get summaries of articles, papers, or documents.
To see an example, see the quickstart article.
You can use the sentenceCount parameter to guide how many sentences are returned, with 3
being the default. The range is from 1 to 20.
You can also use the sortby parameter to specify in what order the extracted sentences are
returned - either Offset or Rank , with Offset being the default.
ï¾‰

Expand table

parameter
value

Description

Rank

Order sentences according to their relevance to the input document, as decided by
the service.

Offset

Keeps the original order in which the sentences appear in the input document.

Try text abstractive summarization
The following example gets you started with text abstractive summarization:
1. Copy the following command into a text editor. The BASH example uses the \ line
continuation character. If your console or terminal uses a different line continuation
character, use that character instead.
Bash
curl -i -X POST https://<your-language-resource-endpoint>/language/analyzetext/jobs?api-version=2023-04-01 \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>" \
-d \
'
{
"displayName": "Text Abstractive Summarization Task Example",
"analysisInput": {
"documents": [
{
"id": "1",
"language": "en",
"text": "At Microsoft, we have been on a quest to advance AI beyond existing
techniques, by taking a more holistic, human-centric approach to learning and
understanding. As Chief Technology Officer of Foundry Tools, I have been working
with a team of amazing scientists and engineers to turn this quest into a reality.
In my role, I enjoy a unique perspective in viewing the relationship among three
attributes of human cognition: monolingual text (X), audio or visual sensory
signals, (Y) and multilingual (Z). At the intersection of all three, there's magicâ€”
what we call XYZ-code as illustrated in Figure 1â€”a joint representation to create
more powerful AI that can speak, hear, see, and understand humans better. We believe
XYZ-code enables us to fulfill our long-term vision: cross-domain transfer learning,
spanning modalities and languages. The goal is to have pretrained models that can
jointly learn representations to support a broad range of downstream AI tasks, much
in the way humans do today. Over the past five years, we have achieved human
performance on benchmarks in conversational speech recognition, machine translation,
conversational question answering, machine reading comprehension, and image
captioning. These five breakthroughs provided us with strong signals toward our more
ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory
and multilingual learning that is closer in line with how humans learn and
understand. I believe the joint XYZ-code is a foundational component of this
aspiration, if grounded with external knowledge sources in the downstream AI tasks."

}
]
},
"tasks": [
{
"kind": "AbstractiveSummarization",
"taskName": "Text Abstractive Summarization Task 1",
}
]
}
'

2. Make the following changes in the command where needed:
Replace the value your-language-resource-key with your key.
Replace the first part of the request URL your-language-resource-endpoint with your
endpoint URL.
3. Open a command prompt window (for example: BASH).
4. Paste the command from the text editor into the command prompt window, then run the
command.
5. Get the operation-location from the response header. The value looks similar to the
following URL:
HTTP
https://<your-language-resource-endpoint>/language/analyze-text/jobs/12345678-12341234-1234-12345678?api-version=2022-10-01-preview

6. To get the results of the request, use the following cURL command. Be sure to replace
<my-job-id> with the numerical ID value you received from the previous operationlocation response header:

Bash
curl -X GET https://<your-language-resource-endpoint>/language/analyzetext/jobs/<my-job-id>?api-version=2022-10-01-preview \
-H "Content-Type: application/json" \
-H "Ocp-Apim-Subscription-Key: <your-language-resource-key>"

Abstractive text summarization example JSON response
JSON

{
"jobId": "cd6418fe-db86-4350-aec1-f0d7c91442a6",
"lastUpdateDateTime": "2022-09-08T16:45:14Z",
"createdDateTime": "2022-09-08T16:44:53Z",
"expirationDateTime": "2022-09-09T16:44:53Z",
"status": "succeeded",
"errors": [],
"displayName": "Text Abstractive Summarization Task Example",
"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "AbstractiveSummarizationLROResults",
"taskName": "Text Abstractive Summarization Task 1",
"lastUpdateDateTime": "2022-09-08T16:45:14.0717206Z",
"status": "succeeded",
"results": {
"documents": [
{
"summaries": [
{
"text": "Microsoft is taking a more holistic,
human-centric approach to AI. We've developed a joint representation to create more
powerful AI that can speak, hear, see, and understand humans better. We've achieved
human performance on benchmarks in conversational speech recognition, machine
translation, ...... and image captions.",
"contexts": [
{
"offset": 0,
"length": 247
}
]
}
],
"id": "1"
}
],
"errors": [],
"modelVersion": "latest"
}
}
]
}
}

ï¾‰

Expand table

parameter

Description

-X POST <endpoint>

Specifies your Language resource endpoint for accessing the
API.

--header Content-Type:

The content type for sending JSON data.

application/json
--header "Ocp-Apim-Subscription-Key:

Specifies Azure Language resource key for accessing the API.

<key>
-data

The JSON file containing the data you want to pass with your
request.

The following cURL commands are executed from a BASH shell. Edit these commands with
your own resource name, resource key, and JSON values. Try analyzing native documents by
selecting the Personally Identifiable Information (PII) or Document Summarization code
sample project:

Summarization sample document
For this project, you need a source document uploaded to your source container. You can
download our Microsoft Word sample document

or Adobe PDF

for this quickstart. The

source language is English.

Build the POST request
1. Using your preferred editor or IDE, create a new directory for your app named nativedocument .

2. Create a new json file called document-summarization.json in your native-document
directory.
3. Copy and paste the Document Summarization request sample into your documentsummarization.json file. Replace {your-source-container-SAS-URL} and {your-targetcontainer-SAS-URL} with values from your Azure portal Storage account containers

instance:
Request sample
JSON
{
"tasks": [
{

"kind": "ExtractiveSummarization",
"parameters": {
"sentenceCount": 6
}
}
],
"analysisInput": {
"documents": [
{
"source": {
"location": "{your-source-blob-SAS-URL}"
},
"targets": {
"location": "{your-target-container-SAS-URL}"
}
}
]
}
}

Run the POST request
Before you run the POST request, replace {your-language-resource-endpoint} and {your-key}
with the endpoint value from your Azure portal Language resource instance.
ï¼‰ Important
Remember to remove the key from your code when you're done, and never post it
publicly. For production, use a secure way of storing and accessing your credentials like
Azure Key Vault. For more information, see Foundry Tools security.
PowerShell
PowerShell
cmd /c curl "{your-language-resource-endpoint}/language/analyze-documents/jobs?apiversion=2024-11-15-preview" -i -X POST --header "Content-Type: application/json" -header "Ocp-Apim-Subscription-Key: {your-key}" --data "@document-summarization.json"

command prompt / terminal
Bash
curl -v -X POST "{your-language-resource-endpoint}/language/analyze-documents/jobs?
api-version=2024-11-15-preview" --header "Content-Type: application/json" --header
"Ocp-Apim-Subscription-Key: {your-key}" --data "@document-summarization.json"

Sample response:
HTTP
HTTP/1.1 202 Accepted
Content-Length: 0
operation-location: https://{your-language-resource-endpoint}/language/analyzedocuments/jobs/f1cc29ff-9738-42ea-afa5-98d2d3cabf94?api-version=2024-11-15-preview
apim-request-id: e7d6fa0c-0efd-416a-8b1e-1cd9287f5f81
x-ms-region: West US 2
Date: Thu, 25 Jan 2024 15:12:32 GMT

POST response ( jobId)
You receive a 202 (Success) response that includes a read-only Operation-Location header. The
value of this header contains a jobId that can be queried to get the status of the asynchronous
operation and retrieve the results using a GET request:

Get analyze results (GET request)
1. After your successful POST request, poll the operation-location header returned in the
POST request to view the processed data.
2. Here's the structure of the GET request:
Bash
GET {cognitive-service-endpoint}/language/analyze-documents/jobs/{jobId}?apiversion=2024-11-15-preview

3. Before you run the command, make these changes:
Replace {jobId} with the Operation-Location header from the POST response.
Replace {your-language-resource-endpoint} and {your-key} with the values from
your Language instance in the Azure portal.

Get request
PowerShell

cmd /c curl "{your-language-resource-endpoint}/language/analyzedocuments/jobs/{jobId}?api-version=2024-11-15-preview" -i -X GET --header "ContentType: application/json" --header "Ocp-Apim-Subscription-Key: {your-key}"

Bash
curl -v -X GET "{your-language-resource-endpoint}/language/analyzedocuments/jobs/{jobId}?api-version=2024-11-15-preview" --header "Content-Type:
application/json" --header "Ocp-Apim-Subscription-Key: {your-key}"

Examine the response
You receive a 200 (Success) response with JSON output. The status field indicates the result of
the operation. If the operation isn't complete, the value of status is "running" or "notStarted",
and you should call the API again, either manually or through a script. We recommend an
interval of one second or more between calls.

Sample response
JSON
{
"jobId": "f1cc29ff-9738-42ea-afa5-98d2d3cabf94",
"lastUpdatedDateTime": "2024-01-24T13:17:58Z",
"createdDateTime": "2024-01-24T13:17:47Z",
"expirationDateTime": "2024-01-25T13:17:47Z",
"status": "succeeded",
"errors": [],
"tasks": {
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "ExtractiveSummarizationLROResults",
"lastUpdateDateTime": "2024-01-24T13:17:58.33934Z",
"status": "succeeded",
"results": {
"documents": [
{
"id": "doc_0",
"source": {
"kind": "AzureBlob",
"location": "https://myaccount.blob.core.windows.net/sampleinput/input.pdf"
},
"targets": [

{
"kind": "AzureBlob",
"location": "https://myaccount.blob.core.windows.net/sampleoutput/df6611a3-fe74-44f8-b8d4-58ac7491cb13/ExtractiveSummarization0001/input.result.json"
}
],
"warnings": []
}
],
"errors": [],
"modelVersion": "2023-02-01-preview"
}
}
]
}
}

Upon successful completion:
The analyzed documents can be found in your target container.
The successful POST method returns a 202 Accepted response code indicating that the
service created the batch request.
The POST request also returned response headers including Operation-Location that
provides a value used in subsequent GET requests.

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
Document summarization overview

Last updated on 11/18/2025

Use summarization Docker containers onpremises
Containers enable you to host the Summarization API on your own infrastructure. If you have
security or data governance requirements that can't be fulfilled by calling Summarization
remotely, then containers might be a good option.

Prerequisites
If you don't have an Azure subscription, create a free account
Docker

.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts
A Language resource

.

with the free (F0) or standard (S) pricing tier

. For disconnected

containers, the DC0 tier is required.

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:
Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the
summarization container skills. Listed CPU/memory combinations are for a 4000 token input
(conversation consumption is for all the aspects in the same request).
ï¾‰

Expand table

Container Type

Summarization CPU

Recommended number

Recommended

of CPU cores

memory

16

48 GB

2

24 GB

Notes

container
Summarization GPU
container

Requires an NVIDIA GPU that
supports Cuda 11.8 with 16GB
VRAM.

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Get the container image with docker pull
The Summarization container image can be found on the mcr.microsoft.com container registry
syndicate. It resides within the azure-cognitive-services/textanalytics/ repository and is
named summarization . The fully qualified container image name is, mcr.microsoft.com/azurecognitive-services/textanalytics/summarization

To use the latest version of the container, you can use the latest tag. You can also find a full
list of tags on the MCR
Use the docker pull

.

command to download a container image from the Microsoft Container

Registry.

docker pull mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu

for CPU containers,

docker pull mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:gpu

for GPU containers.
îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded

container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Download the summarization container models
A pre-requisite for running the summarization container is to download the models first. This
can be done by running one of the following commands using a CPU container image as an
example:
Bash
docker run -v {HOST_MODELS_PATH}:/models mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu downloadModels=ExtractiveSummarization
billing={ENDPOINT_URI} apikey={API_KEY}
docker run -v {HOST_MODELS_PATH}:/models mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu downloadModels=AbstractiveSummarization
billing={ENDPOINT_URI} apikey={API_KEY}
docker run -v {HOST_MODELS_PATH}:/models mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu downloadModels=ConversationSummarization
billing={ENDPOINT_URI} apikey={API_KEY}

It's not recommended to download models for all skills inside the same HOST_MODELS_PATH , as
the container loads all models inside the HOST_MODELS_PATH . Doing so would use a large
amount of memory. We recommend that you only download the model for the skill you need
in a particular HOST_MODELS_PATH .
In order to ensure compatibility between models and the container, re-download the utilized
models whenever you create a container using a new image version. When using a
disconnected container, the license should be downloaded again after downloading the
models.

Run the container with docker run
Once the Summarization container is on the host computer, use the following docker run
command to run the containers. The container will continue to run until you stop it. Replace
the placeholders with your own values:

ï¾‰

Expand table

Placeholder

Value

Format or example

{HOST_MODELS_PATH}

The host computer volume
mount , which Docker uses
to persist the model.

An example is c:\SummarizationModel where
the c:\ drive is located on the host machine.

{ENDPOINT_URI}

The endpoint for accessing

https://<your-custom-

the summarization API. You
can find it on your resource's
Key and endpoint page, on

subdomain>.cognitiveservices.azure.com

the Azure portal.
{API_KEY}

The key for your Language
resource. You can find it on

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

your resource's Key and
endpoint page, on the Azure
portal.

Bash
docker run -p 5000:5000 -v {HOST_MODELS_PATH}:/models mcr.microsoft.com/azurecognitive-services/textanalytics/summarization:cpu eula=accept rai_terms=accept
billing={ENDPOINT_URI} apikey={API_KEY}

Or if you're running a GPU container, use this command instead.
Bash
docker run -p 5000:5000 --gpus all -v {HOST_MODELS_PATH}:/models
mcr.microsoft.com/azure-cognitive-services/textanalytics/summarization:gpu
eula=accept rai_terms=accept billing={ENDPOINT_URI} apikey={API_KEY}

If there is more than one GPU on the machine, replace --gpus all with --gpus device=
{DEVICE_ID} .

ï¼‰ Important
The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove this based on your host operating
system's requirements.
The Eula , Billing , rai_terms and ApiKey options must be specified to run the
container; otherwise, the container won't start. For more information, see Billing.

This command:
Runs a Summarization container from the container image
Allocates one CPU core and 4 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Automatically removes the container after it exits. The container image is still available on
the host computer.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Expand table

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes .

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request

Request URL

Purpose
can be used for Kubernetes liveness and readiness probes

http://localhost:5000/swagger

.

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

Run the container disconnected from the internet
To use this container disconnected from the internet, you must first request access by filling
out an application, and purchasing a commitment plan. See Use Docker containers in
disconnected environments for more information.
If you have been approved to run the container disconnected from the internet, use the
following example shows the formatting of the docker run command you use, with
placeholder values. Replace these placeholder values with your own values.
The DownloadLicense=True parameter in your docker run command will download a license file
that will enable your Docker container to run when it isn't connected to the internet. It also
contains an expiration date, after which the license file will be invalid to run the container. You
can only use a license file with the appropriate container that you've been approved for. For
example, you can't use a license file for a speech to text container with a Languages container.

Download the summarization disconnected
container models
A pre-requisite for running the summarization container is to download the models first. This
can be done by running one of the following commands using a CPU container image as an
example:
Bash
docker run -v {HOST_MODELS_PATH}:/models mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu downloadModels=ExtractiveSummarization
billing={ENDPOINT_URI} apikey={API_KEY}
docker run -v {HOST_MODELS_PATH}:/models mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu downloadModels=AbstractiveSummarization
billing={ENDPOINT_URI} apikey={API_KEY}
docker run -v {HOST_MODELS_PATH}:/models mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu downloadModels=ConversationSummarization
billing={ENDPOINT_URI} apikey={API_KEY}

It's not recommended to download models for all skills inside the same HOST_MODELS_PATH , as
the container loads all models inside the HOST_MODELS_PATH . Doing so would use a large
amount of memory. We recommend that you only download the model for the skill you need
in a particular HOST_MODELS_PATH .
In order to ensure compatibility between models and the container, re-download the utilized
models whenever you create a container using a new image version. When using a
disconnected container, the license should be downloaded again after downloading the
models.

Run the disconnected container with docker run
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitiveservices/textanalytics/summarization:cpu

{LICENSE_MOUNT}

The path where the
license will be
downloaded, and
mounted.

/host/license:/path/to/license/directory

{HOST_MODELS_PATH}

The path where the
models were

/host/models:/models

Placeholder

Value

Format or example

downloaded, and
mounted.
{ENDPOINT_URI}

The endpoint for
authenticating your

https://<your-customsubdomain>.cognitiveservices.azure.com

service request. You
can find it on your
resource's Key and
endpoint page, on the
Azure portal.
{API_KEY}

The key for your Text

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Analytics resource. You
can find it on your
resource's Key and
endpoint page, on the
Azure portal.
{CONTAINER_LICENSE_DIRECTORY}

Location of the license

/path/to/license/directory

folder on the
container's local
filesystem.

Bash
docker run --rm -it -p 5000:5000 \
-v {LICENSE_MOUNT} \
-v {HOST_MODELS_PATH} \
{IMAGE} \
eula=accept \
rai_terms=accept \
billing={ENDPOINT_URI} \
apikey={API_KEY} \
DownloadLicense=True \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}

Once the license file has been downloaded, you can run the container in a disconnected
environment. The following example shows the formatting of the docker run command you
use, with placeholder values. Replace these placeholder values with your own values.
Wherever the container is run, the license file must be mounted to the container and the
location of the license folder on the container's local filesystem must be specified with
Mounts:License= . An output mount must also be specified so that billing usage records can be

written.
ï¾‰

Expand table

Placeholder

Value

Format or example

{IMAGE}

The container image
you want to use.

mcr.microsoft.com/azure-cognitive-

{MEMORY_SIZE}

The appropriate size
of memory to
allocate for your

services/textanalytics/summarization:cpu
4g

container.
{NUMBER_CPUS}

The appropriate
number of CPUs to
allocate for your
container.

4

{LICENSE_MOUNT}

The path where the
license will be

/host/license:/path/to/license/directory

located and
mounted.
{HOST_MODELS_PATH}

The path where the
models were
downloaded, and
mounted.

/host/models:/models

{OUTPUT_PATH}

The output path for

/host/output:/path/to/output/directory

logging usage
records.
{CONTAINER_LICENSE_DIRECTORY}

Location of the
license folder on the
container's local
filesystem.

/path/to/license/directory

{CONTAINER_OUTPUT_DIRECTORY}

Location of the
output folder on the
container's local
filesystem.

/path/to/output/directory

Bash
docker run --rm -it -p 5000:5000 --memory {MEMORY_SIZE} --cpus {NUMBER_CPUS} \
-v {LICENSE_MOUNT} \
-v {HOST_MODELS_PATH} \
-v {OUTPUT_PATH} \
{IMAGE} \
eula=accept \
rai_terms=accept \
Mounts:License={CONTAINER_LICENSE_DIRECTORY}
Mounts:Output={CONTAINER_OUTPUT_DIRECTORY}

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
The summarization containers send billing information to Azure, using a Language resource on
your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure
The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If
the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.

Expand table

The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .
Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

For more information about these options, see Configure containers.

Summary
In this article, you learned concepts and workflow for downloading, installing, and running
summarization containers. In summary:
Summarization provides Linux containers for Docker
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You must specify billing information when instantiating a container.
ï¼‰ Important
This container is not licensed to run without being connected to Azure for metering.
Customers need to enable the containers to communicate billing information with the
metering service at all times. Azure AI containers do not send customer data (e.g. text that
is being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Language docker containers
Language provides each container with a common configuration framework, so that you can
easily configure and manage storage, logging, and security settings for your containers. This
article applies to the following containers:
Sentiment Analysis
Language Detection
Key Phrase Extraction
Text Analytics for Health
Summarization
Named Entity Recognition (NER)
Personally Identifiable (PII) detection
Conversational Language Understanding (CLU)

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important

The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container doesn't start.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the key and it must be a valid key for the Language
resource specified for the Billing configuration setting.

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Expand table

Required

Name

Data
type

Description

No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
ï¾‰

Expand table

Required

Name

Data type

Description

Yes

Billing

String

Billing endpoint URI.

End-user license agreement (EULA) setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

Name

Data
type

Description

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \

Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \

<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

The Language containers don't use input or output mounts to store training or service data.
The exact syntax of the host mount location varies depending on the host operating system.
The host computer's mount location may not be accessible due to a conflict between the
docker service account permissions and the host mount location permissions.
ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Language containers don't use this data type.

Optional

Output

String

The target of the output mount. The default value is /output . It's the
location of the logs. This output includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Use more Azure AI containers

Last updated on 12/05/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

Use Docker containers in disconnected
environments
Containers enable you to run Foundry Tools APIs in your own environment, and are great for
your specific security and data governance requirements. Disconnected containers enable you
to use several of these APIs disconnected from the internet. Currently, the following containers
can be run in this manner:
Speech to text
Custom Speech to text
Neural Text to speech
Text Translation (Standard)
Azure Language in Foundry Tools
Sentiment Analysis
Key Phrase Extraction
Language Detection
Summarization
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)
Azure Vision in Foundry Tools - Read
Document Intelligence
Before attempting to run a Docker container in an offline environment, make sure you know
the steps to successfully download and use the container. For example:
Host computer requirements and recommendations.
The Docker pull command you use to download the container.
How to validate that a container is running.
How to send queries to the container's endpoint, once it's running.

Request access to use containers in disconnected
environments
Fill out and submit the request form

to request access to the containers disconnected from

the internet.
The form requests information about you, your company, and the user scenario for which you'll
use the container. After you submit the form, the Foundry Tools team reviews it and emails you
with a decision within 10 business days.

ï¼‰ Important
On the form, you must use an email address associated with an Azure subscription
ID.
The Azure resource you use to run the container must have been created with the
approved Azure subscription ID.
Check your email (both inbox and junk folders) for updates on the status of your
application from Microsoft.
After you're approved, you'll be able to run the container after you download it from the
Microsoft Container Registry (MCR), described later in the article.
You won't be able to run the container if your Azure subscription hasn't been approved.
Access is limited to customers that meet the following requirements:
Your organization should be identified as strategic customer or partner with Microsoft.
Disconnected containers are expected to run fully offline, hence your use cases must
meet one of these or similar requirements:
Environments or devices with zero connectivity to internet.
Remote location that occasionally has internet access.
Organization under strict regulation of not sending any kind of data back to cloud.
Application completed as instructed - Pay close attention to guidance provided
throughout the application to ensure you provide all the necessary information required
for approval.

Purchase a commitment tier pricing plan for
disconnected containers
Create a new resource
1. Sign in to the Azure portal

and select Create a new resource for one of the applicable

Foundry Tools listed.
2. Enter the applicable information to create your resource. Be sure to select Commitment
tier disconnected containers as your pricing tier.
ï¼— Note

You only see the option to purchase a commitment tier if you have been
approved by Microsoft.
Pricing details are only examples.
3. Select Review + Create at the bottom of the page. Review the information, and select
Create.

Configure container for disconnected usage
See the following documentation for steps on downloading and configuring the container for
disconnected usage:
Vision - Read
Language Understanding (LUIS)
Text Translation (Standard)
Document Intelligence
Speech service
Speech to text
Custom Speech to text
Neural Text to speech
Language service
Sentiment Analysis
Key Phrase Extraction
Language Detection
Named Entity Recognition
Personally Identifiable Information (PII) detection
Conversational Language Understanding (CLU)

Environment variable names in Kubernetes
deployments
Some Azure AI Containers, for example Translator, require users to pass environmental variable
names that include colons ( : ) when running the container. This works fine when using Docker,
but Kubernetes doesn't accept colons in environmental variable names. To resolve this, you can
replace colons with double underscore characters ( __ ) when deploying to Kubernetes. See the
following example of an acceptable format for environment variable names:

Kubernetes
env:
- name: Mounts__License
value: "/license"
- name: Mounts__Output
value: "/output"

This example replaces the default format for the Mounts:License and Mounts:Output
environment variable names in the docker run command.

Container image and license updates
Container license files are used as keys to decrypt certain files within each container image. If
these encrypted files happen to be updated within a new container image, the license file you
have may fail to start the container even if it worked with the previous version of the container
image. To avoid this issue, we recommend that you download a new license file from the
resource endpoint for your container provided in Azure portal after you pull new image
versions from mcr.microsoft.com.
To download a new license file, you can add DownloadLicense=True to your docker run
command along with a license mount, your API Key, and your billing endpoint. Refer to your
container's documentation for detailed instructions.

Usage records
When operating Docker containers in a disconnected environment, the container writes usage
records to a volume where they're collected over time. You can also call a REST endpoint to
generate a report about service usage.

Arguments for storing logs
When run in a disconnected environment, an output mount must be available to the container
to store usage logs. For example, you would include -v /host/output:{OUTPUT_PATH} and
Mounts:Output={OUTPUT_PATH} in the example below, replacing {OUTPUT_PATH} with the path

where the logs are stored:
Docker
docker run -v /host/output:{OUTPUT_PATH} ... <image> ... Mounts:Output={OUTPUT_PATH}

Get records using the container endpoints

The container provides two endpoints for returning records about its usage.

Get all records
The following endpoint provides a report summarizing all of the usage collected in the
mounted billing record directory.
HTTP
https://<service>/records/usage-logs/

It returns JSON similar to the example below.
JSON
{
"apiType": "noop",
"serviceName": "noop",
"meters": [
{
"name": "Sample.Meter",
"quantity": 253
}
]
}

Get records for a specific month
The following endpoint provides a report summarizing usage over a specific month and year.
HTTP
https://<service>/records/usage-logs/{MONTH}/{YEAR}

It returns a JSON response similar to the example below:
JSON
{
"apiType": "string",
"serviceName": "string",
"meters": [
{
"name": "string",
"quantity": 253
}

]
}

Purchase a commitment plan to use containers in
disconnected environments
Commitment plans for disconnected containers have a calendar year commitment period.
When you purchase a plan, you are charged the full price immediately. During the commitment
period, you can't change your commitment plan, however you can purchase more units at a
pro-rated price for the remaining days in the year. You have until midnight (UTC) on the last
day of your commitment, to end a commitment plan.
You can choose a different commitment plan in the Commitment Tier pricing settings of your
resource.

End a commitment plan
If you decide that you don't want to continue purchasing a commitment plan, you can set your
resource's auto-renewal to Do not auto-renew. Your commitment plan expires on the
displayed commitment end date. After this date, you won't be charged for the commitment
plan. You are able to continue using the Azure resource to make API calls, charged at Standard
pricing. You have until midnight (UTC) on the last day of the year to end a commitment plan for
disconnected containers, and not be charged for the following year.

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files that are helpful to troubleshoot issues that happen while starting or running the
container.
îª€ Tip
For more troubleshooting information and guidance, see Disconnected containers
Frequently asked questions (FAQ).

Next steps
Azure AI containers overview

Last updated on 10/02/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

Transparency note for summarization
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance.
Microsoft transparency notes are intended to help you understand how our AI technology
works, and the choices that you as a system owner can make that influence system
performance and behavior. It's important to think about the whole system, including the
technology, the people, and the environment. You can use transparency notes when you
develop or deploy your own system, or share them with the people who will use or be affected
by your system.
Transparency notes are part of a broader effort at Microsoft to put our AI principles into
practice. To find out more, see Microsoft AI principles

.

The basics of Summarization
Introduction
Summarization uses natural language processing techniques to condense articles, papers, or
documents into key sentences. This feature is provided as an API for developers to build
intelligent solutions based on the relevant information extracted and can support various use
cases.

Capabilities
Document summarization

Document summarization uses natural language processing techniques to generate a
summary for documents. There are two general approaches to auto-summarization:

extractive and abstractive.

The basics of document extractive summarization
This feature extracts sentences that collectively represent the most important or relevant
information within the original content. It locates key sentences in an unstructured text
document. These sentences collectively convey the main idea of the document.

The basics of document abstractive summarization
Different from extractive summarization, document abstractive summarization generates a
summary with concise, coherent sentences or words which are not simply extracted from
the original document.

Example use cases
You can use document summarization in multiple scenarios, across a variety of industries.
For example, you can use extractive summarization to:
Assist the processing of documents to improve efficiency. Distill critical information
from lengthy documents, reports, and other text forms,highlight key sentences in
documents, andquickly skim documents in a library.
Extract key information from public news articles to produce insights such as trends
and news spotlights, and generate news feed content.
Classify or cluster documents by their contents. Use Summarization to surface key
concepts from documents and use those key concepts to group documents that are
similar.
Distill important information from long documents to empower solutions such as
search, question and answering, and decision support.

Considerations when you choose a use case
We encourage you to come up with use cases that most closely match your own particular
context and requirements. Draw on actionable information that enables responsible integration
in your use cases, and conduct your own testing specific to your scenarios.
The summarization models reflect certain societal views that are over-represented in the
training data, relative to other, marginalized perspectives. The models reflect societal biases
and other undesirable content present in the training data. As a result, we caution against

using the models in high-stakes scenarios, where unfair, unreliable, or offensive behavior might
be extremely costly or lead to harm.
Avoid real-time, critical safety alerting. Don't rely on this feature for scenarios that
require real-time alerts to trigger intervention to prevent injury. For example, don't rely
on summarization for turning off a piece of heavy machinery when a harmful action is
present.
The feature isn't suitable for scenarios where up-to-date, factually accurate information
is crucial, unless you have human reviewers. The service doesn't have information about
current events after its training date, probably has missing knowledge about some topics,
and might not always produce factually accurate information.
Avoid scenarios in which the use or misuse of the system could have a consequential
impact on life opportunities or legal status. For example, avoid scenarios in which the AI
system could affect an individual's legal status or legal rights. Additionally, avoid
scenarios in which the AI system could affect an individual's access to credit, education,
employment, healthcare, housing, insurance, social welfare benefits, services,
opportunities, or the terms on which they are provided.
Legal and regulatory considerations: Organizations need to evaluate potential specific
legal and regulatory obligations when using any AI services and solutions, which may not
be appropriate for use in every industry or scenario. Additionally, AI services or solutions
are not designed for and may not be used in ways prohibited in applicable terms of
service and relevant codes of conduct.

Next steps
Transparency note for Azure Language in Foundry Tools
Transparency note for named entity recognition
Transparency note for health
Transparency note for key phrase extraction
Transparency note for sentiment analysis
Guidance for integration and responsible use with language
Data privacy for language

Last updated on 08/17/2025

Guidance for integration and responsible
use with summarization
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you develop and deploy solutions that use the summarization feature
in a responsible manner. Microsoft takes a principled approach to uphold personal agency and
dignity, by considering the following aspects of an AI system: fairness, reliability and safety,
privacy and security, inclusiveness, transparency, and human accountability. These
considerations reflect our commitment to developing Responsible AI

.

General guidelines
When you're getting ready to integrate and responsibly use AI-powered products or features,
the following activities help to set you up for success. Although each guideline is unlikely to
apply to all scenarios, consider them as a starting point for mitigating possible risks:
Understand what it can do, and how it might be misused. Fully assess the capabilities of
any AI system you're using to understand its capabilities and limitations. The particular
testing that Microsoft conducts might not reflect your scenario. Understand how it will
perform in your particular scenario, by thoroughly testing it with real-life conditions and
diverse data that reflect your context. Include fairness considerations.
Test with real, diverse data. Understand how your system will perform in your scenario.
Test it thoroughly with real-life conditions, and data that reflects the diversity in your
users, geography, and deployment contexts. Small datasets, synthetic data, and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Evaluate the system. Consider using adversarial testing, where trusted testers attempt to
find system failures, poor performance, or undesirable behaviors. This information helps
you to understand risks and how to mitigate them. Communicate the capabilities and
limitations to stakeholders. To help you evaluate your system, you might find some of
these resources useful: Checklist on GitHub

, Stereotyping Norwegian Salmon: An

Inventory of Pitfalls in Fairness Benchmark Datasets

(Blodgett et al., 2021), and On the

Dangers of Stochastic Parrots: Ca Language Models Be Too Big?

(Bender et al., 2021).

Learn about fairness. AI systems can behave unfairly for a variety of reasons. Some are
social, some are technical, and some are a combination of the two. There are seldom
clear-cut solutions. Mitigation methods are usually context dependent. Learning about
fairness can help you learn what to expect and how you might mitigate potential harms.
To learn more about the approach that Microsoft uses, see Responsible AI resources
the AI fairness checklist , and resources from Microsoft Research

,

.

Respect an individual's right to privacy. Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Conduct user testing during development, and solicit feedback after deployment.
Consider using value-sensitive design to identify stakeholders. Work with them to identify
their values to design systems that support those values. Seek feedback from a diverse
group of people during the development and evaluation process. Use strategies like
Community Jury.
Undertake user testing with diverse stakeholders. Then analyze the results broken down
by stakeholder groups. Include stakeholders from different demographic groups.
Consider conducting online experiments, ring testing, dogfooding, field trials, or pilots in
deployment contexts.
Limit the length, structure, rate, and source of inputs and outputs. Restricting input and
output length can reduce the likelihood of risks. Such risks include producing undesirable
content, misuse for an overly general-purpose beyond the intended application use cases,
or other harmful, disallowed, or unintended scenarios.
Consider requiring prompts to be structured a certain way. They can be confined to a
particular topic, or drawn from validated sources, like a dropdown field. Consider
structuring the output so it isn't overly open-ended. Consider returning outputs from
validated, reliable source materials (such as existing support articles), rather than
connecting to the internet. This restriction can help your application stay on task and
mitigate unfair, unreliable, or offensive behavior. Putting rate limits in place can further
reduce misuse.
Implement blocklists and content moderation. Keep your application on topic. Consider
blocklists and content moderation strategies to check inputs and outputs for undesirable
content. The definition of undesired content depends on your scenario, and can change
over time. It might include hate speech, text that contains profane words or phrases,
misinformation, and text that relates to sensitive or emotionally charged topics. Checking
inputs can help keep your application on topic, even if a malicious user tries to produce
undesired content. Checking API outputs allows you to detect undesired content

produced by the system. You can then replace it, report it, ask the user to enter different
input, or provide input examples.
Authenticate users. To make misuse more difficult, consider requiring that customers sign
in and, if appropriate, link a valid payment method. Consider working only with known,
trusted customers in the early stages of development.
Ensure human oversight. Especially in higher-stakes scenarios, maintain the role of
humans in decision making. Disclose what the AI has done versus what a human has
done.
Based on your scenario, there are various stages in the lifecycle in which you can add
human oversight. Ensure you can have real-time human intervention in the solution to
prevent harm. For example, when generating summaries, editors should review the
summaries before publication. Ideally, assess the effectiveness of human oversight prior
to deployment, through user testing, and after deployment.
Have a customer feedback loop. Provide a feedback channel that allows users and
individuals to report issues with the service after deployment. Issues might include unfair
or undesirable behaviors. After you've deployed an AI-powered product or feature, it
requires ongoing monitoring and improvement. Establish channels to collect questions
and concerns from stakeholders who might be directly or indirectly affected by the
system, such as employees, visitors, and the general public. Examples include:
Feedback features built into app experiences.
An easy-to-remember email address for feedback.
Conduct a legal review. Obtain appropriate legal advice to review your solution,
particularly if you'll use it in sensitive or high-risk applications. Know what restrictions you
might need to work within. Understand your responsibility to resolve any issues that
might come up in the future. Ensure the appropriate use of datasets.
Conduct a system review. You might plan to integrate and responsibly use an AIpowered product or feature into an existing system of software, customers, and
organizational processes. If so, take the time to understand how each part of your system
will be affected. Consider how your AI solution aligns with the principles of responsible AI
that Microsoft uses.
Security. Ensure that your solution is secure, and has adequate controls to preserve the
integrity of your content and prevent any unauthorized access.

Recommended content
Assess your application for alignment with Responsible AI principles

.

Use the Microsoft HAX Toolkit

. The toolkit recommends best practices for how AI

systems should behave on initial interaction, during regular interaction, when they're
inevitably wrong, and over time.
Follow the Microsoft guidelines for responsible development of conversational AI
systems

. Use the guidelines when you develop and deploy language models that

power chat bots, or other conversational AI systems.
Use Microsoft Inclusive Design Guidelines

Last updated on 08/17/2025

to build inclusive solutions.

Characteristics and limitations for
Summarization
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Large-scale, natural language models are trained with publicly available text data which
typically contain societal biases. Such data can potentially behave in ways that are unfair,
unreliable, or offensive. This behavior, in turn, may cause harms of varying severities. These
types of harms aren't mutually exclusive. A single model can exhibit more than one type of
harm, potentially relating to multiple groups of people. For example:
Allocation: It's possible to use language models in ways that lead to unfair allocation of
resources or opportunities. For example, automated systems that screen resumes can
withhold employment opportunities from women, if these systems are trained on resume
data that reflects the existing gender imbalance in the technology industries.
Quality of service: Language models can fail to provide the same quality of service to
some people as they do to others. For example, summary generation can work less well
for some dialects or language varieties, because of their lack of representation in the
training data. The models are trained primarily on English text. English language varieties
less well represented in the training data might experience worse performance.
Stereotyping: Language models can reinforce stereotypes. For example, when translating
He is a nurse and She is a doctor into a genderless language, such as Turkish, and then
back into English, you can get an error. Many machine translation systems yield the
stereotypical (and incorrect) results of She is a nurse and He is a doctor.
Demeaning: Language models can demean people. For example, an open-ended content
generation system with inappropriate mitigation might produce offensive text, targeted
at a particular group of people.
Over- and underrepresentation: Language models can over- or under-represent groups
of people, or even erase them entirely. For example, toxicity detection systems that rate
text containing the word gay as toxic might lead to the under-representation, or even
erasure, of legitimate text written by or about the LGBTQ community.
Inappropriate or offensive content: Language models can produce other types of
inappropriate or offensive content. Examples include:

Hate speech.
Text that contains profane words or phrases.
Text that relates to illicit activities.
Text that relates to contested, controversial, or ideologically polarizing topics.
Misinformation.
Text that's manipulative.
Text that relates to sensitive or emotionally charged topics.
For example, suggested-reply systems that are restricted to positive replies

can suggest

inappropriate or insensitive replies for messages about negative events.
False information: The service doesn't check facts or verify content provided by
customers or users. Depending on how you've developed your application, it might
promote false information unless you've built in an effective mitigation for this possibility.
Inaccurate summary: The feature uses an abstractive summarization method, in which the
model doesn't simply extract contexts from the input text. Instead, the model tries to
understand the input and paraphrase the key information in succinct natural sentences.
However, there can be information or accuracy loss.
Genre consideration: The training data used to train the summarization feature in
Foundry Tools for language is mainly texts and transcripts between two participants. The
model might perform with lower accuracy for the input text in other types of genres, such
as documents or reports, which are less represented in the training data.
Language support: Most of the training data is in English, and in other commonly used
languages like German and Spanish. The trained models might not perform as well on
input in other languages, because these languages are less represented in the training
data. Microsoft is invested in expanding the language support of this feature.

Best practices for improving system performance
The performance of the models varies based on the scenario and input data. The following
sections are designed to help you understand key concepts about performance.
Document summarization

You can use document summarization in a wide range of applications, each with different
focuses and performance metrics. Here, we broadly consider performance to mean the
application performs as you expect, including the absence of harmful outputs. There are
several steps you can take to mitigate some of the concerns mentioned earlier in this
article, and to improve performance:

*Because the document summarization feature is trained on document-based texts, such
as news articles, scientific reports, and legal documents, when used with texts in different
genres that are less represented in the training data, such as conversations and
transcriptions, the system might product output with lower accuracy.
When used with texts that may contain errors or are less similar to well-formed
sentences, such as texts extracted from lists, tables, charts, or scanned in via OCR
(Optical Character Recognition), the document summarization feature may produce
output with lower accuracy.
Most of the training data is in commonly used languages such as English, German,
French, Chinese, Japanese, and Korean. The trained models may not perform as well
on input in other languages.
Documents must be "cracked," or converted, from their original format into plain and
unstructured text.
Although the service can handle a maximum of 25 documents per request, the
latency performance of the API increases with larger documents (it becomes slower).
This is especially true if the documents contain close to the maximum 125,000
characters. Learn more about system limits
The extractive summarization gives a score between 0 and 1 to each sentence and
returns the highest scored sentences per request. If you request a three-sentence
summary, the service returns the three highest scored sentences. If you request a
five-sentence summary from the same document, the service returns the next two
highest scored sentences in addition to the first three sentences.
The extractive summarization returns extracted sentences in their chronological order
by default. To change the order, specify sortBy. The accepted values for sortBy are
Offset (default). The value of Offset is the character positions of the extracted
sentences and the value of Rank is the rank scores of the extracted sentences.

Next steps
Transparency note for Azure Language in Foundry Tools
Transparency note for named entity recognition
Transparency note for health
Transparency note for key phrase extraction
Transparency note for sentiment analysis
Guidance for integration and responsible use with language
Data privacy for language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

What is Text Analytics for health?
ï¼‰ Important
Text Analytics for health is a capability provided â€œAS ISâ€ and â€œWITH ALL FAULTS.â€ Text
Analytics for health is not intended or made available for use as a medical device, clinical
support, diagnostic tool, or other technology intended to be used in the diagnosis, cure,
mitigation, treatment, or prevention of disease or other conditions, and no license or right
is granted by Microsoft to use this capability for such purposes. This capability is not
designed or intended to be implemented or deployed as a substitute for professional
medical advice or healthcare opinion, diagnosis, treatment, or the clinical judgment of a
healthcare professional, and should not be used as such. The customer is solely
responsible for any use of Text Analytics for health. The customer must separately license
any and all source vocabularies it intends to use under the terms set for that UMLS
Metathesaurus License Agreement Appendix

or any future equivalent link. The

customer is responsible for ensuring compliance with those license terms, including any
geographic or other applicable restrictions.
Text Analytics for health now allows extraction of Social Determinants of Health (SDOH)
and ethnicity mentions in text. This capability may not cover all potential SDOH and does
not derive inferences based on SDOH or ethnicity (for example, substance use information
is surfaced, but substance abuse is not inferred). All decisions leveraging outputs of the
Text Analytics for health that impact individuals or resource allocation (including, but not
limited to, those related to billing, human resources, or treatment managing care) should
be made with human oversight and not be based solely on the findings of the model. The
purpose of the SDOH and ethnicity extraction capability is to help providers improve
health outcomes and it should not be used to stigmatize or draw negative inferences
about the users or consumers of SDOH data, or patient populations beyond the stated
purpose of helping providers improving health outcomes.
Text Analytics for health is one of the prebuilt features offered by Azure Language in Foundry
Tools. Text Analytics for health uses machine learning to identify and label medical information
in unstructured text such as doctor's notes, clinical documents, and electronic health records. It
extracts key data from sources like discharge summaries to support healthcare analysis.
îª€ Tip
Try out Text Analytics for health in Microsoft Foundry portal . There you can utilize a
currently existing Language Studio resource or create a new Foundry resource in order
to use this service.

This documentation contains the following types of articles:
The quickstart article provides a short tutorial that guides you with making your first
request to the service.
The how-to guides contain detailed instructions on how to make calls to the service
using the hosted API or using the on-premises Docker container.
The conceptual articles provide in-depth information on each of the service's features,
named entity recognition, relation extraction, entity linking, and assertion detection.

Text Analytics for health features
Text Analytics for health performs four key functions, all with a single API call:
Named entity recognition
Relation extraction
Entity linking
Assertion detection
Named Entity Recognition

Named entity recognition is used to perform a semantic extraction of words and phrases
mentioned from unstructured text that are associated with any of the supported entity
types, such as diagnosis, medication name, symptom/sign, or age.

Text Analytics for health can receive unstructured text in English, German, French, Italian,
Spanish, Portuguese, and Hebrew.
Additionally, Text Analytics for health can return the processed output using the Fast
Healthcare Interoperability Resources (FHIR) structure that enables the service's integration
with other electronic health systems.

https://learn.microsoft.com/Shows/AI-Show/Introducing-Text-Analytics-for-Health/player

Usage scenarios
Text Analytics for health can be used in multiple scenarios across various industries. Some
common customer motivations for using Text Analytics for health include:
Assisting and automating the processing of medical documents by proper medical coding
to ensure accurate care and billing.
Increasing the efficiency of analyzing healthcare data to help drive the success of valuebased care models similar to Medicare.
Minimizing healthcare provider effort by automating the aggregation of key patient data
for trend and pattern monitoring.
Facilitating and supporting the adoption of HL7 standards across the healthcare industry.
By doing so, we help improve the exchange, integration, sharing, retrieval, and delivery of
electronic health information in all areas of healthcare services.

Example use cases:
ï¾‰

Expand table

Use case

Description

Extract insights and statistics

Identify medical entities such as symptoms, medications, diagnosis from
clinical and research documents in order to extract insights and statistics
for different patient cohorts.

Develop predictive
models using historic data

Power solutions for planning, decision support, risk analysis and more,
based on prediction models created from historic data.

Annotate and curate medical
information

Support solutions for clinical data annotation and curation such as
automating clinical coding and digitizing manually created data.

Review and report medical
information

Potential medical information errors found during quality assurance
reviews.

Assist with decision support

Enable solutions that provide humans with assistive information relating
to patients' medical information for faster and more reliable decisions.

Get started with Text Analytics for health
To use Text Analytics for health, you submit raw unstructured text for analysis and handle the
API output in your application. Analysis is performed as-is, with no additional customization to

the model used on your data. There are two ways to use Text Analytics for health:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use entity linking with text
examples with your own data when you sign up. For more information, see the
Foundry website or Foundry documentation.

REST API or Client

Integrate Text Analytics for health into your applications using the REST API, or

library (Azure SDK)

the client library available in a variety of languages. For more information, see
the Text Analytics for health quickstart.

Docker container

Use the available Docker container to deploy this feature on-premises. These
docker containers enable you to bring the service closer to your data for
compliance, security, or other operational reasons.

Input requirements and service limits
Text Analytics for health is designed to receive unstructured text for analysis. For more
information, see data and service limits.
Text Analytics for health works with various input languages. For more information, see
language support.

Reference documentation and code samples
As you use this feature in your applications, see the following reference documentation and
samples for Azure Language in Foundry Tools:
ï¾‰

Expand table

Development option / language

Reference documentation

Samples

REST API

REST API documentation

C#

C# documentation

C# samples

Java

Java documentation

Java Samples

JavaScript

JavaScript documentation

JavaScript samples

Python

Python documentation

Python samples

Responsible use of AI
An AI system includes the technology, the individuals who operate the system, the people who
experience its effects, and the broader environment where the system functions all play a role.
Read the transparency note for Text Analytics for health to learn about responsible AI use and
deployment in your systems.
Transparency note for Azure Language in Foundry Tools
Integration and responsible use
Data, privacy, and security

Last updated on 12/15/2025

Quickstart: Using Text Analytics for health
client library and REST API
This article contains Text Analytics for health quickstarts that help with using the supported
client libraries, C#, Java, NodeJS, and Python and the REST API.
îª€ Tip
You can use Microsoft Foundry

to try Azure Language features without needing to

write code.

Prerequisites
Create a Project in Foundry in the Microsoft Foundry portal

Navigate to the Foundry Playground
Using the left side pane, select Playgrounds. Then select the Try Azure Language Playground
button.

ï Š

Use Text Analytics for Health in the Foundry
Playground
The Language Playground consists of four sections:
Top banner: You can select any of the currently available Languages here.
Right pane: This pane is where you can find the Configuration options for the service,
such as the API and model version, along with features specific to the service.
Center pane: This pane is where you enter your text for processing. After the operation is
run, some results are shown here.
Right pane: This pane is where Details of the run operation are shown.
Here you can select the Text Analytics for Health capability by choosing the top banner tile,
Extract health information.

Use Extract health information
Extract health information is designed to identify and extract health information in text.
In Configuration there are the following options:
ï¾‰

Expand table

Option

Description

Select API version

Select which version of the API to use.

Select model version

Select which version of the model to use.

Select text language

Select which language the language is input in.

Return output in FHIR

Returns the output in the Fast Healthcare Interoperability Resources (FHIR)

structure

structure.

After your operation is completed, the type of entity is displayed beneath each entity in the
center pane and the Details section contains the following fields for each entity:
ï¾‰

Expand table

Field

Description

Entity

The detected entity.

Category

The type of entity that was detected.

Confidence

How confident the model is in the correctness of identification of entity's type.

ï Š

Clean up resources
To clean up and remove an Azure AI resource, you can delete either the individual resource or
the entire resource group. If you delete the resource group, all resources contained within are
also deleted.
Azure portal
Azure CLI

Next steps
How to call the hosted API
How to use the service with Docker containers

Last updated on 12/15/2025

Language support for Text Analytics for
health
Use this article to learn which natural languages are supported by Text Analytics for health and
its Docker container.

Hosted API Service
The hosted API service supports the English, Spanish, French, German, Italian, and Portuguese
languages.
When structuring the API request, the relevant language tags must be added for these
languages:

English â€“ â€œenâ€
Spanish â€“ â€œesâ€
French - â€œfrâ€
German â€“ â€œdeâ€
Italian â€“ â€œitâ€
Portuguese â€“ â€œptâ€

JSON
json
{
"analysisInput": {
"documents": [
{
"text": "El mÃ©dico prescriÃ³ 200 mg de ibuprofeno.",
"language": "es",
"id": "1"
}
]
},
"tasks": [
{
"taskName": "analyze 1",
"kind": "Healthcare",
"parameters":
{
"modelVersion": "2022-08-15-preview"
}
}

]
}

Docker container
The docker container supports the English, Spanish, French, German, Italian, Portuguese and
Hebrew languages. Full details for deploying the service in a container can be found here.
In order to download the new container images from the Microsoft public container registry,
use the following docker pull

command.

For English, Spanish, Italian, French, German and Portuguese:

docker pull mcr.microsoft.com/azure-cognitiveservices/textanalytics/healthcare:latin

For Hebrew:

docker pull mcr.microsoft.com/azure-cognitiveservices/textanalytics/healthcare:semitic

When structuring the API request, the relevant language tags must be added for these
languages:

English â€“ â€œenâ€
Spanish â€“ â€œesâ€
French - â€œfrâ€
German â€“ â€œdeâ€
Italian â€“ â€œitâ€
Portuguese â€“ â€œptâ€
Hebrew â€“ â€œheâ€

The following json is an example of a JSON file attached to Azure Language request's POST
body, for a Spanish document:
JSON
json
{
"analysisInput": {

"documents": [
{
"text": "El mÃ©dico prescriÃ³ 200 mg de ibuprofeno.",
"language": "es",
"id": "1"
}
]
},
"tasks": [
{
"taskName": "analyze 1",
"kind": "Healthcare",
}
]
}

See also
Text Analytics for health overview

Last updated on 11/18/2025

Transparency note for Text Analytics for
health
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a transparency note?
ï¼‰ Important
Text Analytics for health is a capability provided â€œAS ISâ€ and â€œWITH ALL FAULTS.â€ Text
Analytics for health is not intended or made available for use as a medical device, clinical
support, diagnostic tool, or other technology intended to be used in the diagnosis, cure,
mitigation, treatment, or prevention of disease or other conditions, and no license or right
is granted by Microsoft to use this capability for such purposes. This capability is not
designed or intended to be implemented or deployed as a substitute for professional
medical advice or healthcare opinion, diagnosis, treatment, or the clinical judgment of a
healthcare professional, and should not be used as such. The customer is solely
responsible for any use of Text Analytics for health. The customer must separately license
any and all source vocabularies it intends to use under the terms set for that UMLS
Metathesaurus License Agreement Appendix or any future equivalent link. The customer is
responsible for ensuring compliance with those license terms, including any geographic or
other applicable restrictions.
Text Analytics for health now allows extraction of Social Determinants of Health (SDOH)
and ethnicity mentions in text. This capability may not cover all potential SDOH and does
not derive inferences based on SDOH or ethnicity (for example, substance use information
is surfaced, but substance abuse is not inferred). All decisions leveraging outputs of the
Text Analytics for health that impact individuals or resource allocation (including, but not
limited to, those related to billing, human resources, or treatment managing care) should
be made with human oversight and not be based solely on the findings of the model. The
purpose of the SDOH and ethnicity extraction capability is to help providers improve
health outcomes and it should not be used to stigmatize or draw negative inferences
about the users or consumers of SDOH data, or patient populations beyond the stated
purpose of helping providers improving health outcomes.

An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, what its
capabilities and limitations are, and how to achieve the best performance. Microsoftâ€™s
Transparency Notes are intended to help you understand how our AI technology works, the
choices system owners can make that influence system performance and behavior, and the
importance of thinking about the whole system, including the technology, the people, and the
environment. You can use Transparency Notes when developing or deploying your own system,
or share them with the people who will use or be affected by your system.
Microsoft's Transparency notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Responsible AI principles

from Microsoft.

The basics of Text Analytics for health
Introduction
The Text Analytics for health feature of Azure Language in Foundry Tools uses natural language
processing techniques to find and label valuable health information such as diagnoses,
symptoms, medications, and treatments in unstructured text. The service can be used for
diverse types of unstructured medical documents, including discharge summaries, clinical
notes, clinical trial protocols, medical publications, and more. Text Analytics for health performs
Named Entity Recognition (NER), extracts relations between identified entities, surfaces
assertions such as negation and conditionality, and links detected entities to common
vocabularies.
Text Analytics for health can receive unstructured text in English as part of its general
availability offering. Additional languages are currently supported in a preview offering. For
more information, see Language support.
You can read an overview of the API and its capabilities. Also, see supported entities and
relations.
Additionally, customization is now offered for Text Analytics for health under the new preview
feature, custom Text Analytics for health. Custom Text Analytics for health allows customers to
use their own data train a custom NER model, designed for healthcare, to extract their domain
specific categories, extending the existing Text Analytics for health entity map. Customers can
also define lexicon or specific vocabulary for the newly defined custom entities as well as
existing Text Analytics for health entities such as Medication Name. Therefore, custom Text
Analytics for health offers the same capabilities offered by Text Analytics for health along with

the ability to extend the existing entity map by adding new ML entities and adding custom
vocabulary to existing entities.

Key terms
Text Analytics for health currently performs Named Entity Recognition (NER), relation
extraction, assertion detection, and entity linking for biomedical text. It can also be
supplemented with additional custom entity extraction using entity learned and list
components, now available using custom Text Analytics for health.
ï¾‰

Expand table

Term

Definition

Named Entity

Detects words and phrases that are mentioned in unstructured text that can be

Recognition

associated with one or more semantic types, such as diagnosis, medication name,
symptom or sign, or age.

Relation

Identifies meaningful connections between concepts that are mentioned in text. For

extraction

example, a "time of condition" relation is found by associating a condition name with a
time.

Assertion
detection

Surfaces entity modifiers that are mentioned in text, such as negation or conditionality.
The meaning of medical content might be highly affected by these modifiers.

Entity linking

Disambiguates distinct entities by associating named entities that are mentioned in text
with concepts that are found in a predefined database of concepts, such as in the
Unified Medical Language System (UMLS).

Entity Learned
Component

Allows the definition of new custom entities such as treatment, facility, or medical
instrument through training a custom model with labeled data.

Entity List

Allows the extraction of new custom entities or existing Text Analytics for health entities

Component

using a lexicon recognizer by defining a list of synonyms or vocabulary corresponding
to the entities of choice. For example, â€œMedication Aâ€ can be defined as a new list value
under the medication name entity.

Capabilities
System Behavior
To use Text Analytics for health, you input raw, unstructured text for analysis, and the API
output is handled in your application. Four key functions are performed in a single API call:
entity recognition, relation extraction, entity linking, and assertion detection. Analysis is
performed as-is, with no additional customization of the pretrained model. You can use Text

Analytics for health either through a hosted API or by deploying it in a container in your onpremises environment. For more information, see how to call Text Analytics for health.
To customize Text Analytics for health, use custom Text Analytics for healthâ€™s authoring
experience to create new entities that will extend the existing prebuilt entity map. You can also
define new vocabulary to be recognized using exact matching for new custom entities as well
as existing prebuilt entity categories such as Medication Name. After defining your projectâ€™s
entity map, you can train and deploy the custom model to make predictions. The deployed
custom model, by default, supports all the capabilities already included in Text Analytics for
health for the prebuilt entity categories. Additionally, the custom model features custom NER
for the new entity categories as well as any dictionary defined for the prebuilt entities.
Therefore, predictions to the custom model performs named entity recognition, relation
extraction, entity linking, and assertion detection for the Text Analytics for health entities and
custom named entity recognition to extract customer defined entity categories along with
defined vocabulary for new and existing entity categories. All the data used to train your
custom model will be stored in your private blob storage. Additionally, calling your custom
model requires your APIM subscription key, which means that your custom model is available
only to users with whom you have shared your secret key.

Intended use cases
Text Analytics for health can be used in multiple scenarios across a variety of industries that this
type of system supports. Some common customer motivations for using Text Analytics for
health include:
Assist and automate the processing of medical documents for proper coding to improve
accuracy of care and billing.
Increase efficiency of analyzing healthcare data to help drive success of value-based care
models (for example, Medicare).
Improve the aggregation of key data for tracking trends of patient care and history
without adding overhead to healthcare providers.
Make progress toward adopting HL7 standards, which is the framework for the exchange,
integration, sharing, and retrieval of electronic health information in support of the daily
clinical practice and management and overall delivery and evaluation of health services.
The same use cases and considerations apply to custom Text Analytics for health, but custom
Text Analytics for health is better suited for scenarios where the customer has data and would
like to extend the existing prebuilt entity map by creating their own entity categories or
defining vocabulary for new and existing entity categories.

Example use cases

The following use cases are popular examples for applications of the Text Analytics for health
and custom Text Analytics for health features:
Insights and statistics extraction. Identify medical entities such as symptoms,
medications, and diagnoses in clinical notes and diverse clinical documents. Use this
information to produce insights and statistics about patient populations, to search clinical
documents, and to research documents and publications.
Creation of predictive analytics and predictive models from historic data. Enables the
development of solutions for planning, decision support, risk analysis, and more based on
prediction models created by using historic data.
Assisted annotation and curation. Support solutions for clinical data annotation and
curation. For example, to support clinical coding, digitization of data that was manually
created, and automation of registry reporting.
Support solutions for displaying or analyzing health-related information. Support
solutions to display or analyze health-related information. For example, for reporting
purposes, support quality assurance processes or flag possible errors to be reviewed by a
human.

Considerations when choosing a use case
Text Analytics for health is a valuable tool when you manage and extract knowledge from
unstructured medical text. However, given the sensitive nature of health-related data, it's
important to consider your use cases carefully. In all cases, a human should be making
decisions assisted by the information the system returns, and in all cases, you should have a
way to review the source data and correct errors. Here are some additional considerations
when choosing a use case:
Avoid scenarios that use this service as a medical device, to provide clinical support, or
as a diagnostic tool to be used in the diagnosis, cure, mitigation, treatment, or
prevention of disease or other conditions without human intervention. A qualified
medical professional should always do due diligence and verify source data that might
influence patient care decisions.
Avoid scenarios related to automatically granting or denying medical services or health
insurance without human intervention. Because decisions that affect coverage levels are
extremely impactful, source data should always be verified in these scenarios.
Avoid scenarios that use personal health information for a purpose not permitted by
patient consent or applicable law. Health information has special protections regarding
privacy and consent. Make sure that all data you use has patient consent for the way you
use the data in your system or you are otherwise compliant with applicable law as it
relates to the use of health information.

Carefully consider using detected entities to automatically update patient records
without human intervention. Make sure that there is always a way to report, trace, and
correct any errors to avoid propagating incorrect data to other systems. Ensure that any
updates to patient records are reviewed and approved by qualified professionals.
Carefully consider using detected entities in patient billing without human
intervention. Make sure that providers and patients always have a way to report, trace,
and correct data that generates incorrect billing.
Carefully consider scenarios that make use of the detected Social Determinants of
Health and ethnicity entities. Always make sure that there is a way to report, trace, and
correct any errors to avoid erroneous substance use inference or offering an incorrect
form of care based on social and demographic factors.
Carefully consider scenarios that use an automated feedback loop in finetuning the
custom Text Analytics for health model. Always make sure to test and evaluate the
model prior to deploying to a production environment to avoid model quality regression
because custom model training is an iterative process that is very sensitive to the input
training data.
Legal and regulatory considerations: Organizations need to evaluate potential specific
legal and regulatory obligations when using any AI services and solutions, which may not
be appropriate for use in every industry or scenario. Additionally, AI services or solutions
are not designed for and may not be used in ways prohibited in applicable terms of
service and relevant codes of conduct.

Social Determinants of Health and Ethnicity
Text Analytics for health allows extraction of Social Determinants of Health (SDOH) and
ethnicity mentions in text. Using social and demographics entities might help you unlock
mentions of an array of factors besides direct medical care that can drive health outcomes,
such as underlying genetics, health behaviors, and social and environmental factors. By
leveraging the Text Analytics for health SDOH entity extraction capability, you might be able to
reduce health disparities that are often rooted in social and economic disadvantages, improve
care, assess health inequity issues, and incorporate underrepresented groups into clinical trials
and research. For more information, see Social determinants of health

, FDA Takes Important

Steps to Increase Racial and Ethnic Diversity in Clinical Trials | FDA and County Health Rankings:
Relationships Between Determinant Factors and Health Outcomes

.

This capability doesnâ€™t derive inferences based on SDOH or ethnicity (for example, substance
use information is surfaced from the input text, but substance abuse is not inferred based on
extracted entities). All decisions that rely on the outputs of Text Analytics for health and that
impact individuals or resource allocation (including but not limited to decisions related to
billing, human resources, or managing care) should be made with human oversight and not be
based solely on the findings of the model. The purpose of the SDOH and ethnicity extraction

capabilities is to help providers improve health outcomes. They should not be used to
stigmatize or draw negative inferences about the users or consumers of SDOH data or of
patient populations beyond the stated purpose of helping providers improve health outcomes.
As with other extracted entities, the Text Analytics for health response also returns a confidence
score for living status, employment, substance use, and ethnicity entities. Carefully consider the
confidence score in the context of an entityâ€™s intended use.

Custom Text Analytics for health
Text Analytics for health enables developers to process and extract insights from unstructured
medical data. Although the health feature is capable of processing and extracting a broad
range of data types and entity categories, there are still cases where the customer might like to
add a new entity type specific to their data or even define additional medical vocabulary in an
existing entity category.
Therefore, the purpose of Custom Text Analytics for health is to provide a means of
customizing on top of Text Analytics for health by giving customers the ability to extend the
entity map with completely new entity categories specific to their data, as well as the ability to
add custom vocabulary to the existing entity categories.
Custom Text Analytics for health allows customers to train a custom healthcare entity extraction
ML model using their labeled data and custom dictionaries/vocabularies. This will allow
customers to define new medical entities that are specific to their data. The service will also
internally call Text Analytics for health, providing all the features and entity map already given
in Text Analytics for health. As an added level of customization, customers will be able to add
their own vocabulary to existing Text Analytics for health entities in order to supplement the
prebuilt response with their data.
The customer is responsible for providing sufficient labelled data and vocabulary to train the
custom model; therefore, performance of the model may vary depending on the quality and
comprehensiveness of the labeled training data used by the customer relative to the new entity
categories to be defined. It is recommended to always test and evaluate the model prior to
deploying to a production environment to avoid model quality regression because custom
model training is an iterative process that is very sensitive to the input training data.

Limitations
Coverage: SDOH extraction capability might not cover all potential SDOH. Recognition is
limited to ethnicity and the entity types listed here Entity categories recognized by Text
Analytics for health - Foundry Tools | Microsoft Learn.

Languages: Currently, SDOH and ethnicity extraction capabilities are enabled for English
text only. Text Analytics for health can receive unstructured text in English as part of its
general availability offering. Additional languages are currently supported in a preview
offering.
Spelling: Incorrect spelling might affect the output. Specifically, entity linking looks for
terms and synonyms based only on a specific, correct spelling. If a drug name, for
example, is misspelled, the system might have enough information to recognize that the
text is a drug name, but it might not identify the link as it would for the correctly spelled
drug name.
Performance: Potential error types have been outlined in the System performance section
below.
Custom Text Analytics for health (in preview): Supports all the languages supported by
Text Analytics for health. To train a custom model you need to supply the training service
with a minimum of 10 labels for each newly defined custom entity category. In order to
train a custom model, the customer must add a minimum of 10 documents to the
projectâ€™s dataset. Lexicon recognizers used for extracting customer defined vocabulary
rely on exact case matching in the specified language, meaning that the customer must
add all variants of the specific word and include it for all the input languages for their
project. When using custom Text Analytics for health, entity linking, relationship
extraction, and assertion detection will be supported for Text Analytics for health entities
but will not be returned for any newly defined custom entity categories.

System Performance
Text Analytics for health and custom Text Analytics for health in general might have both false
positive errors and false negative errors for each capability supported by the health feature.
Several examples of the potential error types are described in the next sections.

Named Entity Recognition (NER)
False positive
In NER, a false positive occurs when the system incorrectly identifies an entity as belonging in a
category. In the following example, COVID-19 is mislabeled as EXAMINATION_NAME. In fact,
COVID-19 is a diagnosis, not the name of an examination. So, this is a false positive for
EXAMINATION_NAME.
In the second example, vodka is a false positive for MEDICATION_NAME. Instead, it should be
classified as SUBSTANCE_USE.

False negative
A false negative in NER occurs when an entity should have been identified as belonging in a
category, but it wasn't. In the following example, the entity ER should have been identified as
CARE_ENVIRONMENT, but it wasnâ€™t. If an entity isnâ€™t properly recognized, the linked code wonâ€™t
be recognized either.

In the next two examples, a second mention of ETHNICITY and information about previous
employment arenâ€™t properly recognized.

Relation Extraction
False positive
In relation extraction, a false positive occurs when a relation should not have been identified,
but it was. In the next example, the value of the AST examination was incorrectly attributed to
the ALT examination, which already has a measurement value assigned to it.

False negative
A false negative in relation extraction occurs when a relation should have been recognized, but
it wasn't. In the preceding example, the measurement value of 45 was not assigned to the AST
examination, and it should have been.

Entity Linking
False positive
Entity linking is achieved by looking for an exact match between concepts in common
vocabularies and the recognized entity. A false positive for entity linking would happen in the
rare cases when an entity is captured while it shouldnâ€™t have been (false positive NER) and a
matching concept appears to exist in the vocabulary. A false positive for entity linking might
also happen for ambiguous terms having several distinct matching concepts in the common
vocabularies.
False negative
Because entity linking is an exact match with the original text, you can get a false negative if
there's enough signal to properly recognize the entity but the spelling of that entity is not
correct in the text. For example, in the following text where therapies is misspelled, you would
not get the appropriate linked entity UMLS: C0087111.

Assertion Detection
False positive
In assertion detection, a false positive occurs when the system identifies an assertion that
should not exist in the text. In the following example, the entity respiratory disease is
incorrectly negated as a DIAGNOSIS for COVID-19.

False negative
A false negative in assertion detection occurs when an assertion is not captured. In the
following example, the symptom â€œrespondâ€ should be negated because there was no response

to the mentioned medication.

Best practices for improving system performance
Custom Text Analytics for healthâ€™s custom vocabulary uses exact word matching;
therefore, incorrect spelling may affect entity extraction.
To improve the quality of custom Text Analytics for healthâ€™s ML-based entity extraction
using Learned entity components, it is recommended to include an equal distribution of
labels for each custom entity as well as a minimum of 15 labels for each entity of
examples that are representative of the input data.

Evaluation of Text Analytics for Health
Evaluation methods
Text Analytics for health is trained and evaluated on diverse types of unstructured medical
documents, including discharge summaries, clinical notes, clinical trial protocols, medical
publications, and more. The SDOH model, which surfaces living status, employment, and
substance use entities, is trained and evaluated on a manually annotated dataset that comes
from two independent sources: approximately 750 randomly sampled proprietary clinical notes
and about 1,500 clinical notes randomly sampled from a corpus provided by a US medical
center and focused mostly on adult patients. The original corpus covers more than 10 years of
collected data and thousands of patient admissions. It provides almost equal representation of
male and female patients. It should be noted that no further analysis of the training data
representativeness (for example, geographic, demographic, or ethnographic representation)
has been performed. Even though internal tests demonstrate the modelâ€™s potential to
generalize to different populations and geographies, you should carefully consider how the
training and evaluation data is representative in the context of your intended use. To evaluate
the system in relation to potential fairness harms, the evaluation dataset was split into
subgroups of documents by social and demographic factors, such as gender, age, ethnicity,
employment, and living status. Targeted minimum performance levels for each group were
evaluated, as were relative performance differences between groups.

Evaluation of custom Text Analytics for Health
Custom Text Analytics for health leverages a custom healthcare base model that gets finetuned
by customer- provided data in addition to the prebuilt Text Analytics for health model. The

healthcare base model used is the same base model that the Text Analytics for health entity
map is built on.
Custom Text Analytics for health features an internal evaluation as part of the authoring
experience; this enables the customer to create a testing dataset and review the F1, precision,
and recall scores for the defined custom entity categories. Text Analytics for health prebuilt
entities are not included in the internal evaluation. The experience also features model
guidance to provide the customer with ways of improving the resulting scores from the testing
such as recommending additional labels for entities that are not performing well.

Evaluating and integrating Text Analytics for health for your
use
Microsoft wants to help you responsibly develop and deploy solutions that use Language.
These considerations are in line with our commitment to developing responsible AI. When you
decide how to use and implement products and solutions powered by Language features,
consider the following factors.

General guidelines
When you're getting ready to deploy Text Analytics for health, the following activities help set
you up for success:
Understand what it can do: Fully assess the capabilities of Text Analytics for health to
understand its capabilities and limitations. Understand how it will perform in your
scenario and context.
Test with real, diverse data: Understand Text Analytics for health will perform in your
scenario by thoroughly testing it by using real-life conditions and data that reflect the
diversity in your users, geography, and deployment contexts. Small datasets, synthetic
data, and tests that don't reflect your end-to-end scenario are unlikely to sufficiently
represent your production performance.
Respect an individual's right to privacy: Only collect or use data and information from
individuals for lawful and justifiable purposes. Use only the data and information that you
have consent to use or are legally permitted to use.
Legal review: Obtain appropriate legal review of your solution, particularly if you will use
it in sensitive or high-risk applications. Understand what restrictions you might need to
work within and any risks that need to be mitigated prior to use. It is your responsibility
to mitigate such risks and resolve any issues that might come up.
System review: If you plan to integrate and responsibly use an AI-powered product or
feature into an existing system for software or customer or organizational processes, take

time to understand how each part of your system will be affected. Consider how your AI
solution aligns with Microsoft Responsible AI principles.
Human in the loop: Keep a human in the loop and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and ensuring the role of humans in making any decisions
that are based on the modelâ€™s output. To prevent harm and to manage how the AI model
performs, ensure that humans have a way to intervene in the solution in real time.
Security: Ensure that your solution is secure and that it has adequate controls to preserve
the integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that users and individuals can use
to report issues with the service after it's deployed. After you deploy an AI-powered
product or feature, it requires ongoing monitoring and improvement. Have a plan and be
ready to implement feedback and suggestions for improvement.

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language
Guidance for integration and responsible use with Language

Last updated on 08/17/2025

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

How to use Text Analytics for health
ï¼‰ Important
Text Analytics for health is a capability provided â€œAS ISâ€ and â€œWITH ALL FAULTS.â€ Text
Analytics for health is not intended or made available for use as a medical device, clinical
support, diagnostic tool, or other technology intended to be used in the diagnosis, cure,
mitigation, treatment, or prevention of disease or other conditions, and no license or right
is granted by Microsoft to use this capability for such purposes. This capability is not
designed or intended to be implemented or deployed as a substitute for professional
medical advice or healthcare opinion, diagnosis, treatment, or the clinical judgment of a
healthcare professional, and should not be used as such. The customer is solely
responsible for any use of Text Analytics for health. The customer must separately license
any and all source vocabularies it intends to use under the terms set for that UMLS
Metathesaurus License Agreement Appendix

or any future equivalent link. The

customer is responsible for ensuring compliance with those license terms, including any
geographic or other applicable restrictions.
Text Analytics for health now allows extraction of Social Determinants of Health (SDOH)
and ethnicity mentions in text. This capability may not cover all potential SDOH and does
not derive inferences based on SDOH or ethnicity (for example, substance use information
is surfaced, but substance abuse is not inferred). All decisions leveraging outputs of the
Text Analytics for health that impact individuals or resource allocation (including, but not
limited to, those related to billing, human resources, or treatment managing care) should
be made with human oversight and not be based solely on the findings of the model. The
purpose of the SDOH and ethnicity extraction capability is to help providers improve
health outcomes and it should not be used to stigmatize or draw negative inferences
about the users or consumers of SDOH data, or patient populations beyond the stated
purpose of helping providers improving health outcomes.
Text Analytics for health can be used to extract relevant medical information from unstructured
texts. These texts may include doctors' notes, discharge summaries, clinical documents, and
electronic health records. The tool can also label the extracted information for easier analysis
and reference. The service performs named entity recognition, relation extraction, entity
linking , and assertion detection to uncover insights from the input text. For information on
the returned confidence scores, see the transparency note.
îª€ Tip
If you want to test out the feature without writing any code, use Microsoft Foundry

.

There are two ways to call the service:
Using a Docker container (synchronous).
Using the web-based API and client libraries (asynchronous).

Development options
To use Text Analytics for health, you submit raw unstructured text for analysis and handle the
API output in your application. Analysis is performed as-is, with no additional customization to
the model used on your data. There are two ways to use Text Analytics for health:
ï¾‰

Expand table

Development option

Description

Microsoft Foundry

Foundry is a web-based platform that lets you use entity linking with text
examples with your own data when you sign up. For more information, see the
Foundry website or Foundry documentation.

REST API or Client
library (Azure SDK)

Integrate Text Analytics for health into your applications using the REST API, or
the client library available in a variety of languages. For more information, see
the Text Analytics for health quickstart.

Docker container

Use the available Docker container to deploy this feature on-premises. These
docker containers enable you to bring the service closer to your data for
compliance, security, or other operational reasons.

Input languages
The Text Analytics for health supports English in addition to multiple languages that are
currently in preview. You can use the hosted API or deploy the API in a container, as detailed
under Text Analytics for health languages support.

Submitting data
To send an API request, you need your Language resource endpoint and key.
ï¼— Note
You can find the key and endpoint for your Language resource on the Azure portal.
They're located on the resource's Key and endpoint page, under resource management.

Analysis is performed upon receipt of the request. If you send a request using the REST API or
client library, the results are returned asynchronously. If you're using the Docker container,
they're returned synchronously.
When using this feature asynchronously, the API results are available for 24 hours from the
time the request was ingested, and is indicated in the response. After this time period, the
results are purged and are no longer available for retrieval.

Submitting a Fast Healthcare Interoperability
Resources (FHIR) request
Fast Healthcare Interoperability Resources (FHIR) is the health industry communication
standard developed by the Health Level Seven International (HL7) organization. The standard
defines the data formats (resources) and API structure for exchanging electronic healthcare
data. To receive your result using the FHIR structure, you must send the FHIR version in the API
request body.
ï¾‰

Parameter Name

Type

Value

fhirVersion

string

4.0.1

Expand table

Getting results from the feature
Depending on your API request, and the data you submit to the Text Analytics for health, you
get:
Named Entity Recognition

Named entity recognition is used to perform a semantic extraction of words and phrases
mentioned from unstructured text that are associated with any of the supported entity
types, such as diagnosis, medication name, symptom/sign, or age.

Service and data limits
For information on the size and number of requests you can send per minute and second, see
the service limits article.

See also
Text Analytics for health overview
Text Analytics for health entity categories

Last updated on 11/18/2025

Use Text Analytics for health containers
Containers enable you to host the Text Analytics for health API on your own infrastructure. If
you have security or data governance requirements that can't be fulfilled by calling Text
Analytics for health remotely, then containers might be a good option.
If you don't have an Azure subscription, create a free account

before you begin.

Prerequisites
You must meet the following prerequisites before using Text Analytics for health containers. If
you don't have an Azure subscription, create a free account
Docker

before you begin.

installed on a host computer. Docker must be configured to allow the

containers to connect with and send billing data to Azure.
On Windows, Docker must also be configured to support Linux containers.
You should have a basic understanding of Docker concepts
A Language resource

.

with the free (F0) or standard (S) pricing tier

.

Gather required parameters
Three primary parameters for all Azure AI containers are required. The Microsoft Software
License Terms must be present with a value of accept. An Endpoint URI and API key are also
needed.

Endpoint URI
The {ENDPOINT_URI} value is available on the Azure portal Overview page of the corresponding
Foundry Tools resource. Go to the Overview page, hover over the endpoint, and a Copy to
clipboard ï¼ icon appears. Copy and use the endpoint where needed.

Keys
The {API_KEY} value is used to start the container and is available on the Azure portal's Keys
page of the corresponding Foundry Tools resource. Go to the Keys page, and select the Copy
to clipboard ï¼ icon.

ï¼‰ Important
These subscription keys are used to access your Foundry Tools API. Don't share your keys.
Store them securely. For example, use Azure Key Vault. We also recommend that you
regenerate these keys regularly. Only one key is necessary to make an API call. When you
regenerate the first key, you can use the second key for continued access to the service.

Host computer requirements and
recommendations
The host is an x64-based computer that runs the Docker container. It can be a computer on
your premises or a Docker hosting service in Azure, such as:
Azure Kubernetes Service.
Azure Container Instances.
A Kubernetes

cluster deployed to Azure Stack. For more information, see Deploy

Kubernetes to Azure Stack.
The following table describes the minimum and recommended specifications for the Text
Analytics for health containers. Each CPU core must be at least 2.6 gigahertz (GHz) or faster.
The allowable Transactions Per Second (TPS) are also listed.
ï¾‰

Expand table

1 document/request

Minimum host

Recommended host

Minimum

Maximum

specs

specs

TPS

TPS

4 core, 12GB

6 core, 12GB memory

15

30

8 core, 20GB memory

15

30

memory
10

6 core, 16GB

documents/request

memory

CPU core and memory correspond to the --cpus and --memory settings, which are used as part
of the docker run command.

Get the container image with docker pull
The Text Analytics for health container image can be found on the mcr.microsoft.com container
registry syndicate. It resides within the azure-cognitive-services/textanalytics/ repository
and is named healthcare . The fully qualified container image name is
mcr.microsoft.com/azure-cognitive-services/textanalytics/healthcare

To use the latest version of the container, you can use the latest tag. You can also find a full
list of tags on the MCR
Use the docker pull

.

command to download this container image from the Microsoft public

container registry. You can find the featured tags on the Microsoft Container Registry

docker pull mcr.microsoft.com/azure-cognitive-services/textanalytics/healthcare:
<tag-name>

îª€ Tip
You can use the docker images

command to list your downloaded container images.

For example, the following command lists the ID, repository, and tag of each downloaded
container image, formatted as a table:

docker images --format "table {{.ID}}\t{{.Repository}}\t{{.Tag}}"
IMAGE ID
<image-id>

REPOSITORY
<repository-path/name>

TAG
<tag-name>

Run the container with docker run
Once the container is on the host computer, use the docker run

command and run the

containers. The container continues to run until you stop it.
ï¼‰ Important

The docker commands in the following sections use the back slash, \ , as a line
continuation character. Replace or remove the back slash based on your host
operating system's requirements.
The Eula , Billing , and ApiKey options must be specified to run the container;
otherwise, the container doesn't start. For more information, see Billing.
The responsible AI (RAI) acknowledgment must also be present with a value of
accept .

The sentiment analysis and language detection containers use v3 of the API, and are
generally available. The key phrase extraction container uses v2 of the API, and is in
preview.
There are multiple ways you can install and run the Text Analytics for health container.
Use the Azure portal to create a Language resource, and use Docker to get your
container.
Use an Azure virtual machine (VM) with Docker and run the container.
Use the following PowerShell and Azure CLI scripts to automate resource deployment and
container configuration.
When you use the Text Analytics for health container, the data contained in your API requests
and responses isn't visible to Microsoft, and isn't used for training the model applied to your
data.

Run the container locally
To run the container in your own environment after downloading the container image, execute
the following docker run command. Replace the placeholders with your own values:
ï¾‰

Expand table

Placeholder

Value

Format or example

{API_KEY}

The key for your Language resource.
You can find it on your resource's

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Placeholder

Value

Format or example

Key and endpoint page, on the
Azure portal.
{ENDPOINT_URI}

The endpoint for accessing the API.
You can find it on your resource's
Key and endpoint page, on the

https://<your-customsubdomain>.cognitiveservices.azure.com

Azure portal.

Bash
docker run --rm -it -p 5000:5000 --cpus 6 --memory 12g \
mcr.microsoft.com/azure-cognitive-services/textanalytics/healthcare:<tag-name> \
Eula=accept \
rai_terms=accept \
Billing={ENDPOINT_URI} \
ApiKey={API_KEY}

This command:
Runs the Text Analytics for health container from the container image
Allocates 6 CPU core and 12 gigabytes (GB) of memory
Exposes TCP port 5000 and allocates a pseudo-TTY for the container
Accepts the end user license agreement (EULA) and responsible AI (RAI) terms
Automatically removes the container after it exits. The container image is still available on
the host computer.

Demo UI to visualize output
The container provides REST-based query prediction endpoint APIs. We also provide a
visualization tool in the container that is accessible by appending /demo to the endpoint of the
container. For example:

http://<serverURL>:5000/demo

Use the following example cURL request to submit a query to the container you deployed by
replacing the serverURL variable with the appropriate value.
Bash
curl -X POST 'http://<serverURL>:5000/text/analytics/v3.1/entities/health' --header
'Content-Type: application/json' --header 'accept: application/json' --data-binary

@example.json

Install the container using Azure Web App for Containers
Azure Web App for Containers

is an Azure resource dedicated to running containers in the

cloud. It offers built-in features like autoscaling, support for Docker containers and Docker
Compose, and HTTPS support.
ï¼— Note
With Azure Web App, you automatically get a domain in the form of
<appservice_name>.azurewebsites.net

Run this PowerShell script using the Azure CLI to create a Web App for Containers, using your
subscription and the container image over HTTPS. Wait for the script to complete
(approximately 25-30 minutes) before submitting the first request.
Azure CLI
$subscription_name = ""
you resource to be created on.
$resource_group_name = ""
the AppServicePlan
$resources_location = ""
AppServicePlan to be deployed to.

# THe name of the subscription you want
# The name of the resource group you want
#
and AppService to be attached to.
# This is the location you wish the
#

You can use the "az account list-

#

get the list of available locations

locations -o table" command to
and location code names.
$appservice_plan_name = ""
# This is the AppServicePlan name you
wish to have.
$appservice_name = ""
# This is the AppService resource name
you wish to have.
$TEXT_ANALYTICS_RESOURCE_API_KEY = ""
# This should be taken from Azure
Language resource.
$TEXT_ANALYTICS_RESOURCE_API_ENDPOINT = "" # This should be taken from Azure
Language resource.
$DOCKER_IMAGE_NAME = "mcr.microsoft.com/azure-cognitiveservices/textanalytics/healthcare:latest"
az login
az account set -s $subscription_name
az appservice plan create -n $appservice_plan_name -g $resource_group_name --islinux -l $resources_location --sku P3V2
az webapp create -g $resource_group_name -p $appservice_plan_name -n
$appservice_name -i $DOCKER_IMAGE_NAME

az webapp config appsettings set -g $resource_group_name -n $appservice_name -settings Eula=accept rai_terms=accept Billing=$TEXT_ANALYTICS_RESOURCE_API_ENDPOINT
ApiKey=$TEXT_ANALYTICS_RESOURCE_API_KEY
# Once deployment complete, the resource should be available at:
https://<appservice_name>.azurewebsites.net

Install the container using Azure Container Instance
You can also use an Azure Container Instance (ACI) to make deployment easier. ACI is a
resource that allows you to run Docker containers on-demand in a managed, serverless Azure
environment.
See How to use Azure Container Instances for steps on deploying an ACI resource using the
Azure portal. You can also use the following PowerShell script using Azure CLI, which creates an
ACI on your subscription using the container image. Wait for the script to complete
(approximately 25-30 minutes) before submitting the first request. Due to the limit on the
maximum number of CPUs per ACI resource, don't select this option if you expect to submit
more than five large documents (approximately 5,000 characters each) per request. See the ACI
regional support article for availability information.
ï¼— Note
Azure Container Instances don't include HTTPS support for the builtin domains. If you
need HTTPS, you need to manually configure it, including creating a certificate and
registering a domain. You can find instructions with the following NGINX example:

Azure CLI
$subscription_name = ""
you resource to be created on.
$resource_group_name = ""
the AppServicePlan
$resources_location = ""
app to be deployed to.

# The name of the subscription you want
# The name of the resource group you want
# and AppService to be attached to.
# This is the location you wish the web
# You can use the "az account list-

locations -o table" command to
# Get the list of available locations and
location code names.
$azure_container_instance_name = ""
# This is the AzureContainerInstance name
you wish to have.
$TEXT_ANALYTICS_RESOURCE_API_KEY = ""
# This should be taken from Azure
Language resource.
$TEXT_ANALYTICS_RESOURCE_API_ENDPOINT = "" # This should be taken from Azure
Language resource.

$DNS_LABEL = ""
# This is the DNS label name you wish
your ACI will have
$DOCKER_IMAGE_NAME = "mcr.microsoft.com/azure-cognitiveservices/textanalytics/healthcare:latest"
az login
az account set -s $subscription_name
az container create --resource-group $resource_group_name --name
$azure_container_instance_name --image $DOCKER_IMAGE_NAME --cpu 4 --memory 12 --port
5000 --dns-name-label $DNS_LABEL --environment-variables Eula=accept
rai_terms=accept Billing=$TEXT_ANALYTICS_RESOURCE_API_ENDPOINT
ApiKey=$TEXT_ANALYTICS_RESOURCE_API_KEY
# Once deployment complete, the resource should be available at:
http://<unique_dns_label>.<resource_group_region>.azurecontainer.io:5000

Secure ACI connectivity
By default, ACI with the container API doesn't provide securityâ€”containers usually run inside a
pod, and a network bridge isolates the pod from external access. You can, however, modify a
container with a front-facing component, keeping the container endpoint private. The
following examples use NGINX

as an ingress gateway to support HTTPS/SSL and client-

certificate authentication.
ï¼— Note
NGINX is an open-source, high-performance HTTP server, and proxy. An NGINX container
can be used to terminate a TLS connection for a single container. More complex NGINX
ingress-based TLS termination solutions are also possible.

Set up NGINX as an ingress gateway
NGINX uses configuration files

to enable features at runtime. In order to enable TLS

termination for another service, you must specify an SSL certificate to terminate the TLS
connection and proxy_pass to specify an address for the service. A sample is provided:
ï¼— Note
ssl_certificate expects a path to be specified within the NGINX container's local

filesystem. The address specified for proxy_pass must be available from within the NGINX
container's network.

The NGINX container loads all of the files in the _.conf_ that are mounted under
/etc/nginx/conf.d/ into the HTTP configuration path.

nginx
server {
listen
80;
return 301 https://$host$request_uri;
}
server {
listen
443 ssl;
# replace with .crt and .key paths
ssl_certificate
/cert/Local.crt;
ssl_certificate_key /cert/Local.key;
location / {
proxy_pass http://cognitive-service:5000;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_set_header X-Real-IP $remote_addr;
}
}

Compose file example
The following example shows how a docker compose

file can be created to deploy NGINX

and health containers:
YAML
version: "3.7"
services:
cognitive-service:
image: {IMAGE_ID}
ports:
- 5000:5000
environment:
- eula=accept
- billing={ENDPOINT_URI}
- apikey={API_KEY}
volumes:
# replace with path to logs folder
- <path-to-logs-folder>:/output
nginx:
image: nginx
ports:
- 443:443
volumes:
# replace with paths for certs and conf folders
- <path-to-certs-folder>:/cert
- <path-to-conf-folder>:/etc/nginx/conf.d/

To initiate this Docker compose file, execute the following command from a console at the root
level of the file:
Bash
docker-compose up

For more information, see NGINX's documentation on NGINX SSL Termination

.

Run multiple containers on the same host
If you intend to run multiple containers with exposed ports, make sure to run each container
with a different exposed port. For example, run the first container on port 5000 and the second
container on port 5001.
You can have this container and a different Foundry Tools container running on the HOST
together. You also can have multiple containers of the same Foundry Tools container running.

Query the container's prediction endpoint
The container provides REST-based query prediction endpoint APIs.
Use the host, http://localhost:5000 , for container APIs.

Validate that a container is running
There are several ways to validate that the container is running. Locate the External IP address
and exposed port of the container in question, and open your favorite web browser. Use the
various request URLs that follow to validate the container is running. The example request URLs
listed here are http://localhost:5000 , but your specific container might vary. Make sure to rely
on your container's External IP address and exposed port.
ï¾‰

Expand table

Request URL

Purpose

http://localhost:5000/

The container provides a home page.

http://localhost:5000/ready

Requested with GET, this URL provides a verification that the container
is ready to accept a query against the model. This request can be used
for Kubernetes liveness and readiness probes

.

Request URL

Purpose

http://localhost:5000/status

Also requested with GET, this URL verifies if the api-key used to start
the container is valid without causing an endpoint query. This request
can be used for Kubernetes liveness and readiness probes .

http://localhost:5000/swagger

The container provides a full set of documentation for the endpoints
and a Try it out feature. With this feature, you can enter your settings
into a web-based HTML form and make the query without having to
write any code. After the query returns, an example CURL command is
provided to demonstrate the HTTP headers and body format that's
required.

Structure the API request for the container
You can use the Visual Studio Code REST Client extension

or the following example cURL

request to submit a query to the container you deployed, replacing the serverURL variable with
the appropriate value. Note the version of the API in the URL for the container is different than
the hosted API.
Language API (Preview)

ï¼— Note
Fast Healthcare Interoperability Resources (FHIR) feature is available in the latest
container, and is exposed through the new language REST API.

Bash
curl -i -X POST 'http://<serverURL>:5000/language/analyze-text/jobs?apiversion=2022-04-01-preview' --header 'Content-Type: application/json' --header -data-binary @example.json

The following JSON is an example of a JSON file attached to Azure Language request's
POST body:
JSON
example.json
{
"analysisInput": {
"documents": [
{
"text": "The doctor prescried 200mg Ibuprofen.",
"language": "en",
"id": "1"
}
]
},
"tasks": [
{
"taskName": "analyze 1",
"kind": "Healthcare",
"parameters": {
"fhirVersion": "4.0.1"
}
}
]
}

Container response body
The following JSON is an example of Azure Language response body from the
containerized synchronous call:
JSON
{
"jobId": "{JOB-ID}",
"lastUpdateDateTime": "2022-04-18T15:50:16Z",
"createdDateTime": "2022-04-18T15:50:14Z",
"expirationDateTime": "2022-04-19T15:50:14Z",
"status": "succeeded",
"errors": [],
"tasks": {
"completed": 1,

"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind": "HealthcareLROResults",
"taskName": "analyze 1",
"lastUpdateDateTime": "2022-04-18T15:50:16.7046515Z",
"status": "succeeded",
"results": {
"documents": [
{
"id": "1",
"entities": [
{
"offset": 4,
"length": 6,
"text": "doctor",
"category": "HealthcareProfession",
"confidenceScore": 0.76
},
{
"offset": 21,
"length": 5,
"text": "200mg",
"category": "Dosage",
"confidenceScore": 0.99
},
{
"offset": 27,
"length": 9,
"text": "Ibuprofen",
"category": "MedicationName",
"confidenceScore": 1.0,
"name": "ibuprofen",
"links": [
{ "dataSource": "UMLS", "id": "C0020740" },
{ "dataSource": "AOD", "id": "0000019879" },
{ "dataSource": "ATC", "id": "M01AE01" },
{ "dataSource": "CCPSS", "id": "0046165" },
{ "dataSource": "CHV", "id": "0000006519" },
{ "dataSource": "CSP", "id": "2270-2077" },
{ "dataSource": "DRUGBANK", "id": "DB01050" },
{ "dataSource": "GS", "id": "1611" },
{ "dataSource": "LCH_NW", "id": "sh97005926" },
{ "dataSource": "LNC", "id": "LP16165-0" },
{ "dataSource": "MEDCIN", "id": "40458" },
{ "dataSource": "MMSL", "id": "d00015" },
{ "dataSource": "MSH", "id": "D007052" },
{ "dataSource": "MTHSPL", "id": "WK2XYI10QM" },
{ "dataSource": "NCI", "id": "C561" },
{ "dataSource": "NCI_CTRP", "id": "C561" },
{ "dataSource": "NCI_DCP", "id": "00803" },
{ "dataSource": "NCI_DTP", "id": "NSC0256857" },
{ "dataSource": "NCI_FDA", "id": "WK2XYI10QM" },

{ "dataSource": "NCI_NCI-GLOSS", "id": "CDR0000613511" },
{ "dataSource": "NDDF", "id": "002377" },
{ "dataSource": "PDQ", "id": "CDR0000040475" },
{ "dataSource": "RCD", "id": "x02MO" },
{ "dataSource": "RXNORM", "id": "5640" },
{ "dataSource": "SNM", "id": "E-7772" },
{ "dataSource": "SNMI", "id": "C-603C0" },
{ "dataSource": "SNOMEDCT_US", "id": "387207008" },
{ "dataSource": "USP", "id": "m39860" },
{ "dataSource": "USPMG", "id": "MTHU000060" },
{ "dataSource": "VANDF", "id": "4017840" }
]
}
],
"relations": [
{
"relationType": "DosageOfMedication",
"entities": [
{
"ref": "#/results/documents/0/entities/1",
"role": "Dosage"
},
{
"ref": "#/results/documents/0/entities/2",
"role": "Medication"
}
]
}
],
"warnings": [],
"fhirBundle": {
"resourceType": "Bundle",
"id": "bae9d4e0-191e-48e6-9c24-c1ff6097c439",
"meta": {
"profile": [
"http://hl7.org/fhir/4.0.1/StructureDefinition/Bundle"
]
},
"identifier": {
"system": "urn:ietf:rfc:3986",
"value": "urn:uuid:bae9d4e0-191e-48e6-9c24-c1ff6097c439"
},
"type": "document",
"entry": [
{
"fullUrl": "Composition/9044c2cc-dcec-4b9d-b005bfa8be978aa8",
"resource": {
"resourceType": "Composition",
"id": "9044c2cc-dcec-4b9d-b005-bfa8be978aa8",
"status": "final",
"type": {
"coding": [
{
"system": "http://loinc.org",

"code": "11526-1",
"display": "Pathology study"
}
],
"text": "Pathology study"
},
"subject": {
"reference": "Patient/5c554347-4290-4b05-83ac6637ff3bfb40",
"type": "Patient"
},
"encounter": {
"reference": "Encounter/6fe12f5b-e35c-4c92-a49296feda5a1a3b",
"type": "Encounter",
"display": "unknown"
},
"date": "2022-04-18",
"author": [
{
"reference": "Practitioner/fb5da4d8-e0f0-4434-8d294419b065c4d7",
"type": "Practitioner",
"display": "Unknown"
}
],
"title": "Pathology study",
"section": [
{
"title": "General",
"code": {
"coding": [
{
"system": "",
"display": "Unrecognized Section"
}
],
"text": "General"
},
"text": {
"div": "
<div>\r\n\t\t\t\t\t\t\t<h1>General</h1>\r\n\t\t\t\t\t\t\t<p>The doctor prescried
200mg Ibuprofen.</p>\r\n\t\t\t\t\t</div>"
},
"entry": [
{
"reference": "List/db388912-b5fb-4073-a74c2751fd3374dd",
"type": "List",
"display": "General"
}
]
}
]
}

},
{
"fullUrl": "Practitioner/fb5da4d8-e0f0-4434-8d294419b065c4d7",
"resource": {
"resourceType": "Practitioner",
"id": "fb5da4d8-e0f0-4434-8d29-4419b065c4d7",
"extension": [
{
"extension": [
{ "url": "offset", "valueInteger": -1 },
{ "url": "length", "valueInteger": 7 }
],
"url":
"http://hl7.org/fhir/StructureDefinition/derivation-reference"
}
],
"name": [{ "text": "Unknown", "family": "Unknown" }]
}
},
{
"fullUrl": "Patient/5c554347-4290-4b05-83ac-6637ff3bfb40",
"resource": {
"resourceType": "Patient",
"id": "5c554347-4290-4b05-83ac-6637ff3bfb40",
"gender": "unknown"
}
},
{
"fullUrl": "Encounter/6fe12f5b-e35c-4c92-a492-96feda5a1a3b",
"resource": {
"resourceType": "Encounter",
"id": "6fe12f5b-e35c-4c92-a492-96feda5a1a3b",
"meta": {
"profile": [
"http://hl7.org/fhir/us/core/StructureDefinition/uscore-encounter"
]
},
"status": "finished",
"class": {
"system": "http://terminology.hl7.org/CodeSystem/v3ActCode",
"display": "unknown"
},
"subject": {
"reference": "Patient/5c554347-4290-4b05-83ac6637ff3bfb40",
"type": "Patient"
}
}
},
{
"fullUrl": "MedicationStatement/24e860ce-2fdc-4745-aa9e7d30bb487c4e",

"resource": {
"resourceType": "MedicationStatement",
"id": "24e860ce-2fdc-4745-aa9e-7d30bb487c4e",
"extension": [
{
"extension": [
{ "url": "offset", "valueInteger": 27 },
{ "url": "length", "valueInteger": 9 }
],
"url":
"http://hl7.org/fhir/StructureDefinition/derivation-reference"
}
],
"status": "active",
"medicationCodeableConcept": {
"coding": [
{
"system": "http://www.nlm.nih.gov/research/umls",
"code": "C0020740",
"display": "Ibuprofen"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/aod",
"code": "0000019879"
},
{
"system": "http://www.whocc.no/atc",
"code": "M01AE01"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/ccpss",
"code": "0046165"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/chv",
"code": "0000006519"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/csp",
"code": "2270-2077"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/drugbank",
"code": "DB01050"
},
{
"system": "http://www.nlm.nih.gov/research/umls/gs",
"code": "1611"
},
{

"system":
"http://www.nlm.nih.gov/research/umls/lch_nw",
"code": "sh97005926"
},
{ "system": "http://loinc.org", "code": "LP16165-0" },
{
"system":
"http://www.nlm.nih.gov/research/umls/medcin",
"code": "40458"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/mmsl",
"code": "d00015"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/msh",
"code": "D007052"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/mthspl",
"code": "WK2XYI10QM"
},
{
"system": "http://ncimeta.nci.nih.gov",
"code": "C561"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/nci_ctrp",
"code": "C561"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/nci_dcp",
"code": "00803"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/nci_dtp",
"code": "NSC0256857"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/nci_fda",
"code": "WK2XYI10QM"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/nci_nci-gloss",
"code": "CDR0000613511"
},
{

"system":
"http://www.nlm.nih.gov/research/umls/nddf",
"code": "002377"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/pdq",
"code": "CDR0000040475"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/rcd",
"code": "x02MO"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/rxnorm",
"code": "5640"
},
{
"system": "http://snomed.info/sct",
"code": "E-7772"
},
{
"system":
"http://snomed.info/sct/900000000000207008",
"code": "C-603C0"
},
{
"system": "http://snomed.info/sct/731000124108",
"code": "387207008"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/usp",
"code": "m39860"
},
{
"system":
"http://www.nlm.nih.gov/research/umls/uspmg",
"code": "MTHU000060"
},
{
"system": "http://hl7.org/fhir/ndfrt",
"code": "4017840"
}
],
"text": "Ibuprofen"
},
"subject": {
"reference": "Patient/5c554347-4290-4b05-83ac6637ff3bfb40",
"type": "Patient"
},
"context": {

"reference": "Encounter/6fe12f5b-e35c-4c92-a49296feda5a1a3b",
"type": "Encounter",
"display": "unknown"
},
"dosage": [
{
"text": "200mg",
"doseAndRate": [{ "doseQuantity": { "value": 200 } }]
}
]
}
},
{
"fullUrl": "List/db388912-b5fb-4073-a74c-2751fd3374dd",
"resource": {
"resourceType": "List",
"id": "db388912-b5fb-4073-a74c-2751fd3374dd",
"status": "current",
"mode": "snapshot",
"title": "General",
"subject": {
"reference": "Patient/5c554347-4290-4b05-83ac6637ff3bfb40",
"type": "Patient"
},
"encounter": {
"reference": "Encounter/6fe12f5b-e35c-4c92-a49296feda5a1a3b",
"type": "Encounter",
"display": "unknown"
},
"entry": [
{
"item": {
"reference": "MedicationStatement/24e860ce-2fdc4745-aa9e-7d30bb487c4e",
"type": "MedicationStatement",
"display": "Ibuprofen"
}
}
]
}
}
]
}
}
],
"errors": [],
"modelVersion": "2022-03-01"
}
}
]
}

}

Run the container with client library support
Starting with container version 3.0.017010001-onprem-amd64 (or if you use the latest
container), you can run the Text Analytics for health container using the client library. To do so,
add the following parameter to the docker run command:
enablelro=true

Afterwards when you authenticate the client object, use the endpoint that your container is
running on:
http://localhost:5000

For example, if you're using C# you would use the following code:
C#
var client = new TextAnalyticsClient("http://localhost:5000", "your-text-analyticskey");

Stop the container
To shut down the container, in the command-line environment where the container is running,
select Ctrl+C .

Troubleshooting
If you run the container with an output mount and logging enabled, the container generates
log files. These log files are useful for troubleshooting issues that may occur while the
container is starting or running.
îª€ Tip
For more troubleshooting information and guidance, see Azure AI containers frequently
asked questions (FAQ).

Billing
Text Analytics for health containers send billing information to Azure, using a Language
resource on your Azure account.
Queries to the container are billed at the pricing tier of the Azure resource that's used for the
ApiKey parameter.

Foundry Tools containers aren't licensed to run without being connected to the metering or
billing endpoint. You must enable the containers to communicate billing information with the
billing endpoint at all times. Foundry Tools containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft.

Connect to Azure
The container needs the billing argument values to run. These values allow the container to
connect to the billing endpoint. The container reports usage about every 10 to 15 minutes. If
the container doesn't connect to Azure within the allowed time window, the container
continues to run but doesn't serve queries until the billing endpoint is restored. The connection
is attempted 10 times at the same time interval of 10 to 15 minutes. If it can't connect to the
billing endpoint within the 10 tries, the container stops serving requests. See the Foundry Tools
container FAQ for an example of the information sent to Microsoft for billing.

Billing arguments
The docker run ï¼­

command will start the container when all three of the following options

are provided with valid values:
ï¾‰

Expand table

Option

Description

ApiKey

The API key of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to an API key for the provisioned resource that's specified
in Billing .

Billing

The endpoint of the Foundry Tools resource that's used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.

Eula

Indicates that you accepted the license for the container.
The value of this option must be set to accept.

Summary
In this article, you learned concepts and workflow for downloading, installing, and running Text
Analytics for health containers. In summary:
Text Analytics for health provides a Linux container for Docker
Container images are downloaded from the Microsoft Container Registry (MCR).
Container images run in Docker.
You can use either the REST API or SDK to call operations in Text Analytics for health
containers by specifying the host URI of the container.
You must specify billing information when instantiating a container.
ï¼‰ Important
Azure AI containers aren't licensed to run without being connected to Azure for metering.
Customers must ensure that the containers are always able to communicate billing
information to the metering service. Azure AI containers don't send customer data (for
example, text that is being analyzed) to Microsoft.

Next steps
See Configure containers for configuration settings.

Last updated on 11/18/2025

Configure Text Analytics for health docker
containers
Text Analytics for health provides each container with a common configuration framework, so
that you can easily configure and manage storage, logging and telemetry, and security settings
for your containers. Several example docker run commands are also available.

Configuration settings
The container has the following configuration settings:
ï¾‰

Expand table

Required

Setting

Purpose

Yes

ApiKey

Tracks billing information.

No

ApplicationInsights

Enables adding Azure Application Insights telemetry support to your
container.

Yes

Billing

Specifies the endpoint URI of the service resource on Azure.

Yes

Eula

Indicates that you've accepted the license for the container.

No

Fluentd

Writes log and, optionally, metric data to a Fluentd server.

No

HTTP Proxy

Configures an HTTP proxy for making outbound requests.

No

Logging

Provides ASP.NET Core logging support for your container.

No

Mounts

Reads and writes data from the host computer to the container and
from the container back to the host computer.

ï¼‰ Important
The ApiKey, Billing, and Eula settings are used together, and you must provide valid
values for all three of them; otherwise your container won't start. For more information
about using these configuration settings to instantiate a container, see Billing.

ApiKey configuration setting
The ApiKey setting specifies the Azure resource key used to track billing information for the
container. You must specify a value for the ApiKey and the value must be a valid key for the

Language resource specified for the Billing configuration setting.
This setting can be found in the following place:
Azure portal: Language resource management, under Keys and endpoint

ApplicationInsights setting
The ApplicationInsights setting allows you to add Azure Application Insights telemetry
support to your container. Application Insights provides in-depth monitoring of your container.
You can easily monitor your container for availability, performance, and usage. You can also
quickly identify and diagnose errors in your container.
The following table describes the configuration settings supported under the
ApplicationInsights section.

ï¾‰

Required

Name

Data

Expand table

Description

type
No

InstrumentationKey

String

The instrumentation key of the Application Insights instance
to which telemetry data for the container is sent. For more
information, see Application Insights for ASP.NET Core.
Example:
InstrumentationKey=123456789

Billing configuration setting
The Billing setting specifies the endpoint URI of the Language resource on Azure used to
meter billing information for the container. You must specify a value for this configuration
setting, and the value must be a valid endpoint URI for a Language resource on Azure. The
container reports usage about every 10 to 15 minutes.
This setting can be found in the following place:
Azure portal: Language Overview, labeled Endpoint
ï¾‰

Expand table

Required

Name

Data

Description

type
Yes

Billing

String

Billing endpoint URI. For more information on obtaining the billing URI,
see gather required parameters. For more information and a complete
list of regional endpoints, see Custom subdomain names for Foundry
Tools.

Eula setting
The Eula setting indicates that you've accepted the license for the container. You must specify
a value for this configuration setting, and the value must be set to accept .
ï¾‰

Required

Name

Data type

Description

Yes

Eula

String

License acceptance

Expand table

Example:
Eula=accept

Foundry Tools containers are licensed under your agreement

governing your use of Azure. If

you do not have an existing agreement governing your use of Azure, you agree that your
agreement governing use of Azure is the Microsoft Online Subscription Agreement
incorporates the Online Services Terms

, which

. For previews, you also agree to the Supplemental

Terms of Use for Microsoft Azure Previews

. By using the container you agree to these terms.

Fluentd settings
Fluentd is an open-source data collector for unified logging. The Fluentd settings manage the
container's connection to a Fluentd

server. The container includes a Fluentd logging

provider, which allows your container to write logs and, optionally, metric data to a Fluentd
server.
The following table describes the configuration settings supported under the Fluentd section.
ï¾‰

Expand table

Name

Data
type

Description

Host

String

The IP address or DNS host name of the Fluentd
server.

Port

Integer

The port of the Fluentd server.
The default value is 24224.

HeartbeatMs

Integer

The heartbeat interval, in milliseconds. If no event
traffic has been sent before this interval expires, a
heartbeat is sent to the Fluentd server. The default
value is 60000 milliseconds (1 minute).

Integer

SendBufferSize

The network buffer space, in bytes, allocated for send
operations. The default value is 32768 bytes (32
kilobytes).

TlsConnectionEstablishmentTimeoutMs

Integer

The timeout, in milliseconds, to establish a SSL/TLS
connection with the Fluentd server. The default value
is 10000 milliseconds (10 seconds).
If UseTLS is set to false, this value is ignored.

Boolean

UseTLS

Indicates whether the container should use SSL/TLS
for communicating with the Fluentd server. The
default value is false.

Http proxy credentials settings
If you need to configure an HTTP proxy for making outbound requests, use these two
arguments:
ï¾‰

Name

Data
type

Description

HTTP_PROXY

string

The proxy to use, for example, http://proxy:8888

Expand table

<proxy-url>

HTTP_PROXY_CREDS

string

Any credentials needed to authenticate against the proxy, for example,
username:password . This value must be in lower-case.

<proxy-user>

string

The user for the proxy.

<proxy-password>

string

The password associated with <proxy-user> for the proxy.

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
HTTP_PROXY=<proxy-url> \
HTTP_PROXY_CREDS=<proxy-user>:<proxy-password> \

Logging settings
The Logging settings manage ASP.NET Core logging support for your container. You can use
the same configuration settings and values for your container that you use for an ASP.NET
Core application.
The following logging providers are supported by the container:
ï¾‰

Expand table

Provider

Purpose

Console

The ASP.NET Core Console logging provider. All of the ASP.NET Core configuration settings
and default values for this logging provider are supported.

Debug

The ASP.NET Core Debug logging provider. All of the ASP.NET Core configuration settings and
default values for this logging provider are supported.

Disk

The JSON logging provider. This logging provider writes log data to the output mount.

This container command stores logging information in the JSON format to the output mount:
Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
--mount type=bind,src=/home/azureuser/output,target=/output \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Disk:Format=json \
Mounts:Output=/output

This container command shows debugging information, prefixed with dbug , while the
container is running:

Bash
docker run --rm -it -p 5000:5000 \
--memory 2g --cpus 1 \
<registry-location>/<image-name> \
Eula=accept \
Billing=<endpoint> \
ApiKey=<api-key> \
Logging:Console:LogLevel:Default=Debug

Disk logging
The Disk logging provider supports the following configuration settings:
ï¾‰

Expand table

Name

Data
type

Description

Format

String

The output format for log files.
Note: This value must be set to json to enable the logging provider. If this
value is specified without also specifying an output mount while instantiating a
container, an error occurs.

MaxFileSize

Integer

The maximum size, in megabytes (MB), of a log file. When the size of the current
log file meets or exceeds this value, a new log file is started by the logging
provider. If -1 is specified, the size of the log file is limited only by the maximum
file size, if any, for the output mount. The default value is 1.

For more information about configuring ASP.NET Core logging support, see Settings file
configuration.

Mount settings
Use bind mounts to read and write data to and from the container. You can specify an input
mount or output mount by specifying the --mount option in the docker run

command.

Text Analytics for health containers don't use input or output mounts to store training or
service data.
The exact syntax of the host mount location varies depending on the host operating system.
Additionally, the host computer's mount location may not be accessible due to a conflict
between permissions used by the docker service account and the host mount location
permissions.

ï¾‰

Expand table

Optional

Name

Data
type

Description

Not
allowed

Input

String

Text Analytics for health containers do not use this.

Optional

Output

String

The target of the output mount. The default value is /output . This is the
location of the logs. This includes container logs.
Example:
--mount type=bind,src=c:\output,target=/output

Next steps
Review How to install and run containers
Use more Azure AI containers

Last updated on 11/18/2025

Deploy and run containers on Azure
Container Instance
With the following steps, scale Foundry Tools applications in the cloud easily with Azure
Container Instances. Containerization helps you focus on building your applications instead of
managing the infrastructure. For more information on using containers, see features and
benefits.

Prerequisites
The recipe works with any Foundry Tools container. The Foundry resource must be created
before using the recipe. Each Foundry Tool that supports containers has a "How to install"
article for installing and configuring the service for a container. Some services require a file or
set of files as input for the container, it is important that you understand and have used the
container successfully before using this solution.
An Azure resource for the Foundry Tool that you're using.
Azure resource endpoint URL - review your specific service's "How to install" for the
container, to find where the endpoint URL is from within the Azure portal, and what a
correct example of the URL looks like. The exact format can change from service to
service.
Azure resource key - the keys are on the Keys page for the Azure resource. You only need
one of the two keys. The key is a string of 84 alpha-numeric characters.
A single Foundry Tools container on your local host (your computer). Make sure you can:
Pull down the image with a docker pull command.
Run the local container successfully with all required configuration settings with a
docker run command.

Call the container's endpoint, getting a response of HTTP 2xx and a JSON response
back.
All variables in angle brackets, <> , need to be replaced with your own values. This replacement
includes the angle brackets.
ï¼‰ Important
The LUIS container requires a .gz model file that is pulled in at runtime. The container
must be able to access this model file via a volume mount from the container instance. To
upload a model file, follow these steps:

1. Create an Azure file share. Take note of the Azure Storage account name, key, and
file share name as you'll need them later.
2. export your LUIS model (packaged app) from the LUIS portal.
3. In the Azure portal, navigate to the Overview page of your storage account resource,
and select File shares.
4. Select the file share name that you recently created, then select Upload. Then upload
your packaged app.

Azure portal

Create an Azure Container Instance resource
using the Azure portal
1. Go to the Create

page for Container Instances.

2. On the Basics tab, enter the following details:
ï¾‰

Expand table

Setting

Value

Subscription

Select your subscription.

Resource

Select the available resource group or create a new one such as cognitive-

group

services .

Container

Enter a name such as cognitive-container-instance . The name must be in

name

lower caps.

Location

Select a region for deployment.

Image type

If your container image is stored in a container registry that doesnâ€™t require
credentials, choose Public . If accessing your container image requires
credentials, choose Private . Refer to container repositories and images for
details on whether or not the container image is Public or Private ("Public
Preview").

Image name

Enter the Foundry Tools container location. The location is what's used as an
argument to the docker pull command. Refer to the container repositories
and images for the available image names and their corresponding repository.
The image name must be fully qualified specifying three parts. First, the
container registry, then the repository, finally the image name: <container-

Setting

Value
registry>/<repository>/<image-name> .

Here is an example, mcr.microsoft.com/azure-cognitive-services/keyphrase
would represent the Key Phrase Extraction image in the Microsoft Container
Registry under the Foundry Tools repository. Another example is,
containerpreview.azurecr.io/microsoft/cognitive-services-speech-to-text

which would represent the Speech to text image in the Microsoft repository of
the Container Preview container registry.
OS type

Linux

Size

Change size to the suggested recommendations for your specific Azure AI
container:
2 CPU cores
4 GB

3. On the Networking tab, enter the following details:
ï¾‰

Setting

Value

Ports

Set the TCP port to 5000 . Exposes the container on port 5000.

Expand table

4. On the Advanced tab, enter the required Environment Variables for the container
billing settings of the Azure Container Instance resource:
ï¾‰

Expand table

Key

Value

ApiKey

Copied from the Keys and endpoint page of the resource. It is a 84 alphanumericcharacter string with no spaces or dashes, xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx .

Billing

Your endpoint URL copied from the Keys and endpoint page of the resource.

Eula

accept

5. Select Review and Create
6. After validation passes, click Create to finish the creation process
7. When the resource is successfully deployed, it's ready

Use the Container Instance
Azure portal

1. Select the Overview and copy the IP address. It will be a numeric IP address such as
55.55.55.55 .

2. Open a new browser tab and use the IP address, for example, http://<IPaddress>:5000 (http://55.55.55.55:5000 ). You will see the container's home page,

letting you know the container is running.

3. Select Service API Description to view the swagger page for the container.
4. Select any of the POST APIs and select Try it out. The parameters are displayed
including the input. Fill in the parameters.
5. Select Execute to send the request to your Container Instance.
You have successfully created and used Azure AI containers in Azure Container
Instance.

Last updated on 11/18/2025

What are Azure containers?
Foundry Tools provide several Docker containers

that let you use the same APIs that are

available in Azure, on-premises. Using these containers gives you the flexibility to bring
Foundry Tools closer to your data for compliance, security or other operational reasons.
Container support is currently available for a subset of Foundry Tools.
https://www.youtube-nocookie.com/embed/hdfbn4Q8jbo
Containerization is an approach to software distribution in which an application or service,
including its dependencies & configuration, is packaged together as a container image. With
little or no modification, a container image can be deployed on a container host. Containers
are isolated from each other and the underlying operating system, with a smaller footprint than
a virtual machine. Containers can be instantiated from container images for short-term tasks,
and removed when no longer needed.

Features and benefits
Immutable infrastructure: Enable DevOps teams to leverage a consistent and reliable set
of known system parameters, while being able to adapt to change. Containers provide the
flexibility to pivot within a predictable ecosystem and avoid configuration drift.
Control over data: Choose where your data gets processed by Foundry Tools. This can be
essential if you can't send data to the cloud but need access to Foundry Tools APIs.
Support consistency in hybrid environments â€“ across data, management, identity, and
security.
Control over model updates: Flexibility in versioning and updating of models deployed in
their solutions.
Portable architecture: Enables the creation of a portable application architecture that can
be deployed on Azure, on-premises and the edge. Containers can be deployed directly to
Azure Kubernetes Service, Azure Container Instances, or to a Kubernetes

cluster

deployed to Azure Stack. For more information, see Deploy Kubernetes to Azure Stack.
High throughput / low latency: Provide customers the ability to scale for high
throughput and low latency requirements by enabling Foundry Tools to run physically
close to their application logic and data. Containers don't cap transactions per second
(TPS) and can be made to scale both up and out to handle demand if you provide the
necessary hardware resources.
Scalability: With the ever growing popularity of containerization and container
orchestration software, such as Kubernetes; scalability is at the forefront of technological
advancements. Building on a scalable cluster foundation, application development caters
to high availability.

Containers in Foundry Tools
Azure containers provide the following set of Docker containers, each of which contains a
subset of functionality from services in Foundry Tools. You can find instructions and image
locations in the tables below.
ï¼— Note
See Install and run Document Intelligence containers for Azure Document Intelligence
container instructions and image locations.

Decision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Anomaly

Anomaly

The Anomaly Detector API enables you to monitor

Generally

detector

Detector

and detect abnormalities in your time series data with

available

(image

)

machine learning.

Language containers
ï¾‰

Service

Container

LUIS

LUIS (image

)

Expand table

Description

Availability

Loads a trained or published Language

Generally

Understanding model, also known as a LUIS app,

available.

into a docker container and provides access to
the query predictions from the container's API
endpoints. You can collect query logs from the
container and upload these back to the LUIS
portal to improve the app's prediction
accuracy.
Language

Key Phrase

Extracts key phrases to identify the main points.

Generally

Extraction

For example, for the input text "The food was

available.

(image

delicious and there were wonderful staff," the

This container can

API returns the main talking points: "food" and
"wonderful staff".

also run in
disconnected

)

environments.

Service

Container

Description

Availability

Language

Text Language

For up to 120 languages, detects which

Generally

language the input text is written in and report a

available.

single language code for every document
submitted on the request. The language code is

This container can
also run in

paired with a score indicating the strength of the

disconnected

score.

environments.

Sentiment Analysis

Analyzes raw text for clues about positive or

Generally

(image

negative sentiment. This version of sentiment
analysis returns sentiment labels (for example

available.
This container can

positive or negative) for each document and

also run in

sentence within it.

disconnected
environments.

Detection (image

Language

)

)

Language

Text Analytics for
health (image )

Extract and label medical information from
unstructured clinical text.

Generally available

Language

Named Entity
Recognition

Extract named entities from text.

Generally
available.

(image

)

This container can
also run in
disconnected
environments.

Language

Personally

Detect and redact personally identifiable

Generally

Identifiable

information entities from text.

available.

Information (PII)
detection (image

This container can
also run in

)

disconnected
environments.
Language

Custom Named
Entity Recognition
(image )

Extract named entities from text, using a custom
model you create using your data.

Generally available

Language

Summarization

Summarize text from various sources.

Public preview.

(image

)

This container can
also run in
disconnected
environments.

Language

Conversational
Language
Understanding
(image )

Interpret conversational language.

Generally
Available.
This container can
also run in
disconnected
environments.

Service

Container

Description

Availability

Translator

Translator
(image )

Translate text in several languages and dialects.

Generally
available. Gated request access .
This container can
also run in
disconnected
environments.

Speech containers
ï¾‰

Expand table

Service

Container

Description

Availability

Speech
API

Speech to text
(image )

Transcribes continuous real-time speech
into text.

Generally available.
This container can also
run in disconnected
environments.

Speech
API

Custom Speech to
text (image )

Transcribes continuous real-time speech
into text using a custom model.

Generally available
This container can also
run in disconnected
environments.

Speech
API

Neural Text to
speech (image

)

Speech

Speech language

API

identification
(image )

Converts text to natural-sounding speech
using deep neural network technology,

Generally available.
This container can also

allowing for more natural synthesized
speech.

run in disconnected
environments.

Determines the language of spoken audio.

Preview

Vision containers
ï¾‰

Expand table

Service

Container

Description

Availability

Vision

Read OCR

The Read OCR container allows you to extract

Generally Available.

(image

printed and handwritten text from images and
documents with support for JPEG, PNG, BMP, PDF,
and TIFF file formats. For more information, see the

This container can
also run in
disconnected

Read API documentation.

environments.

)

Service

Container

Description

Availability

Spatial
Analysis

Spatial
analysis

Analyzes real-time streaming video to understand
spatial relationships between people, their

Preview

(image

)

movement, and interactions with objects in physical
environments.

Additionally, some containers are supported in the Foundry Tools multi-service resource
offering. You can create one single Foundry Tools resource and use the same billing key across
supported services for the following services:
Vision
LUIS
Language

Prerequisites
You must satisfy the following prerequisites before using Azure containers:
Docker Engine: You must have Docker Engine installed locally. Docker provides packages that
configure the Docker environment on macOS , Linux , and Windows

. On Windows,

Docker must be configured to support Linux containers. Docker containers can also be
deployed directly to Azure Kubernetes Service or Azure Container Instances.
Docker must be configured to allow the containers to connect with and send billing data to
Azure.
Familiarity with Microsoft Container Registry and Docker: You should have a basic
understanding of both Microsoft Container Registry and Docker concepts, like registries,
repositories, containers, and container images, as well as knowledge of basic docker
commands.
For a primer on Docker and container basics, see the Docker overview

.

Individual containers can have their own requirements, as well, including server and memory
allocation requirements.

Foundry Tools container security
Security should be a primary focus whenever you're developing applications. The importance
of security is a metric for success. When you're architecting a software solution that includes
Azure AI containers, it's vital to understand the limitations and capabilities available to you. For
more information about network security, see Configure Foundry Tools virtual networks.

ï¼‰ Important
By default there is no security on the Foundry Tools container API. The reason for this is
that most often the container will run as part of a pod which is protected from the outside
by a network bridge. However, it is possible for users to construct their own authentication
infrastructure to approximate the authentication methods used when accessing the cloudbased Foundry Tools.
The following diagram illustrates the default and non-secure approach:

As an example of an alternative and secure approach, consumers of Azure AI containers could
augment a container with a front-facing component, keeping the container endpoint private.
Let's consider a scenario where we use Istio

as an ingress gateway. Istio supports HTTPS/TLS

and client-certificate authentication. In this scenario, the Istio frontend exposes the container
access, presenting the client certificate that is approved beforehand with Istio.
Nginx

is another popular choice in the same category. Both Istio and Nginx act as a service

mesh and offer additional features including things like load-balancing, routing, and ratecontrol.

Container networking
The Azure AI containers are required to submit metering information for billing purposes.
Failure to allowlist various network channels that the Azure AI containers rely on will prevent

the container from working.

Allowlist Foundry Tools domains and ports
The host should allowlist port 443 and the following domains:
*.cognitive.microsoft.com
*.cognitiveservices.azure.com

If you are using the Azure Translator in Foundry Tools on-premise, you need to additionally
allow the following URLs to download files
translatoronprem.blob.core.windows.net

Disable deep packet inspection
Deep packet inspection (DPI)

is a type of data processing that inspects in detail the data sent

over a computer network, and usually takes action by blocking, rerouting, or logging it
accordingly.
Disable DPI on the secure channels that the Azure AI containers create to Microsoft servers.
Failure to do so will prevent the container from functioning correctly.

Developer samples
Developer samples are available at our GitHub repository .

Next steps
Learn about container recipes you can use with the Foundry Tools.
Install and explore the functionality provided by containers in Foundry Tools:
Anomaly Detector containers
Vision containers
Language Understanding (LUIS) containers
Speech API containers
Language containers
Translator containers

Last updated on 11/18/2025

Supported Text Analytics for health entity
categories
Text Analytics for health processes and extracts insights from unstructured medical data. The
service detects and surfaces medical concepts, assigns assertions to concepts, infers semantic
relations between concepts and links them to common medical ontologies.
Text Analytics for health detects medical concepts that fall under the following categories.

Anatomy
Entities
BODY_STRUCTURE - Body systems, anatomic locations or regions, and body sites. For
example, arm, knee, abdomen, nose, liver, head, respiratory system, lymphocytes.

ï Š

An example of the body structure entity.

Demographics
Entities
AGE - All age terms and phrases, including ones for patients, family members, and others. For
example, 40-year-old, 51 yo, 3 months old, adult, infant, elderly, young, minor, middle-aged.
ETHNICITY - Phrases that indicate the ethnicity of the subject. For example, African American
or Asian.

ï Š

GENDER - Terms that disclose the gender of the subject. For example, male, female, woman,
gentleman, lady.

ï Š

Examinations

Entities
EXAMINATION_NAME â€“ Diagnostic procedures and tests, including vital signs and body
measurements. For example, MRI, ECG, HIV test, hemoglobin, platelets count, scale systems
such as Bristol stool scale.

ï Š

External Influence
Entities
ALLERGEN â€“ an antigen triggering an allergic reaction. For example, cats, peanuts.

ï Š

General attributes
Entities
COURSE - Description of a change in another entity over time, such as condition progression
(for example: improvement, worsening, resolution, remission), a course of treatment or
medication (for example: increase in medication dosage).

ï Š

DATE - Full date relating to a medical condition, examination, treatment, medication, or
administrative event.

ï Š

DIRECTION â€“ Directional terms that may relate to a body structure, medical condition,
examination, or treatment, such as: left, lateral, upper, posterior.

ï Š

FREQUENCY - Describes how often a medical condition, examination, treatment, or medication
occurred, occurs, or should occur.

ï Š

TIME - Temporal terms relating to the beginning and/or length (duration) of a medical
condition, examination, treatment, medication, or administrative event.
MEASUREMENT_UNIT â€“ The unit of measurement related to an examination or a medical
condition measurement.
MEASUREMENT_VALUE â€“ The value related to an examination or a medical condition
measurement.

ï Š

RELATIONAL_OPERATOR - Phrases that express the quantitative relation between an entity
and some additional information.

ï Š

Genomics

Entities
VARIANT - All mentions of gene variations and mutations. For example, c.524C>T ,
(MTRR):r.1462_1557del96

GENE_OR_PROTEIN â€“ All mentions of names and symbols of human genes as well as
chromosomes and parts of chromosomes and proteins. For example, MTRR, F2.
MUTATION_TYPE - Description of the mutation, including its type, effect, and location. For
example, trisomy, germline mutation, loss of function.

ï Š

EXPRESSION - Gene expression level. For example, positive for-, negative for-, overexpressed,
detected in high/low levels, elevated.

ï Š

Healthcare
Entities
ADMINISTRATIVE_EVENT â€“ Events that relate to the healthcare system but of an
administrative/semi-administrative nature. For example, registration, admission, trial, study
entry, transfer, discharge, hospitalization, hospital stay.
CARE_ENVIRONMENT â€“ An environment or location where patients are given care. For
example, emergency room, physicianâ€™s office, cardio unit, hospice, hospital.

ï Š

An example of a healthcare event entity.
HEALTHCARE_PROFESSION â€“ A healthcare practitioner licensed or non-licensed. For example,
dentist, pathologist, neurologist, radiologist, pharmacist, nutritionist, physical therapist,
chiropractor.

ï Š

Medical condition
Entities
DIAGNOSIS â€“ Disease, syndrome, poisoning. For example, breast cancer, Alzheimerâ€™s, HTN,
CHF, spinal cord injury.
SYMPTOM_OR_SIGN â€“ Subjective or objective evidence of disease or other diagnoses. For
example, chest pain, headache, dizziness, rash, SOB, abdomen was soft, good bowel sounds,
well nourished.

ï Š

CONDITION_QUALIFIER - Qualitative terms that are used to describe a medical condition. All
the following subcategories are considered qualifiers:
Time-related expressions: those are terms that describe the time dimension qualitatively,
such as sudden, acute, chronic, longstanding.
Quality expressions: Those are terms that describe the â€œnatureâ€ of the medical condition,
such as burning, sharp.
Severity expressions: severe, mild, a bit, uncontrolled.
Extensivity expressions: local, focal, diffuse.

ï Š

CONDITION_SCALE â€“ Qualitative terms that characterize the condition by a scale, which is a
finite ordered list of values.

ï Š

Medication
Entities
MEDICATION_CLASS â€“ A set of medications that have a similar mechanism of action, a related
mode of action, a similar chemical structure, and/or are used to treat the same disease. For
example, ACE inhibitor, opioid, antibiotics, pain relievers.

ï Š

MEDICATION_NAME â€“ Medication mentions, including copyrighted brand names, and nonbrand names. For example, Ibuprofen.
DOSAGE - Amount of medication ordered. For example, Infuse Sodium Chloride solution 1000
mL.
MEDICATION_FORM - The form of the medication. For example, solution, pill, capsule, tablet,
patch, gel, paste, foam, spray, drops, cream, syrup.

ï Š

MEDICATION_ROUTE - The administration method of medication. For example, oral, topical,
inhaled.

ï Š

Social
Entities
FAMILY_RELATION â€“ Mentions of family relatives of the subject. For example, father, daughter,
siblings, parents.

ï Š

EMPLOYMENT â€“ Mentions of employment status including specific profession, such as
unemployed, retired, firefighter, student.

ï Š

LIVING_STATUS â€“ Mentions of the housing situation, including homeless, living with parents,
living alone, living with others.

ï Š

SUBSTANCE_USE â€“ Mentions of use of legal or illegal drugs, tobacco or alcohol. For example,
smoking, drinking, or heroin use.

ï Š

SUBSTANCE_USE_AMOUNT â€“ Mentions of specific amounts of substance use. For example, a
pack (of cigarettes) or a few glasses (of wine).

ï Š

Treatment
Entities

TREATMENT_NAME â€“ Therapeutic procedures. For example, knee replacement surgery, bone
marrow transplant, TAVI, diet.

ï Š

Next steps
How to call the Text Analytics for health

Last updated on 11/18/2025

Relation extraction
Text Analytics for health features relation extraction, which is used to identify meaningful
connections between concepts, or entities, mentioned in the text. For example, a "time of
condition" relation is found by associating a condition name with a time. Another example is a
"dosage of medication" relation, which is found by relating an extracted medication to its
extracted dosage. The following example shows how relations are expressed in the JSON
output.
ï¼— Note
Relations referring to CONDITION may refer to either the DIAGNOSIS entity type or
the SYMPTOM_OR_SIGN entity type.
Relations referring to MEDICATION may refer to either the MEDICATION_NAME
entity type or the MEDICATION_CLASS entity type.
Relations referring to TIME may refer to either the TIME entity type or the DATE
entity type.
Relation extraction output contains URI references and assigned roles of the entities of the
relation type. For example, in the following JSON:
JSON
"relations": [
{
"relationType": "DosageOfMedication",
"entities": [
{
"ref": "#/results/documents/0/entities/0",
"role": "Dosage"
},
{
"ref": "#/results/documents/0/entities/1",
"role": "Medication"
}
]
},
{
"relationType": "RouteOfMedication",
"entities": [
{
"ref": "#/results/documents/0/entities/1",
"role": "Medication"
},
{
"ref": "#/results/documents/0/entities/2",

"role": "Route"
}
]
}
]

Recognized relations
The following list presents all the recognized relations by the Text Analytics for health API.
ABBREVIATION
AMOUNT_OF_SUBSTANCE_USE
BODY_SITE_OF_CONDITION
BODY_SITE_OF_TREATMENT
COURSE_OF_CONDITION
COURSE_OF_EXAMINATION
COURSE_OF_MEDICATION
COURSE_OF_TREATMENT
DIRECTION_OF_BODY_STRUCTURE
DIRECTION_OF_CONDITION
DIRECTION_OF_EXAMINATION
DIRECTION_OF_TREATMENT
DOSAGE_OF_MEDICATION
EXAMINATION_FINDS_CONDITION
EXPRESSION_OF_GENE
EXPRESSION_OF_VARIANT
FORM_OF_MEDICATION
FREQUENCY_OF_CONDITION
FREQUENCY_OF_MEDICATION
FREQUENCY_OF_SUBSTANCE_USE

FREQUENCY_OF_TREATMENT
MUTATION_TYPE_OF_GENE
MUTATION_TYPE_OF_VARIANT
QUALIFIER_OF_CONDITION
RELATION_OF_EXAMINATION
ROUTE_OF_MEDICATION
SCALE_OF_CONDITION
TIME_OF_CONDITION
TIME_OF_EVENT
TIME_OF_EXAMINATION
TIME_OF_MEDICATION
TIME_OF_TREATMENT
UNIT_OF_CONDITION
UNIT_OF_EXAMINATION
VALUE_OF_CONDITION
VALUE_OF_EXAMINATION
VARIANT_OF_GENE

Next steps
How to call the Text Analytics for health

Last updated on 11/18/2025

Assertion detection
The meaning of medical content is highly affected by modifiers, such as negative or conditional
assertions, which can have critical implications if misrepresented. Text Analytics for health
supports four categories of assertion detection for entities in the text:
Certainty
Conditional
Association
Temporal

Assertion output
Text Analytics for health returns assertion modifiers, which are informative attributes assigned
to medical concepts that provide a deeper understanding of the conceptsâ€™ context within the
text. These modifiers are divided into four categories, each focusing on a different aspect and
containing a set of mutually exclusive values. Only one value per category is assigned to each
entity. The most common value for each category is the Default value. The serviceâ€™s output
response contains only assertion modifiers that are different from the default value. In other
words, if no assertion is returned, the implied assertion is the default value.
CERTAINTY â€“ provides information regarding the presence (present vs. absent) of the concept
and how certain the text is regarding its presence (definite vs. possible).
Positive [Default]: the concept exists or has happened.
Negative: the concept does not exist now or never happened.
Positive_Possible: the concept likely exists but there is some uncertainty.
Negative_Possible: the conceptâ€™s existence is unlikely but there is some uncertainty.
Neutral_Possible: the concept may or may not exist without a tendency to either side.
An example of assertion detection is shown below where a negated entity is returned with a
negative value for the certainty category:
JSON
{
"offset": 381,
"length": 3,
"text": "SOB",
"category": "SymptomOrSign",
"confidenceScore": 0.98,
"assertion": {
"certainty": "negative"
},
"name": "Dyspnea",

"links": [
{
"dataSource": "UMLS",
"id": "C0013404"
},
{
"dataSource": "AOD",
"id": "0000005442"
},
...
}

CONDITIONALITY â€“ provides information regarding whether the existence of a concept
depends on certain conditions.
None [Default]: the concept is a fact and not hypothetical and does not depend on
certain conditions.
Hypothetical: the concept may develop or occur in the future.
Conditional: the concept exists or occurs only under certain conditions.
ASSOCIATION â€“ describes whether the concept is associated with the subject of the text or
someone else.
Subject [Default]: the concept is associated with the subject of the text, usually the
patient.
Other: the concept is associated with someone who is not the subject of the text.
TEMPORAL - provides additional temporal information for a concept detailing whether it is an
occurrence related to the past, present, or future.
Current [Default]: the concept is related to conditions/events that belong to the current
encounter. For example, medical symptoms that have brought the patient to seek medical
attention (e.g., â€œstarted having headaches 5 days prior to their arrival to the ERâ€). This
includes newly made diagnoses, symptoms experienced during or leading to this
encounter, treatments and examinations done within the encounter.
Past: the concept is related to conditions, examinations, treatments, medication events
that are mentioned as something that existed or happened prior to the current
encounter, as might be indicated by hints like s/p, recently, ago, previously, in childhood,
at age X. For example, diagnoses that were given in the past, treatments that were done,
past examinations and their results, past admissions, etc. Medical background is
considered as PAST.
Future: the concept is related to conditions/events that are planned/scheduled/suspected
to happen in the future, e.g., will be obtained, will undergo, is scheduled in two weeks
from now.

Next steps
How to call the Text Analytics for health

Last updated on 11/18/2025

Utilizing Fast Healthcare Interoperability
Resources (FHIR) structuring in Text
Analytics for Health
When you process unstructured data using Text Analytics for health, you can request that the
output response includes a Fast Healthcare Interoperability Resources (FHIR) resource bundle.
The FHIR resource bundle output is enabled by passing the FHIR version as part of the options
in each request. How you pass the FHIR version differs depending on whether you're using the
SDK or the REST API.

Use the REST API
When you use the REST API as part of building the request payload, you include a Tasks object.
Each of the Tasks can have parameters. One of the options for parameters is fhirVersion . By
including the fhirVersion parameter in the Task object parameters, you're requesting the
output to include a FHIR resource bundle in addition to the normal Text Analytics for health
output. The following example shows the inclusion of fhirVersion in the request parameters.
JSON
{
"analysis input": {
"documents:"[
{
text:"54 year old patient had pain in the left elbow with no relief
from 100 mg Ibuprofen",
"language":"en",
"id":"1"
}
]
},
"tasks"[
{
"taskId":"analyze 1",
"kind":"Healthcare",
"parameters":
{
"fhirVersion":"4.0.1"
}
}
]
}

Once the request completes processing by Text Analytics for health and you pull the response
from the REST API, you can find the FHIR resource bundle in the output. You can locate the

FHIR resource bundle inside each document processed using the property name fhirBundle .
The following partial sample is output highlighting the fhirBundle .
JSON
{
"jobID":"50d11b05-7a03-a611-6f1e95ebde07",
"lastUpdatedDateTime":"2024-06-05T17:29:51Z",
"createdDateTime:"2024-06-05T17:29:40Z",
"expirationDateTime":"2024-06-05T17:29:40Z",
"status":"succeeded",
"errors":[],
"tasks":{
"completed": 1,
"failed": 0,
"inProgress": 0,
"total": 1,
"items": [
{
"kind":"HealthcareLROResults",
"lastUpdatedDateTime":"2024-06-05T17:29:51.5839858Z",
"status":"succeeded",
"results": {
"documents": [
{
"id": "1",
"entities": [...
],
"relations": [...
].
"warnings":[],
"fhirBundle": {
"resourceType": "Bundle",
"id": "b4d907ed-0334-4186-9e21-8ed4d79e709f",
"meta": {
"profile": [
"http://hl7.org/fhir/4.0.1/StructureDefinition/Bundle"
]
},

Use the REST SDK
You can also use the SDK to make the request for Text Analytics for health to include the FHIR
resource bundle in the output. To accomplish this request with the SDK, you would create an
instance of AnalyzeHealthcareEntitiesOptions and populate the FhirVersion property with the
FHIR version. This options object is then passed to each StartAnalyzeHealthcareEntitiesAsync
method call to configure the request to include a FHIR resource bundle in the output.

Next steps
How to call the Text Analytics for health

Last updated on 12/05/2025

SDK and REST developer guide for Azure
Language
Use this article to find information on integrating Azure Language SDK and REST API into your
applications.

Development options
The Language provides support through a REST API, and client libraries in several languages.
Client library (Azure SDK)

Client libraries (Azure SDK)
The Language provides three namespaces for using the available features. Depending on
which features and programming language you're using, you need to download one or
more of the following packages, and have the following framework/language version
support:
ï¾‰

Expand table

Framework/Language

Minimum supported version

.NET

.NET Framework 4.6.1 or newer, or .NET (formerly .NET Core) 2.0 or
newer.

Java

v8 or later

JavaScript

v14 LTS or later

Python

v3.7 or later

Azure.AI.TextAnalytics
ï¼— Note
If you're using custom named entity recognition or custom text classification, you
need to create a project and train a model before using the SDK. The SDK only allows
for you to analyze text using models you create. See the following quickstarts for
information on creating a model.

Custom named entity recognition
Custom text classification
The Azure.AI.TextAnalytics namespace enables you to use the following Language
features. Use the following links for articles to help you send API requests using the SDK.
Custom named entity recognition
Custom text classification
Document summarization
Entity linking
Key phrase extraction
Named entity recognition (NER)
Personally Identifying Information (PII) detection
Sentiment analysis and opinion mining
Text analytics for health
As you use these features in your application, use the following documentation and code
samples for additional information.
ï¾‰

Expand table

Language â†’ Latest GA version

Reference documentation

Samples

C#/.NET â†’ v5.2.0

C# documentation

C# samples

Java â†’ v5.2.0

Java documentation

Java Samples

JavaScript â†’ v1.0.0

JavaScript documentation

JavaScript samples

Python â†’ v5.2.0

Python documentation

Python samples

Azure.AI.Language.Conversations
ï¼— Note
If you're using conversational language understanding or orchestration workflow, you
need to create a project and train a model before using the SDK. The SDK only allows
you to analyze text using models you create. For more information, see:
Conversational language understanding
Orchestration workflow

The Azure.AI.Language.Conversations namespace enables you to use the following
Language features. Use the following links for articles to help you send API requests using
the SDK.
Conversational language understanding
Orchestration workflow
Conversation summarization (Python only)
Personally Identifying Information (PII) detection for conversations
As you use these features in your application, use the following documentation and code
samples for additional information.
ï¾‰

Expand table

Language â†’ Latest GA version

Reference documentation

Samples

C#/.NET â†’ v1.0.0

C# documentation

C# samples

Python â†’ v1.0.0

Python documentation

Python samples

Azure.AI.Language.QuestionAnswering
The Azure.AI.Language.QuestionAnswering namespace enables you to use the following
Language features:
Question answering
Authoring - Automate common tasks like adding new question answer pairs and
working with projects/knowledge bases.
Prediction - Answer questions based on passages of text.
As you use these features in your application, use the following documentation and code
samples for additional information.
ï¾‰

Expand table

Language â†’ Latest GA version

Reference documentation

Samples

C#/.NET â†’ v1.0.0

C# documentation

C# samples

Python â†’ v1.0.0

Python documentation

Python samples

See also

Azure Language in Foundry Tools overview

Last updated on 11/18/2025

Azure Language tools and agents
Azure Language integrates with Foundry Tools to provide agents and endpoints for building
conversational applications. These tools combine Azure Language's natural language
processing capabilities with AI agent frameworks.

Azure Language MCP server ðŸ†•
The Azure Language MCP server in Foundry

connects AI agents to Azure Language services

through the Model Context Protocol. This integration enables developers to build
conversational applications with natural language processing while maintaining compliance
and transparency.
The server transforms Azure Language services into agent-friendly endpoints that support realtime workflows. Implementing standard MCP protocols ensures consistent communication
between AI agents and language services.

Core capabilities
Language processing: Access to Azure Language's comprehensive natural language
processing (NLP) services, including Named Entity Recognition, Text Analytics for health,
Conversational Language Understanding, Custom Question Answering, Language
Detection, Sentiment Analysis, Summarization, Key Phrase Extraction, and PII redaction.
These services process text with accuracy and support multiple languages.
Local deployment: Azure Language also provides local MCP server where developers can
host the server in their own environment. You can find the local MCP server and setup
instructions in the Quickstart for Language MCP Server

sample in our GitHub

repository.
Remote MCP Server Endpoint
Bash
https://{foundry-resource-name}.cognitiveservices.azure.com/language/mcp?apiversion=2025-11-15-preview

Azure Language Intent Routing agent ðŸ†•
The Intent Routing agent in Foundry

manages conversation flows by combining intent

classification with answer delivery. This agent creates a framework that ensures users receive

accurate responses while maintaining operational control.
The agent, which is built on Azure Language's natural language understanding capabilities,
processes user input through layers. The system analyzes messages to understand intentions,
then users can implement logic to route requests through appropriate channels based on
confidence levels.
The agent prioritizes deterministic behavior, making it suitable for enterprise applications
where consistency is important.

Prerequisites
Before setting up the Intent Routing agent, ensure you have the following resources and
configurations in place:
Foundry resource: You need an active Foundry resource to host your agent.
Project resources: Create your CLU and CQA projects using one of the following resource
types:
Foundry resource.
AI hub resource.
Azure Language in Foundry Tools resource.
Project deployments: Deploy the following required projects:
Custom Question Answering (CQA) deployment - see CQA Overview.
Conversational Language Understanding (CLU) deployment - see CLU Overview.
Custom connection setup: Configure a custom connection between your agent project
and the Language resources:
In your agent project management center, use "Custom keys" connection when adding
the custom connection in the connected resources page.
Add a key-value pair with Ocp-Apim-Subscription-Key as the key name and your
resource key as the value.
For Foundry and AI hub resources, find the resource key in the resource overview page
in the Foundry portal management center.
For any resource type, you can also find the key in the Azure portal.
For detailed connection instructions, see Create a connection.

Key capabilities
Intent classification: Conversational Language Understanding (CLU) analyzes user
utterances to identify intents and extract entities. The system recognizes conversation
patterns and understands context.

Response delivery: Custom Question Answering (CQA) provides responses drawn from
curated knowledge sources. This capability ensures users receive consistent information
that aligns with organizational standards.
Knowledge management: Users can manage their intent definitions in CLU projects and
manage pairs of question-answers in CQA projects. This capability provides oversight for
the agent's knowledge base and response capabilities.
Fallback processing: Users can easily add retrieval-augmented generation (RAG) to the
agent to handle edge cases and uncommon questions by using approved knowledge
sources.
Download intent routing template code with Azure Developer CLI (azd)
Azure CLI
azd ai agent init -m azureml://registries/azuremlstaging/agentmanifests/intent_routing_agent/versions/1

Azure Language Exact Question Answering agent

ðŸ†•

The Exact Question Answering agent in Foundry

delivers responses to frequently asked

business questions through a fully managed, no-code solution. This agent provides consistent
answers to queries while maintaining governance and quality control.
The agent combines Azure AI Agent Service capabilities with Custom Question Answering
technology. This integration creates a solution with minimal setup while delivering
performance and oversight.
The agent works well for scenarios where answer accuracy is important, such as customer
service, help desk operations, or compliance information delivery.
In addition to creating the agent from the Exact Question Answering Agent template in Agent
Catalog, users can also create the agent directly from their CQA project in the Foundry portal.
More details can be found in Create and deploy a CQA agent.

Prerequisites
Before setting up the Exact Question Answering agent, ensure you have the following
resources and configurations in place:
Foundry resource: You need an active Foundry resource to host your agent.

Project resources: Create your CQA project using one of the following resource types:
Foundry resource.
AI hub resource.
Language resource.
Project deployment: Deploy the following required project:
Custom Question Answering (CQA) deployment - see CQA Overview.
Custom connection setup: Configure a custom connection between your agent project
and the Language resources:
In your agent project management center, use "Custom keys" connection when adding
the custom connection in the connected resources page.
Add a key-value pair with Ocp-Apim-Subscription-Key as the key name and your
resource key as the value.
For Foundry and AI hub resources, find the resource key in the resource overview page
in the Foundry portal management center.
For any resource type, you can also find the key in the Azure portal.
For detailed connection instructions, see Create a connection.
Download exact question answering template code with Azure Developer CLI (azd)
Azure CLI
azd ai agent init -m azureml://registries/azuremlstaging/agentmanifests/exact_question_answering_agent/versions/1

Key capabilities
Azure integration: The agent integrates Azure AI Agent Service with Custom Question
Answering capabilities within Azure Language services. This integration eliminates
complex configuration requirements and provides access to enterprise security and
monitoring features.
No-code deployment: Organizations can deploy and configure the agent through
Foundry's visual interface without writing custom code. This approach enables business
stakeholders to participate in knowledge base creation and maintenance.
Knowledge management: Users can manage question-answer pairs in CQA projects,
providing control over the agent's knowledge base and ensuring response accuracy.
Deterministic answering: The agent returns exact verbatim responses as defined in the
CQA project answers, ensuring consistent and controllable responses to questions.

Fallback processing: Users can easily add retrieval-augmented generation (RAG) to
handle queries outside the predefined knowledge base by using approved organizational
content sources.

Related content
Configure Azure resources for Foundry

Last updated on 11/18/2025

Language role-based access control
Azure Language in Foundry Tools supports Azure role-based access control (Azure RBAC), an
authorization system for managing individual access to Azure resources. Using Azure RBAC,
you assign different team members different levels of permissions for your projects authoring
resources. For more information, see the Azure RBAC documentation.

Enable Microsoft Entra authentication
To use Azure RBAC, you must enable Microsoft Entra authentication. You can create a new
resource with a custom subdomain or create a custom subdomain for your existing resource.

Add role assignment to Azure resource
Azure RBAC can be assigned to an Azure resource. To do so, you can add a role assignment.
1. In the Azure portal

, select All services.

2. Select Foundry Tools, and navigate to your specific Azure resource.
ï¼— Note
You can also set up Azure RBAC for whole resource groups, subscriptions, or
management groups. Complete your configuration by selecting the desired scope
level and then navigating to the desired item. For example, selecting Resource
groups and then navigating to a specific resource group.
3. Select Access control (IAM) on the left pane.
4. Select Add, then select Add role assignment.
5. On the Role tab on the next screen, select a role you want to add.
6. On the Members tab, select a user, group, service principal, or managed identity.
7. On the Review + assign tab, select Review + assign to assign the role.
Within a few minutes, the target is assigned to the selected role at the selected scope. For help
with these steps, see Assign Azure roles using the Azure portal.

Language role types

Use the following table to determine access needs for your Language projects.
These custom roles only apply to Language resources.
ï¼— Note
All prebuilt capabilities are accessible to all roles.
Owner and Contributor roles take priority over the custom language roles.
Microsoft Entra ID is only used with custom Language roles.
If you're assigned as a Contributor on Azure, your role is shown as Owner in
Language studio portal.

Cognitive Services Language Reader
A user that should only be validating and reviewing Azure Language apps, typically a tester to
ensure the application is performing well before deploying the project. They might want to
review the application's assets to notify the app developers of any changes that need to be
made, but don't have direct access to make them. Readers have access to view the evaluation
results.
Capabilities
API Access
Read
Test
All GET APIs under:
Language authoring conversational language understanding APIs
Language authoring text analysis APIs
Question answering projects Only TriggerExportProjectJob POST operation under:
Language authoring conversational language understanding export API
Language authoring text analysis export API Only Export POST operation under:
Question Answering Projects All the Batch Testing Web APIs *Language Runtime CLU APIs
*Language Runtime Text Analysis APIs

Cognitive Services Language Writer
A user responsible for building and modifying an application as a collaborator in a larger team.
The collaborator can modify Azure Language apps in any way, train those changes, and
validate/test those changes in the portal. However, this user shouldn't have access to

deploying this application to the runtime, as they might accidentally reflect their changes in
production. They also shouldn't be able to delete the application or alter its prediction
resources and endpoint settings (assigning or unassigning prediction resources, making the
endpoint public). This restriction prevents the role from altering an application currently being
used in production. They might also create new applications under this resource, but with the
restrictions mentioned.
Capabilities
API Access
All functionalities under Cognitive Services Language Reader.
Ability to:
Train
Write
All APIs under Language reader
All POST, PUT, and PATCH APIs under:
Language conversational language understanding APIs
Language text analysis APIs
question answering projects Except for
Delete deployment
Delete trained model
Delete Project
Deploy Model

Cognitive Services Language Owner
ï¼— Note
If you're assigned as an Owner and Language Owner,* you considered a Cognitive Services
Language Owner by Azure Language studio portal.
These users are the gatekeepers for Azure Language applications in production environments.
They should have full access to any of the underlying functions and thus can view everything in
the application and have direct access to edit any changes for both authoring and runtime
environments
Functionality
API Access

All functionalities under Cognitive Services Language Writer
Deploy
Delete
All APIs available under:
Language authoring conversational language understanding APIs
Language authoring text analysis APIs
question answering projects

Last updated on 11/18/2025

Multilingual and emoji support in
Language features
Multilingual and emoji support results in Unicode encodings that use more than one code
point

to represent a single displayed character, called a grapheme. For example, emojis like

ðŸŒ· and ðŸ‘ may use several characters to compose the shape with added characters for visual
attributes, such as skin tone. Similarly, the Hindi word à¤…à¤¨à¥à¤šà¥à¤›à¥‡ à¤¦ is encoded as five letters and
three combining marks.
Because of the different lengths of possible multilingual and emoji encodings, Language
features may return offsets in the response.

Offsets in the API response
Whenever offsets are returned in the API response, remember:
Elements in the response may be specific to the endpoint that was called.
HTTP POST/GET payloads are encoded in UTF-8

, which may or may not be the default

character encoding on your client-side compiler or operating system.
Offsets refer to grapheme counts based on the Unicode 8.0.0

standard, not character

counts.

Extracting substrings from text with offsets
Offsets can cause problems when using character-based substring methods, for example the
.NET substring() method. One problem is that an offset may cause a substring method to end
in the middle of a multi-character grapheme encoding instead of the end.
In .NET, consider using the StringInfo class, which enables you to work with a string as a series
of textual elements, rather than individual character objects. You can also look for grapheme
splitter libraries in your preferred software environment.
The Language features returns these textual elements as well, for convenience.
Endpoints that return an offset support the stringIndexType parameter. This parameter adjusts
the offset and length attributes in the API output to match the requested string iteration
scheme. Currently, we support three types:
textElement_v8 (default): iterates over graphemes as defined by the Unicode 8.0.0

standard
unicodeCodePoint : iterates over Unicode Code Points

, the default scheme for Python 3

utf16CodeUnit : iterates over UTF-16 Code Units

, the default scheme for JavaScript, Java,

and .NET
If the stringIndexType requested matches the programming environment of choice, substring
extraction can be done using standard substring or slice methods.

See also
Language overview

Last updated on 11/18/2025

Model lifecycle
Language features utilize AI models. We update Azure Language with new model versions to
improve accuracy, support, and quality. As models become older, they're retired. Use this
article for information on that process, and what you can expect for your applications.

Prebuilt features
Our standard (not customized) Language features are built on AI models that we call
pretrained or prebuilt models.
We regularly update Azure Language with new model versions to improve model accuracy,
support, and quality.
By default, all API requests use the latest Generally Available (GA) model.

Choose the model-version used on your data
We recommend using the latest model version to utilize the latest and highest quality
models. As our models improve, it's possible that some of your model results may change.
Model versions may be deprecated, so we no longer accept specified GA model versions in
your implementation.
Preview models used for preview features don't maintain a minimum retirement period and
may be deprecated at any time.
By default, API and SDK requests use the latest Generally Available model. To use a model in
preview, you can use an optional parameter modelVersion to select the preview version of the
model to be used (not recommended for GA models).
ï¼— Note
If you're using a model version that isn't listed in the table, then it was subjected to the
expiration policy.

Model versions
Use the following table to find which model versions support each feature:
ï¾‰

Expand table

Feature

Supported generally available (GA)
version

Sentiment Analysis and
opinion mining

latest

Language Detection

latest

Entity Linking

latest

Named Entity

latest

Recognition (NER)
Personally Identifiable
Information (PII)

latest

Latest
supported
preview versions

Other
supported
verision

2025-08-01-

2025-04-15-

preview

preview

2025-08-01-

2025-04-15-

preview

preview

2024-11-01-

2023-04-15-

preview

preview

detection
PII detection for

latest

conversations
Question answering

latest

Text Analytics for health

latest

2023-04-15preview

Key phrase extraction

latest

Summarization

latest . Note: 2025-06-10 is only

available for issue and resolution
aspects in conversation summarization.

Custom features
Expiration timeline
For custom features, there are two key parts of the AI implementation: training and
deployment. New configurations are released regularly with regular AI improvements, so older
and less accurate configurations are retired.
Use the following table to find which model versions support each feature:
ï¾‰

Expand table

Feature

Supported Training

Training Config

Deployment

Config Versions

Expiration

Expiration

2022-09-01 (latest)**

August 26, 2025

August 26, 2026

Orchestration workflow

2022-09-01 (latest)**

October 22, 2025

October 22, 2026

Custom named entity

2022-05-01 (latest)**

October 22, 2025

October 22, 2026

2022-05-01 (latest)**

October 22, 2025

October 22, 2026

Conversational language
understanding

recognition
Custom text classification

** For latest training configuration versions, the posted expiration dates are subject to availability
of a newer model version. If no newer model versions are available, the expiration date may be
extended.
Training configurations are typically available for six months after its release. If you assigned a
trained configuration to a deployment, this deployment expires after twelve months from the
training config expiration. If your models are about to expire, you can retrain and redeploy
your models with the latest training configuration version.
îª€ Tip
We recommend that you use the latest supported configuration version.
After the training config expiration date, you have to use another supported training
configuration version to submit any training or deployment jobs. After the deployment
expiration date, your deployed model is unavailable to be used for prediction.
After training config version expires, API calls returns an error when called or used if called with
an expired configuration version. By default, training requests use the latest available training
configuration version. To change the configuration version, use the trainingConfigVersion
parameter when submitting a training job and assign the version you want.

API versions
When you're making API calls to the following features, you need to specify the API-VERISON
you want to use to complete your request. We recommend that you use the latest available API
version.
If you're using Language Studio

for your projects, you use the latest API version available.

Other API versions are only available through the REST APIs and client libraries.

Use the following table to find which API versions support each feature:
ï¾‰

Feature

Custom text classification

Supported versions

2022-05-01 , 2022-10-01-preview ,

Latest Generally

Latest

Available version

preview
version

2022-05-01

2022-10-01-

2023-04-01

Conversational language
understanding

2022-05-01 , 2022-10-01-preview ,

Custom named entity
recognition

2022-05-01 , 2022-10-01-preview ,

Expand table

preview
2023-04-01

2023-04-01

2022-10-01preview

2023-04-15

2023-04-01 , 2023-04-15 , 2023-04-15-

2023-04-15preview

preview

Orchestration workflow

2022-05-01 , 2022-10-01-preview ,

2023-04-01

2023-04-01

Named Entity Recognition
(NER)

2025-05-15-preview , 2024-11-01
(GA) , 2024-11-15-preview

Personally Identifiable

2025-05-15-preview , 2024-11-01

Information (PII) detection

(GA) , 2024-11-15-preview

PII detection for

2025-05-15-preview , 2024-11-01

conversations

(GA) , 2024-11-15-preview

Next steps
Azure Language in Foundry Tools overview

Last updated on 11/18/2025

2022-10-01preview

2024-11-01 (GA)

2025-05-15preview

2024-11-01 (GA)

2025-05-15preview

2024-11-01 (GA)

2025-05-15preview

How to use Language features
asynchronously
The Language enables you to send API requests asynchronously, using either the REST API or
client library. You can also include multiple different Language features in your request, to be
performed on your data at the same time.
Currently, the following features are available to be used asynchronously:
Entity linking
Document summarization
Conversation summarization
Key phrase extraction
Language detection
Named Entity Recognition (NER)
Customer content detection
Sentiment analysis and opinion mining
Text Analytics for health
Personal Identifiable information (PII)
When you send asynchronous requests, you incur charges based on number of text records
you include in your request, for each feature use. For example, if you send a text record for
sentiment analysis and NER, the data is counted as sending two text records, and you're
charged for both according to your pricing tier

.

Submit an asynchronous job using the REST API
To submit an asynchronous job, review the reference documentation for the JSON body you
send in your request.
1. Add your documents to the analysisInput object.
2. In the tasks object, include the operations you want performed on your data. For
example, if you wanted to perform sentiment analysis, you would include the
SentimentAnalysisLROTask object.

3. You can optionally:
a. Choose a specific version of the model used on your data.
b. Include added Language features in the tasks object, to be performed on your data at
the same time.
Once you create the JSON body for your request, add your key to the Ocp-Apim-SubscriptionKey header. Then send your API request to job creation endpoint. For example:

HTTP
POST https://your-endpoint.cognitiveservices.azure.com/language/analyze-text/jobs?
api-version=2022-05-01

A successful call returns a 202 response code. The operation-location in the response header
is the URL you use to retrieve the API results. The value looks similar to the following URL:
HTTP
GET {Endpoint}/language/analyze-text/jobs/12345678-1234-1234-1234-12345678?apiversion=2022-05-01

To get the status and retrieve the results of the request, send a GET request to the URL you
received in the operation-location header from the previous API response. Remember to
include your key in the Ocp-Apim-Subscription-Key . The response includes the results of your
API call.

Send asynchronous API requests using the client
library
First, make sure you have the client library installed for your language of choice. For steps on
installing the client library, see the quickstart article for the feature you want to use.
Afterwards, use the client object to send asynchronous calls to the API. The method calls to use
vary depending on your language. Use the available samples and reference documentation to
help you get started.
C#
Java
JavaScript
Python

Result availability
For this feature, used asynchronously, the API results are available for 24 hours from the time
the request was ingested, and is indicated in the response. After this time period, the results
are purged and are no longer available for retrieval.

Automatic language detection

Starting in version 2022-07-01-preview of the REST API, you can request automatic language
detection on your documents. By setting the language parameter to auto , the detected
language code of the text is returned as a language value in the response. This language
detection doesn't incur extra charges to your Language resource.

Data limits
ï¼— Note
If you need to analyze larger documents than the limit allows, you can break the text
into smaller chunks of text before sending them to the API.
A document is a single string of text characters.
You can send up to 125,000 characters across all documents contained in the asynchronous
request, as measured by StringInfo.LengthInTextElements. This character limit is higher than the
limit for synchronous requests, to enable higher throughput.
If a document exceeds the character limit, the API rejects the entire request and return a 400
bad request error if any document within it exceeds the maximum size.

See also
Azure Language in Foundry Tools overview
Multilingual and emoji support
What's new

Last updated on 11/18/2025

Transparency Note for Azure Language in
Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.

What is a Transparency Note?
An AI system includes not only the technology, but also the people who will use it, the people
who will be affected by it, and the environment in which it is deployed. Creating a system that
is fit for its intended purpose requires an understanding of how the technology works, its
capabilities and limitations, and how to achieve the best performance. Microsoft's Transparency
Notes are intended to help you understand how our AI technology works, the choices system
owners can make that influence system performance and behavior, and the importance of
thinking about the whole system, including the technology, the people, and the environment.
You can use Transparency Notes when developing or deploying your own system, or share
them with the people who will use or be affected by your system.
Microsoft's Transparency Notes are part of a broader effort at Microsoft to put our AI principles
into practice. To find out more, see Microsoft AI principles .

The basics of Azure Language in Foundry Tools
Introduction
Language is a cloud-based service that provides Natural Language Processing (NLP) features
for text mining and text analysis, including the following features:
Named Entity Recognition (NER), Personally Identifying Information (PII)
Text analytics for health
Key phrase extraction
Language detection
Sentiment analysis and opinion mining
Question answering
Summarization
Custom Named Entity Recognition (Custom NER)

Custom text classification
Conversational language understanding
Read the overview to get an introduction to each feature and review the example use cases.
See the How-to guides and the API reference to understand more details about what each
feature does and what gets returned by the system.
This article contains basic guidelines for how to use Language features responsibly. Read the
general information first and then jump to the specific article if you're using one of the features
below.
Transparency note for Named Entity Recognition
[Transparency note for Personally Identifying Information](/azure/ai-foundry/responsibleai/language-service/transparency-note-personally-identifiable-information
[Transparency note for text analytics for health](/azure/ai-foundry/responsibleai/language-service/transparency-note-health
Transparency note for key phrase extraction
Transparency note for language detection
Transparency note for sentiment analysis
Transparency note for question answering
Transparency note for summarization
Transparency note for custom Named Entity Recognition (custom NER)
Transparency note for custom text classification
Transparency note for conversational language understanding

Capabilities
Use cases
Language services can be used in multiple scenarios across a variety of industries. Some
examples listed by feature are:
Use Custom Named Entity Recognition for knowledge mining to enhance semantic
search. Search is foundational to any app that surfaces text content to users. Common
scenarios include catalog or document search, retail product search, or knowledge mining
for data science. Many enterprises across various industries want to build a rich search
experience over private, heterogeneous content, which includes both structured and
unstructured documents. As a part of their pipeline, developers can use custom NER for
extracting entities from the text that are relevant to their industry. These entities can be
used to enrich the indexing of the file for a more customized search experience.

Use Named Entity Recognition to enhance or automate business processes. For
example, when reviewing insurance claims, recognized entities like name and location
could be highlighted to facilitate the review. Or a support ticket could be generated with
a customer's name and company automatically from an email.
Use Personally Identifiable Information to redact some categories of personal
information from documents to protect privacy. For example, if customer contact
records are accessible to first line support representatives, the company may want to
redact unnecessary customer's personal information from customer history to preserve
the customer's privacy.
Use Language Detection to detect languages for business workflow. For example, if a
company receives email in various languages from customers, they could use language
detection to route the emails by language to native speakers for ease of communication
with those customers.
Use Sentiment Analysis to monitor for positive and negative feedback trends in
aggregate. After the introduction of a new product, a retailer could use the sentiment
service to monitor multiple social media outlets for mentions of the product with their
sentiment. They could review the trending sentiment in their weekly product meetings.
Use Summarization to extract key information from public news articles. To produce
insights such as trends and news spotlights.
Use Key Phrase Extraction to view aggregate trends in text data. For example, a word
cloud can be generated with key phrases to help visualize key concepts in text comments
or feedback. For example, a hotel could generate a word cloud based on key phrases
identified in their comments and might see that people are commenting most frequently
about the location, cleanliness and helpful staff.
Use Text Analytics for Health for insights and statistics extraction. Identify medical
entities such as symptoms, medications, and diagnoses in clinical notes and diverse
clinical documents. Use this information for producing insights and statistics on patient
populations, searching clinical documents, research documents and publications.
Use Custom Text Classification for automatic email or ticket triaging. Support centers of
all types receive a high volume of emails or tickets containing unstructured, freeform text
and attachments. Timely review, acknowledgment, and routing to subject matter experts
within internal teams is critical. Email triage at this scale requires people to review and
route to the right departments, which takes time and resources. Custom text classification
can be used to analyze incoming text, and triage and categorize the content to be
automatically routed to the relevant departments for further action.

Use Conversational Language Understanding to build end-to-end conversational bots.
Use CLU to build and train a custom natural language understanding model based on a
specific domain and the expected users' utterances. Integrate it with any end-to-end
conversational bot so that it can process and analyze incoming text in real time to identify
the intention of the text and extract important information from it. Have the bot perform
the desired action based on the intention and extracted information. An example would
be a customized retail bot for online shopping or food ordering.
Use Question Answering for customer support. In most customer support scenarios,
common questions are asked frequently. Question Answering lets you instantly create a
chat bot from existing support content, and this bot can act as the front-line system for
handling customer queries. If the questions can't be answered by the bot, then additional
components can help identify and flag the question for human intervention.

Limitations
The quality of the incoming text to the system will affect your
results.
Language features only process text. The fidelity and formatting of the incoming text will affect
the performance of the system. Make sure you consider the following:
Speech transcription quality may affect the quality of the results. If your source data is
voice, make sure you use the highest quality combination of automatic and human
transcription to ensure the best performance. Consider using custom speech models for
better quality results.
Lack of standard punctuation or casing may affect the quality of your results. If you are
using a speech system, like Azure Speech in Foundry Tools to Text, be sure to select the
option to include punctuation.
Optical character recognition (OCR) quality may affect the quality of the system. If your
source data is images and you use OCR technology to generate the text, incorrectly
generated text may affect the performance of the system. Consider using custom OCR
models to help improve the quality of results.
If your data includes frequent misspellings, consider using Bing Spell Check to correct
misspellings.
Tabular data may not be identified correctly depending on how you send the table text to
the system. Assess how you send text from tables in source documents to the service. For
tables in documents, consider using Azure Document Intelligence in Foundry Tools or a

similar service. This will allow you to get the appropriate keys and values to send to
Language with contextual keys that are close enough to the values for the system to
properly recognize the entities.
Microsoft trained its Language feature models (with the exception of language detection)
using natural language text data that is comprised primarily of fully formed sentences and
paragraphs. Therefore, using this service for data that most closely resembles this type of
text will yield the best performance. We recommend avoiding use of this service to
evaluate incomplete sentences and phrases where possible, as the performance may be
reduced.
The service only supports single language text. If your text includes multiple languages
for example "the sandwich was bueno", the output may not be accurate.
The language code must match the input text language to get accurate results. If you are
unsure about the input language you can use the language detection feature.

Best practices for improving system performance
Some features of Language return confidence scores and can be evaluated using the approach
described in the following sections. Other features which do not return a confidence score
(such as key word extraction and summarization) will need to be evaluated using different
methods.

Understand confidence scores for sentiment analysis, named
entity recognition, language detection, and health functions
The sentiment, named entity recognition, language detection and health functions all return a
confidence score as a part of the system response. This is an indicator of how confident the
service is with the system's response. A higher value indicates that the service is more
confident that the result is accurate. For example, the system recognizes entity of category U.S.
Driver's License Number on the text 555 555 555 when given the text "My NY driver's license
number is 555 555 555" with a score of .75 and might recognize category U.S. Driver's License
Number on the text 555 555 555 with a score of .65 when given the text "My NY DL number is
555 555 555". Given the more specific context in the first example, the system is more
confident in its response. In many cases, the system response can be used without examining
the confidence score. In other cases, you can choose to use a response only if its confidence
score is above a specified confidence score threshold.

Understand and measuring performance

The performance of Language features is measured by examining how well the system
recognizes the supported NLP concepts (at a given threshold value in comparison with a
human judge.) For named entity extraction (NER), for example, one might count the true
number of phone number entities in some text based on human judgement, and then compare
with the output of the system from processing the same text. Comparing human judgement
with the system recognized entities would allow you to classify the events into two kinds of
correct (or "true") events and two kinds of incorrect (or "false") events.
ï¾‰

Expand table

Outcome

Correct/Incorrect

Definition

Example

True

Correct

The system returns the

The system correctly recognizes PII entity

same result that would be
expected from a human
judge.

of category Phone Number on the text 1234-567-8910 when given the text: "You
can reach me at my office number 1-234567-9810."

The system does not return
a result, and this aligns

The system does not recognize any PII
entity when given the text: "You can

with what would be
expected from human
judge.

reach me at my office number."

Positive

True
Negative

Correct

False
Positive

Incorrect

The system returns a result
where a human judge
would not.

The system incorrectly recognizes PII
entity of category Phone Number for the
text office number when given the text:
"You can reach me at my office number."

False
Negative

Incorrect

The system does not return
a result when a human
judge would.

The system incorrectly misses a Phone
Number PII entity on the text 1-234-5678910 when given the text: "You can reach
me at my office number 1-234-5679810."

Language features will not always be correct. You'll likely experience both false negative and
false positive errors. It's important to consider how each type of error will affect your system.
Carefully think through scenarios where true events won't be recognized and where incorrect
events will be recognized and what the downstream effects might be in your implementation.
Make sure to build in ways to identify, report and respond to each type of error. Plan to
periodically review the performance of your deployed system to ensure errors are being
handled appropriately.

How to set confidence score thresholds

You can choose to make decisions in your system based on the confidence score the system
returns. You can adjust the confidence score threshold your system uses to meet your needs. If
it is more important to identify all potential instances of the NLP concepts you want, you can
use a lower threshold. This means that you may get more false positives but fewer false
negatives. If it is more important for your system to recognize only true instances of the feature
you're calling, you can use a higher threshold. If you use a higher threshold, you may get fewer
false positives but more false negatives. Different scenarios call for different approaches. In
addition, threshold values may not have consistent behavior across individual features of
Language and categories of entities. For example, do not make assumptions that using a
certain threshold for NER category Phone Number would be sufficient for another NER
category, or that a threshold you use in NER would work similarly for Sentiment Analysis.
Therefore, it is critical that you test your system with any thresholds you are considering using
with real data to determine the effects of various threshold values of your system in the
context that it will be used.

Fairness
At Microsoft, we strive to empower every person on the planet to achieve more. An essential
part of this goal is working to create technologies and products that are fair and inclusive.
Fairness is a multi-dimensional, sociotechnical topic and impacts many different aspects of our
product development. You can learn more about Microsoftâ€™s approach to fairness here

.

One dimension we need to consider is how well the system performs for different groups of
people. This may include looking at the accuracy of the model as well as measuring the
performance of the complete system. Research has shown that without conscious effort
focused on improving performance for all groups, it is often possible for the performance of an
AI system to vary across groups based on factors such as race, ethnicity, language, gender, and
age.
Each service and feature is different, and our testing may not perfectly match your context or
cover all scenarios required for your use case. We encourage developers to thoroughly
evaluate error rates for the service with real-world data that reflects your use case, including
testing with users from different demographic groups.
For Language, certain dialects and language varieties within our supported languages and text
from some demographic groups may not yet have enough representation in our current
training datasets. We encourage you to review our responsible use guidelines, and if you
encounter performance differences, we encourage you to let us know.

Performance varies across features and languages

Various languages are supported for each Language feature. You may find that performance for
a particular feature is not consistent with another feature. Also, you may find that for a
particular feature that performance is not consistent across various languages.

Next steps
If you are using any of the features below, be sure to review the specific information for that
feature.

See also
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for text analytics for health
Transparency note for key phrase extraction
Transparency note for language detection
Transparency note for question answering
Transparency note for summarization
Transparency note for sentiment analysis
Transparency note for custom Named Entity Recognition (NER)
Transparency note for custom text classification
Transparency note for conversational language understanding
Also, make sure to review:
Guidance for integration and responsible use with Language
Data Privacy for Language

Last updated on 11/08/2025

Guidance for integration and responsible
use with Azure Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
Microsoft wants to help you responsibly develop and deploy solutions that use Azure
Language in Foundry Tools. We are taking a principled approach to upholding personal agency
and dignity by considering the fairness, reliability & safety, privacy & security, inclusiveness,
transparency, and human accountability of our AI systems. These considerations are in line with
our commitment to developing Responsible AI.
This article discusses Language features and the key considerations for making use of this
technology responsibly. Consider the following factors when you decide how to use and
implement AI-powered products and features.

General guidelines
When you're getting ready to deploy AI-powered products or features, the following activities
help to set you up for success:
Understand what it can do: Fully assess the capabilities of any AI model you are using to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context.
Test with real, diverse data: Understand how your system will perform in your scenario by
thoroughly testing it with real life conditions and data that reflects the diversity in your
users, geography and deployment contexts. Small datasets, synthetic data and tests that
don't reflect your end-to-end scenario are unlikely to sufficiently represent your
production performance.
Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that you
have consent to use for this purpose.
Legal review: Obtain appropriate legal advice to review your solution, particularly if you
will use it in sensitive or high-risk applications. Understand what restrictions you might
need to work within and your responsibility to resolve any issues that might come up in
the future. Do not provide any legal advice or guidance.

System review: If you're planning to integrate and responsibly use an AI-powered
product or feature into an existing system of software, customers or organizational
processes, take the time to understand how each part of your system will be affected.
Consider how your AI solution aligns with Microsoft's Responsible AI principles.
Human in the loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means constant human oversight of the AIpowered product or feature and maintaining the role of humans in decision-making.
Ensure you can have real-time human intervention in the solution to prevent harm. This
enables you to manage where the AI model doesn't perform as required.
Security: Ensure your solution is secure and has adequate controls to preserve the
integrity of your content and prevent unauthorized access.
Customer feedback loop: Provide a feedback channel that allows users and individuals to
report issues with the service once it's been deployed. Once you've deployed an AIpowered product or feature it requires ongoing monitoring and improvement â€“ be ready
to implement any feedback and suggestions for improvement.

Learn more about Responsible AI
Microsoft Responsible AI principles
Microsoft Responsible AI resources
Microsoft Azure Learning courses on Responsible AI

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Data Privacy and Security for Language

Last updated on 08/17/2025

Data, privacy, and security for Azure
Language in Foundry Tools
ï¼‰ Important
Non-English translations are provided for convenience only. Please consult the EN-US
version of this document for the definitive version.
This article provides details regarding how Azure Language in Foundry Tools processes your
data. Language is designed with compliance, privacy, and security in mind. However, you are
responsible for its use and the implementation of this technology. It's your responsibility to
comply with all applicable laws and regulations in your jurisdiction.

What data does Language process and how does it
process it?
Language processes text data that is sent by the customer to the system for the purposes
of getting a response from one of the available features.
All results of the requested feature are sent back to the customer in the API response as
specified in the API reference. For example, if Language Detection is requested, the
language code is returned along with a confidence score for each text record.
Language uses aggregate telemetry such as which APIs are used and the number of calls
from each subscription and resource for service monitoring purposes.
Language doesn't store or process customer data outside the region where the customer
deploys the service instance.
Language encrypts all content, including customer data, at rest.

How is data retained and what customer controls
are available?
Data sent in synchronous or asynchronous calls may be temporarily stored by Language
for up to 48 hours only and is purged thereafter. This data is encrypted and is only
accessible to authorized on call engineers when service support is needed for debugging
purposes in the event of a catastrophic failure. To prevent this temporary storage of input
data, the LoggingOptOut query parameter can be set accordingly. By default, this
parameter is set to false for Language Detection, Key Phrase Extraction, Sentiment
Analysis and Named Entity Recognition endpoints. The LoggingOptOut parameter is true

by default for the PII and health feature endpoints. More information on the
LoggingOptOut query parameter is available in the API reference.
To learn more about Microsoft's privacy and security commitments, visit the Microsoft Trust
Center

See also
Transparency note for Language
Transparency note for Named Entity Recognition and Personally Identifying Information
Transparency note for the health feature
Transparency note for Key Phrase Extraction
Transparency note for Language Detection
Transparency note for Question answering
Transparency note for Summarization
Transparency note for Sentiment Analysis
Guidance for integration and responsible use with Language

Last updated on 11/18/2025

Deploy a key phrase extraction container to
Azure Kubernetes Service
Learn how to deploy a key phrase extraction Docker container image to Azure Kubernetes
Service (AKS). This procedure shows how to create a Language resource, how to associate a
container image, and how to exercise this orchestration of the two from a browser. Using
containers can shift your attention away from managing infrastructure to instead focusing on
application development. While this article uses the key phrase extraction container as an
example, you can use this process for other containers offered by Azure Language in Foundry
Tools

Prerequisites
This procedure requires several tools that must be installed and run locally. Don't use Azure
Cloud Shell. You need the following:
An Azure subscription. If you don't have an Azure subscription, create a free account
before you begin.
A text editor, for example, Visual Studio Code .
The Azure CLI installed.
The Kubernetes CLI

installed.

An Azure resource with the correct pricing tier. Not all pricing tiers work with this
container:
Language resource with F0 or standard pricing tiers only.
Foundry Tools resource with the S0 pricing tier.

Create an Azure Language in Foundry Tools
resource
1. Sign in to the Azure portal

.

2. Select Create a resource, and then go to AI + Machine Learning > Language. Or, go to
Create a Language resource .
3. Enter all the required settings:
ï¾‰

Expand table

Setting

Value

Name

Enter a name (2-64 characters).

Subscription

Select the appropriate subscription.

Location

Select a nearby location.

Pricing tier

Enter S, the standard pricing tier.

Resource group

Select an available resource group.

4. Select Create, and wait for the resource to be created. Your browser automatically
redirects to the newly created resource page.
5. Collect the configured endpoint and an API key:
ï¾‰

Resource
tab in

Setting

Value

Endpoint

Copy the endpoint. It appears similar to https://my-

Expand table

portal
Overview

resource.cognitiveservices.azure.com/text/analytics/v3.0 .

Keys

API Key

Copy one of the two keys. It's a 32-character alphanumeric string with no
spaces or dashes: < xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx >.

Create an Azure Kubernetes Service cluster
resource
1. Go to Azure Kubernetes Service

, and select Create.

2. On the Basics tab, enter the following information:
ï¾‰

Setting

Value

Subscription

Select an appropriate subscription.

Resource group

Select an available resource group.

Kubernetes cluster name

Enter a name (lowercase).

Region

Select a nearby location.

Expand table

Setting

Value

Kubernetes version

Whatever value is marked as (default).

DNS name prefix

Created automatically, but you can override.

Node size

Standard DS2 v2:
2 vCPUs , 7 GB

Node count

Leave the slider at the default value.

3. On the Node pools tab, leave Virtual nodes and VM scale sets set to their default values.
4. On the Authentication tab, leave Service principal and Enable RBAC set to their default
values.
5. On the Networking tab, enter the following selections:
ï¾‰

Setting

Value

HTTP application routing

No

Networking configuration

Basic

Expand table

6. On the Integrations tab, make sure that Container monitoring is set to Enabled, and
leave Log Analytics workspace as the default value.
7. On the Tags tab, leave the name/value pairs blank for now.
8. Select Review and Create.
9. After validation passes, select Create.
ï¼— Note
If validation fails, it might be because of a "Service principal" error. Go back to the
Authentication tab and then go back to Review + create, where validation should run and
then pass.

Deploy the Key Phrase Extraction container to an
AKS cluster

1. Open the Azure CLI, and sign in to Azure.
Azure CLI
az login

2. Sign in to the AKS cluster. Replace your-cluster-name and your-resource-group with the
appropriate values.
Azure CLI
az aks get-credentials -n your-cluster-name -g -your-resource-group

After this command runs, it reports a message similar to the following:
Output
Merged "your-cluster-name" as current context in /home/username/.kube/config

ï¼’ Warning
If you have multiple subscriptions available to you on your Azure account and the az
aks get-credentials command returns with an error, a common problem is that

you're using the wrong subscription. Set the context of your Azure CLI session to use
the same subscription that you created the resources with and try again.
Azure CLI
az account set -s subscription-id

3. Open the text editor of choice. This example uses Visual Studio Code.
Console
code .

4. Within the text editor, create a new file named keyphrase.yaml, and paste the following
YAML into it. Be sure to replace billing/value and apikey/value with your own
information.
YAML
apiVersion: apps/v1beta1
kind: Deployment

metadata:
name: keyphrase
spec:
template:
metadata:
labels:
app: keyphrase-app
spec:
containers:
- name: keyphrase
image: mcr.microsoft.com/azure-cognitive-services/keyphrase
ports:
- containerPort: 5000
resources:
requests:
memory: 2Gi
cpu: 1
limits:
memory: 4Gi
cpu: 1
env:
- name: EULA
value: "accept"
- name: billing
value: # {ENDPOINT_URI}
- name: apikey
value: # {API_KEY}
--apiVersion: v1
kind: Service
metadata:
name: keyphrase
spec:
type: LoadBalancer
ports:
- port: 5000
selector:
app: keyphrase-app

ï¼‰ Important
Remember to remove the key from your code when you're done, and never post it
publicly. For production, use a secure way of storing and accessing your credentials like
Azure Key Vault. See the Foundry Tools security article for more information.
1. Save the file, and close the text editor.
2. Run the Kubernetes apply command with the keyphrase.yaml file as its target:

Console
kubectl apply -f keyphrase.yaml

After the command successfully applies the deployment configuration, a message
appears similar to the following output:
Output
deployment.apps "keyphrase" created
service "keyphrase" created

3. Verify that the pod was deployed:
Console
kubectl get pods

The output for the running status of the pod:
Output
NAME
keyphrase-5c9ccdf575-mf6k5

READY
1/1

STATUS
Running

RESTARTS
0

AGE
1m

4. Verify that the service is available, and get the IP address.
Console
kubectl get services

The output for the running status of the keyphrase service in the pod:
Output
NAME
kubernetes
keyphrase

TYPE
ClusterIP
LoadBalancer

CLUSTER-IP
10.0.0.1
10.0.100.64

EXTERNAL-IP
<none>
168.61.156.180

PORT(S)
443/TCP
5000:31234/TCP

AGE
2m
2m

Verify the Key Phrase Extraction container instance
1. Select the Overview tab, and copy the IP address.
2. Open a new browser tab, and enter the IP address. For example, enter http://<IPaddress>:5000 (http://55.55.55.55:5000 ). The container's home page is displayed, which

lets you know the container is running.

3. Select the Service API Description link to go to the container's Swagger page.
4. Choose any of the POST APIs, and select Try it out. The parameters are displayed, which
includes this example input:
JSON
{
"documents": [
{
"id": "1",
"text": "Hello world"
},
{
"id": "2",
"text": "Bonjour tout le monde"
},
{
"id": "3",
"text": "La carretera estaba atascada. HabÃ­a mucho trÃ¡fico el dÃ­a de
ayer."
},
{
"id": "4",
"text": ":) :( :D"
}
]
}

5. Replace the input with the following JSON content:
JSON
{
"documents": [
{
"language": "en",
"id": "7",
"text": "I was fortunate to attend the KubeCon Conference in Barcelona,
it is one of the best conferences I have ever attended. Great people, great
sessions and I thoroughly enjoyed it!"
}
]
}

6. Set showStats to true .
7. Select Execute to determine the sentiment of the text.
The model that's packaged in the container generates a score that ranges from 0 to 1,
where 0 is negative and 1 is positive.
The JSON response that's returned includes sentiment for the updated text input:
JSON
{
"documents": [
{
"id": "7",
"keyPhrases": [
"Great people",
"great sessions",
"KubeCon Conference",
"Barcelona",
"best conferences"
],
"statistics": {
"charactersCount": 176,
"transactionsCount": 1
}
}
],
"errors": [],
"statistics": {
"documentsCount": 1,
"validDocumentsCount": 1,
"erroneousDocumentsCount": 0,
"transactionsCount": 1

}
}

We can now correlate the document id of the response payload's JSON data to the original
request payload document id . The resulting document has a keyPhrases array, which contains
the list of key phrases that have been extracted from the corresponding input document.
Additionally, there are various statistics such as characterCount and transactionCount for each
resulting document.

Next steps
Use more Azure AI containers
Key phrase extraction overview

Last updated on 11/18/2025

Use Azure Language in Power Automate
You can use Power Automate flows to automate repetitive tasks and bring efficiency to your
organization. Using Azure Language in Foundry Tools, you can automate tasks like:
Send incoming emails to different departments based on their contents.
Analyze the sentiment of new tweets.
Extract entities from incoming documents.
Summarize meetings.
Remove personal data from files before saving them.
In this tutorial, you'll create a Power Automate flow to extract entities found in text, using
Named entity recognition.

Prerequisites
Azure subscription - Create one for free
Once you have your Azure subscription, create a Language resource

in the Azure portal

to get your key and endpoint. After it deploys, select Go to resource.
You will need the key and endpoint from the resource you create to connect your
application to the API. You'll paste your key and endpoint into the code below later in
the quickstart.
You can use the free pricing tier ( Free F0 ) to try the service, and upgrade later to a
paid tier for production.
Optional for this tutorial: A trained model is required if you're using a custom capability
such as custom NER, custom text classification, or conversational language
understanding.

Create a Power Automate flow
For this tutorial, you will create a flow that extracts named entities from text.
1. Sign in to power automate
2. From the left side menu, select My flows. Then select New flow > Automated cloud flow.

ï Š

3. Enter a name for your flow such as LanguageFlow . Then select Skip to continue without
choosing a trigger.

ï Š

4. Under Triggers select Manually trigger a flow.

ï Š

5. Select + New step to begin adding a Language connector.
6. Under Choose an operation search for Language. Then select Language. This will narrow
down the list of actions to only those that are available for Language.

ï Š

7. Under Actions search for Named Entity Recognition, and select the connector.

ï Š

8. Get the endpoint and key for your Language resource, which will be used for
authentication. You can find your key and endpoint by navigating to your resource in the
Azure portal

, and selecting Keys and Endpoint from the left side menu.

ï Š

9. Once you have your key and endpoint, add it to the connector in Power Automate.

ï Š

10. Add the data in the connector

ï Š

ï¼— Note
You will need deployment name and project name if you're using custom language
capability.
11. From the top navigation menu, save the flow and select Test the flow. In the window that
appears, select Test.

ï Š

12. After the flow runs, you can see the response in the outputs field.

ï Š

Next steps
Triage incoming emails with custom text classification
Available Language connectors

Last updated on 11/18/2025

Native document support for Azure
Language in Foundry Tools (preview)
ï¼‰ Important
Azure Language in Foundry Tools public preview releases provide early access to
features that are in active development.
Features, approaches, and processes can change, before General Availability (GA),
based on user feedback.
Language is a cloud-based service that applies Natural Language Processing (NLP) features to
text-based data. The native document support capability enables you to send API requests
asynchronously, using an HTTP POST request body to send your data and HTTP GET request
query string to retrieve the status results. Your processed documents are located in your Azure
Blob Storage target container.
A native document refers to the file format used to create the original document such as
Microsoft Word (docx) or a portable document file (pdf). Native document support eliminates
the need for text preprocessing before using Language resource capabilities. Currently, native
document support is available for the following capabilities:
Personally Identifiable Information (PII). The PII detection feature can identify, categorize,
and redact sensitive information in unstructured text. The PiiEntityRecognition API
supports native document processing.
Document summarization. Document summarization uses natural language processing to
generate extractive (salient sentence extraction) or abstractive (contextual word
extraction) summaries for documents. Both AbstractiveSummarization and
ExtractiveSummarization APIs support native document processing.

Supported document formats
Applications use native file formats to create, save, or open native documents. Currently PII and
Document summarization capabilities supports the following native document formats:
ï¾‰

File type

File extension

Description

Text

.txt

An unformatted text document.

Expand table

File type

File extension

Description

Adobe PDF

.pdf

A portable document file formatted document.

Microsoft Word

.docx

A Microsoft Word document file.

Input guidelines
Supported file formats

Type

support and limitations

PDFs

Fully scanned PDFs aren't supported.

Text within images

Digital images with embedded text aren't supported.

Digital tables

Tables in scanned documents aren't supported.

ï¾‰

Expand table

ï¾‰

Expand table

Document Size

Attribute

Input limit

Total number of documents per request

â‰¤ 20

Total content size per request

â‰¤ 10 MB

Request headers and parameters
ï¾‰

Expand table

parameter

Description

-X POST <endpoint>

Specifies your Language resource endpoint for accessing the
API.

--header Content-Type:

The content type for sending JSON data.

application/json
--header "Ocp-Apim-Subscription-Key:
<key>

Specifies Azure Language resource key for accessing the API.

parameter

Description

-data

The JSON file containing the data you want to pass with your
request.

Related content
PII detection overview

Last updated on 11/18/2025

Document Summarization overview

SAS tokens for your storage containers
Learn to create user delegation, shared access signature (SAS) tokens, using the Azure portal.
User delegation SAS tokens are secured with Microsoft Entra credentials. SAS tokens provide
secure, delegated access to resources in your Azure storage account.

îª€ Tip
Role-based access control (managed identities) provide an alternate method for granting
access to your storage data without the need to include SAS tokens with your HTTP
requests.
Using managed identities grants access to any resource that supports Microsoft
Entra authentication, including your own applications.
Using managed identities replaces the requirement for you to include shared access
signature tokens (SAS) with your source and target URLs.
Using managed identities doesn't require an added cost in Azure.
At a high level, here's how SAS tokens work:
Your application submits the SAS token to Azure Storage as part of a REST API request.
The storage service verifies that the SAS is valid and then the request is authorized. If the
SAS token is deemed invalid, the request is declined, and the error code 403 (Forbidden)
is returned.
Azure Blob Storage offers three resource types:
Storage accounts provide a unique namespace in Azure for your data.
Data storage containers are located in storage accounts and organize sets of blobs (files,
text, or images).
Blobs are located in containers and store text and binary data such as files, text, and
images.
ï¼‰ Important
SAS tokens are used to grant permissions to storage resources, and should be
protected in the same manner as an account key.

Operations that use SAS tokens should be performed only over an HTTPS
connection, and SAS URI s should only be distributed on a secure connection such as
HTTPS.

Prerequisites
To get started, you need the following resources:
An active Azure account . If you don't have one, you can create a free account .
An Azure Language in Foundry Tools

resource.

A standard performance Azure Blob Storage account

. You also need to create

containers to store and organize your files within your storage account. If you don't know
how to create an Azure storage account with a storage container, follow these quickstarts:
Create a storage account. When you create your storage account, select Standard
performance in the Instance details > Performance field.
Create a container. When you create your container, set Public access level to
Container (anonymous read access for containers and files) in the New Container
window.

Create SAS tokens in the Azure portal
Go to the Azure portal

and navigate to your container or a specific file as follows and

continue with these steps:
Workflow: Your storage account â†’ containers â†’ your container â†’ your file
1. Right-click the container or file and select Generate SAS from the drop-down menu.
2. Select Signing method â†’ User delegation key.
3. Define Permissions by checking and/or clearing the appropriate check box:
Your source file must designate read and list access.
Your target file must designate write and list access.
4. Specify the signed key Start and Expiry times.
When you create a shared access signature (SAS), the default duration is 48 hours.
After 48 hours, you'll need to create a new token.

Consider setting a longer duration period for the time you're using your storage
account for Language operations.
The value of the expiry time is determined by whether you're using an Account key
or User delegation key Signing method:
Account key: No imposed maximum time limit; however, best practices
recommended that you configure an expiration policy to limit the interval and
minimize compromise. Configure an expiration policy for shared access
signatures.
User delegation key: The value for the expiry time is a maximum of seven days
from the creation of the SAS token. The SAS is invalid after the user delegation
key expires, so a SAS with an expiry time of greater than seven days will still only
be valid for seven days. For more information, see Use Microsoft Entra credentials
to secure a SAS.
5. The Allowed IP addresses field is optional and specifies an IP address or a range of IP
addresses from which to accept requests. If the request IP address doesn't match the IP
address or address range specified on the SAS token, authorization fails. The IP address or
a range of IP addresses must be public IPs, not private. For more information, see, Specify
an IP address or IP range.
6. The Allowed protocols field is optional and specifies the protocol permitted for a request
made with the SAS. The default value is HTTPS.
7. Review then select Generate SAS token and URL.
8. The Blob SAS token query string and Blob SAS URL are displayed in the lower area of
window.
9. Copy and paste the Blob SAS token and URL values in a secure location. They'll only be
displayed once and cannot be retrieved once the window is closed.
10. To construct a SAS URL, append the SAS token (URI) to the URL for a storage service.

Use your SAS URL to grant access
The SAS URL includes a special set of query parameters. Those parameters indicate how the
client accesses the resources.
You can include your SAS URL with REST API requests in two ways:
Use the SAS URL as your sourceURL and targetURL values.
Append the SAS query string to your existing sourceURL and targetURL values.

Here's a sample REST API request:
JSON
{
"analysisInput": {
"documents": [
{
"id": "doc_0",
"language": "en",
"source": {
"location": "myaccount.blob.core.windows.net/sample-input/input.pdf?{SASToken}"
},
"target": {
"location": "https://myaccount.blob.core.windows.net/sample-output?{SASToken}"
}
}
]
}
}

That's it! You learned how to create SAS tokens to authorize how clients access your data.

Next step
Learn more about native document support
Learn more about granting access with SAS

Last updated on 11/18/2025

Managed identities for Language resources
Managed identities for Azure resources are service principals that create a Microsoft Entra
identity and specific permissions for Azure managed resources. Managed identities are a safer
way to grant access to storage data and replace the requirement for you to include shared
access signature tokens (SAS) with your source and target container URLs.
Screenshot of managed identity flow (`RBAC`).
You can use managed identities to grant access to any resource that supports Microsoft
Entra authentication, including your own applications.
To grant access to an Azure resource, assign an Azure role to a managed identity using
Azure role-based access control (Azure RBAC).
There's no added cost to use managed identities in Azure.
ï¼‰ Important
When using managed identities, don't include a SAS token URL with your HTTP
requests. Using managed identities replaces the requirement for you to include
shared access signature tokens (SAS) with your source and target container URLs.
To use managed identities for Language operations, you must create your Language
resource

in a specific geographic Azure region such as East US. If your Language

resource region is set to Global, then you can't use managed identity authentication.
You can, however, still use Shared Access Signature (SAS) tokens.

Prerequisites
To get started, you need the following resources:
An active Azure account

. If you don't have one, you can create a free account .

An single-service Azure Language in Foundry Tools

resource created in a regional

location.
A brief understanding of Azure role-based access control ( Azure RBAC ) using the Azure
portal.
An Azure Blob Storage account

in the same region as your Language resource. You

also need to create containers to store and organize your blob data within your storage

account.
If your storage account is behind a firewall, you must enable the following
configuration:
1. Go to the Azure portal

and sign in to your Azure account.

2. Select your Storage account.
3. In the Security + networking group in the left pane, select Networking.
4. In the Firewalls and virtual networks tab, select Enabled from selected virtual
networks and IP addresses.

5. Deselect all check boxes.
6. Make sure Microsoft network routing is selected.
7. Under the Resource instances section, select Microsoft.CognitiveServices/accounts
as the resource type and select your Language resource as the instance name.
8. Make certain that the Allow Azure services on the trusted services list to access
this storage account box is checked. For more information about managing
exceptions, see Configure Azure Storage firewalls and virtual networks.
Screenshot that shows the allow trusted services checkbox in the Azure portal.
9. Select Save.
ï¼— Note
It may take up to 5 minutes for the network changes to propagate.

Although network access is now permitted, your Language resource is still unable to
access the data in your Storage account. You need to create a managed identity for and
assign a specific access role to your Language resource.

Managed identity assignments
There are two types of managed identities: system-assigned and user-assigned. Currently,
Document Translation supports system-assigned managed identity:
A system-assigned managed identity is enabled directly on a service instance. It isn't
enabled by default; you must go to your resource and update the identity setting.
The system-assigned managed identity is tied to your resource throughout its lifecycle. If
you delete your resource, the managed identity is deleted as well.
In the following steps, we enable a system-assigned managed identity and grant your
Language resource limited access to your Azure Blob Storage account.

Enable a system-assigned managed identity
You must grant Azure Language resource access to your storage account before it can create,
read, or delete blobs. Once you enabled Azure Language resource with a system-assigned
managed identity, you can use Azure role-based access control ( Azure RBAC ), to give Language
features access to your Azure storage containers.
1. Go to the Azure portal

and sign in to your Azure account.

2. Select your Language resource.
3. In the Resource Management group in the left pane, select Identity. If your resource was
created in the global region, the Identity tab isn't visible. You can still use Shared Access
Signature (SAS) tokens for authentication.
4. Within the System assigned tab, turn on the Status toggle.
Screenshot that shows the resource management identity tab in the Azure portal.
ï¼‰ Important
User assigned managed identities don't meet the requirements for the batch
processing storage account scenario. Be sure to enable system assigned managed
identity.

5. Select Save.

Grant storage account access for your Language
resource
ï¼‰ Important
To assign a system-assigned managed identity role, you need
Microsoft.Authorization/roleAssignments/write permissions, such as Owner or User
Access Administrator at the storage scope for the storage resource.
1. Go to the Azure portal

and sign in to your Azure account.

2. Select your Language resource.
3. In the Resource Management group in the left pane, select Identity.
4. Under Permissions select Azure role assignments:
Screenshot that shows the enable system-assigned managed identity in Azure portal.
5. On the Azure role assignments page that opened, choose your subscription from the
drop-down menu then select + Add role assignment.
Screenshot that shows the Azure role assignments page in the Azure portal.
6. Next, assign a Storage Blob Data Contributor role to your Language resource. The
Storage Blob Data Contributor role gives Language (represented by the system-assigned
managed identity) read, write, and delete access to the blob container and data. In the
Add role assignment pop-up window, complete the fields as follows and select Save:
ï¾‰

Field

Value

Scope

Storage.

Subscription

The subscription associated with your storage resource.

Resource

The name of your storage resource.

Role

Storage Blob Data Contributor.

Screenshot that shows the role assignments page in the Azure portal.

Expand table

7. After the Added Role assignment confirmation message appears, refresh the page to see
the added role assignment.
Screenshot that shows the added role assignment confirmation pop-up message.
8. If you don't see the new role assignment right away, wait and try refreshing the page
again. When you assign or remove role assignments, it can take up to 30 minutes for
changes to take effect.

HTTP requests
A native document Language operation request is submitted to your Language endpoint
via a POST request.
With managed identity and Azure RBAC , you no longer need to include SAS URLs.
If successful, the POST method returns a 202 Accepted response code and the service
creates a request.
The processed documents appear in your target container.

Next steps
Native document support

Last updated on 11/18/2025

Call center overview
Azure Language in Foundry Tools and Azure Speech in Foundry Tools can help you realize
partial or full automation of telephony-based customer interactions, and provide accessibility
across multiple channels. With Azure Language and Speech services, you can further analyze
call center transcriptions, extract and redact conversation (PII), summarize the transcription,
and detect the sentiment.
Some example scenarios for the implementation of Foundry Tools in call and contact centers
are:
Virtual agents: Conversational AI-based telephony-integrated voice bots and voiceenabled chatbots
Agent-assist: Real-time transcription and analysis of a call to improve the customer
experience by providing insights and suggest actions to agents
Post-call analytics: Post-call analysis to create insights into customer conversations to
improve understanding and support continuous improvement of call handling,
optimization of quality assurance and compliance control as well as other insight driven
optimizations.
îª€ Tip
Try the Language Studio

or Speech Studio

for a demonstration on how to use Azure

Language and Speech services to analyze call center conversations.
To deploy a call center transcription solution to Azure with a no-code approach, try the
Ingestion Client.

Foundry Tools features for call centers
A holistic call center implementation typically incorporates technologies from Azure Language
and Speech services.
Audio data typically used in call centers generated through landlines, mobile phones, and
radios are often narrowband, in the range of 8 KHz, which can create challenges when you're
converting speech to text. The Speech service recognition models are trained to ensure that
you can get high-quality transcriptions, however you choose to capture the audio.
Once you transcribe your audio with the Speech service, you can use Azure Language service
to perform analytics on your call center data such as: sentiment analysis, summarizing the
reason for customer calls, how they were resolved, extracting and redacting conversation PII,
and more.

Speech service
The Speech service offers the following features that can be used for call center use cases:
Real-time speech to text: Recognize and transcribe audio in real-time from multiple
inputs. For example, with virtual agents or agent-assist, you can continuously recognize
audio input and control how to process results based on multiple events.
Batch speech to text: Transcribe large amounts of audio files asynchronously including
speaker diarization and is typically used in post-call analytics scenarios. Diarization is the
process of recognizing and separating speakers in mono channel audio data.
Text to speech: Text to speech enables your applications, tools, or devices to convert text
into human like synthesized speech.
Speaker identification: Helps you determine an unknown speakerâ€™s identity within a group
of enrolled speakers and is typically used for call center customer verification scenarios or
fraud detection.
Language Identification: Identify languages spoken in audio and can be used in real-time
and post-call analysis for insights or to control the environment (such as output language
of a virtual agent).
You might want to further customize and fine-tune the experience for your product or
environment. Typical examples for Speech fine-tuning include:
ï¾‰

Speech

Expand table

Description

customization
Custom speech

A speech to text feature used to evaluate and improve the speech recognition
accuracy of use-case specific entities (such as alpha-numeric customer, case, and
contract IDs, license plates, and names). You can also train a custom model with your
own product names and industry terminology.

Custom voice

A text to speech feature that lets you create a one-of-a-kind, customized, synthetic
voice for your applications.

Language service
The Language service offers the following features that can be used for call center use cases:
Personally Identifiable Information (PII) extraction and redaction: Identify, categorize, and
redact sensitive information in conversation transcription.
Conversation summarization: Summarize in abstract text what each conversation
participant said about the issues and resolutions. For example, a call center can group
product issues that have a high volume.

Sentiment analysis and opinion mining: Analyze transcriptions and associate positive,
neutral, or negative sentiment at the utterance and conversation-level.
You might want to further customize and fine-tune models to extract more information from
your data. Typical examples for Language customization include:
ï¾‰

Expand table

Language customization

Description

Custom NER (named entity

Improve the detection and extraction of entities in transcriptions.

recognition)
Custom text classification

Classify and label transcribed utterances with either single or
multiple classifications.

You can find an overview of all Language service features and customization options here.

Next steps
Post-call transcription and analytics quickstart
Try out Azure Language Studio
Try out the Speech Studio

Last updated on 11/18/2025

Quickstart: Post-call transcription and
analytics
Language service documentation | Language Studio

| Speech service documentation |

Speech Studio
In this C# quickstart, you perform sentiment analysis and conversation summarization of call
center transcriptions. The sample will automatically identify, categorize, and redact sensitive
information. The quickstart implements a cross-service scenario that uses features of the Azure
Cognitive Speech and Azure Cognitive Language services.
îª€ Tip
Try the Language Studio

or Speech Studio

for a demonstration on how to use Azure

Language and Speech services to analyze call center conversations.
To deploy a call center transcription solution to Azure with a no-code approach, try the
Ingestion Client.
The following Foundry Tools for Speech features are used in the quickstart:
Batch transcription: Submit a batch of audio files for transcription.
Speaker separation: Separate multiple speakers through diarization of mono 16khz 16 bit
PCM wav files.
The Language service offers the following features that are used:
Personally Identifiable Information (PII) extraction and redaction: Identify, categorize, and
redact sensitive information in conversation transcription.
Conversation summarization: Summarize in abstract text what each conversation
participant said about the issues and resolutions. For example, a call center can group
product issues that have a high volume.
Sentiment analysis and opinion mining: Analyze transcriptions and associate positive,
neutral, or negative sentiment at the utterance and conversation-level.

Prerequisites
ï¼‚ Azure subscription - Create one for free
ï¼‚ Create a multi-service resource

in the Azure portal. This quickstart only requires one

Foundry Tools multi-service resource. The sample code allows you to specify separate
Language and Speech resource keys.

ï¼‚ Get the resource key and region. After your Microsoft Foundry resource is deployed, select
Go to resource to view and manage keys.
ï¼‰ Important
This quickstart requires access to conversation summarization. To get access, you must
submit an online request

and have it approved.

The --languageKey and --languageEndpoint values in this quickstart must correspond to a
resource that's in one of the regions supported by the conversation summarization
API

: eastus , northeurope , and uksouth .

Run post-call transcription analysis with C#
Follow these steps to build and run the post-call transcription analysis quickstart code example.
1. Copy the scenarios/csharp/dotnetcore/call-center/
have Git installed

sample files from GitHub. If you

, open a command prompt and run the git clone command to

download the Speech SDK samples repository.
.NET CLI
git clone https://github.com/Azure-Samples/cognitive-services-speech-sdk.git

2. Open a command prompt and change to the project directory.
.NET CLI
cd <your-local-path>/scenarios/csharp/dotnetcore/call-center/call-center/

3. Build the project with the .NET CLI.
.NET CLI
dotnet build

4. Run the application with your preferred command line arguments. See usage and
arguments for the available options.
Here's an example that transcribes from an example audio file at GitHub :
.NET CLI

dotnet run --languageKey YourResourceKey --languageEndpoint
YourResourceEndpoint --speechKey YourResourceKey --speechRegion
YourResourceRegion --input "https://github.com/Azure-Samples/cognitiveservices-speech-sdk/raw/master/scenarios/callcenter/sampledata/Call1_separated_16k_health_insurance.wav" --stereo --output
summary.json

If you already have a transcription for input, here's an example that only requires a
Language resource:
.NET CLI
dotnet run --languageKey YourResourceKey --languageEndpoint
YourResourceEndpoint --jsonInput "YourTranscriptionFile.json" --stereo
output summary.json

--

Replace YourResourceKey with your Microsoft Foundry resource key, replace
YourResourceRegion with your Microsoft Foundry resource region (such as eastus ), and

replace YourResourceEndpoint with your Foundry Tools endpoint. Make sure that the
paths specified by --input and --output are valid. Otherwise you must change the paths.
ï¼‰ Important
Remember to remove the key from your code when you're done, and never post it
publicly. For production, use a secure way of storing and accessing your credentials
like Azure Key Vault. See the Foundry Tools security article for more information.

Check results
The console output shows the full conversation and summary. Here's an example of the overall
summary, with redactions for brevity:
Output
Conversation summary:
issue: Customer wants to sign up for insurance.
resolution: Customer was advised that customer would be contacted by the
insurance company.

If you specify the --output FILE optional argument, a JSON version of the results are written
to the file. The file output is a combination of the JSON responses from the batch transcription
(Speech), sentiment (Language), and conversation summarization (Language) APIs.

The transcription property contains a JSON object with the results of sentiment analysis
merged with batch transcription. Here's an example, with redactions for brevity:
JSON
{
"source": "https://github.com/Azure-Samples/cognitive-services-speechsdk/raw/master/scenarios/callcenter/sampledata/Call1_separated_16k_health_insurance.wav",
// Example results redacted for brevity
"nBest": [
{
"confidence": 0.77464247,
"lexical": "hello thank you for calling contoso who am i speaking with
today",
"itn": "hello thank you for calling contoso who am i speaking with
today",
"maskedITN": "hello thank you for calling contoso who am i speaking with
today",
"display": "Hello, thank you for calling Contoso. Who am I speaking with
today?",
"sentiment": {
"positive": 0.78,
"neutral": 0.21,
"negative": 0.01
}
},
]
// Example results redacted for brevity
}

The conversationAnalyticsResults property contains a JSON object with the results of the
conversation PII and conversation summarization analysis. Here's an example, with redactions
for brevity:
JSON
{
"conversationAnalyticsResults": {
"conversationSummaryResults": {
"conversations": [
{
"id": "conversation1",
"summaries": [
{
"aspect": "issue",
"text": "Customer wants to sign up for insurance"
},
{
"aspect": "resolution",
"text": "Customer was advised that customer would be contacted by the
insurance company"

}
],
"warnings": []
}
],
"errors": [],
"modelVersion": "2022-05-15-preview"
},
"conversationPiiResults": {
"combinedRedactedContent": [
{
"channel": "0",
"display": "Hello, thank you for calling Contoso. Who am I speaking with
today? Hi, ****. Uh, are you calling because you need health insurance?", // Example
results redacted for brevity
"itn": "hello thank you for calling contoso who am i speaking with today
hi **** uh are you calling because you need health insurance", // Example results
redacted for brevity
"lexical": "hello thank you for calling contoso who am i speaking with
today hi **** uh are you calling because you need health insurance" // Example
results redacted for brevity
},
{
"channel": "1",
"display": "Hi, my name is **********. I'm trying to enroll myself with
Contoso. Yes. Yeah, I'm calling to sign up for insurance.", // Example results
redacted for brevity
"itn": "hi my name is ********** i'm trying to enroll myself with contoso
yes yeah i'm calling to sign up for insurance", // Example results redacted for
brevity
"lexical": "hi my name is ********** i'm trying to enroll myself with
contoso yes yeah i'm calling to sign up for insurance" // Example results redacted
for brevity
}
],
"conversations": [
{
"id": "conversation1",
"conversationItems": [
{
"id": "0",
"redactedContent": {
"itn": "hello thank you for calling contoso who am i speaking with
today",
"lexical": "hello thank you for calling contoso who am i speaking
with today",
"text": "Hello, thank you for calling Contoso. Who am I speaking
with today?"
},
"entities": [],
"channel": "0",
"offset": "PT0.77S"
},
{
"id": "1",

"redactedContent": {
"itn": "hi my name is ********** i'm trying to enroll myself with
contoso",
"lexical": "hi my name is ********** i'm trying to enroll myself
with contoso",
"text": "Hi, my name is **********. I'm trying to enroll myself with
Contoso."
},
"entities": [
{
"text": "Mary Rondo",
"category": "Name",
"offset": 15,
"length": 10,
"confidenceScore": 0.97
}
],
"channel": "1",
"offset": "PT4.55S"
},
{
"id": "2",
"redactedContent": {
"itn": "hi **** uh are you calling because you need health
insurance",
"lexical": "hi **** uh are you calling because you need health
insurance",
"text": "Hi, ****. Uh, are you calling because you need health
insurance?"
},
"entities": [
{
"text": "Mary",
"category": "Name",
"offset": 4,
"length": 4,
"confidenceScore": 0.93
}
],
"channel": "0",
"offset": "PT9.55S"
},
{
"id": "3",
"redactedContent": {
"itn": "yes yeah i'm calling to sign up for insurance",
"lexical": "yes yeah i'm calling to sign up for insurance",
"text": "Yes. Yeah, I'm calling to sign up for insurance."
},
"entities": [],
"channel": "1",
"offset": "PT13.09S"
},
// Example results redacted for brevity
],

"warnings": []
}
]
}
}
}

Usage and arguments
Usage: call-center -- [...]
ï¼‰ Important
You can use a multi-service

resource or separate Language

and Speech

resources.

In either case, the --languageKey and --languageEndpoint values must correspond to a
resource that's in one of the regions supported by the conversation summarization
API

: eastus , northeurope , and uksouth .

Connection options include:
--speechKey KEY : Your Foundry

resource key. Required for audio transcriptions with the

--input from URL option.
--speechRegion REGION : Your Foundry

resource region. Required for audio

transcriptions with the --input from URL option. Examples: eastus , northeurope
--languageKey KEY : Your Foundry

resource key. Required.

--languageEndpoint ENDPOINT : Your Foundry

resource endpoint. Required. Example:

https://YourResourceName.cognitiveservices.azure.com

Input options include:
--input URL : Input audio from URL. You must set either the --input or --jsonInput

option.
--jsonInput FILE : Input an existing batch transcription JSON result from FILE. With this

option, you only need a Language resource to process a transcription that you already
have. With this option, you don't need an audio file or a Foundry resource for Speech.
Overrides --input . You must set either the --input or --jsonInput option.
--stereo : Indicates that the audio via ```input URL` should be in stereo format. If stereo

isn't specified, then mono 16khz 16 bit PCM wav files are assumed. Diarization of mono

files is used to separate multiple speakers. Diarization of stereo files isn't supported, since
2-channel stereo files should already have one speaker per channel.
--certificate : The PEM certificate file. Required for C++.

Language options include:
--language LANGUAGE : The language to use for sentiment analysis and conversation

analysis. This value should be a two-letter ISO 639-1 code. The default value is en .
--locale LOCALE : The locale to use for batch transcription of audio. The default value is
en-US .

Output options include:
--help : Show the usage help and stop
--output FILE : Output the transcription, sentiment, conversation PII, and conversation

summaries in JSON format to a text file. For more information, see output examples.

Clean up resources
You can use the Azure portal or Azure Command Line Interface (CLI) to remove the Microsoft
Foundry resource you created.

Next steps
Try the Ingestion Client

Last updated on 11/18/2025

Ingestion Client with Foundry Tools
The Ingestion Client is a tool released by Microsoft on GitHub that helps you quickly deploy a
call center transcription solution to Azure with a no-code approach.
îª€ Tip
You can use the tool and resulting solution in production to process a high volume of
audio.
Ingestion Client uses the Azure Language in Foundry Tools, Azure Speech in Foundry Tools,
Azure storage

, and Azure Functions

.

Get started with the Ingestion Client
An Azure account and a multi-service Microsoft Foundry resource are needed to run the
Ingestion Client.
Azure subscription - Create one for free
Create a Foundry resource

in the Azure portal.

Get the resource key and region. After your resource is deployed, select Go to resource to
view and manage keys. For more information about Microsoft Foundry resources, see this
quickstart.

Ingestion Client Features
The Ingestion Client works by connecting a dedicated Azure storage
Azure Functions

account to custom

in a serverless fashion to pass transcription requests to the service. The

transcribed audio files land in the dedicated Azure Storage container

.

ï¼‰ Important
Pricing varies depending on the mode of operation (batch vs real-time) as well as the
Azure Function SKU selected. By default the tool will create a Premium Azure Function
SKU to handle large volume. Visit the Pricing

page for more information.

Internally, the tool uses Speech and Language services, and follows best practices to handle
scale-up, retries and failover. The following schematic describes the resources and connections.

The following Speech service feature is used by the Ingestion Client:
Batch speech to text: Transcribe large amounts of audio files asynchronously including
speaker diarization and is typically used in post-call analytics scenarios. Diarization is the
process of recognizing and separating speakers in mono channel audio data.
Here are some Language service features that are used by the Ingestion Client:
Personally Identifiable Information (PII) extraction and redaction: Identify, categorize, and
redact sensitive information in conversation transcription.
Sentiment analysis and opinion mining: Analyze transcriptions and associate positive,
neutral, or negative sentiment at the utterance and conversation-level.
Besides Foundry Tools, these Azure products are used to complete the solution:
Azure storage

: Used for storing telephony data and the transcripts that batch

transcription API returns. This storage account should use notifications, specifically for
when new files are added. These notifications are used to trigger the transcription
process.
Azure Functions

: Used for creating the shared access signature (SAS) URI for each

recording, and triggering the HTTP POST request to start a transcription. Additionally, you
use Azure Functions to create requests to retrieve and delete transcriptions by using the
Batch Transcription API.

Tool customization

The tool is built to show customers results quickly. You can customize the tool to your
preferred SKUs and setup. The SKUs can be edited from the Azure portal
is available on GitHub

and the code itself

.

ï¼— Note
We suggest creating the resources in the same dedicated resource group to understand
and track costs more easily.

Next steps
Learn more about Foundry Tools features for call center
Explore the Language service features
Explore the Speech service features

Last updated on 11/18/2025

Azure AI Language REST API reference
10/21/2025

The API reference for authoring and runtime APIs for Azure AI Language.

Links to services
ï¾‰

Expand table

Service

Description

API Reference
(Latest GA
version)

API Reference
(Latest Preview
version)

Text - runtime

Runtime prediction calls to the
following features:
* entity linking
* key phrase extraction
* language detection
* Named Entity Recognition
* language detection
* Personally Identifiable Information
(PII)
* sentiment analysis
* text analytics for health
* summarization
* custom text classification
* custom Named Entity Recognition
(NER)

2024-11-01

2025-05-15preview

Documents - runtime

* Personally Identifiable Information

Not available

2024-11-15-

(PII)
* summarization
Conversation - runtime

* Personally Identifiable Information
(PII)

preview

2024-11-01

2025-05-15preview

2023-04-01

2025-05-15-

* summarization
Text - authoring

Authoring API calls to create, build,
train, and deploy your projects Custom
Text Classification or Custom Named
Entity Recognition projects.

Conversational Language
Understanding &

Authoring API calls to create, build,
manage, train, and deploy your CLU

Orchestration workflow authoring

projects

preview

2023-04-01

2025-05-15preview

Service

Description

API Reference
(Latest GA
version)

API Reference
(Latest Preview
version)

Conversational Language
Understanding &

Runtime prediction calls to query your
deployed CLU project

2024-11-01

2025-05-15preview

Custom Question
Answering - authoring

Authoring API calls to create, build,
and deploy your projects

2023-04-01

2025-05-15preview

Custom Question
Answering - runtime

Runtime prediction calls to query
custom question answering models.

2023-04-01

2025-05-15preview

Question Answering -

Runtime prediction calls to query

2023-04-01

2025-05-15-

runtime

question answering models.

Orchestration workflow runtime

preview

Azure Cognitive Services Text Analytics
client library for .NET - version 5.3.0
Article â€¢ 06/20/2023

Text Analytics is part of the Azure Cognitive Service for Language, a cloud-based service that
provides Natural Language Processing (NLP) features for understanding and analyzing text.
This client library offers the following features:
Language detection
Sentiment analysis
Key phrase extraction
Named entity recognition (NER)
Personally identifiable information (PII) entity recognition
Entity linking
Text analytics for health
Custom named entity recognition (Custom NER)
Custom text classification
Extractive text summarization
Abstractive text summarization
Source code

| Package (NuGet)

| API reference documentation

| Product documentation

| Samples

Getting started
Install the package
Install the Azure Text Analytics client library for .NET with NuGet

:

.NET CLI

dotnet add package Azure.AI.TextAnalytics

This table shows the relationship between SDK versions and supported API versions of the
service:
Note that 5.2.0 is the first stable version of the client library that targets the Azure
Cognitive Service for Language APIs which includes the existing text analysis and natural
language processing features found in the Text Analytics client library. In addition, the
service API has changed from semantic to date-based versioning.

ï¾‰

SDK version

Supported API version of service

5.3.X

3.0, 3.1, 2022-05-01, 2023-04-01 (default)

5.2.X

3.0, 3.1, 2022-05-01 (default)

5.1.X

3.0, 3.1 (default)

5.0.X

3.0

1.0.X

3.0

Expand table

Prerequisites
An Azure subscription

.

An existing Cognitive Services or Language service resource.

Create a Cognitive Services resource or a Language service resource
Azure Cognitive Service for Language supports both multi-service and single-service access.
Create a Cognitive Services resource if you plan to access multiple cognitive services under a
single endpoint and API key. To access the features of the Language service only, create a
Language service resource instead.
You can create either resource via the Azure portal or, alternatively, you can follow the steps in
this document to create it using the Azure CLI.

Authenticate the client
Interaction with the service using the client library begins with creating an instance of the
TextAnalyticsClient

class. You will need an endpoint, and either an API key or

TokenCredential to instantiate a client object. For more information regarding authenticating

with cognitive services, see Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and API key from the Cognitive Services resource or Language
service resource information in the Azure Portal

.

Alternatively, use the Azure CLI snippet below to get the API key from the Language service
resource.

PowerShell

az cognitiveservices account keys list --resource-group <your-resource-group-name>
--name <your-resource-name>

Create a TextAnalyticsClient using an API key credential
Once you have the value for the API key, create an AzureKeyCredential . This will allow you to
update the API key without creating a new client.
With the value of the endpoint and an AzureKeyCredential , you can create the
TextAnalyticsClient :
C#

Uri endpoint = new("<endpoint>");
AzureKeyCredential credential = new("<apiKey>");
TextAnalyticsClient client = new(endpoint, credential);

Create a TextAnalyticsClient with an Azure Active Directory credential
Client API key authentication is used in most of the examples in this getting started guide, but
you can also authenticate with Azure Active Directory using the Azure Identity library

. Note

that regional endpoints do not support AAD authentication. Create a custom subdomain for
your resource in order to use this type of authentication.
To use the DefaultAzureCredential

provider shown below, or other credential providers

provided with the Azure SDK, please install the Azure.Identity package:
.NET CLI

dotnet add package Azure.Identity

You will also need to register a new AAD application and grant access to the Language service
by assigning the "Cognitive Services User" role to your service principal.
Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET.
C#

Uri endpoint = new("<endpoint>");
TextAnalyticsClient client = new(endpoint, new DefaultAzureCredential());

Key concepts
TextAnalyticsClient
A TextAnalyticsClient is the primary interface for developers using the Text Analytics client
library. It provides both synchronous and asynchronous operations to access a specific use of
text analysis, such as language detection or key phrase extraction.

Input
A document, is a single unit of input to be analyzed by the predictive models in the Language
service. Operations on TextAnalyticsClient may take a single document or a collection of
documents to be analyzed as a batch. For document length limits, maximum batch size, and
supported text encoding see here

.

Operation on multiple documents
For each supported operation, TextAnalyticsClient provides a method that accepts a batch of
documents as strings, or a batch of either TextDocumentInput or DetectLanguageInput objects.
This methods allow callers to give each document a unique ID, indicate that the documents in
the batch are written in different languages, or provide a country hint about the language of
the document.
Note: It is recommended to use the batch methods when working on production environments
as they allow you to send one request with multiple documents. This is more performant than
sending a request per each document.

Return value
Return values, such as AnalyzeSentimentResult , is the result of a Text Analytics operation,
containing a prediction or predictions about a single document. An operation's return value
also may optionally include information about the document and how it was processed.

Return value Collection
A Return value collection, such as AnalyzeSentimentResultCollection , is a collection of
operation results, where each corresponds to one of the documents provided in the input
batch. A document and its result will have the same index in the input and result collections.
The return value also contains a HasError property that allows to identify if an operation

executed was successful or unsuccessful for the given document. It may optionally include
information about the document batch and how it was processed.

Long-Running Operations
For large documents which take a long time to execute, these operations are implemented as
long-running operations . Long-running operations consist of an initial request sent to the
service to start an operation, followed by polling the service at intervals to determine whether
the operation has completed or failed, and if it has succeeded, to get the result.
For long running operations in the Azure SDK, the client exposes a Start<operation-name>
method that returns an Operation<T> or a PageableOperation<T> . You can use the extension
method WaitForCompletionAsync() to wait for the operation to complete and obtain its result.
A sample code snippet is provided to illustrate using long-running operations below.

Thread safety
We guarantee that all client instance methods are thread-safe and independent of each other
(guideline ). This ensures that the recommendation of reusing client instances is always safe,
even across threads.

Additional concepts
Client options

| Accessing the response

| Handling failures

| Diagnostics

| Mocking

|

Client lifetime

Examples
The following section provides several code snippets using the client created above, and
covers the main features present in this client library. Although most of the snippets below
make use of synchronous service calls, keep in mind that the Azure.AI.TextAnalytics package
supports both synchronous and asynchronous APIs.

Sync examples
Detect Language
Analyze Sentiment
Extract Key Phrases
Recognize Named Entities
Recognize PII Entities

Recognize Linked Entities

Async examples
Detect Language Asynchronously
Recognize Named Entities Asynchronously
Analyze Healthcare Entities Asynchronously
Run multiple actions Asynchronously

Detect Language
Run a predictive model to determine the language that the passed-in document or batch of
documents are written in.
C#

string document =
"Este documento estÃ¡ escrito en un lenguaje diferente al inglÃ©s. Su objectivo
es demostrar cÃ³mo"
+ " invocar el mÃ©todo de DetecciÃ³n de Lenguaje del servicio de Text Analytics
en Microsoft Azure."
+ " TambiÃ©n muestra cÃ³mo acceder a la informaciÃ³n retornada por el servicio.
Esta funcionalidad es"
+ " Ãºtil para los sistemas de contenido que recopilan texto arbitrario, donde
el lenguaje no se conoce"
+ " de antemano. Puede usarse para detectar una amplia gama de lenguajes,
variantes, dialectos y"
+ " algunos idiomas regionales o culturales.";
try
{
Response<DetectedLanguage> response = client.DetectLanguage(document);
DetectedLanguage language = response.Value;
Console.WriteLine($"Detected language is {language.Name} with a confidence
score of {language.ConfidenceScore}.");
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

For samples on using the production recommended option DetectLanguageBatch see here

.

Please refer to the service documentation for a conceptual discussion of language detection.

Analyze Sentiment
Run a predictive model to determine the positive, negative, neutral or mixed sentiment
contained in the passed-in document or batch of documents.
C#

string document =
"I had the best day of my life. I decided to go sky-diving and it made me
appreciate my whole life so"
+ "much more. I developed a deep-connection with my instructor as well, and I
feel as if I've made a"
+ "life-long friend in her.";
try
{
Response<DocumentSentiment> response = client.AnalyzeSentiment(document);
DocumentSentiment docSentiment = response.Value;
Console.WriteLine($"Document sentiment is {docSentiment.Sentiment} with: ");
Console.WriteLine($" Positive confidence score:
{docSentiment.ConfidenceScores.Positive}");
Console.WriteLine($" Neutral confidence score:
{docSentiment.ConfidenceScores.Neutral}");
Console.WriteLine($" Negative confidence score:
{docSentiment.ConfidenceScores.Negative}");
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

For samples on using the production recommended option AnalyzeSentimentBatch see here
To get more granular information about the opinions related to targets of a product/service,
also known as Aspect-based Sentiment Analysis in Natural Language Processing (NLP), see a
sample on sentiment analysis with opinion mining here

.

Please refer to the service documentation for a conceptual discussion of sentiment analysis.

Extract Key Phrases
Run a model to identify a collection of significant phrases found in the passed-in document or
batch of documents.
C#

.

string document =
"My cat might need to see a veterinarian. It has been sneezing more than
normal, and although my"
+ " little sister thinks it is funny, I am worried it has the cold that I got
last week. We are going"
+ " to call tomorrow and try to schedule an appointment for this week.
Hopefully it will be covered by"
+ " the cat's insurance. It might be good to not let it sleep in my room for a
while.";
try
{
Response<KeyPhraseCollection> response = client.ExtractKeyPhrases(document);
KeyPhraseCollection keyPhrases = response.Value;
Console.WriteLine($"Extracted {keyPhrases.Count} key phrases:");
foreach (string keyPhrase in keyPhrases)
{
Console.WriteLine($" {keyPhrase}");
}
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

For samples on using the production recommended option ExtractKeyPhrasesBatch see
here

.

Please refer to the service documentation for a conceptual discussion of key phrase extraction.

Recognize Named Entities
Run a predictive model to identify a collection of named entities in the passed-in document or
batch of documents and categorize those entities into categories such as person, location, or
organization. For more information on available categories, see Text Analytics Named Entity
Categories.
C#

string document =
"We love this trail and make the trip every year. The views are breathtaking
and well worth the hike!"
+ " Yesterday was foggy though, so we missed the spectacular views. We tried
again today and it was"
+ " amazing. Everyone in my family liked the trail although it was too
challenging for the less"
+ " athletic among us. Not necessarily recommended for small children. A hotel

close to the trail"
+ " offers services for childcare in case you want that.";
try
{
Response<CategorizedEntityCollection> response =
client.RecognizeEntities(document);
CategorizedEntityCollection entitiesInDocument = response.Value;
Console.WriteLine($"Recognized {entitiesInDocument.Count} entities:");
foreach (CategorizedEntity entity in entitiesInDocument)
{
Console.WriteLine($" Text: {entity.Text}");
Console.WriteLine($" Offset: {entity.Offset}");
Console.WriteLine($" Length: {entity.Length}");
Console.WriteLine($" Category: {entity.Category}");
if (!string.IsNullOrEmpty(entity.SubCategory))
Console.WriteLine($" SubCategory: {entity.SubCategory}");
Console.WriteLine($" Confidence score: {entity.ConfidenceScore}");
Console.WriteLine();
}
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

For samples on using the production recommended option RecognizeEntitiesBatch see
here

.

Please refer to the service documentation for a conceptual discussion of named entity
recognition.

Recognize PII Entities
Run a predictive model to identify a collection of entities containing Personally Identifiable
Information found in the passed-in document or batch of documents, and categorize those
entities into categories such as US social security number, drivers license number, or credit card
number.
C#

string document =
"Parker Doe has repaid all of their loans as of 2020-04-25. Their SSN is 85998-0987. To contact them,"
+ " use their phone number 800-102-1100. They are originally from Brazil and
have document ID number"
+ " 998.214.865-68.";

try
{
Response<PiiEntityCollection> response =
client.RecognizePiiEntities(document);
PiiEntityCollection entities = response.Value;
Console.WriteLine($"Redacted Text: {entities.RedactedText}");
Console.WriteLine();
Console.WriteLine($"Recognized {entities.Count} PII entities:");
foreach (PiiEntity entity in entities)
{
Console.WriteLine($" Text: {entity.Text}");
Console.WriteLine($" Category: {entity.Category}");
if (!string.IsNullOrEmpty(entity.SubCategory))
Console.WriteLine($" SubCategory: {entity.SubCategory}");
Console.WriteLine($" Confidence score: {entity.ConfidenceScore}");
Console.WriteLine();
}
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

For samples on using the production recommended option RecognizePiiEntitiesBatch see
here

.

Please refer to the service documentation for supported PII entity types.

Recognize Linked Entities
Run a predictive model to identify a collection of entities found in the passed-in document or
batch of documents, and include information linking the entities to their corresponding entries
in a well-known knowledge base.
C#

string document =
"Microsoft was founded by Bill Gates with some friends he met at Harvard. One
of his friends, Steve"
+ " Ballmer, eventually became CEO after Bill Gates as well. Steve Ballmer
eventually stepped down as"
+ " CEO of Microsoft, and was succeeded by Satya Nadella. Microsoft originally
moved its headquarters"
+ " to Bellevue, Washington in Januaray 1979, but is now headquartered in
Redmond.";
try
{

Response<LinkedEntityCollection> response =
client.RecognizeLinkedEntities(document);
LinkedEntityCollection linkedEntities = response.Value;
Console.WriteLine($"Recognized {linkedEntities.Count} entities:");
foreach (LinkedEntity linkedEntity in linkedEntities)
{
Console.WriteLine($" Name: {linkedEntity.Name}");
Console.WriteLine($" Language: {linkedEntity.Language}");
Console.WriteLine($" Data Source: {linkedEntity.DataSource}");
Console.WriteLine($" URL: {linkedEntity.Url}");
Console.WriteLine($" Entity Id in Data Source:
{linkedEntity.DataSourceEntityId}");
foreach (LinkedEntityMatch match in linkedEntity.Matches)
{
Console.WriteLine($"
Match Text: {match.Text}");
Console.WriteLine($"
Offset: {match.Offset}");
Console.WriteLine($"
Length: {match.Length}");
Console.WriteLine($"
Confidence score: {match.ConfidenceScore}");
}
Console.WriteLine();
}
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

For samples on using the production recommended option RecognizeLinkedEntitiesBatch see
here

.

Please refer to the service documentation for a conceptual discussion of entity linking.

Detect Language Asynchronously
Run a predictive model to determine the language that the passed-in document or batch of
documents are written in.
C#

string document =
"Este documento estÃ¡ escrito en un lenguaje diferente al inglÃ©s. Su objectivo
es demostrar cÃ³mo"
+ " invocar el mÃ©todo de DetecciÃ³n de Lenguaje del servicio de Text Analytics
en Microsoft Azure."
+ " TambiÃ©n muestra cÃ³mo acceder a la informaciÃ³n retornada por el servicio.
Esta funcionalidad es"
+ " Ãºtil para los sistemas de contenido que recopilan texto arbitrario, donde
el lenguaje no se conoce"
+ " de antemano. Puede usarse para detectar una amplia gama de lenguajes,

variantes, dialectos y"
+ " algunos idiomas regionales o culturales.";
try
{
Response<DetectedLanguage> response = await
client.DetectLanguageAsync(document);
DetectedLanguage language = response.Value;
Console.WriteLine($"Detected language is {language.Name} with a confidence
score of {language.ConfidenceScore}.");
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

Recognize Named Entities Asynchronously
Run a predictive model to identify a collection of named entities in the passed-in document or
batch of documents and categorize those entities into categories such as person, location, or
organization. For more information on available categories, see Text Analytics Named Entity
Categories.
C#

string document =
"We love this trail and make the trip every year. The views are breathtaking
and well worth the hike!"
+ " Yesterday was foggy though, so we missed the spectacular views. We tried
again today and it was"
+ " amazing. Everyone in my family liked the trail although it was too
challenging for the less"
+ " athletic among us. Not necessarily recommended for small children. A hotel
close to the trail"
+ " offers services for childcare in case you want that.";
try
{
Response<CategorizedEntityCollection> response = await
client.RecognizeEntitiesAsync(document);
CategorizedEntityCollection entitiesInDocument = response.Value;
Console.WriteLine($"Recognized {entitiesInDocument.Count} entities:");
foreach (CategorizedEntity entity in entitiesInDocument)
{
Console.WriteLine($" Text: {entity.Text}");
Console.WriteLine($" Offset: {entity.Offset}");
Console.WriteLine($" Length: {entity.Length}");
Console.WriteLine($" Category: {entity.Category}");

if (!string.IsNullOrEmpty(entity.SubCategory))
Console.WriteLine($" SubCategory: {entity.SubCategory}");
Console.WriteLine($" Confidence score: {entity.ConfidenceScore}");
Console.WriteLine();
}
}
catch (RequestFailedException exception)
{
Console.WriteLine($"Error Code: {exception.ErrorCode}");
Console.WriteLine($"Message: {exception.Message}");
}

Analyze Healthcare Entities Asynchronously
Text Analytics for health is a containerized service that extracts and labels relevant medical
information from unstructured texts such as doctor's notes, discharge summaries, clinical
documents, and electronic health records. For more information see How to: Use Text Analytics
for health.
C#

string documentA =
"RECORD #333582770390100 | MH | 85986313 | | 054351 | 2/14/2001 12:00:00 AM |"
+ " CORONARY ARTERY DISEASE | Signed | DIS |"
+ Environment.NewLine
+ " Admission Date: 5/22/2001 Report Status: Signed Discharge Date: 4/24/2001"
+ " ADMISSION DIAGNOSIS: CORONARY ARTERY DISEASE."
+ Environment.NewLine
+ " HISTORY OF PRESENT ILLNESS: The patient is a 54-year-old gentleman with a
history of progressive"
+ " angina over the past several months. The patient had a cardiac
catheterization in July of this"
+ " year revealing total occlusion of the RCA and 50% left main disease, with
a strong family history"
+ " of coronary artery disease with a brother dying at the age of 52 from a
myocardial infarction and"
+ " another brother who is status post coronary artery bypass grafting. The
patient had a stress"
+ " echocardiogram done on July, 2001, which showed no wall motion
abnormalities, but this was a"
+ " difficult study due to body habitus. The patient went for six minutes with
minimal ST depressions"
+ " in the anterior lateral leads, thought due to fatigue and wrist pain, his
anginal equivalent. Due"
+ " to the patient'sincreased symptoms and family history and history left
main disease with total"
+ " occasional of his RCA was referred for revascularization with open heart
surgery.";
string documentB = "Prescribed 100mg ibuprofen, taken twice daily.";

// Prepare the input of the text analysis operation. You can add multiple
documents to this list and
// perform the same operation on all of them simultaneously.
List<string> batchedDocuments = new()
{
documentA,
documentB
};
// Perform the text analysis operation.
AnalyzeHealthcareEntitiesOperation operation = await
client.AnalyzeHealthcareEntitiesAsync(WaitUntil.Completed, batchedDocuments);
Console.WriteLine($"The operation has completed.");
Console.WriteLine();
// View the operation status.
Console.WriteLine($"Created On
: {operation.CreatedOn}");
Console.WriteLine($"Expires On
: {operation.ExpiresOn}");
Console.WriteLine($"Id
: {operation.Id}");
Console.WriteLine($"Status
: {operation.Status}");
Console.WriteLine($"Last Modified: {operation.LastModified}");
Console.WriteLine();
// View the operation results.
await foreach (AnalyzeHealthcareEntitiesResultCollection documentsInPage in
operation.Value)
{
Console.WriteLine($"Analyze Healthcare Entities, model version: \"
{documentsInPage.ModelVersion}\"");
Console.WriteLine();
foreach (AnalyzeHealthcareEntitiesResult documentResult in documentsInPage)
{
if (documentResult.HasError)
{
Console.WriteLine($" Error!");
Console.WriteLine($" Document error code:
{documentResult.Error.ErrorCode}");
Console.WriteLine($" Message: {documentResult.Error.Message}");
continue;
}
Console.WriteLine($" Recognized the following
{documentResult.Entities.Count} healthcare entities:");
Console.WriteLine();
// View the healthcare entities that were recognized.
foreach (HealthcareEntity entity in documentResult.Entities)
{
Console.WriteLine($" Entity: {entity.Text}");
Console.WriteLine($" Category: {entity.Category}");
Console.WriteLine($" Offset: {entity.Offset}");
Console.WriteLine($" Length: {entity.Length}");
Console.WriteLine($" NormalizedText: {entity.NormalizedText}");

Console.WriteLine($"

Links:");

// View the entity data sources.
foreach (EntityDataSource entityDataSource in entity.DataSources)
{
Console.WriteLine($"
Entity ID in Data Source:
{entityDataSource.EntityId}");
Console.WriteLine($"
DataSource: {entityDataSource.Name}");
}
// View the entity assertions.
if (entity.Assertion is not null)
{
Console.WriteLine($" Assertions:");
if (entity.Assertion?.Association is not null)
{
Console.WriteLine($"
Association:
{entity.Assertion?.Association}");
}
if (entity.Assertion?.Certainty is not null)
{
Console.WriteLine($"
Certainty:
{entity.Assertion?.Certainty}");
}
if (entity.Assertion?.Conditionality is not null)
{
Console.WriteLine($"
Conditionality:
{entity.Assertion?.Conditionality}");
}
}
}
Console.WriteLine($" We found {documentResult.EntityRelations.Count}
relations in the current document:");
Console.WriteLine();
// View the healthcare entity relations that were recognized.
foreach (HealthcareEntityRelation relation in
documentResult.EntityRelations)
{
Console.WriteLine($"
Relation: {relation.RelationType}");
if (relation.ConfidenceScore is not null)
{
Console.WriteLine($"
ConfidenceScore:
{relation.ConfidenceScore}");
}
Console.WriteLine($"
For this relation there are
{relation.Roles.Count} roles");
// View the relation roles.
foreach (HealthcareEntityRelationRole role in relation.Roles)
{

Console.WriteLine($"
Console.WriteLine($"
{role.Entity.Text}");
Console.WriteLine($"
{role.Entity.Category}");
Console.WriteLine();
}

Role Name: {role.Name}");
Associated Entity Text:
Associated Entity Category:

Console.WriteLine();
}
Console.WriteLine();
}
}

Run multiple actions Asynchronously
This functionality allows running multiple actions in one or more documents. Actions include:
Named Entities Recognition
PII Entities Recognition
Linked Entity Recognition
Key Phrase Extraction
Sentiment Analysis
Healthcare Entities Recognition (see sample here

)

Custom Named Entities Recognition (see sample here
Custom Single Label Classification (see sample here
Custom Multi Label Classification (see sample here

)
)

)

C#

string documentA =
"We love this trail and make the trip every year. The views are
breathtaking and well worth the hike!"
+ " Yesterday was foggy though, so we missed the spectacular views. We
tried again today and it was"
+ " amazing. Everyone in my family liked the trail although it was too
challenging for the less"
+ " athletic among us.";
string documentB =
"Last week we stayed at Hotel Foo to celebrate our anniversary. The staff
knew about our anniversary"
+ " so they helped me organize a little surprise for my partner. The room
was clean and with the"
+ " decoration I requested. It was perfect!";
// Prepare the input of the text analysis operation. You can add multiple

documents to this list and
// perform the same operation on all of them simultaneously.
List<string> batchedDocuments = new()
{
documentA,
documentB
};
TextAnalyticsActions actions = new()
{
ExtractKeyPhrasesActions = new List<ExtractKeyPhrasesAction>() { new
ExtractKeyPhrasesAction() { ActionName = "ExtractKeyPhrasesSample" } },
RecognizeEntitiesActions = new List<RecognizeEntitiesAction>() { new
RecognizeEntitiesAction() { ActionName = "RecognizeEntitiesSample" } },
DisplayName = "AnalyzeOperationSample"
};
// Perform the text analysis operation.
AnalyzeActionsOperation operation = await
client.AnalyzeActionsAsync(WaitUntil.Completed, batchedDocuments, actions);
// View the operation status.
Console.WriteLine($"Created On
: {operation.CreatedOn}");
Console.WriteLine($"Expires On
: {operation.ExpiresOn}");
Console.WriteLine($"Id
: {operation.Id}");
Console.WriteLine($"Status
: {operation.Status}");
Console.WriteLine($"Last Modified: {operation.LastModified}");
Console.WriteLine();
if (!string.IsNullOrEmpty(operation.DisplayName))
{
Console.WriteLine($"Display name: {operation.DisplayName}");
Console.WriteLine();
}
Console.WriteLine($"Total actions: {operation.ActionsTotal}");
Console.WriteLine($" Succeeded actions: {operation.ActionsSucceeded}");
Console.WriteLine($" Failed actions: {operation.ActionsFailed}");
Console.WriteLine($" In progress actions: {operation.ActionsInProgress}");
Console.WriteLine();
await foreach (AnalyzeActionsResult documentsInPage in operation.Value)
{
IReadOnlyCollection<ExtractKeyPhrasesActionResult> keyPhrasesResults =
documentsInPage.ExtractKeyPhrasesResults;
IReadOnlyCollection<RecognizeEntitiesActionResult> entitiesResults =
documentsInPage.RecognizeEntitiesResults;
Console.WriteLine("Recognized Entities");
int docNumber = 1;
foreach (RecognizeEntitiesActionResult entitiesActionResults in
entitiesResults)
{
Console.WriteLine($" Action name:
{entitiesActionResults.ActionName}");

Console.WriteLine();
foreach (RecognizeEntitiesResult documentResult in
entitiesActionResults.DocumentsResults)
{
Console.WriteLine($" Document #{docNumber++}");
Console.WriteLine($" Recognized {documentResult.Entities.Count}
entities:");
foreach (CategorizedEntity entity in documentResult.Entities)
{
Console.WriteLine();
Console.WriteLine($"
Entity: {entity.Text}");
Console.WriteLine($"
Category: {entity.Category}");
Console.WriteLine($"
Offset: {entity.Offset}");
Console.WriteLine($"
Length: {entity.Length}");
Console.WriteLine($"
ConfidenceScore:
{entity.ConfidenceScore}");
Console.WriteLine($"
SubCategory: {entity.SubCategory}");
}
Console.WriteLine();
}
}
Console.WriteLine("Extracted Key Phrases");
docNumber = 1;
foreach (ExtractKeyPhrasesActionResult keyPhrasesActionResult in
keyPhrasesResults)
{
Console.WriteLine($" Action name:
{keyPhrasesActionResult.ActionName}");
Console.WriteLine();
foreach (ExtractKeyPhrasesResult documentResults in
keyPhrasesActionResult.DocumentsResults)
{
Console.WriteLine($" Document #{docNumber++}");
Console.WriteLine($" Recognized the following
{documentResults.KeyPhrases.Count} Keyphrases:");
foreach (string keyphrase in documentResults.KeyPhrases)
{
Console.WriteLine($"
{keyphrase}");
}
Console.WriteLine();
}
}
}
}

Troubleshooting
General

When you interact with the Cognitive Services for Language using the .NET Text Analytics SDK,
errors returned by the Language service correspond to the same HTTP status codes returned
for REST API requests.
For example, if you submit a batch of text document inputs containing duplicate document ids,
a 400 error is returned, indicating "Bad Request".
C#

try
{
DetectedLanguage result = client.DetectLanguage(document);
}
catch (RequestFailedException e)
{
Console.WriteLine(e.ToString());
}

You will notice that additional information is logged, like the client request ID of the operation.
text

Message:
Azure.RequestFailedException:
Status: 400 (Bad Request)
Content:
{"error":{"code":"InvalidRequest","innerError":
{"code":"InvalidDocument","message":"Request contains duplicated Ids. Make sure
each document has a unique Id."},"message":"Invalid document in request."}}
Headers:
Transfer-Encoding: chunked
x-aml-ta-request-id: 146ca04a-af54-43d4-9872-01a004bee5f8
X-Content-Type-Options: nosniff
x-envoy-upstream-service-time: 6
apim-request-id: c650acda-2b59-4ff7-b96a-e316442ea01b
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
Date: Wed, 18 Dec 2019 16:24:52 GMT
Content-Type: application/json; charset=utf-8

Setting up console logging
The simplest way to see the logs is to enable the console logging. To create an Azure SDK log
listener that outputs messages to console use AzureEventSourceListener.CreateConsoleLogger
method.
C#

// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();

To learn more about other logging mechanisms see here

.

Next steps
Samples showing how to use this client library are available in this GitHub repository. Samples
are provided for each main functional area, and for each area, samples are provided for
analyzing a single document, and a collection of documents in both sync and async mode.
Detect Language
Analyze Sentiment
Extract Key Phrases
Recognize Named Entities
Recognize PII Entities
Recognize Linked Entities
Recognize Healthcare Entities
Custom Named Entities Recognition
Custom Single Label Classification
Custom Multi Label Classification
Extractive Summarization
Abstractive Summarization

Advanced samples
Understand how to work with long-running operations
Running multiple actions in one or more documents
Analyze Sentiment with Opinion Mining
Mock a client for testing

using the Moq

library

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .

When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

or contact opencode@microsoft.com with any additional

Azure Cognitive Language Services
Question Answering client library for .NET version 1.1.0
The Question Answering service is a cloud-based API service that lets you create a
conversational question-and-answer layer over your existing data. Use it to build a knowledge
base by extracting questions and answers from your semi-structured content, including FAQ,
manuals, and documents. Answer usersâ€™ questions with the best answers from the QnAs in your
knowledge baseâ€”automatically. Your knowledge base gets smarter, too, as it continually learns
from user behavior.
Source code
Samples

| Package (NuGet)

| API reference documentation | Product documentation |

| Migration guide

Getting started
Install the package
Install the Azure Cognitive Language Services Question Answering client library for .NET with
NuGet

:

.NET CLI
dotnet add package Azure.AI.Language.QuestionAnswering

Prerequisites
An Azure subscription
An existing Question Answering resource
Though you can use this SDK to create and import conversation projects, Language Studio
the preferred method for creating projects.

Authenticate the client
In order to interact with the Question Answering service, you'll need to either create an
instance of the QuestionAnsweringClient

class for querying existing projects or an instance

of the QuestionAnsweringAuthoringClient

for managing projects within your resource. You

will need an endpoint, and an API key to instantiate a client object. For more information

is

regarding authenticating with Cognitive Services, see Authenticate requests to Azure Cognitive
Services.

Get an API key
You can get the endpoint and an API key from the Cognitive Services resource or Question
Answering resource in the Azure Portal

.

Alternatively, use the Azure CLI command shown below to get the API key from the Question
Answering resource.
PowerShell
az cognitiveservices account keys list --resource-group <resource-group-name> --name
<resource-name>

Create a QuestionAnsweringClient
To use the QuestionAnsweringClient , make sure you use the right namespaces:
C#
using Azure.Core;
using Azure.AI.Language.QuestionAnswering;

With your endpoint and API key you can instantiate a QuestionAnsweringClient :
C#
Uri endpoint = new Uri("https://myaccount.cognitiveservices.azure.com/");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
QuestionAnsweringClient client = new QuestionAnsweringClient(endpoint, credential);

Create a QuestionAnsweringAuthoringClient
To use the QuestionAnsweringAuthoringClient , use the following namespace in addition to
those above, if needed.
C#
using Azure.AI.Language.QuestionAnswering.Authoring;

With your endpoint and API key, you can instantiate a QuestionAnsweringAuthoringClient :

C#
Uri endpoint = new Uri("https://myaccount.cognitiveservices.azure.com/");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
QuestionAnsweringAuthoringClient client = new
QuestionAnsweringAuthoringClient(endpoint, credential);

Create a client using Azure Active Directory authentication
You can also create a QuestionAnsweringClient or QuestionAnsweringAuthoringClient using
Azure Active Directory (AAD) authentication. Your user or service principal must be assigned
the "Cognitive Services Language Reader" role. Using the DefaultAzureCredential

you can

authenticate a service using Managed Identity or a service principal, authenticate as a
developer working on an application, and more all without changing code.
Before you can use the DefaultAzureCredential , or any credential type from Azure.Identity
you'll first need to install the Azure.Identity package

,

.

To use DefaultAzureCredential with a client ID and secret, you'll need to set the
AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET environment variables;

alternatively, you can pass those values to the ClientSecretCredential also in Azure.Identity.
Make sure you use the right namespace for DefaultAzureCredential at the top of your source
file:
C#
using Azure.Identity;

Then you can create an instance of DefaultAzureCredential and pass it to a new instance of
your client:
C#
Uri endpoint = new Uri("https://myaccount.cognitiveservices.azure.com");
DefaultAzureCredential credential = new DefaultAzureCredential();
QuestionAnsweringClient client = new QuestionAnsweringClient(endpoint, credential);

Note that regional endpoints do not support AAD authentication. Instead, create a custom
domain name for your resource to use AAD authentication.

Key concepts

QuestionAnsweringClient
The QuestionAnsweringClient

is the primary interface for asking questions using a

knowledge base with your own information, or text input using pre-trained models. It provides
both synchronous and asynchronous APIs to ask questions.

QuestionAnsweringAuthoringClient
The QuestionAnsweringAuthoringClient

provides an interface for managing Question

Answering projects. Examples of the available operations include creating and deploying
projects, updating your knowledge sources, and updating question and answer pairs. It
provides both synchronous and asynchronous APIs.

Thread safety
We guarantee that all client instance methods are thread-safe and independent of each other
(guideline ). This ensures that the recommendation of reusing client instances is always safe,
even across threads.

Additional concepts
Client options
Diagnostics

| Accessing the response
| Mocking

| Long-running operations

| Handling failures

| Client lifetime

Examples
QuestionAnsweringClient
The Azure.AI.Language.QuestionAnswering client library provides both synchronous and
asynchronous APIs.
The following examples show common scenarios using the client created above.

Ask a question
The only input required to a ask a question using an existing knowledge base is just the
question itself:
C#

|

string projectName = "{ProjectName}";
string deploymentName = "{DeploymentName}";
QuestionAnsweringProject project = new QuestionAnsweringProject(projectName,
deploymentName);
Response<AnswersResult> response = client.GetAnswers("How long should my Surface
battery last?", project);
foreach (KnowledgeBaseAnswer answer in response.Value.Answers)
{
Console.WriteLine($"({answer.Confidence:P2}) {answer.Answer}");
Console.WriteLine($"Source: {answer.Source}");
Console.WriteLine();
}

You can set additional properties on QuestionAnsweringClientOptions to limit the number of
answers, specify a minimum confidence score, and more.

Ask a follow-up question
If your knowledge base is configured for chit-chat, you can ask a follow-up question provided
the previous question-answering ID and, optionally, the exact question the user asked:
C#
string projectName = "{ProjectName}";
string deploymentName = "{DeploymentName}";
// Answers are ordered by their ConfidenceScore so assume the user choose the first
answer below:
KnowledgeBaseAnswer previousAnswer = answers.Answers.First();
QuestionAnsweringProject project = new QuestionAnsweringProject(projectName,
deploymentName);
AnswersOptions options = new AnswersOptions
{
AnswerContext = new KnowledgeBaseAnswerContext(previousAnswer.QnaId.Value)
};
Response<AnswersResult> response = client.GetAnswers("How long should charging
take?", project, options);
foreach (KnowledgeBaseAnswer answer in response.Value.Answers)
{
Console.WriteLine($"({answer.Confidence:P2}) {answer.Answer}");
Console.WriteLine($"Source: {answer.Source}");
Console.WriteLine();
}

QuestionAnsweringAuthoringClient

The following examples show common scenarios using the QuestionAnsweringAuthoringClient
instance created in this section.

Create a new project
To create a new project, you must specify the project's name and a create a RequestContent
instance with the parameters needed to set up the project.
C#
// Set project name and request content parameters
string newProjectName = "{ProjectName}";
RequestContent creationRequestContent = RequestContent.Create(
new {
description = "This is the description for a test project",
language = "en",
multilingualResource = false,
settings = new {
defaultAnswer = "No answer found for your question."
}
}
);
Response creationResponse = client.CreateProject(newProjectName,
creationRequestContent);
// Projects can be retrieved as follows
Pageable<BinaryData> projects = client.GetProjects();
Console.WriteLine("Projects: ");
foreach (BinaryData project in projects)
{
Console.WriteLine(project);
}

Deploy your project
Your projects can be deployed using the DeployProjectAsync or the synchronous
DeployProject . All you need to specify is the project's name and the deployment name that

you wish to use. Please note that the service will not allow you to deploy empty projects.
C#
// Set deployment name and start operation
string newDeploymentName = "{DeploymentName}";
Operation<BinaryData> deploymentOperation =
client.DeployProject(WaitUntil.Completed, newProjectName, newDeploymentName);

// Deployments can be retrieved as follows
Pageable<BinaryData> deployments = client.GetDeployments(newProjectName);
Console.WriteLine("Deployments: ");
foreach (BinaryData deployment in deployments)
{
Console.WriteLine(deployment);
}

Add a knowledge source
One way to add content to your project is to add a knowledge source. The following example
shows how you can set up a RequestContent instance to add a new knowledge source of the
type "url".
C#
// Set request content parameters for updating our new project's sources
string sourceUri = "{KnowledgeSourceUri}";
RequestContent updateSourcesRequestContent = RequestContent.Create(
new[] {
new {
op = "add",
value = new
{
displayName = "MicrosoftFAQ",
source = sourceUri,
sourceUri = sourceUri,
sourceKind = "url",
contentStructureKind = "unstructured",
refresh = false
}
}
});
Operation<Pageable<BinaryData>> updateSourcesOperation =
client.UpdateSources(WaitUntil.Completed, newProjectName,
updateSourcesRequestContent);
// Knowledge Sources can be retrieved as follows
Pageable<BinaryData> sources = updateSourcesOperation.Value;
Console.WriteLine("Sources: ");
foreach (BinaryData source in sources)
{
Console.WriteLine(source);
}

Troubleshooting

General
When you interact with the Cognitive Language Services Question Answering client library
using the .NET SDK, errors returned by the service correspond to the same HTTP status codes
returned for REST API requests.
For example, if you submit a question to a non-existant knowledge base, a 400 error is
returned indicating "Bad Request".
C#
try
{
QuestionAnsweringProject project = new QuestionAnsweringProject("invalidknowledgebase", "test");
Response<AnswersResult> response = client.GetAnswers("Does this knowledge base
exist?", project);
}
catch (RequestFailedException ex)
{
Console.WriteLine(ex.ToString());
}

You will notice that additional information is logged, like the client request ID of the operation.
text
Azure.RequestFailedException: Please verify azure search service is up, restart the
WebApp and try again
Status: 400 (Bad Request)
ErrorCode: BadArgument
Content:
{
"error": {
"code": "BadArgument",
"message": "Please verify azure search service is up, restart the WebApp and try
again"
}
}
Headers:
x-envoy-upstream-service-time: 23
apim-request-id: 76a83876-22d1-4977-a0b1-559a674f3605
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
X-Content-Type-Options: nosniff
Date: Wed, 30 Jun 2021 00:32:07 GMT
Content-Length: 139
Content-Type: application/json; charset=utf-8

Setting up console logging
The simplest way to see the logs is to enable console logging. To create an Azure SDK log
listener that outputs messages to the console use the
AzureEventSourceListener.CreateConsoleLogger method.

C#
// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();

To learn more about other logging mechanisms see here

.

Next steps
View our samples

.

Read about the different features
Try our service demos

of the Question Answering service.

.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 10/18/2022

or contact opencode@microsoft.com with any additional

Azure Text Analytics client library for
Python - version 5.3.0
The Azure Cognitive Service for Language is a cloud-based service that provides Natural
Language Processing (NLP) features for understanding and analyzing text, and includes the
following main features:
Sentiment Analysis
Named Entity Recognition
Language Detection
Key Phrase Extraction
Entity Linking
Multiple Analysis
Personally Identifiable Information (PII) Detection
Text Analytics for Health
Custom Named Entity Recognition
Custom Text Classification
Extractive Text Summarization
Abstractive Text Summarization
Source code

| Package (PyPI)

| Package (Conda)

| API reference documentation

|

Product documentation | Samples

Getting started
Prerequisites
Python 3.7 later is required to use this package.
You must have an Azure subscription

and a Cognitive Services or Language service

resource to use this package.

Create a Cognitive Services or Language service resource
The Language service supports both multi-service and single-service access. Create a Cognitive
Services resource if you plan to access multiple cognitive services under a single endpoint/key.
For Language service access only, create a Language service resource. You can create the
resource using the Azure Portal

or Azure CLI following the steps in this document.

Interaction with the service using the client library begins with a client. To create a client object,
you will need the Cognitive Services or Language service endpoint to your resource and a

credential that allows you access:

Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
credential = AzureKeyCredential("<api_key>")
text_analytics_client = TextAnalyticsClient(endpoint="https://<resourcename>.cognitiveservices.azure.com/", credential=credential)

Note that for some Cognitive Services resources the endpoint might look different from the
above code snippet. For example, https://<region>.api.cognitive.microsoft.com/ .

Install the package
Install the Azure Text Analytics client library for Python with pip

:

Bash
pip install azure-ai-textanalytics

Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))

Note that 5.2.X and newer targets the Azure Cognitive Service for Language APIs. These
APIs include the text analysis and natural language processing features found in the
previous versions of the Text Analytics client library. In addition, the service API has
changed from semantic to date-based versioning. This version of the client library defaults
to the latest supported API version, which currently is 2023-04-01 .
This table shows the relationship between SDK versions and supported API versions of the
service
ï¾‰

Expand table

SDK version

Supported API version of service

5.3.X - Latest stable release

3.0, 3.1, 2022-05-01, 2023-04-01 (default)

5.2.X

3.0, 3.1, 2022-05-01 (default)

5.1.0

3.0, 3.1 (default)

5.0.0

3.0

API version can be selected by passing the api_version

keyword argument into the client. For

the latest Language service features, consider selecting the most recent beta API version. For
production scenarios, the latest stable version is recommended. Setting to an older version
may result in reduced feature compatibility.

Authenticate the client
Get the endpoint
You can find the endpoint for your Language service resource using the Azure Portal or Azure
CLI:
Bash
# Get the endpoint for the Language service resource
az cognitiveservices account show --name "resource-name" --resource-group "resourcegroup-name" --query "properties.endpoint"

Get the API Key
You can get the API key from the Cognitive Services or Language service resource in the Azure
Portal. Alternatively, you can use Azure CLI snippet below to get the API key of your resource.
az cognitiveservices account keys list --name "resource-name" --resource-group "resourcegroup-name"

Create a TextAnalyticsClient with an API Key Credential
Once you have the value for the API key, you can pass it as a string into an instance of
AzureKeyCredential
Python

. Use the key as the credential parameter to authenticate the client:

import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(endpoint, AzureKeyCredential(key))

Create a TextAnalyticsClient with an Azure Active Directory Credential
To use an Azure Active Directory (AAD) token credential, provide an instance of the desired
credential type obtained from the azure-identity

library. Note that regional endpoints do not

support AAD authentication. Create a custom subdomain name for your resource in order to
use this type of authentication.
Authentication with AAD requires some initial setup:
Install azure-identity
Register a new AAD application
Grant access to the Language service by assigning the "Cognitive Services Language
Reader" role to your service principal.

After setup, you can choose which type of credential
example, DefaultAzureCredential

from azure.identity to use. As an

can be used to authenticate the client:

Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
Use the returned token credential to authenticate the client:
Python
import os
from azure.ai.textanalytics import TextAnalyticsClient
from azure.identity import DefaultAzureCredential
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
credential = DefaultAzureCredential()
text_analytics_client = TextAnalyticsClient(endpoint, credential=credential)

Key concepts
TextAnalyticsClient

The Text Analytics client library provides a TextAnalyticsClient

to do analysis on batches of

documents. It provides both synchronous and asynchronous operations to access a specific use
of text analysis, such as language detection or key phrase extraction.

Input
A document is a single unit to be analyzed by the predictive models in the Language service.
The input for each operation is passed as a list of documents.
Each document can be passed as a string in the list, e.g.
Python
documents = ["I hated the movie. It was so slow!", "The movie made it into my top
ten favorites. What a great movie!"]

or, if you wish to pass in a per-item document id or language / country_hint , they can be
passed as a list of DetectLanguageInput

or TextDocumentInput

or a dict-like

representation of the object:
Python
documents = [
{"id": "1", "language": "en", "text": "I hated the movie. It was so slow!"},
{"id": "2", "language": "en", "text": "The movie made it into my top ten
favorites. What a great movie!"},
]

See service limitations

for the input, including document length limits, maximum batch size,

and supported text encoding.

Return Value
The return value for a single document can be a result or error object. A heterogeneous list
containing a collection of result and error objects is returned from each operation. These
results/errors are index-matched with the order of the provided documents.
A result, such as AnalyzeSentimentResult , is the result of a text analysis operation and
contains a prediction or predictions about a document input.
The error object, DocumentError

, indicates that the service had trouble processing the

document and contains the reason it was unsuccessful.

Document Error Handling

You can filter for a result or error object in the list by using the is_error attribute. For a result
object this is always False and for a DocumentError

this is True .

For example, to filter out all DocumentErrors you might use list comprehension:
Python
response = text_analytics_client.analyze_sentiment(documents)
successful_responses = [doc for doc in response if not doc.is_error]

You can also use the kind attribute to filter between result types:
Python
poller = text_analytics_client.begin_analyze_actions(documents, actions)
response = poller.result()
for result in response:
if result.kind == "SentimentAnalysis":
print(f"Sentiment is {result.sentiment}")
elif result.kind == "KeyPhraseExtraction":
print(f"Key phrases: {result.key_phrases}")
elif result.is_error is True:
print(f"Document error: {result.code}, {result.message}")

Long-Running Operations
Long-running operations are operations which consist of an initial request sent to the service
to start an operation, followed by polling the service at intervals to determine whether the
operation has completed or failed, and if it has succeeded, to get the result.
Methods that support healthcare analysis, custom text analysis, or multiple analyses are
modeled as long-running operations. The client exposes a begin_<method-name> method that
returns a poller object. Callers should wait for the operation to complete by calling result()
on the poller object returned from the begin_<method-name> method. Sample code snippets are
provided to illustrate using long-running operations below.

Examples
The following section provides several code snippets covering some of the most common
Language service tasks, including:
Analyze Sentiment
Recognize Entities
Recognize Linked Entities

Recognize PII Entities
Extract Key Phrases
Detect Language
Healthcare Entities Analysis
Multiple Analysis
Custom Entity Recognition
Custom Single Label Classification
Custom Multi Label Classification
Extractive Summarization
Abstractive Summarization

Analyze Sentiment
analyze_sentiment

looks at its input text and determines whether its sentiment is positive,

negative, neutral or mixed. It's response includes per-sentence sentiment analysis and
confidence scores.
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
documents = [
"""I had the best day of my life. I decided to go sky-diving and it made me
appreciate my whole life so much more.
I developed a deep-connection with my instructor as well, and I feel as if I've
made a life-long friend in her.""",
"""This was a waste of my time. All of the views on this drop are extremely
boring, all I saw was grass. 0/10 would
not recommend to any divers, even first timers.""",
"""This was pretty good! The sights were ok, and I had fun with my instructors!
Can't complain too much about my experience""",
"""I only have one word for my experience: WOW!!! I can't believe I have had
such a wonderful skydiving company right
in my backyard this whole time! I will definitely be a repeat customer, and I
want to take my grandmother skydiving too,
I know she'll love it!"""
]

result = text_analytics_client.analyze_sentiment(documents,
show_opinion_mining=True)

docs = [doc for doc in result if not doc.is_error]
print("Let's visualize the sentiment of each of these documents")
for idx, doc in enumerate(docs):
print(f"Document text: {documents[idx]}")
print(f"Overall sentiment: {doc.sentiment}")

The returned response is a heterogeneous list of result and error objects:
list[AnalyzeSentimentResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of sentiment analysis. To
see how to conduct more granular analysis into the opinions related to individual aspects (such
as attributes of a product or service) in a text, see here

.

Recognize Entities
recognize_entities

recognizes and categories entities in its input text as people, places,

organizations, date/time, quantities, percentages, currencies, and more.
Python
import os
import typing
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
reviews = [
"""I work for Foo Company, and we hired Contoso for our annual founding
ceremony. The food
was amazing and we all can't say enough good words about the quality and the
level of service.""",
"""We at the Foo Company re-hired Contoso after all of our past successes with
the company.
Though the food was still great, I feel there has been a quality drop since
their last time
catering for us. Is anyone else running into the same problem?""",
"""Bar Company is over the moon about the service we received from Contoso, the
best sliders ever!!!!"""
]
result = text_analytics_client.recognize_entities(reviews)
result = [review for review in result if not review.is_error]
organization_to_reviews: typing.Dict[str, typing.List[str]] = {}
for idx, review in enumerate(result):
for entity in review.entities:

print(f"Entity '{entity.text}' has category '{entity.category}'")
if entity.category == 'Organization':
organization_to_reviews.setdefault(entity.text, [])
organization_to_reviews[entity.text].append(reviews[idx])
for organization, reviews in organization_to_reviews.items():
print(
"\n\nOrganization '{}' has left us the following review(s): {}".format(
organization, "\n\n".join(reviews)
)
)

The returned response is a heterogeneous list of result and error objects:
list[RecognizeEntitiesResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of named entity
recognition and supported types

.

Recognize Linked Entities
recognize_linked_entities

recognizes and disambiguates the identity of each entity found in

its input text (for example, determining whether an occurrence of the word Mars refers to the
planet, or to the Roman god of war). Recognized entities are associated with URLs to a wellknown knowledge base, like Wikipedia.
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
documents = [
"""
Microsoft was founded by Bill Gates with some friends he met at Harvard. One of
his friends,
Steve Ballmer, eventually became CEO after Bill Gates as well. Steve Ballmer
eventually stepped
down as CEO of Microsoft, and was succeeded by Satya Nadella.
Microsoft originally moved its headquarters to Bellevue, Washington in January
1979, but is now
headquartered in Redmond.
"""
]
result = text_analytics_client.recognize_linked_entities(documents)

docs = [doc for doc in result if not doc.is_error]
print(
"Let's map each entity to it's Wikipedia article. I also want to see how many
times each "
"entity is mentioned in a document\n\n"
)
entity_to_url = {}
for doc in docs:
for entity in doc.entities:
print("Entity '{}' has been mentioned '{}' time(s)".format(
entity.name, len(entity.matches)
))
if entity.data_source == "Wikipedia":
entity_to_url[entity.name] = entity.url

The returned response is a heterogeneous list of result and error objects:
list[RecognizeLinkedEntitiesResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of entity linking and
supported types

.

Recognize PII Entities
recognize_pii_entities

recognizes and categorizes Personally Identifiable Information (PII)

entities in its input text, such as Social Security Numbers, bank account information, credit card
numbers, and more.
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(
endpoint=endpoint, credential=AzureKeyCredential(key)
)
documents = [
"""Parker Doe has repaid all of their loans as of 2020-04-25.
Their SSN is 859-98-0987. To contact them, use their phone number
555-555-5555. They are originally from Brazil and have Brazilian CPF number
998.214.865-68"""
]
result = text_analytics_client.recognize_pii_entities(documents)
docs = [doc for doc in result if not doc.is_error]
print(

"Let's compare the original document with the documents after redaction. "
"I also want to comb through all of the entities that got redacted"
)
for idx, doc in enumerate(docs):
print(f"Document text: {documents[idx]}")
print(f"Redacted document text: {doc.redacted_text}")
for entity in doc.entities:
print("...Entity '{}' with category '{}' got redacted".format(
entity.text, entity.category
))

The returned response is a heterogeneous list of result and error objects:
list[RecognizePiiEntitiesResult

, DocumentError

]

Please refer to the service documentation for supported PII entity types

.

Note: The Recognize PII Entities service is available in API version v3.1 and newer.

Extract Key Phrases
extract_key_phrases

determines the main talking points in its input text. For example, for the

input text "The food was delicious and there were wonderful staff", the API returns: "food" and
"wonderful staff".
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
articles = [
"""
Washington, D.C. Autumn in DC is a uniquely beautiful season. The leaves fall
from the trees
in a city chock-full of forests, leaving yellow leaves on the ground and a
clearer view of the
blue sky above...
""",
"""
Redmond, WA. In the past few days, Microsoft has decided to further postpone the
start date of
its United States workers, due to the pandemic that rages with no end in
sight...
""",
"""
Redmond, WA. Employees at Microsoft can be excited about the new coffee shop

that will open on campus
once workers no longer have to work remotely...
"""
]
result = text_analytics_client.extract_key_phrases(articles)
for idx, doc in enumerate(result):
if not doc.is_error:
print("Key phrases in article #{}: {}".format(
idx + 1,
", ".join(doc.key_phrases)
))

The returned response is a heterogeneous list of result and error objects:
list[ExtractKeyPhrasesResult , DocumentError

]

Please refer to the service documentation for a conceptual discussion of key phrase extraction.

Detect Language
detect_language

determines the language of its input text, including the confidence score of

the predicted language.
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
documents = [
"""
The concierge Paulette was extremely helpful. Sadly when we arrived the elevator
was broken, but with Paulette's help we barely noticed this inconvenience.
She arranged for our baggage to be brought up to our room with no extra charge
and gave us a free meal to refurbish all of the calories we lost from
walking up the stairs :). Can't say enough good things about my experience!
""",
"""
æœ€è¿‘ç”±äºŽå·¥ä½œåŽ‹åŠ›å¤ªå¤§ï¼Œæˆ‘ä»¬å†³å®šåŽ»å¯Œé…’åº—åº¦å‡ã€‚é‚£å„¿çš„æ¸©æ³‰å®žåœ¨å¤ªèˆ’æœäº†ï¼Œæˆ‘è·Ÿæˆ‘ä¸ˆå¤«éƒ½å®Œå…¨æ¢å¤
äº†å·¥ä½œå‰çš„é’æ˜¥ç²¾ç¥žï¼åŠ æ²¹ï¼
"""
]
result = text_analytics_client.detect_language(documents)
reviewed_docs = [doc for doc in result if not doc.is_error]

print("Let's see what language each review is in!")
for idx, doc in enumerate(reviewed_docs):
print("Review #{} is in '{}', which has ISO639-1 name '{}'\n".format(
idx, doc.primary_language.name, doc.primary_language.iso6391_name
))

The returned response is a heterogeneous list of result and error objects:
list[DetectLanguageResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of language detection
and language and regional support.

Healthcare Entities Analysis
Long-running operation begin_analyze_healthcare_entities

extracts entities recognized within

the healthcare domain, and identifies relationships between entities within the input document
and links to known sources of information in various well known databases, such as UMLS,
CHV, MSH, etc.
Python
import os
import typing
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(
endpoint=endpoint,
credential=AzureKeyCredential(key),
)
documents = [
"""
Patient needs to take 100 mg of ibuprofen, and 3 mg of potassium. Also needs to
take
10 mg of Zocor.
""",
"""
Patient needs to take 50 mg of ibuprofen, and 2 mg of Coumadin.
"""
]
poller = text_analytics_client.begin_analyze_healthcare_entities(documents)
result = poller.result()
docs = [doc for doc in result if not doc.is_error]

print("Let's first visualize the outputted healthcare result:")
for doc in docs:
for entity in doc.entities:
print(f"Entity: {entity.text}")
print(f"...Normalized Text: {entity.normalized_text}")
print(f"...Category: {entity.category}")
print(f"...Subcategory: {entity.subcategory}")
print(f"...Offset: {entity.offset}")
print(f"...Confidence score: {entity.confidence_score}")
if entity.data_sources is not None:
print("...Data Sources:")
for data_source in entity.data_sources:
print(f"......Entity ID: {data_source.entity_id}")
print(f"......Name: {data_source.name}")
if entity.assertion is not None:
print("...Assertion:")
print(f"......Conditionality: {entity.assertion.conditionality}")
print(f"......Certainty: {entity.assertion.certainty}")
print(f"......Association: {entity.assertion.association}")
for relation in doc.entity_relations:
print(f"Relation of type: {relation.relation_type} has the following roles")
for role in relation.roles:
print(f"...Role '{role.name}' with entity '{role.entity.text}'")
print("------------------------------------------")
print("Now, let's get all of medication dosage relations from the documents")
dosage_of_medication_relations = [
entity_relation
for doc in docs
for entity_relation in doc.entity_relations if entity_relation.relation_type ==
HealthcareEntityRelation.DOSAGE_OF_MEDICATION
]

Note: Healthcare Entities Analysis is only available with API version v3.1 and newer.

Multiple Analysis
Long-running operation begin_analyze_actions

performs multiple analyses over one set of

documents in a single request. Currently it is supported using any combination of the following
Language APIs in a single request:
Entities Recognition
PII Entities Recognition
Linked Entity Recognition
Key Phrase Extraction
Sentiment Analysis
Custom Entity Recognition (API version 2022-05-01 and newer)
Custom Single Label Classification (API version 2022-05-01 and newer)

Custom Multi Label Classification (API version 2022-05-01 and newer)
Healthcare Entities Analysis (API version 2022-05-01 and newer)
Extractive Summarization (API version 2023-04-01 and newer)
Abstractive Summarization (API version 2023-04-01 and newer)
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import (
TextAnalyticsClient,
RecognizeEntitiesAction,
RecognizeLinkedEntitiesAction,
RecognizePiiEntitiesAction,
ExtractKeyPhrasesAction,
AnalyzeSentimentAction,
)
endpoint = os.environ["AZURE_LANGUAGE_ENDPOINT"]
key = os.environ["AZURE_LANGUAGE_KEY"]
text_analytics_client = TextAnalyticsClient(
endpoint=endpoint,
credential=AzureKeyCredential(key),
)
documents = [
'We went to Contoso Steakhouse located at midtown NYC last week for a dinner
party, and we adore the spot! '
'They provide marvelous food and they have a great menu. The chief cook happens
to be the owner (I think his name is John Doe) '
'and he is super nice, coming out of the kitchen and greeted us all.'
,
'We enjoyed very much dining in the place! '
'The Sirloin steak I ordered was tender and juicy, and the place was impeccably
clean. You can even pre-order from their '
'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to
order@contososteakhouse.com! '
'The only complaint I have is the food didn\'t come fast enough. Overall I
highly recommend it!'
]
poller = text_analytics_client.begin_analyze_actions(
documents,
display_name="Sample Text Analysis",
actions=[
RecognizeEntitiesAction(),
RecognizePiiEntitiesAction(),
ExtractKeyPhrasesAction(),
RecognizeLinkedEntitiesAction(),
AnalyzeSentimentAction(),
],

)
document_results = poller.result()
for doc, action_results in zip(documents, document_results):
print(f"\nDocument text: {doc}")
for result in action_results:
if result.kind == "EntityRecognition":
print("...Results of Recognize Entities Action:")
for entity in result.entities:
print(f"......Entity: {entity.text}")
print(f".........Category: {entity.category}")
print(f".........Confidence Score: {entity.confidence_score}")
print(f".........Offset: {entity.offset}")
elif result.kind == "PiiEntityRecognition":
print("...Results of Recognize PII Entities action:")
for pii_entity in result.entities:
print(f"......Entity: {pii_entity.text}")
print(f".........Category: {pii_entity.category}")
print(f".........Confidence Score: {pii_entity.confidence_score}")
elif result.kind == "KeyPhraseExtraction":
print("...Results of Extract Key Phrases action:")
print(f"......Key Phrases: {result.key_phrases}")
elif result.kind == "EntityLinking":
print("...Results of Recognize Linked Entities action:")
for linked_entity in result.entities:
print(f"......Entity name: {linked_entity.name}")
print(f".........Data source: {linked_entity.data_source}")
print(f".........Data source language: {linked_entity.language}")
print(
f".........Data source entity ID:
{linked_entity.data_source_entity_id}"
)
print(f".........Data source URL: {linked_entity.url}")
print(".........Document matches:")
for match in linked_entity.matches:
print(f"............Match text: {match.text}")
print(f"............Confidence Score: {match.confidence_score}")
print(f"............Offset: {match.offset}")
print(f"............Length: {match.length}")
elif result.kind == "SentimentAnalysis":
print("...Results of Analyze Sentiment action:")
print(f"......Overall sentiment: {result.sentiment}")
print(
f"......Scores: positive={result.confidence_scores.positive}; \
neutral={result.confidence_scores.neutral}; \
negative={result.confidence_scores.negative} \n"
)
elif result.is_error is True:
print(
f"...Is an error with code '{result.error.code}' and message

'{result.error.message}'"
)
print("------------------------------------------")

The returned response is an object encapsulating multiple iterables, each representing results
of individual analyses.
Note: Multiple analysis is available in API version v3.1 and newer.

Optional Configuration
Optional keyword arguments can be passed in at the client and per-operation level. The azurecore reference documentation

describes available configurations for retries, logging,

transport protocols, and more.

Troubleshooting
General
The Text Analytics client will raise exceptions defined in Azure Core

.

Logging
This library uses the standard logging

library for logging. Basic information about HTTP

sessions (URLs, headers, etc.) is logged at INFO level.
Detailed DEBUG level logging, including request/response bodies and unredacted headers, can
be enabled on a client with the logging_enable keyword argument:
Python
import sys
import logging
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalyticsClient
# Create a logger for the 'azure' SDK
logger = logging.getLogger('azure')
logger.setLevel(logging.DEBUG)
# Configure a console output
handler = logging.StreamHandler(stream=sys.stdout)
logger.addHandler(handler)

endpoint = "https://<resource-name>.cognitiveservices.azure.com/"
credential = DefaultAzureCredential()
# This client will log detailed information about its HTTP sessions, at DEBUG level
text_analytics_client = TextAnalyticsClient(endpoint, credential,
logging_enable=True)
result = text_analytics_client.analyze_sentiment(["I did not like the restaurant.
The food was too spicy."])

Similarly, logging_enable can enable detailed logging for a single operation, even when it isn't
enabled for the client:
Python
result = text_analytics_client.analyze_sentiment(documents, logging_enable=True)

Next steps
More sample code
These code samples show common scenario operations with the Azure Text Analytics client
library.
Authenticate the client with a Cognitive Services/Language service API key or a token
credential from azure-identity
sample_authentication.py

:
(async version )

Common scenarios
Analyze sentiment: sample_analyze_sentiment.py

(async version )

Recognize entities: sample_recognize_entities.py

(async version )

Recognize personally identifiable information: sample_recognize_pii_entities.py
version

(async

)

Recognize linked entities: sample_recognize_linked_entities.py
Extract key phrases: sample_extract_key_phrases.py
Detect language: sample_detect_language.py

(async version )

(async version )

(async version )

Healthcare Entities Analysis: sample_analyze_healthcare_entities.py
Multiple Analysis: sample_analyze_actions.py

(async version )

(async version )

Custom Entity Recognition: sample_recognize_custom_entities.py

(async_version

Custom Single Label Classification: sample_single_label_classify.py

(async_version

Custom Multi Label Classification: sample_multi_label_classify.py
Extractive text summarization: sample_extract_summary.py

(async_version

(async version )

)
)
)

Abstractive text summarization: sample_abstract_summary.py

(async version )

Advanced scenarios
Opinion Mining: sample_analyze_sentiment_with_opinion_mining.py

(async_version

)

Additional documentation
For more extensive documentation on Azure Cognitive Service for Language, see the Language
Service documentation on docs.microsoft.com.

Contributing
This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 06/21/2023

or contact opencode@microsoft.com with any additional

Azure Cognitive Language Service
Question Answering client library for
Python - version 1.1.0
Question Answering is a cloud-based API service that lets you create a conversational
question-and-answer layer over your existing data. Use it to build a knowledge base by
extracting questions and answers from your semi-structured content, including FAQ, manuals,
and documents. Answer usersâ€™ questions with the best answers from the QnAs in your
knowledge baseâ€”automatically. Your knowledge base gets smarter, too, as it continually learns
from users' behavior.
Source code

| Package (PyPI)

| API reference documentation

| Product documentation

| Samples

Disclaimer
Azure SDK Python packages support for Python 2.7 ended 01 January 2022. For more
information and questions, please refer to https://github.com/Azure/azure-sdk-forpython/issues/20691

Getting started
Prerequisites
Python 3.7 or later is required to use this package.
An Azure subscription
A Language Service

resource

Install the package
Install the Azure Question Answering client library for Python with pip

:

Bash
pip install azure-ai-language-questionanswering

Note: this version of the client library defaults to the service API version 2021-10-01 .

Authenticate the client
In order to interact with the Question Answering service, you'll need to create an instance of
the QuestionAnsweringClient

class or an instance of the AuthoringClient

for managing

projects within your resource. You will need an endpoint, and an API key to instantiate a client
object. For more information regarding authenticating with Cognitive Services, see
Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and an API key from the Language resource in the Azure Portal

.

Alternatively, use the Azure CLI command shown below to get the API key from the Language
resource.
PowerShell
az cognitiveservices account keys list --resource-group <resource-group-name> --name
<resource-name>

Create QuestionAnsweringClient
Once you've determined your endpoint and API key you can instantiate a
QuestionAnsweringClient

:

Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering import QuestionAnsweringClient
endpoint = "https://{myaccount}.api.cognitive.microsoft.com"
credential = AzureKeyCredential("{api-key}")
client = QuestionAnsweringClient(endpoint, credential)

Create AuthoringClient
With your endpoint and API key, you can instantiate a AuthoringClient :
Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering.authoring import AuthoringClient
endpoint = "https://{myaccount}.api.cognitive.microsoft.com"

credential = AzureKeyCredential("{api-key}")
client = AuthoringClient(endpoint, credential)

Create a client with an Azure Active Directory Credential
To use an Azure Active Directory (AAD) token credential, provide an instance of the desired
credential type obtained from the azure-identity

library. Note that regional endpoints do not

support AAD authentication. Create a custom subdomain name for your resource in order to
use this type of authentication.
Authentication with AAD requires some initial setup:
Install azure-identity
Register a new AAD application
Grant access to the Language service by assigning the "Cognitive Services Language
Reader" role to your service principal.
After setup, you can choose which type of credential
example, DefaultAzureCredential

from azure.identity to use. As an

can be used to authenticate the client:

Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID , AZURE_TENANT_ID , AZURE_CLIENT_SECRET
Use the returned token credential to authenticate the client:
Python
from azure.ai.language.questionanswering import QuestionAnsweringClient
from azure.identity import DefaultAzureCredential
credential = DefaultAzureCredential()
client = QuestionAnsweringClient(endpoint="https://<my-customsubdomain>.cognitiveservices.azure.com/", credential=credential)

Key concepts
QuestionAnsweringClient
The QuestionAnsweringClient

is the primary interface for asking questions using a

knowledge base with your own information, or text input using pre-trained models. For
asynchronous operations, an async QuestionAnsweringClient is in the
azure.ai.language.questionanswering.aio namespace.

AuthoringClient
The AuthoringClient

provides an interface for managing Question Answering projects.

Examples of the available operations include creating and deploying projects, updating your
knowledge sources, and updating question and answer pairs. It provides both synchronous and
asynchronous APIs.

Examples
QuestionAnsweringClient
The azure-ai-language-questionanswering client library provides both synchronous and
asynchronous APIs.
Ask a question
Ask a follow-up question
Create a new project
Add a knowledge source
Deploy your project
Asynchronous operations

Ask a question
The only input required to ask a question using a knowledge base is just the question itself:
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering import QuestionAnsweringClient
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
key = os.environ["AZURE_QUESTIONANSWERING_KEY"]
client = QuestionAnsweringClient(endpoint, AzureKeyCredential(key))
output = client.get_answers(
question="How long should my Surface battery last?",
project_name="FAQ",
deployment_name="test"
)
for candidate in output.answers:
print("({}) {}".format(candidate.confidence, candidate.answer))
print("Source: {}".format(candidate.source))

You can set additional keyword options to limit the number of answers, specify a minimum
confidence score, and more.

Ask a follow-up question
If your knowledge base is configured for chit-chat, the answers from the knowledge base may
include suggested prompts for follow-up questions

to initiate a conversation. You can ask a

follow-up question by providing the ID of your chosen answer as the context for the continued
conversation:
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering import QuestionAnsweringClient
from azure.ai.language.questionanswering import models
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
key = os.environ["AZURE_QUESTIONANSWERING_KEY"]
client = QuestionAnsweringClient(endpoint, AzureKeyCredential(key))
output = client.get_answers(
question="How long should charging take?",
answer_context=models.KnowledgeBaseAnswerContext(
previous_qna_id=previous_answer.qna_id
),
project_name="FAQ",
deployment_name="live"
)
for candidate in output.answers:
print("({}) {}".format(candidate.confidence, candidate.answer))
print("Source: {}".format(candidate.source))

Create a new project
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering.authoring import AuthoringClient
# get service secrets
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
key = os.environ["AZURE_QUESTIONANSWERING_KEY"]
# create client
client = AuthoringClient(endpoint, AzureKeyCredential(key))
with client:

# create project
project_name = "IssacNewton"
project = client.create_project(
project_name=project_name,
options={
"description": "biography of Sir Issac Newton",
"language": "en",
"multilingualResource": True,
"settings": {
"defaultAnswer": "no answer"
}
})
print("view created project info:")
print("\tname: {}".format(project["projectName"]))
print("\tlanguage: {}".format(project["language"]))
print("\tdescription: {}".format(project["description"]))

Add a knowledge source
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering.authoring import AuthoringClient
# get service secrets
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
key = os.environ["AZURE_QUESTIONANSWERING_KEY"]
# create client
client = AuthoringClient(endpoint, AzureKeyCredential(key))
project_name = "IssacNewton"
update_sources_poller = client.begin_update_sources(
project_name=project_name,
sources=[
{
"op": "add",
"value": {
"displayName": "Issac Newton Bio",
"sourceUri": "https://wikipedia.org/wiki/Isaac_Newton",
"sourceKind": "url"
}
}
]
)
update_sources_poller.result()
# list sources
print("list project sources")
sources = client.list_sources(

project_name=project_name
)
for source in sources:
print("project: {}".format(source["displayName"]))
print("\tsource: {}".format(source["source"]))
print("\tsource Uri: {}".format(source["sourceUri"]))
print("\tsource kind: {}".format(source["sourceKind"]))

Deploy your project
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering.authoring import AuthoringClient
# get service secrets
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
key = os.environ["AZURE_QUESTIONANSWERING_KEY"]
# create client
client = AuthoringClient(endpoint, AzureKeyCredential(key))
project_name = "IssacNewton"
# deploy project
deployment_poller = client.begin_deploy_project(
project_name=project_name,
deployment_name="production"
)
deployment_poller.result()
# list all deployments
deployments = client.list_deployments(
project_name=project_name
)
print("view project deployments")
for d in deployments:
print(d)

Asynchronous operations
The above examples can also be run asynchronously using the clients in the aio namespace:
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering.aio import QuestionAnsweringClient

endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
key = os.environ["AZURE_QUESTIONANSWERING_KEY"]
client = QuestionAnsweringClient(endpoint, AzureKeyCredential(key))
output = await client.get_answers(
question="How long should my Surface battery last?",
project_name="FAQ",
deployment_name="production"
)

Optional Configuration
Optional keyword arguments can be passed in at the client and per-operation level. The azurecore reference documentation

describes available configurations for retries, logging,

transport protocols, and more.

Troubleshooting
General
Azure Question Answering clients raise exceptions defined in Azure Core

. When you interact

with the Cognitive Language Service Question Answering client library using the Python SDK,
errors returned by the service correspond to the same HTTP status codes returned for REST API
requests.
For example, if you submit a question to a non-existent knowledge base, a 400 error is
returned indicating "Bad Request".
Python
from azure.core.exceptions import HttpResponseError
try:
client.get_answers(
question="Why?",
project_name="invalid-knowledge-base",
deployment_name="test"
)
except HttpResponseError as error:
print("Query failed: {}".format(error.message))

Logging

This library uses the standard logging

library for logging. Basic information about HTTP

sessions (URLs, headers, etc.) is logged at INFO level.
Detailed DEBUG level logging, including request/response bodies and unredacted headers, can
be enabled on a client with the logging_enable argument.
See full SDK logging documentation with examples here.

Next steps
View our samples

.

Read about the different features
Try our service demos

of the Question Answering service.

.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 10/14/2022

or contact opencode@microsoft.com with any additional

Azure Pipelines set up now

Azure Conversational Language
Understanding client library for Python version 1.1.0
Conversational Language Understanding - aka CLU for short - is a cloud-based conversational
AI service which provides many language understanding capabilities like:
Conversation App: It's used in extracting intents and entities in conversations
Workflow app: Acts like an orchestrator to select the best candidate to analyze
conversations to get best response from apps like Qna, Luis, and Conversation App
Conversational Summarization: Used to analyze conversations in the form of
issues/resolution, chapter title, and narrative summarizations
Source code
Samples

| Package (PyPI)

| Package (Conda)

| API reference documentation

|

| Product documentation | REST API documentation

Getting started
Prerequisites
Python 3.7 or later is required to use this package.
An Azure subscription
A Language service resource

Install the package
Install the Azure Conversations client library for Python with pip

:

Bash
pip install azure-ai-language-conversations

Note: This version of the client library defaults to the 2023-04-01 version of the service

Authenticate the client
In order to interact with the CLU service, you'll need to create an instance of the
ConversationAnalysisClient

class, or ConversationAuthoringClient

class. You will need an

endpoint, and an API key to instantiate a client object. For more information regarding
authenticating with Cognitive Services, see Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and an API key from the Cognitive Services resource in the Azure
Portal

.

Alternatively, use the Azure CLI command shown below to get the API key from the Cognitive
Service resource.
PowerShell
az cognitiveservices account keys list --resource-group <resource-group-name> --name
<resource-name>

Create ConversationAnalysisClient
Once you've determined your endpoint and API key you can instantiate a
ConversationAnalysisClient :

Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient
endpoint = "https://<my-custom-subdomain>.cognitiveservices.azure.com/"
credential = AzureKeyCredential("<api-key>")
client = ConversationAnalysisClient(endpoint, credential)

Create ConversationAuthoringClient
Once you've determined your endpoint and API key you can instantiate a
ConversationAuthoringClient :

Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations.authoring import ConversationAuthoringClient
endpoint = "https://<my-custom-subdomain>.cognitiveservices.azure.com/"
credential = AzureKeyCredential("<api-key>")
client = ConversationAuthoringClient(endpoint, credential)

Create a client with an Azure Active Directory Credential
To use an Azure Active Directory (AAD) token credential, provide an instance of the desired
credential type obtained from the azure-identity

library. Note that regional endpoints do not

support AAD authentication. Create a custom subdomain name for your resource in order to
use this type of authentication.
Authentication with AAD requires some initial setup:
Install azure-identity
Register a new AAD application
Grant access to the Language service by assigning the "Cognitive Services Language
Reader" role to your service principal.
After setup, you can choose which type of credential
example, DefaultAzureCredential

from azure.identity to use. As an

can be used to authenticate the client:

Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID , AZURE_TENANT_ID , AZURE_CLIENT_SECRET
Use the returned token credential to authenticate the client:
Python
from azure.ai.language.conversations import ConversationAnalysisClient
from azure.identity import DefaultAzureCredential
credential = DefaultAzureCredential()
client = ConversationAnalysisClient(endpoint="https://<my-customsubdomain>.cognitiveservices.azure.com/", credential=credential)

Key concepts
ConversationAnalysisClient
The ConversationAnalysisClient

is the primary interface for making predictions using your

deployed Conversations models. For asynchronous operations, an async
ConversationAnalysisClient is in the azure.ai.language.conversation.aio namespace.

ConversationAuthoringClient
You can use the ConversationAuthoringClient

to interface with the Azure Language Portal

to carry out authoring operations on your language resource/project. For example, you can use

it to create a project, populate with training data, train, test, and deploy. For asynchronous
operations, an async ConversationAuthoringClient is in the
azure.ai.language.conversation.authoring.aio namespace.

Examples
The azure-ai-language-conversation client library provides both synchronous and
asynchronous APIs.
The following examples show common scenarios using the client created above.

Analyze Text with a Conversation App
If you would like to extract custom intents and entities from a user utterance, you can call the
client.analyze_conversation() method with your conversation's project name as follows:

Python
# import libraries
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient
# get secrets
clu_endpoint = os.environ["AZURE_CONVERSATIONS_ENDPOINT"]
clu_key = os.environ["AZURE_CONVERSATIONS_KEY"]
project_name = os.environ["AZURE_CONVERSATIONS_PROJECT_NAME"]
deployment_name = os.environ["AZURE_CONVERSATIONS_DEPLOYMENT_NAME"]
# analyze quey
client = ConversationAnalysisClient(clu_endpoint, AzureKeyCredential(clu_key))
with client:
query = "Send an email to Carol about the tomorrow's demo"
result = client.analyze_conversation(
task={
"kind": "Conversation",
"analysisInput": {
"conversationItem": {
"participantId": "1",
"id": "1",
"modality": "text",
"language": "en",
"text": query
},
"isLoggingEnabled": False
},
"parameters": {
"projectName": project_name,
"deploymentName": deployment_name,

"verbose": True
}
}
)
# view result
print("query: {}".format(result["result"]["query"]))
print("project kind: {}\n".format(result["result"]["prediction"]["projectKind"]))
print("top intent: {}".format(result["result"]["prediction"]["topIntent"]))
print("category: {}".format(result["result"]["prediction"]["intents"][0]
["category"]))
print("confidence score: {}\n".format(result["result"]["prediction"]["intents"][0]
["confidenceScore"]))
print("entities:")
for entity in result["result"]["prediction"]["entities"]:
print("\ncategory: {}".format(entity["category"]))
print("text: {}".format(entity["text"]))
print("confidence score: {}".format(entity["confidenceScore"]))
if "resolutions" in entity:
print("resolutions")
for resolution in entity["resolutions"]:
print("kind: {}".format(resolution["resolutionKind"]))
print("value: {}".format(resolution["value"]))
if "extraInformation" in entity:
print("extra info")
for data in entity["extraInformation"]:
print("kind: {}".format(data["extraInformationKind"]))
if data["extraInformationKind"] == "ListKey":
print("key: {}".format(data["key"]))
if data["extraInformationKind"] == "EntitySubtype":
print("value: {}".format(data["value"]))

Analyze Text with an Orchestration App
If you would like to pass the user utterance to your orchestrator (worflow) app, you can call the
client.analyze_conversation() method with your orchestration's project name. The

orchestrator project simply orchestrates the submitted user utterance between your language
apps (Luis, Conversation, and Question Answering) to get the best response according to the
user intent. See the next example:
Python
# import libraries
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient
# get secrets
clu_endpoint = os.environ["AZURE_CONVERSATIONS_ENDPOINT"]

clu_key = os.environ["AZURE_CONVERSATIONS_KEY"]
project_name = os.environ["AZURE_CONVERSATIONS_WORKFLOW_PROJECT_NAME"]
deployment_name = os.environ["AZURE_CONVERSATIONS_WORKFLOW_DEPLOYMENT_NAME"]
# analyze query
client = ConversationAnalysisClient(clu_endpoint, AzureKeyCredential(clu_key))
with client:
query = "Reserve a table for 2 at the Italian restaurant"
result = client.analyze_conversation(
task={
"kind": "Conversation",
"analysisInput": {
"conversationItem": {
"participantId": "1",
"id": "1",
"modality": "text",
"language": "en",
"text": query
},
"isLoggingEnabled": False
},
"parameters": {
"projectName": project_name,
"deploymentName": deployment_name,
"verbose": True
}
}
)
# view result
print("query: {}".format(result["result"]["query"]))
print("project kind: {}\n".format(result["result"]["prediction"]["projectKind"]))
# top intent
top_intent = result["result"]["prediction"]["topIntent"]
print("top intent: {}".format(top_intent))
top_intent_object = result["result"]["prediction"]["intents"][top_intent]
print("confidence score: {}".format(top_intent_object["confidenceScore"]))
print("project kind: {}".format(top_intent_object["targetProjectKind"]))
if top_intent_object["targetProjectKind"] == "Luis":
print("\nluis response:")
luis_response = top_intent_object["result"]["prediction"]
print("top intent: {}".format(luis_response["topIntent"]))
print("\nentities:")
for entity in luis_response["entities"]:
print("\n{}".format(entity))

Conversational Summarization
You can use this sample if you need to summarize a conversation in the form of an issue, and
final resolution. For example, a dialog from tech support:

Python
# import libraries
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient
# get secrets
endpoint = os.environ["AZURE_CONVERSATIONS_ENDPOINT"]
key = os.environ["AZURE_CONVERSATIONS_KEY"]
# analyze query
client = ConversationAnalysisClient(endpoint, AzureKeyCredential(key))
with client:
poller = client.begin_conversation_analysis(
task={
"displayName": "Analyze conversations from xxx",
"analysisInput": {
"conversations": [
{
"conversationItems": [
{
"text": "Hello, how can I help you?",
"modality": "text",
"id": "1",
"participantId": "Agent"
},
{
"text": "How to upgrade Office? I am getting error
messages the whole day.",
"modality": "text",
"id": "2",
"participantId": "Customer"
},
{
"text": "Press the upgrade button please. Then sign
in and follow the instructions.",
"modality": "text",
"id": "3",
"participantId": "Agent"
}
],
"modality": "text",
"id": "conversation1",
"language": "en"
},
]
},
"tasks": [
{
"taskName": "Issue task",
"kind": "ConversationalSummarizationTask",
"parameters": {
"summaryAspects": ["issue"]
}
},
{

"taskName": "Resolution task",
"kind": "ConversationalSummarizationTask",
"parameters": {
"summaryAspects": ["resolution"]
}
},
]
}
)
# view result
result = poller.result()
task_results = result["tasks"]["items"]
for task in task_results:
print(f"\n{task['taskName']} status: {task['status']}")
task_result = task["results"]
if task_result["errors"]:
print("... errors occurred ...")
for error in task_result["errors"]:
print(error)
else:
conversation_result = task_result["conversations"][0]
if conversation_result["warnings"]:
print("... view warnings ...")
for warning in conversation_result["warnings"]:
print(warning)
else:
summaries = conversation_result["summaries"]
print("... view task result ...")
for summary in summaries:
print(f"{summary['aspect']}: {summary['text']}")

Import a Conversation Project
This sample shows a common scenario for the authoring part of the SDK
Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations.authoring import ConversationAuthoringClient
clu_endpoint = os.environ["AZURE_CONVERSATIONS_ENDPOINT"]
clu_key = os.environ["AZURE_CONVERSATIONS_KEY"]
project_name = "test_project"
exported_project_assets = {
"projectKind": "Conversation",
"intents": [{"category": "Read"}, {"category": "Delete"}],
"entities": [{"category": "Sender"}],
"utterances": [

{
"text": "Open Blake's email",
"dataset": "Train",
"intent": "Read",
"entities": [{"category": "Sender", "offset": 5, "length": 5}],
},
{
"text": "Delete last email",
"language": "en-gb",
"dataset": "Test",
"intent": "Delete",
"entities": [],
},
],
}
client = ConversationAuthoringClient(
clu_endpoint, AzureKeyCredential(clu_key)
)
poller = client.begin_import_project(
project_name=project_name,
project={
"assets": exported_project_assets,
"metadata": {
"projectKind": "Conversation",
"settings": {"confidenceThreshold": 0.7},
"projectName": "EmailApp",
"multilingual": True,
"description": "Trying out CLU",
"language": "en-us",
},
"projectFileVersion": "2022-05-01",
},
)
response = poller.result()
print(response)

Optional Configuration
Optional keyword arguments can be passed in at the client and per-operation level. The azurecore reference documentation

describes available configurations for retries, logging,

transport protocols, and more.

Troubleshooting
General
The Conversations client will raise exceptions defined in Azure Core

.

Logging
This library uses the standard logging

library for logging. Basic information about HTTP

sessions (URLs, headers, etc.) is logged at INFO level.
Detailed DEBUG level logging, including request/response bodies and unredacted headers, can
be enabled on a client with the logging_enable argument.
See full SDK logging documentation with examples here.
Python
import sys
import logging
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient
# Create a logger for the 'azure' SDK
logger = logging.getLogger('azure')
logger.setLevel(logging.DEBUG)
# Configure a console output
handler = logging.StreamHandler(stream=sys.stdout)
logger.addHandler(handler)
endpoint = "https://<my-custom-subdomain>.cognitiveservices.azure.com/"
credential = AzureKeyCredential("<my-api-key>")
# This client will log detailed information about its HTTP sessions, at DEBUG level
client = ConversationAnalysisClient(endpoint, credential, logging_enable=True)
result = client.analyze_conversation(...)

Similarly, logging_enable can enable detailed logging for a single operation, even when it isn't
enabled for the client:
Python
result = client.analyze_conversation(..., logging_enable=True)

Next steps
More sample code
See the Sample README
CLU Python API.

for several code snippets illustrating common patterns used in the

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 06/14/2023

or contact opencode@microsoft.com with any additional

Azure Text Analytics client library for Java version 5.5.11
10/27/2025

The Azure Cognitive Service for Language is a cloud-based service that provides Natural
Language Processing (NLP) features for understanding and analyzing text, and includes the
following main features:
Sentiment Analysis
Entity Recognition (Named, Linked, and Personally Identifiable Information (PII) entities)
Language Detection
Key Phrase Extraction
Multiple Actions Analysis Per Document
Healthcare Entities Analysis
Abstractive Text Summarization
Extractive Text Summarization
Custom Named Entity Recognition
Custom Text Classification
Source code

| Package (Maven)

| API reference documentation

| Product Documentation

| Samples

Getting started
Prerequisites
A Java Development Kit (JDK), version 8 or later.
Here are details about Java 8 client compatibility with Azure Certificate Authority.
Azure Subscription
Cognitive Services or Language service account to use this package.

Include the Package
Include the BOM file
Please include the azure-sdk-bom to your project to take dependency on GA version of the
library. In the following snippet, replace the {bom_version_to_target} placeholder with the
version number. To learn more about the BOM, see the AZURE SDK BOM README .

XML

<dependencyManagement>
<dependencies>
<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-sdk-bom</artifactId>
<version>{bom_version_to_target}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
</dependencies>
</dependencyManagement>

and then include the direct dependency in the dependencies section without the version tag.
XML

<dependencies>
<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-ai-textanalytics</artifactId>
</dependency>
</dependencies>

Include direct dependency
If you want to take dependency on a particular version of the library that is not present in the
BOM, add the direct dependency to your project as follows.
XML

<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-ai-textanalytics</artifactId>
<version>5.5.11</version>
</dependency>

Note: This version of the client library defaults to the 2023-04-01 version of the service. It is a
newer version than 3_0 , 3_1 and 2022-05-01 .
This table shows the relationship between SDK services and supported API versions of the
service:
ï¾‰

Expand table

SDK version

Supported API version of service

5.3.x

3.0, 3.1, 2022-05-01, 2023-04-01 (default)

5.2.x

3.0, 3.1, 2022-05-01

5.1.x

3.0, 3.1

5.0.x

3.0

Create a Cognitive Services or Language Service resource
The Language service supports both multi-service and single-service access. Create a Cognitive
Services resource if you plan to access multiple cognitive services under a single endpoint/key.
For Language service access only, create a Language service resource.
You can create the resource using the Azure Portal or Azure CLI following the steps in this
document.

Authenticate the client
In order to interact with the Language service, you will need to create an instance of the Text
Analytics client, both the asynchronous and synchronous clients can be created by using
TextAnalyticsClientBuilder invoking buildClient() creates a synchronous client while
buildAsyncClient() creates its asynchronous counterpart.

You will need an endpoint and either a key or AAD TokenCredential to instantiate a client
object.

Looking up the endpoint
You can find the endpoint for your Language service resource in the Azure Portal
"Keys and Endpoint", or Azure CLI.
Bash

# Get the endpoint for the Language service resource
az cognitiveservices account show --name "resource-name" --resource-group
"resource-group-name" --query "endpoint"

Create a Text Analytics client with key credential

under the

Once you have the value for the key, provide it as a string to the AzureKeyCredential
can be found in the Azure Portal

. This

under the "Keys and Endpoint" section in your created

Language service resource or by running the following Azure CLI command:
Bash

az cognitiveservices account keys list --resource-group <your-resource-group-name>
--name <your-resource-name>

Use the key as the credential parameter to authenticate the client:
Java

TextAnalyticsClient textAnalyticsClient = new TextAnalyticsClientBuilder()
.credential(new AzureKeyCredential("{key}"))
.endpoint("{endpoint}")
.buildClient();

The Azure Text Analytics client library provides a way to rotate the existing key.
Java

AzureKeyCredential credential = new AzureKeyCredential("{key}");
TextAnalyticsClient textAnalyticsClient = new TextAnalyticsClientBuilder()
.credential(credential)
.endpoint("{endpoint}")
.buildClient();
credential.update("{new_key}");

Create a Text Analytics client with Azure Active Directory credential
Azure SDK for Java supports an Azure Identity package, making it easy to get credentials from
Microsoft identity platform.
Authentication with AAD requires some initial setup:
Add the Azure Identity package
XML

<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-identity</artifactId>
<version>1.13.1</version>
</dependency>

Register a new Azure Active Directory application
Grant access to Language service by assigning the "Cognitive Services User" role to
your service principal.
After setup, you can choose which type of credential

from azure.identity to use. As an

example, DefaultAzureCredential can be used to authenticate the client: Set the values of the
client ID, tenant ID, and client secret of the AAD application as environment variables:
AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET.
Authorization is easiest using DefaultAzureCredential. It finds the best credential to use in its
running environment. For more information about using Azure Active Directory authorization
with Language service, please refer to the associated documentation.
Java

TokenCredential defaultCredential = new DefaultAzureCredentialBuilder().build();
TextAnalyticsAsyncClient textAnalyticsAsyncClient = new
TextAnalyticsClientBuilder()
.endpoint("{endpoint}")
.credential(defaultCredential)
.buildAsyncClient();

Key concepts
Text Analytics client
The Text Analytics client library provides a TextAnalyticsClient

and TextAnalyticsAsyncClient

to do analysis on batches of documents. It provides both synchronous and asynchronous
operations to access a specific use of Language service, such as language detection or key
phrase extraction.

Input
A text input, also called a document, is a single unit of document to be analyzed by the
predictive models in the Language service. Operations on a Text Analytics client may take a
single document or a collection of documents to be analyzed as a batch. See service limitations
for the document, including document length limits, maximum batch size, and supported text
encoding.

Operation on multiple documents

For each supported operation, the Text Analytics client provides method overloads to take a
single document, a batch of documents as strings, or a batch of either TextDocumentInput or
DetectLanguageInput objects. The overload taking the TextDocumentInput or
DetectLanguageInput batch allows callers to give each document a unique ID, indicate that the

documents in the batch are written in different languages, or provide a country hint about the
language of the document.

Return value
An operation result, such as AnalyzeSentimentResult , is the result of a Language service
operation, containing a prediction or predictions about a single document and a list of
warnings inside of it. An operation's result type also may optionally include information about
the input document and how it was processed. An operation result contains a isError
property that allows to identify if an operation executed was successful or unsuccessful for the
given document. When the operation results an error, you can simply call getError() to get
TextAnalyticsError which contains the reason why it is unsuccessful. If you are interested in

how many characters are in your document, or the number of operation transactions that have
gone through, simply call getStatistics() to get the TextDocumentStatistics which contains
both information.

Return value collection
An operation result collection, such as AnalyzeSentimentResultCollection , which is the
collection of the result of analyzing sentiment operation. It also includes the model version of
the operation and statistics of the batch documents.
Note: It is recommended to use the batch methods when working on production environments
as they allow you to send one request with multiple documents. This is more performant than
sending a request per each document.

Examples
The following sections provide several code snippets covering some of the most common
Language service tasks, including:
Analyze Sentiment
Detect Language
Extract Key Phrases
Recognize Named Entities
Recognize Personally Identifiable Information Entities

Recognize Linked Entities
Analyze Healthcare Entities
Analyze Multiple Actions
Custom Entities Recognition
Custom Text Classification
Abstractive Text Summarization
Extractive Text Summarization

Text Analytics Client
Language service supports both synchronous and asynchronous client creation by using
TextAnalyticsClientBuilder ,
Java

TextAnalyticsClient textAnalyticsClient = new TextAnalyticsClientBuilder()
.credential(new AzureKeyCredential("{key}"))
.endpoint("{endpoint}")
.buildClient();

or
Java

TextAnalyticsAsyncClient textAnalyticsAsyncClient = new
TextAnalyticsClientBuilder()
.credential(new AzureKeyCredential("{key}"))
.endpoint("{endpoint}")
.buildAsyncClient();

Analyze sentiment
Run a predictive model to identify the positive, negative, neutral or mixed sentiment contained
in the provided document or batch of documents.
Java

String document = "The hotel was dark and unclean. I like microsoft.";
DocumentSentiment documentSentiment =
textAnalyticsClient.analyzeSentiment(document);
System.out.printf("Analyzed document sentiment: %s.%n",
documentSentiment.getSentiment());
documentSentiment.getSentences().forEach(sentenceSentiment ->

System.out.printf("Analyzed sentence sentiment: %s.%n",
sentenceSentiment.getSentiment()));

For samples on using the production recommended option AnalyzeSentimentBatch see here

.

To get more granular information about the opinions related to aspects of a product/service,
also knows as Aspect-based Sentiment Analysis in Natural Language Processing (NLP), see
sample on sentiment analysis with opinion mining see here

.

Please refer to the service documentation for a conceptual discussion of sentiment analysis.

Detect language
Run a predictive model to determine the language that the provided document or batch of
documents are written in.
Java

String document = "Bonjour tout le monde";
DetectedLanguage detectedLanguage = textAnalyticsClient.detectLanguage(document);
System.out.printf("Detected language name: %s, ISO 6391 name: %s, confidence
score: %f.%n",
detectedLanguage.getName(), detectedLanguage.getIso6391Name(),
detectedLanguage.getConfidenceScore());

For samples on using the production recommended option DetectLanguageBatch see here

.

Please refer to the service documentation for a conceptual discussion of language detection.

Extract key phrases
Run a model to identify a collection of significant phrases found in the provided document or
batch of documents.
Java

String document = "My cat might need to see a veterinarian.";
System.out.println("Extracted phrases:");
textAnalyticsClient.extractKeyPhrases(document).forEach(keyPhrase ->
System.out.printf("%s.%n", keyPhrase));

For samples on using the production recommended option ExtractKeyPhrasesBatch see
here

. Please refer to the service documentation for a conceptual discussion of key phrase

extraction.

Recognize named entities
Run a predictive model to identify a collection of named entities in the provided document or
batch of documents and categorize those entities into categories such as person, location, or
organization. For more information on available categories, see Named Entity Categories.
Java

String document = "Satya Nadella is the CEO of Microsoft";
textAnalyticsClient.recognizeEntities(document).forEach(entity ->
System.out.printf("Recognized entity: %s, category: %s, subcategory: %s,
confidence score: %f.%n",
entity.getText(), entity.getCategory(), entity.getSubcategory(),
entity.getConfidenceScore()));

For samples on using the production recommended option RecognizeEntitiesBatch see
here

. Please refer to the service documentation for a conceptual discussion of named entity

recognition.

Recognize Personally Identifiable Information entities
Run a predictive model to identify a collection of Personally Identifiable Information(PII)
entities in the provided document. It recognizes and categorizes PII entities in its input text,
such as Social Security Numbers, bank account information, credit card numbers, and more.
This endpoint is only supported for API versions v3.1-preview.1 and above.
Java

String document = "My SSN is 859-98-0987";
PiiEntityCollection piiEntityCollection =
textAnalyticsClient.recognizePiiEntities(document);
System.out.printf("Redacted Text: %s%n", piiEntityCollection.getRedactedText());
piiEntityCollection.forEach(entity -> System.out.printf(
"Recognized Personally Identifiable Information entity: %s, entity category:
%s, entity subcategory: %s,"
+ " confidence score: %f.%n",
entity.getText(), entity.getCategory(), entity.getSubcategory(),
entity.getConfidenceScore()));

For samples on using the production recommended option RecognizePiiEntitiesBatch see
here

. Please refer to the service documentation for supported PII entity types.

Recognize linked entities

Run a predictive model to identify a collection of entities found in the provided document or
batch of documents, and include information linking the entities to their corresponding entries
in a well-known knowledge base.
Java

String document = "Old Faithful is a geyser at Yellowstone Park.";
textAnalyticsClient.recognizeLinkedEntities(document).forEach(linkedEntity -> {
System.out.println("Linked Entities:");
System.out.printf("Name: %s, entity ID in data source: %s, URL: %s, data
source: %s.%n",
linkedEntity.getName(), linkedEntity.getDataSourceEntityId(),
linkedEntity.getUrl(), linkedEntity.getDataSource());
linkedEntity.getMatches().forEach(match ->
System.out.printf("Text: %s, confidence score: %f.%n", match.getText(),
match.getConfidenceScore()));
});

For samples on using the production recommended option RecognizeLinkedEntitiesBatch see
here

. Please refer to the service documentation for a conceptual discussion of entity linking.

Analyze healthcare entities
Text Analytics for health is a containerized service that extracts and labels relevant medical
information from unstructured texts such as doctor's notes, discharge summaries, clinical
documents, and electronic health records.
Healthcare entities recognition
For more information see How to: Use Text Analytics for health.

Custom entities recognition
Custom NER is one of the custom features offered by Azure Cognitive Service for Language. It
is a cloud-based API service that applies machine-learning intelligence to enable you to build
custom models for custom named entity recognition tasks.
Custom entities recognition
For more information see How to use: Custom Entities Recognition.

Custom text classification
Custom text classification is one of the custom features offered by Azure Cognitive Service for
Language. It is a cloud-based API service that applies machine-learning intelligence to enable

you to build custom models for text classification tasks.
Single label classification
Multi label classification
For more information see How to use: Custom Text Classification.

Analyze multiple actions
The Analyze functionality allows choosing which of the supported Language service features to
execute in the same set of documents. Currently, the supported features are:
Named Entities Recognition
PII Entities Recognition
Linked Entity Recognition
Key Phrase Extraction
Sentiment Analysis
Healthcare Analysis
Custom Entity Recognition (API version 2022-05-01 and newer)
Custom Single-Label Classification (API version 2022-05-01 and newer)
Custom Multi-Label Classification (API version 2022-05-01 and newer)
Abstractive Text Summarization (API version 2023-04-01 and newer)
Extractive Text Summarization (API version 2023-04-01 and newer)
Sample: Multiple action analysis
For more examples, such as asynchronous samples, refer to here

.

Troubleshooting
General
Text Analytics clients raise exceptions. For example, if you try to detect the languages of a
batch of text with same document IDs, 400 error is return that indicating bad request. In the
following code snippet, the error is handled gracefully by catching the exception and display
the additional information about the error.
Java

List<DetectLanguageInput> documents = Arrays.asList(
new DetectLanguageInput("1", "This is written in English.", "us"),
new DetectLanguageInput("1", "Este es un documento escrito en EspaÃ±ol.",

"es")
);
try {
textAnalyticsClient.detectLanguageBatchWithResponse(documents, null,
Context.NONE);
} catch (HttpResponseException e) {
System.out.println(e.getMessage());
}

Enable client logging
You can set the AZURE_LOG_LEVEL environment variable to view logging statements made in the
client library. For example, setting AZURE_LOG_LEVEL=2 would show all informational, warning,
and error log messages. The log levels can be found here: log levels

.

Default HTTP Client
All client libraries by default use the Netty HTTP client. Adding the above dependency will
automatically configure the client library to use the Netty HTTP client. Configuring or changing
the HTTP client is detailed in the HTTP clients wiki.

Default SSL library
All client libraries, by default, use the Tomcat-native Boring SSL library to enable native-level
performance for SSL operations. The Boring SSL library is an uber jar containing native libraries
for Linux / macOS / Windows, and provides better performance compared to the default SSL
implementation within the JDK. For more information, including how to reduce the
dependency size, refer to the performance tuning

section of the wiki.

Next steps
Samples are explained in detail here

.

Contributing
This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA)

declaring that you have the right to, and actually

do, grant us the rights to use your contribution.

When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.
Impressions

or contact opencode@microsoft.com with any additional

Azure Text Analysis client library for
JavaScript - version 1.1.0
Article â€¢ 06/20/2023

Azure Cognitive Service for Language

is a cloud-based service that provides advanced

natural language processing over raw text, and includes the following main features:
Note: This SDK targets Azure Cognitive Service for Language API version 2023-04-01.
Language Detection
Sentiment Analysis
Key Phrase Extraction
Named Entity Recognition
Recognition of Personally Identifiable Information
Entity Linking
Healthcare Analysis
Extractive Summarization
Abstractive Summarization
Custom Entity Recognition
Custom Document Classification
Support Multiple Actions Per Document
Use the client library to:
Detect what language input text is written in.
Determine what customers think of your brand or topic by analyzing raw text for clues
about positive or negative sentiment.
Automatically extract key phrases to quickly identify the main points.
Identify and categorize entities in your text as people, places, organizations, date/time,
quantities, percentages, currencies, healthcare specific, and more.
Perform multiple of the above tasks at once.
Key links:
Source code
Package (NPM)
API reference documentation
Product documentation
Samples

Migrating from @azure/ai-text-analytics advisory âš ï¸

Please see the Migration Guide

for detailed instructions on how to update application code

from version 5.x of the AI Text Analytics client library to the new AI Language Text client library.

What's New
Abstractive Summarization
Healthcare Analysis

Getting started
Currently supported environments
LTS versions of Node.js
Latest versions of Safari, Chrome, Edge, and Firefox.
See our support policy

for more details.

Prerequisites
An Azure subscription

.

An existing Cognitive Services or Language resource. If you need to create the resource,
you can use the Azure Portal

or Azure CLI following the steps in this document.

If you use the Azure CLI, replace <your-resource-group-name> and <your-resource-name> with
your own unique names:
PowerShell

az cognitiveservices account create --kind TextAnalytics --resource-group <yourresource-group-name> --name <your-resource-name> --sku <your-sku-name> --location
<your-location>

Install the @azure/ai-language-text package
Install the Azure Text Analysis client library for JavaScript with npm :
Bash

npm install @azure/ai-language-text

Create and authenticate a TextAnalysisClient
To create a client object to access the Language API, you will need the endpoint of your
Language resource and a credential . The Text Analysis client can use either Azure Active
Directory credentials or an API key credential to authenticate.
You can find the endpoint for your Language resource either in the Azure Portal

or by using

the Azure CLI snippet below:
Bash

az cognitiveservices account show --name <your-resource-name> --resource-group
<your-resource-group-name> --query "properties.endpoint"

Using an API Key
Use the Azure Portal

to browse to your Language resource and retrieve an API key, or use

the Azure CLI snippet below:
Note: Sometimes the API key is referred to as a "subscription key" or "subscription API key."
PowerShell

az cognitiveservices account keys list --resource-group <your-resource-group-name>
--name <your-resource-name>

Once you have an API key and endpoint, you can use the AzureKeyCredential class to
authenticate the client as follows:
JavaScript

const { TextAnalysisClient, AzureKeyCredential } = require("@azure/ai-languagetext");
const client = new TextAnalysisClient("<endpoint>", new AzureKeyCredential("<API
key>"));

Using an Azure Active Directory Credential
Client API key authentication is used in most of the examples, but you can also authenticate
with Azure Active Directory using the Azure Identity library
DefaultAzureCredential

. To use the

provider shown below, or other credential providers provided with

the Azure SDK, please install the @azure/identity package:

Bash

npm install @azure/identity

You will also need to register a new AAD application and grant access to Language by
assigning the "Cognitive Services User" role to your service principal (note: other roles such
as "Owner" will not grant the necessary permissions, only "Cognitive Services User" will
suffice to run the examples and the sample code).
Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID , AZURE_TENANT_ID , AZURE_CLIENT_SECRET .
JavaScript

const { TextAnalysisClient } = require("@azure/ai-language-text");
const { DefaultAzureCredential } = require("@azure/identity");
const client = new TextAnalysisClient("<endpoint>", new DefaultAzureCredential());

Key concepts
TextAnalysisClient
TextAnalysisClient is the primary interface for developers using the Text Analysis client library.

Explore the methods on this client object to understand the different features of the Language
service that you can access.

Input
A document represents a single unit of input to be analyzed by the predictive models in the
Language service. Operations on TextAnalysisClient take a collection of inputs to be analyzed
as a batch. The operation methods have overloads that allow the inputs to be represented as
strings, or as objects with attached metadata.
For example, each document can be passed as a string in an array, e.g.
TypeScript

const documents = [
"I hated the movie. It was so slow!",
"The movie made it into my top ten favorites.",

"What a great movie!",
];

or, if you wish to pass in a per-item document id or language / countryHint , they can be given
as a list of TextDocumentInput or DetectLanguageInput depending on the operation;
JavaScript

const textDocumentInputs = [
{ id: "1", language: "en", text: "I hated the movie. It was so slow!" },
{ id: "2", language: "en", text: "The movie made it into my top ten favorites."
},
{ id: "3", language: "en", text: "What a great movie!" },
];

See service limitations for the input, including document length limits, maximum batch size,
and supported text encodings.

Return Value
The return value corresponding to a single document is either a successful result or an error
object. Each TextAnalysisClient method returns a heterogeneous array of results and errors
that correspond to the inputs by index. A text input and its result will have the same index in
the input and result collections.
An result, such as SentimentAnalysisResult , is the result of a Language operation, containing a
prediction or predictions about a single text input. An operation's result type also may
optionally include information about the input document and how it was processed.
The error object, TextAnalysisErrorResult , indicates that the service encountered an error
while processing the document and contains information about the error.

Document Error Handling
In the collection returned by an operation, errors are distinguished from successful responses
by the presence of the error property, which contains the inner TextAnalysisError object if an
error was encountered. For successful result objects, this property is always undefined .
For example, to filter out all errors, you could use the following filter :
JavaScript

const results = await client.analyze("SentimentAnalysis", documents);

const onlySuccessful = results.filter((result) => result.error === undefined);

Note: TypeScript users can benefit from better type-checking of result and error objects if
compilerOptions.strictNullChecks is set to true in the tsconfig.json configuration. For

example:
TypeScript

const [result] = await client.analyze("SentimentAnalysis", ["Hello world!"]);
if (result.error !== undefined) {
// In this if block, TypeScript will be sure that the type of `result` is
// `TextAnalysisError` if compilerOptions.strictNullChecks is enabled in
// the tsconfig.json
console.log(result.error);
}

Samples
Client Usage
Actions Batching
Choose Model Version
Paging
Rehydrate Polling
Get Statistics

Prebuilt Tasks
Abstractive Summarization
Language Detection
Entity Linking
Entity Regconition
Extractive Summarization
Healthcare Analysis
Key Phrase Extraction
Language Detection
Opinion Mining
PII Entity Recognition
Sentiment Analysis

Custom Tasks
Custom Entity Recognition
Custom Single-lable Classfication
Custom Multi-lable Classfication

Troubleshooting
Logging
Enabling logging may help uncover useful information about failures. In order to see a log of
HTTP requests and responses, set the AZURE_LOG_LEVEL environment variable to info .
Alternatively, logging can be enabled at runtime by calling setLogLevel in the @azure/logger :
JavaScript

const { setLogLevel } = require("@azure/logger");
setLogLevel("info");

For more detailed instructions on how to enable logs, you can look at the @azure/logger
package docs

.

Next steps
Please take a look at the samples

directory for detailed examples on how to use this library.

Contributing
If you'd like to contribute to this library, please read the contributing guide
about how to build and test the code.

Related projects
Microsoft Azure SDK for JavaScript

to learn more

Azure Cognitive Language Services Text
client library for .NET - version 1.0.0-beta.4
Text Analytics is part of the Azure Cognitive Service for Language, a cloud-based service that
provides Natural Language Processing (NLP) features for understanding and analyzing text.
This client library offers the following features:
Language detection
Sentiment analysis
Key phrase extraction
Named entity recognition (NER)
Personally identifiable information (PII) entity recognition
Entity linking
Text analytics for health
Custom named entity recognition (Custom NER)
Custom text classification
Extractive text summarization
Abstractive text summarization
Source code

| Package (NuGet) | API reference documentation | Product

documentation | Samples
ï¼— Note
Text Authoring is not supported from version 2.0.0-beta.1. If you use Text Authoring,
please use the separate Text Authoring SDK. You can find the samples

Getting started
Install the package
Install the client library for .NET with NuGet

:

here.

.NET CLI
dotnet add package Azure.AI.Language.Text --prerelease

ï¾‰

Expand table

SDK version

Supported API version of service

1.0.0-beta.4

2022-05-01, 2023-04-01, 2024-11-01, 2025-11-01, 2025-11-15-preview (default)

1.0.0-beta.3

2022-05-01, 2023-04-01, 2024-11-01, 2024-11-15-preview, 2025-05-15-preview (default)

1.0.0-beta.2

2022-05-01, 2023-04-01, 2024-11-01, 2024-11-15-preview (default)

1.0.0-beta.1

2022-05-01, 2023-04-01, 2023-11-15-preview (default)

Prerequisites
An Azure subscription

.

An existing Cognitive Services or Language service resource.

Authenticate the client
In order to interact with the Text service, you'll need to create an instance of the
TextAnalysisClient

class. You will need an endpoint, and an API key to instantiate a client

object. For more information regarding authenticating with Cognitive Services, see
Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and API key from the Cognitive Services resource or Language
service resource information in the Azure Portal

.

Alternatively, use the Azure CLI snippet below to get the API key from the Language service
resource.
PowerShell
az cognitiveservices account keys list --resource-group <your-resource-group-name> -name <your-resource-name>

Migrating from Azure.Ai.TextAnalytics

Check the migration guide

for more information on migrating from Azure.AI.TextAnalytics to

Azure.AI.Language.Text.

Create a TextAnalysisClient
To use the TextAnalysisClient , use the following namespace in addition to those above, if
needed.
C#
using Azure.AI.Language.Text;

With your endpoint and API key, you can instantiate a TextAnalysisClient :
C#
Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
TextAnalysisClient client = new TextAnalysisClient(endpoint, credential);

Create a client using Azure Active Directory authentication
You can also create a TextAnalysisClient using Azure Active Directory (AAD) authentication.
Your user or service principal must be assigned the "Cognitive Services Language Reader" role.
Using the DefaultAzureCredential

you can authenticate a service using Managed Identity or a

service principal, authenticate as a developer working on an application, and more all without
changing code.
Before you can use the DefaultAzureCredential , or any credential type from Azure.Identity
you'll first need to install the Azure.Identity package

,

.

To use DefaultAzureCredential with a client ID and secret, you'll need to set the
AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET environment variables;

alternatively, you can pass those values to the ClientSecretCredential also in Azure.Identity.
Make sure you use the right namespace for DefaultAzureCredential at the top of your source
file:
C#
using Azure.Identity;

Then you can create an instance of DefaultAzureCredential and pass it to a new instance of
your client:
C#
Uri endpoint = new Uri("{endpoint}");
DefaultAzureCredential credential = new DefaultAzureCredential();
TextAnalysisClient client = new TextAnalysisClient(endpoint, credential);

Note that regional endpoints do not support AAD authentication. Instead, create a custom
domain name for your resource to use AAD authentication.

Service API versions
The client library targets the latest service API version by default. A client instance accepts an
optional service API version parameter from its options to specify which API version service to
communicate.

Select a service API version
You have the flexibility to explicitly select a supported service API version when instantiating a
client by configuring its associated options. This ensures that the client can communicate with
services using the specified API version.
For example,
C#
Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
TextAnalysisClientOptions options = new
TextAnalysisClientOptions(TextAnalysisClientOptions.ServiceVersion.V2023_04_01);
var client = new TextAnalysisClient(endpoint, credential, options);

When selecting an API version, it's important to verify that there are no breaking changes
compared to the latest API version. If there are significant differences, API calls may fail due to
incompatibility.
Always ensure that the chosen API version is fully supported and operational for your specific
use case and that it aligns with the service's versioning policy.
If you do not select an api version we will default to the latest version available, which has the
possibility of being a preview version.

Key concepts
TextAnalysisClient
The TextAnalysisClient

is the primary interface for developers using the Azure AI Text client

library. It provides both synchronous and asynchronous operations to access a specific use of
text analysis, such as language detection or key phrase extraction.

Thread safety
We guarantee that all client instance methods are thread-safe and independent of each other
(guideline ). This ensures that the recommendation of reusing client instances is always safe,
even across threads.

Additional concepts
Client options
Diagnostics

| Accessing the response
| Mocking

| Long-running operations

| Client lifetime

Examples
You can familiarize yourself with different APIs using Samples .
Detect Language
Analyze Sentiment
Extract Key Phrases
Recognize Named Entities
Recognize PII Entities
Recognize Linked Entities
Analyze Healthcare Entities
Custom Named Entity Recognition
Custom Single Label Classification
Custom Multi Label Classification
Extractive Summarization
Abstractive Summarization
Perform multiple text analysis actions

Troubleshooting

| Handling failures

|

General
When you interact with the Cognitive Language Services Text client library using the .NET SDK,
errors returned by the service correspond to the same HTTP status codes returned for REST API
requests.
For example, if you submit a utterance to a non-existant project, a 400 error is returned
indicating "Bad Request".
C#
try
{
string textA =
"We love this trail and make the trip every year. The views are breathtaking and
well worth the hike!"
+ " Yesterday was foggy though, so we missed the spectacular views. We tried
again today and it was"
+ " amazing. Everyone in my family liked the trail although it was too
challenging for the less"
+ " athletic among us. Not necessarily recommended for small children. A hotel
close to the trail"
+ " offers services for childcare in case you want that.";
AnalyzeTextInput body = new TextEntityRecognitionInput()
{
TextInput = new MultiLanguageTextInput()
{
MultiLanguageInputs =
{
new MultiLanguageInput("D", textA),
}
},
ActionContent = new EntitiesActionContent()
{
ModelVersion = "NotValid", // Invalid model version will is a bad
request.
}
};
Response<AnalyzeTextResult> response = client.AnalyzeText(body);
}
catch (RequestFailedException ex)
{
Console.WriteLine(ex.ToString());
}

You will notice that additional information is logged, like the client request ID of the operation.
text

Azure.RequestFailedException: The input parameter is invalid.
Status: 400 (Bad Request)
ErrorCode: InvalidArgument
Content:
Azure.RequestFailedException: Invalid Request.
Status: 400 (Bad Request)
ErrorCode: InvalidRequest
Content:
{"error":{"code":"InvalidRequest","message":"Invalid Request.","innererror":
{"code":"ModelVersionIncorrect","message":"Invalid model version. Possible values
are: latest,2021-06-01,2023-09-01,2024-05-01. For additional details see
https://aka.ms/text-analytics-model-versioning"}}}
Headers:
Transfer-Encoding: chunked
x-envoy-upstream-service-time: REDACTED
apim-request-id: REDACTED
Strict-Transport-Security: REDACTED
X-Content-Type-Options: REDACTED
x-ms-region: REDACTED
Date: Wed, 24 Jul 2024 13:39:00 GMT
Content-Type: application/json; charset=utf-8

Setting up console logging
The simplest way to see the logs is to enable the console logging. To create an Azure SDK log
listener that outputs messages to console use AzureEventSourceListener.CreateConsoleLogger
method.
C#
// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();

To learn more about other logging mechanisms see here

Next steps
View our samples

.

Read about the different features of the Text service.

Contributing

.

See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 12/04/2025

or contact opencode@microsoft.com with any additional

Azure Cognitive Language Services
Conversations client library for .NET version 2.0.0-beta.5
The Azure.AI.Language.Conversations client library provides a suite of APIs for conversational
language analysis capabilities like conversation language understanding and orchestration,
conversational summarization and conversational personally identifiable information (PII)
detection.
Conversation Language Understanding - aka CLU for short - is a cloud-based conversational AI
service which provides many language understanding capabilities like:
Conversation App: It's used in extracting intents and entities in conversations
Workflow app: Acts like an orchestrator to select the best candidate to analyze
conversations to get best response from apps like Qna, Luis, and Conversation App
Conversation Summarization is one feature offered by Azure AI Language, which is a
combination of generative Large Language models and task-optimized encoder models that
offer summarization solutions with higher quality, cost efficiency, and lower latency.
Conversation PII detection another feature offered by Azure AI Language, which is a collection
of machine learning and AI algorithms to identify, categorize, and redact sensitive information
in text. The Conversational PII model is a specialized model for handling speech transcriptions
and the more informal, conversational tone of meeting and call transcripts.
Source code

| Package (NuGet)

| API reference documentation | Samples

| Product

documentation | Analysis REST API documentation
ï¼— Note
Conversational Authoring is not supported from version 2.0.0-beta.1. If you use
Conversational Authoring, please use the separate Conversation Authoring SDK. You can
find the samples

here.

Getting started
Install the package
Install the Azure Cognitive Language Services Conversations client library for .NET with
NuGet

:

PowerShell
dotnet add package Azure.AI.Language.Conversations

Prerequisites
An Azure subscription
An existing Azure Language Service Resource
Though you can use this SDK to create and import conversation projects, Azure AI Foundry
the preferred method for creating projects.

Authenticate the client
In order to interact with the Conversations service, you'll need to create an instance of the
ConversationAnalysisClient

class. You will need an endpoint, and an API key to instantiate a

client object. For more information regarding authenticating with Cognitive Services, see
Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and an API key from the Cognitive Services resource in the Azure
Portal

.

Alternatively, use the Azure CLI command shown below to get the API key from the Cognitive
Service resource.
PowerShell
az cognitiveservices account keys list --resource-group <resource-group-name> --name
<resource-name>

Namespaces
Start by importing the namespace for the ConversationAnalysisClient
C#
using Azure.Core;
using Azure.Core.Serialization;
using Azure.AI.Language.Conversations;
using Azure.AI.Language.Conversations.Models;

and related class:

is

Create a ConversationAnalysisClient
Once you've determined your endpoint and API key you can instantiate a
ConversationAnalysisClient :

C#
Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
ConversationAnalysisClient client = new ConversationAnalysisClient(endpoint,
credential);

Create a client using Azure Active Directory authentication
You can also create a ConversationAnalysisClient using Azure Active Directory (AAD)
authentication. Your user or service principal must be assigned the "Cognitive Services
Language Reader" role. Using the DefaultAzureCredential

you can authenticate a service

using Managed Identity or a service principal, authenticate as a developer working on an
application, and more all without changing code.
Before you can use the DefaultAzureCredential , or any credential type from Azure.Identity
you'll first need to install the Azure.Identity package

,

.

To use DefaultAzureCredential with a client ID and secret, you'll need to set the
AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET environment variables;

alternatively, you can pass those values to the ClientSecretCredential also in Azure.Identity.
Make sure you use the right namespace for DefaultAzureCredential at the top of your source
file:
C#
using Azure.Identity;

Then you can create an instance of DefaultAzureCredential and pass it to a new instance of
your client:
C#
Uri endpoint = new Uri("{endpoint}");
DefaultAzureCredential credential = new DefaultAzureCredential();
ConversationAnalysisClient client = new ConversationAnalysisClient(endpoint,
credential);

Note that regional endpoints do not support AAD authentication. Instead, create a custom
domain name for your resource to use AAD authentication.

Service API versions
The client library targets the latest service API version by default. A client instance accepts an
optional service API version parameter from its options to specify which API version service to
communicate.
ï¾‰

SDK

Expand table

Supported API version of service

version
2.0.0beta.5

2022-05-01, 2023-04-01, 2024-05-01, 2024-11-01, 2025-05-15-preview, 2025-11-15preview (default)

2.0.0-

2022-05-01, 2023-04-01, 2024-05-01, 2024-05-15-preview, 2024-11-01, 2024-11-15-

beta.4

preview, 2025-05-15-preview (default)

2.0.0beta.3

2022-05-01, 2023-04-01, 2024-05-01, 2024-05-15-preview, 2024-11-01, 2024-11-15preview, 2025-05-15-preview (default)

2.0.0-

2022-05-01, 2023-04-01, 2024-05-01, 2024-05-15-preview, 2024-11-01, 2024-11-15-

beta.2

preview (default)

2.0.0beta.1

2022-05-01, 2023-04-01, 2024-05-01, 2024-05-15-preview (default)

1.1.0

2022-05-01, 2023-04-01 (default)

1.0.0

2022-05-01 (default)

Select a service API version
You have the flexibility to explicitly select a supported service API version when instantiating a
client by configuring its associated options. This ensures that the client can communicate with
services using the specified API version.
For example,
C#
Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
ConversationsClientOptions options = new
ConversationsClientOptions(ConversationsClientOptions.ServiceVersion.V2025_11_15_Pre

view);
ConversationAnalysisClient client = new ConversationAnalysisClient(endpoint,
credential, options);

When selecting an API version, it's important to verify that there are no breaking changes
compared to the latest API version. If there are significant differences, API calls may fail due to
incompatibility.
Always ensure that the chosen API version is fully supported and operational for your specific
use case and that it aligns with the service's versioning policy.
If you do not select an api version we will default to the latest version available, which has the
possibility of being a preview version.

Key concepts
ConversationAnalysisClient
The ConversationAnalysisClient

is the primary interface for making predictions using your

deployed Conversations models. It provides both synchronous and asynchronous APIs to
submit queries.

Thread safety
We guarantee that all client instance methods are thread-safe and independent of each other
(guideline ). This ensures that the recommendation of reusing client instances is always safe,
even across threads.

Additional concepts
Client options
Diagnostics

| Accessing the response

| Long-running operations

| Mocking | Client lifetime

Examples
You can familiarize yourself with different APIs using Samples .
Analyze a conversation - Conversation project
Analyze a conversation - Orchestration project
Analyze a conversation in a different language
Analyze a conversation using extra options

| Handling failures

|

Analyze a conversation - Conversational AI project
Analyze a conversation with Conversation Summarization
Analyze a conversation with Conversation PII
Analyze a Conversation for PII Using Character Masking
Analyze a Conversation for PII Using Entity Masking
Analyze a Conversation for PII With No Masking

Troubleshooting
General
When you interact with the Cognitive Language Services Conversations client library using the
.NET SDK, errors returned by the service correspond to the same HTTP status codes returned
for REST API requests.
For example, if you submit a utterance to a non-existant project, a 400 error is returned
indicating "Bad Request".
C#
try
{
var data = new
{
analysisInput = new
{
conversationItem = new
{
text = "Send an email to Carol about tomorrow's demo",
id = "1",
participantId = "1",
}
},
parameters = new
{
projectName = "invalid-project",
deploymentName = "production",
// Use Utf16CodeUnit for strings in .NET.
stringIndexType = "Utf16CodeUnit",
},
kind = "Conversation",
};
Response response = client.AnalyzeConversation(RequestContent.Create(data));
}
catch (RequestFailedException ex)
{

Console.WriteLine(ex.ToString());
}

You will notice that additional information is logged, like the client request ID of the operation.
text
Azure.RequestFailedException: The input parameter is invalid.
Status: 400 (Bad Request)
ErrorCode: InvalidArgument
Content:
{
"error": {
"code": "InvalidArgument",
"message": "The input parameter is invalid.",
"innerError": {
"code": "InvalidArgument",
"message": "The input parameter \"payload\" cannot be null or empty."
}
}
}
Headers:
Transfer-Encoding: chunked
pragma: no-cache
request-id: 0303b4d0-0954-459f-8a3d-1be6819745b5
apim-request-id: 0303b4d0-0954-459f-8a3d-1be6819745b5
x-envoy-upstream-service-time: 15
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
Cache-Control: no-store, proxy-revalidate, no-cache, max-age=0, private
Content-Type: application/json

Setting up console logging
The simplest way to see the logs is to enable console logging. To create an Azure SDK log
listener that outputs messages to the console use the
AzureEventSourceListener.CreateConsoleLogger method.

C#
// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();

To learn more about other logging mechanisms see here

.

Next steps
View our samples

.

Read about the different features of the Conversations service.
Try our service demos.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 11/27/2025

or contact opencode@microsoft.com with any additional

Azure Authoring client library for .NET version 1.0.0-beta.3
Azure Conversations Authoring is part of the Azure Cognitive Service for Language, a cloudbased service that provides tools for creating, managing, and deploying conversational AI
solutions. This client library offers the following features:
Creating and managing conversation projects
Importing conversation projects
Exporting conversation projects
Getting project details
Deleting projects
Training models for conversational AI
Canceling active training jobs
Evaluating model summaries
Evaluating model results
Managing project snapshots
Loading snapshots
Deploying conversational AI models
Swapping deployments for active models
Deleting trained models
Deleting deployments
Assigning project resources
Getting deployment resource assignment status
Unassigning project resources
Getting deployment resource unassignment status
Source code

| Package (NuGet) | API reference documentation | Product documentation |

Samples

Getting started
This section should include everything a developer needs to do to install and create their first
client connection very quickly.

Install the package
Install the client library for .NET with NuGet
.NET CLI

:

dotnet add package Azure.AI.Language.Conversations.Authoring --prerelease

ï¾‰

Expand table

SDK version

Supported API version of service

1.0.0-beta.1

2023-04-01, 2023-04-15-preview, 2024-11-15-preview (default)

1.0.0-beta.2

2023-04-01, 2023-04-15-preview, 2024-11-15-preview, 2025-05-15-preview (default)

1.0.0-beta.3

2023-04-01, 2025-11-01, 2025-05-15-preview, 2025-11-15-preview (default)

Prerequisites
An Azure subscription

.

An existing Cognitive Services or Language service resource.

Authenticate the client
In order to interact with the Conversations Authoring service, you'll need to create an instance
of the ConversationAnalysisAuthoringClient

class. You will need an endpoint, and an API key

to instantiate a client object. For more information regarding authenticating with Cognitive
Services, see Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and API key from the Cognitive Services resource or Language
service resource information in the Azure Portal

.

Alternatively, use the Azure CLI snippet below to get the API key from the Language service
resource.
PowerShell
az cognitiveservices account keys list --resource-group <your-resource-group-name> -name <your-resource-name>

Create a AnalyzeConversationAuthoring Client
To use the AnalyzeConversationAuthoring client, include the following namespace in your
project:

C#
using Azure.AI.Language.Conversations.Authoring;

With your endpoint and API key, you can instantiate a ConversationAnalysisAuthoringClient
using specific service options:
C#
Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
ConversationAnalysisAuthoringClientOptions options = new
ConversationAnalysisAuthoringClientOptions(ConversationAnalysisAuthoringClientOption
s.ServiceVersion.V2025_11_15_Preview);
ConversationAnalysisAuthoringClient client = new
ConversationAnalysisAuthoringClient(endpoint, credential, options);

Create a client using Azure Active Directory authentication
You can also create a ConversationAnalysisAuthoringClient using Azure Active Directory (AAD)
authentication. Your user or service principal must be assigned the "Cognitive Services
Language Reader" role. Using the DefaultAzureCredential

you can authenticate a service

using Managed Identity or a service principal, authenticate as a developer working on an
application, and more, all without changing code.
Before you can use the DefaultAzureCredential , or any credential type from Azure.Identity
you'll first need to install the Azure.Identity package

,

.

To use DefaultAzureCredential with a client ID and secret, you'll need to set the
AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET environment variables;

alternatively, you can pass those values to the ClientSecretCredential also in Azure.Identity.
Make sure you use the right namespace for DefaultAzureCredential at the top of your source
file:
C#
using Azure.Identity;
using Azure.Core;

Then you can create an instance of DefaultAzureCredential and pass it to a new instance of
your client:
C#

Uri endpoint = new Uri("{endpoint}");
DefaultAzureCredential credential = new DefaultAzureCredential();
ConversationAnalysisAuthoringClient client = new
ConversationAnalysisAuthoringClient(endpoint, credential);

Note that regional endpoints do not support AAD authentication. Instead, create a custom
domain name for your resource to use AAD authentication.

Service API versions
The client library targets the latest service API version by default. A client instance accepts an
optional service API version parameter from its options to specify which API version service to
communicate.

Select a service API version
You have the flexibility to explicitly select a supported service API version when instantiating a
client by configuring its associated options. This ensures that the client can communicate with
services using the specified API version.
For example,
C#
Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
ConversationAnalysisAuthoringClientOptions options = new
ConversationAnalysisAuthoringClientOptions(ConversationAnalysisAuthoringClientOption
s.ServiceVersion.V2025_11_15_Preview);
ConversationAnalysisAuthoringClient client = new
ConversationAnalysisAuthoringClient(endpoint, credential, options);

When selecting an API version, it's important to verify that there are no breaking changes
compared to the latest API version. If there are significant differences, API calls may fail due to
incompatibility.
Always ensure that the chosen API version is fully supported and operational for your specific
use case and that it aligns with the service's versioning policy.
If you do not select an api version we will default to the latest version available, which has the
possibility of being a preview version.

Key concepts

ConversationAuthoringClientlet
The ConversationAuthoringProject

, ConversationAuthoringDeployment

ConversationAuthoringExportedModel

,

and ConversationAuthoringTrainedModel

are the

clientlets for developers using the Azure AI Conversation Authoring client library. It provides
both synchronous and asynchronous operations to access a specific use of conversation
authoring, such as creating and managing conversation projects.

Thread safety
We guarantee that all client instance methods are thread-safe and independent of each other
(guideline ). This ensures that the recommendation of reusing client instances is always safe,
even across threads.

Additional concepts
Client options
Diagnostics

| Accessing the response

| Long-running operations

| Mocking | Client lifetime

Examples
You can familiarize yourself with different APIs using Samples .
Create a New Project
Import a Project
Export a Project
Get Project Details
Delete a Project
Train a Model
Cancel Training Job
Get Model Evaluation Summary
Get Model Evaluation Results
Load Snapshot
Delete a Trained Model
Swap Deployments
Delete a Deployment
Deploy a Project
Get a Deployment
Assign Project Resources
Get Assign Project Resources Status

| Handling failures

|

Unassign Project Resources
Get Unassign Project Resources Status
List Assigned Resource Deployments
List Project Resources
Delete Deployment From Resources
Get Deployment Delete From Resources Status

Troubleshooting
General
When you interact with the Cognitive Language Services Conversations Authoring client library
using the .NET SDK, errors returned by the service correspond to the same HTTP status codes
returned for REST API requests.
For example, if you attempt to create a project with an invalid configuration, a 400 error is
returned indicating "Bad Request".
C#
try
{
string invalidProjectName = "InvalidProject";
ConversationAuthoringProject projectClient =
client.GetProject(invalidProjectName);
ConversationAuthoringCreateProjectDetails projectData = new
ConversationAuthoringCreateProjectDetails(
projectKind: "Conversation",
projectName: invalidProjectName,
language: "invalid-lang"
)
{
Description = "This is a test for invalid configuration."
};
using RequestContent content = RequestContent.Create(projectData);
Response response = projectClient.CreateProject(content);
}
catch (RequestFailedException ex)
{
Console.WriteLine(ex.ToString());
}

You will notice that additional information is logged, like the client request ID of the operation.
text

Azure.RequestFailedException: The input parameter is invalid.
Status: 400 (Bad Request)
ErrorCode: InvalidArgument
Content:
Azure.RequestFailedException: Invalid Request.
Status: 400 (Bad Request)
ErrorCode: InvalidRequest
Content:
{"error":{"code":"InvalidRequest","message":"Invalid Request.","innererror":
{"code":"LanguageCodeInvalid","message":"The language code is invalid. Possible
values are: en, es, fr, ..."}}}
Headers:
Transfer-Encoding: chunked
x-envoy-upstream-service-time: REDACTED
apim-request-id: REDACTED
Strict-Transport-Security: REDACTED
X-Content-Type-Options: REDACTED
x-ms-region: REDACTED
Date: Wed, 24 Jul 2024 13:39:00 GMT
Content-Type: application/json; charset=utf-8

Setting up console logging
The simplest way to see the logs is to enable the console logging. To create an Azure SDK log
listener that outputs messages to console use AzureEventSourceListener.CreateConsoleLogger
method.
C#
// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();

To learn more about other logging mechanisms see here

.

Next steps
View our samples

.

Read about the different features of the Conversations Authoring.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 12/17/2025

or contact opencode@microsoft.com with any additional

Azure Authoring client library for .NET version 1.0.0-beta.2
07/25/2025

Azure Text Authoring is part of the Azure Cognitive Service for Language, a cloud-based service
that provides tools for creating, managing, and deploying text processing and AI solutions. This
client library offers the following features:
Creating and managing text analysis projects
Importing data into text analysis projects
Exporting text analysis projects
Retrieving project details
Deleting projects
Training models
Canceling training jobs
Retrieving model evaluation summaries
Retrieving model evaluation results
Managing project snapshots
Deleting trained models
Swapping deployments
Deleting deployments
Deploying trained models
Retrieving deployment details
Assigning deployment resources
Checking the status of assigned resource operations
Unassigning deployment resources
Checking the status of unassigned resource operations
Source code

| Package (NuGet)

| API reference documentation | Product documentation |

Samples

Getting started
This section should include everything a developer needs to do to install and create their first
client connection very quickly.

Install the package
Install the client library for .NET with NuGet

:

.NET CLI

dotnet add package Azure.AI.Language.Text.Authoring --prerelease

ï¾‰

Expand table

SDK version

Supported API version of service

1.0.0-beta.1

2023-04-01, 2023-04-15-preview, 2024-11-15-preview (default)

1.0.0-beta.2

2023-04-01, 2023-04-15-preview, 2024-11-15-preview, 2025-05-15-preview (default)

Prerequisites
An Azure subscription

.

An existing Cognitive Services or Language service resource.

Authenticate the client
In order to interact with the Text Authoring service, you'll need to create an instance of the
TextAnalysisAuthoringClient

class. You will need an endpoint, and an API key to instantiate a

client object. For more information regarding authenticating with Cognitive Services, see
Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and API key from the Cognitive Services resource or Language
service resource information in the Azure Portal

.

Alternatively, use the Azure CLI snippet below to get the API key from the Language service
resource.
PowerShell

az cognitiveservices account keys list --resource-group <your-resource-group-name>
--name <your-resource-name>

Create a TextAnalysisAuthoringClient
To use the TextAnalysisAuthoringClient, include the following namespace in your project:
C#

using Azure.AI.Language.Text.Authoring;

With your endpoint and API key, you can instantiate a TextAnalysisAuthoringClient using
specific service options:
C#

Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
TextAnalysisAuthoringClientOptions options = new
TextAnalysisAuthoringClientOptions(TextAnalysisAuthoringClientOptions.ServiceVersi
on.V2025_05_15_Preview);
TextAnalysisAuthoringClient client = new TextAnalysisAuthoringClient(endpoint,
credential, options);

Create a client using Azure Active Directory authentication
You can also create a TextAnalysisAuthoringClient using Azure Active Directory (AAD)
authentication. Your user or service principal must be assigned the "Cognitive Services
Language Reader" role. Using the DefaultAzureCredential

, you can authenticate a service

using Managed Identity or a service principal, authenticate as a developer working on an
application, and more, all without changing code.
Before you can use the DefaultAzureCredential , or any credential type from Azure.Identity
you'll first need to install the Azure.Identity package

.

To use DefaultAzureCredential with a client ID and secret, you'll need to set the
AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET environment variables;
alternatively, you can pass those values to the ClientSecretCredential also in Azure.Identity.
Make sure you use the right namespace for DefaultAzureCredential at the top of your source
file:
C#

using Azure.Identity;
using Azure.Core;
using Microsoft.Extensions.Options;

Then you can create an instance of DefaultAzureCredential and pass it to a new instance of
your client:
C#

,

Uri endpoint = new Uri("{endpoint}");;
DefaultAzureCredential credential = new DefaultAzureCredential();
TextAnalysisAuthoringClient client = new TextAnalysisAuthoringClient(endpoint,
credential);

Note that regional endpoints do not support AAD authentication. Instead, create a custom
domain name for your resource to use AAD authentication.

Service API versions
The client library targets the latest service API version by default. A client instance accepts an
optional service API version parameter from its options to specify which API version service to
communicate.

Select a service API version
You have the flexibility to explicitly select a supported service API version when instantiating a
client by configuring its associated options. This ensures that the client can communicate with
services using the specified API version.
For example:
C#

Uri endpoint = new Uri("{endpoint}");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
TextAnalysisAuthoringClientOptions options = new
TextAnalysisAuthoringClientOptions(TextAnalysisAuthoringClientOptions.ServiceVersi
on.V2025_05_15_Preview);
TextAnalysisAuthoringClient client = new TextAnalysisAuthoringClient(endpoint,
credential, options);

When selecting an API version, it's important to verify that there are no breaking changes
compared to the latest API version. If there are significant differences, API calls may fail due to
incompatibility.
Always ensure that the chosen API version is fully supported and operational for your specific
use case and that it aligns with the service's versioning policy.
If you do not select an API version, we will default to the latest version available, which has the
possibility of being a preview version.

Key concepts

TextAuthoringClientlet
The TextAuthoringProject

, TextAuthoringDeployment

TextAuthoringTrainedModel

, TextAuthoringExportedModel

and

are the clientlets for developers using the Azure AI Text

Authoring client library. It provides both synchronous and asynchronous operations to access a
specific use of text authoring, such as creating and managing text analysis projects.

Thread safety
We guarantee that all client instance methods are thread-safe and independent of each other
(guideline). This ensures that the recommendation of reusing client instances is always safe,
even across threads.

Additional concepts
Client options
Diagnostics

| Accessing the response

| Long-running operations

| Mocking | Client lifetime

Examples
You can familiarize yourself with different APIs using Samples .
Create a New Project
Import a Project
Export a Project
Get Project Details
Delete a Project
Train a Model
Cancel Training Job
Get Model Evaluation Summary
Get Model Evaluation Results
Load Snapshot
Delete a Trained Model
Swap Deployments
Delete a Deployment
Deploy a Project
Get Deployment Details
Assign Deployment Resources
Get Deployment Resources Assignment Status
Unassign Deployment Resources

| Handling failures

|

Get Deployment Resources Unassignment Status

Troubleshooting
General
When you interact with the Cognitive Language Services Text Authoring client library using the
.NET SDK, errors returned by the service correspond to the same HTTP status codes returned
for REST API requests.
For example, if you attempt to create a project with an invalid configuration, a 400 error is
returned indicating "Bad Request".
C#

try
{
string invalidProjectName = "InvalidProject";
TextAuthoringProject projectClient = client.GetProject(invalidProjectName);
var projectData = new TextAuthoringCreateProjectDetails(
projectKind: "Text",
storageInputContainerName: "e2e0test0data",
language: "invalid-lang" // Invalid language code
)
{
Description = "This is a test for invalid configuration."
};
Response response = projectClient.CreateProject(projectData);
}
catch (RequestFailedException ex)
{
Console.WriteLine(ex.ToString());
}

You will notice that additional information is logged, like the client request ID of the operation.
text

Azure.RequestFailedException: The input parameter is invalid.
Status: 400 (Bad Request)
ErrorCode: InvalidArgument
Content:
Azure.RequestFailedException: Invalid Request.
Status: 400 (Bad Request)
ErrorCode: InvalidRequest

Content:
{"error":{"code":"InvalidRequest","message":"Invalid Request.","innererror":
{"code":"LanguageCodeInvalid","message":"The language code is invalid. Possible
values are: en, es, fr, ..."}}}
Headers:
Transfer-Encoding: chunked
x-envoy-upstream-service-time: REDACTED
apim-request-id: REDACTED
Strict-Transport-Security: REDACTED
X-Content-Type-Options: REDACTED
x-ms-region: REDACTED
Date: Wed, 24 Jul 2024 13:39:00 GMT
Content-Type: application/json; charset=utf-8

Setting up console logging
The simplest way to see the logs is to enable the console logging. To create an Azure SDK log
listener that outputs messages to the console, use
AzureEventSourceListener.CreateConsoleLogger method.
C#

// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();

To learn more about other logging mechanisms, see here

.

Next steps
View our samples

. Read about the different features of the Text Authoring.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the

instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

or contact opencode@microsoft.com with any additional

Azure Cognitive Language Services
Question Answering client library for .NET version 1.1.0
The Question Answering service is a cloud-based API service that lets you create a
conversational question-and-answer layer over your existing data. Use it to build a knowledge
base by extracting questions and answers from your semi-structured content, including FAQ,
manuals, and documents. Answer usersâ€™ questions with the best answers from the QnAs in your
knowledge baseâ€”automatically. Your knowledge base gets smarter, too, as it continually learns
from user behavior.
Source code
Samples

| Package (NuGet)

| API reference documentation | Product documentation |

| Migration guide

Getting started
Install the package
Install the Azure Cognitive Language Services Question Answering client library for .NET with
NuGet

:

.NET CLI
dotnet add package Azure.AI.Language.QuestionAnswering

Prerequisites
An Azure subscription
An existing Question Answering resource
Though you can use this SDK to create and import conversation projects, Language Studio
the preferred method for creating projects.

Authenticate the client
In order to interact with the Question Answering service, you'll need to either create an
instance of the QuestionAnsweringClient

class for querying existing projects or an instance

of the QuestionAnsweringAuthoringClient

for managing projects within your resource. You

will need an endpoint, and an API key to instantiate a client object. For more information

is

regarding authenticating with Cognitive Services, see Authenticate requests to Azure Cognitive
Services.

Get an API key
You can get the endpoint and an API key from the Cognitive Services resource or Question
Answering resource in the Azure Portal

.

Alternatively, use the Azure CLI command shown below to get the API key from the Question
Answering resource.
PowerShell
az cognitiveservices account keys list --resource-group <resource-group-name> --name
<resource-name>

Create a QuestionAnsweringClient
To use the QuestionAnsweringClient , make sure you use the right namespaces:
C#
using Azure.Core;
using Azure.AI.Language.QuestionAnswering;

With your endpoint and API key you can instantiate a QuestionAnsweringClient :
C#
Uri endpoint = new Uri("https://myaccount.cognitiveservices.azure.com/");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
QuestionAnsweringClient client = new QuestionAnsweringClient(endpoint, credential);

Create a QuestionAnsweringAuthoringClient
To use the QuestionAnsweringAuthoringClient , use the following namespace in addition to
those above, if needed.
C#
using Azure.AI.Language.QuestionAnswering.Authoring;

With your endpoint and API key, you can instantiate a QuestionAnsweringAuthoringClient :

C#
Uri endpoint = new Uri("https://myaccount.cognitiveservices.azure.com/");
AzureKeyCredential credential = new AzureKeyCredential("{api-key}");
QuestionAnsweringAuthoringClient client = new
QuestionAnsweringAuthoringClient(endpoint, credential);

Create a client using Azure Active Directory authentication
You can also create a QuestionAnsweringClient or QuestionAnsweringAuthoringClient using
Azure Active Directory (AAD) authentication. Your user or service principal must be assigned
the "Cognitive Services Language Reader" role. Using the DefaultAzureCredential

you can

authenticate a service using Managed Identity or a service principal, authenticate as a
developer working on an application, and more all without changing code.
Before you can use the DefaultAzureCredential , or any credential type from Azure.Identity
you'll first need to install the Azure.Identity package

,

.

To use DefaultAzureCredential with a client ID and secret, you'll need to set the
AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET environment variables;

alternatively, you can pass those values to the ClientSecretCredential also in Azure.Identity.
Make sure you use the right namespace for DefaultAzureCredential at the top of your source
file:
C#
using Azure.Identity;

Then you can create an instance of DefaultAzureCredential and pass it to a new instance of
your client:
C#
Uri endpoint = new Uri("https://myaccount.cognitiveservices.azure.com");
DefaultAzureCredential credential = new DefaultAzureCredential();
QuestionAnsweringClient client = new QuestionAnsweringClient(endpoint, credential);

Note that regional endpoints do not support AAD authentication. Instead, create a custom
domain name for your resource to use AAD authentication.

Key concepts

QuestionAnsweringClient
The QuestionAnsweringClient

is the primary interface for asking questions using a

knowledge base with your own information, or text input using pre-trained models. It provides
both synchronous and asynchronous APIs to ask questions.

QuestionAnsweringAuthoringClient
The QuestionAnsweringAuthoringClient

provides an interface for managing Question

Answering projects. Examples of the available operations include creating and deploying
projects, updating your knowledge sources, and updating question and answer pairs. It
provides both synchronous and asynchronous APIs.

Thread safety
We guarantee that all client instance methods are thread-safe and independent of each other
(guideline ). This ensures that the recommendation of reusing client instances is always safe,
even across threads.

Additional concepts
Client options
Diagnostics

| Accessing the response
| Mocking

| Long-running operations

| Handling failures

| Client lifetime

Examples
QuestionAnsweringClient
The Azure.AI.Language.QuestionAnswering client library provides both synchronous and
asynchronous APIs.
The following examples show common scenarios using the client created above.

Ask a question
The only input required to a ask a question using an existing knowledge base is just the
question itself:
C#

|

string projectName = "{ProjectName}";
string deploymentName = "{DeploymentName}";
QuestionAnsweringProject project = new QuestionAnsweringProject(projectName,
deploymentName);
Response<AnswersResult> response = client.GetAnswers("How long should my Surface
battery last?", project);
foreach (KnowledgeBaseAnswer answer in response.Value.Answers)
{
Console.WriteLine($"({answer.Confidence:P2}) {answer.Answer}");
Console.WriteLine($"Source: {answer.Source}");
Console.WriteLine();
}

You can set additional properties on QuestionAnsweringClientOptions to limit the number of
answers, specify a minimum confidence score, and more.

Ask a follow-up question
If your knowledge base is configured for chit-chat, you can ask a follow-up question provided
the previous question-answering ID and, optionally, the exact question the user asked:
C#
string projectName = "{ProjectName}";
string deploymentName = "{DeploymentName}";
// Answers are ordered by their ConfidenceScore so assume the user choose the first
answer below:
KnowledgeBaseAnswer previousAnswer = answers.Answers.First();
QuestionAnsweringProject project = new QuestionAnsweringProject(projectName,
deploymentName);
AnswersOptions options = new AnswersOptions
{
AnswerContext = new KnowledgeBaseAnswerContext(previousAnswer.QnaId.Value)
};
Response<AnswersResult> response = client.GetAnswers("How long should charging
take?", project, options);
foreach (KnowledgeBaseAnswer answer in response.Value.Answers)
{
Console.WriteLine($"({answer.Confidence:P2}) {answer.Answer}");
Console.WriteLine($"Source: {answer.Source}");
Console.WriteLine();
}

QuestionAnsweringAuthoringClient

The following examples show common scenarios using the QuestionAnsweringAuthoringClient
instance created in this section.

Create a new project
To create a new project, you must specify the project's name and a create a RequestContent
instance with the parameters needed to set up the project.
C#
// Set project name and request content parameters
string newProjectName = "{ProjectName}";
RequestContent creationRequestContent = RequestContent.Create(
new {
description = "This is the description for a test project",
language = "en",
multilingualResource = false,
settings = new {
defaultAnswer = "No answer found for your question."
}
}
);
Response creationResponse = client.CreateProject(newProjectName,
creationRequestContent);
// Projects can be retrieved as follows
Pageable<BinaryData> projects = client.GetProjects();
Console.WriteLine("Projects: ");
foreach (BinaryData project in projects)
{
Console.WriteLine(project);
}

Deploy your project
Your projects can be deployed using the DeployProjectAsync or the synchronous
DeployProject . All you need to specify is the project's name and the deployment name that

you wish to use. Please note that the service will not allow you to deploy empty projects.
C#
// Set deployment name and start operation
string newDeploymentName = "{DeploymentName}";
Operation<BinaryData> deploymentOperation =
client.DeployProject(WaitUntil.Completed, newProjectName, newDeploymentName);

// Deployments can be retrieved as follows
Pageable<BinaryData> deployments = client.GetDeployments(newProjectName);
Console.WriteLine("Deployments: ");
foreach (BinaryData deployment in deployments)
{
Console.WriteLine(deployment);
}

Add a knowledge source
One way to add content to your project is to add a knowledge source. The following example
shows how you can set up a RequestContent instance to add a new knowledge source of the
type "url".
C#
// Set request content parameters for updating our new project's sources
string sourceUri = "{KnowledgeSourceUri}";
RequestContent updateSourcesRequestContent = RequestContent.Create(
new[] {
new {
op = "add",
value = new
{
displayName = "MicrosoftFAQ",
source = sourceUri,
sourceUri = sourceUri,
sourceKind = "url",
contentStructureKind = "unstructured",
refresh = false
}
}
});
Operation<Pageable<BinaryData>> updateSourcesOperation =
client.UpdateSources(WaitUntil.Completed, newProjectName,
updateSourcesRequestContent);
// Knowledge Sources can be retrieved as follows
Pageable<BinaryData> sources = updateSourcesOperation.Value;
Console.WriteLine("Sources: ");
foreach (BinaryData source in sources)
{
Console.WriteLine(source);
}

Troubleshooting

General
When you interact with the Cognitive Language Services Question Answering client library
using the .NET SDK, errors returned by the service correspond to the same HTTP status codes
returned for REST API requests.
For example, if you submit a question to a non-existant knowledge base, a 400 error is
returned indicating "Bad Request".
C#
try
{
QuestionAnsweringProject project = new QuestionAnsweringProject("invalidknowledgebase", "test");
Response<AnswersResult> response = client.GetAnswers("Does this knowledge base
exist?", project);
}
catch (RequestFailedException ex)
{
Console.WriteLine(ex.ToString());
}

You will notice that additional information is logged, like the client request ID of the operation.
text
Azure.RequestFailedException: Please verify azure search service is up, restart the
WebApp and try again
Status: 400 (Bad Request)
ErrorCode: BadArgument
Content:
{
"error": {
"code": "BadArgument",
"message": "Please verify azure search service is up, restart the WebApp and try
again"
}
}
Headers:
x-envoy-upstream-service-time: 23
apim-request-id: 76a83876-22d1-4977-a0b1-559a674f3605
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
X-Content-Type-Options: nosniff
Date: Wed, 30 Jun 2021 00:32:07 GMT
Content-Length: 139
Content-Type: application/json; charset=utf-8

Setting up console logging
The simplest way to see the logs is to enable console logging. To create an Azure SDK log
listener that outputs messages to the console use the
AzureEventSourceListener.CreateConsoleLogger method.

C#
// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();

To learn more about other logging mechanisms see here

.

Next steps
View our samples

.

Read about the different features
Try our service demos

of the Question Answering service.

.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 10/18/2022

or contact opencode@microsoft.com with any additional

Azure Text Analytics client library for
Python - version 6.0.0b2
The Azure Cognitive Service for Language is a cloud-based service that provides Natural
Language Processing (NLP) features for understanding and analyzing text, and includes the
following main features:
Sentiment Analysis
Named Entity Recognition
Language Detection
Key Phrase Extraction
Entity Linking
Multiple Analysis
Personally Identifiable Information (PII) Detection
Text Analytics for Health
Custom Named Entity Recognition
Custom Text Classification
Extractive Text Summarization
Abstractive Text Summarization
Source code

| Package (PyPI)

| Package (Conda)

| API reference documentation

|

Product documentation | Samples

Getting started
Prerequisites
Python 3.7 later is required to use this package.
You must have an Azure subscription

and a Cognitive Services or Language service

resource to use this package.

Create a Cognitive Services or Language service resource
The Language service supports both multi-service and single-service access. Create a Cognitive
Services resource if you plan to access multiple cognitive services under a single endpoint/key.
For Language service access only, create a Language service resource. You can create the
resource using the Azure Portal

or Azure CLI following the steps in this document.

Interaction with the service using the client library begins with a client. To create a client object,
you will need the Cognitive Services or Language service endpoint to your resource and a

credential that allows you access:

Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient
credential = AzureKeyCredential("<api_key>")
text_analytics_client = TextAnalyticsClient(endpoint="https://<resourcename>.cognitiveservices.azure.com/", credential=credential)

Note that for some Cognitive Services resources the endpoint might look different from the
above code snippet. For example, https://<region>.api.cognitive.microsoft.com/ .

Install the package
Install the Azure Text Analytics client library for Python with pip

:

Bash
pip install azure-ai-textanalytics

Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalysisClient
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
key = os.environ["AZURE_TEXT_KEY"]
text_client = TextAnalysisClient(endpoint, AzureKeyCredential(key))

Note that 5.2.X and newer targets the Azure Cognitive Service for Language APIs. These
APIs include the text analysis and natural language processing features found in the
previous versions of the Text Analytics client library. In addition, the service API has
changed from semantic to date-based versioning. This version of the client library defaults
to the latest supported API version, which currently is 2023-04-01 .
This table shows the relationship between SDK versions and supported API versions of the
service
ï¾‰

Expand table

SDK version

Supported API version of service

6.0.0b2 - Latest preview

2022-05-01, 2023-04-01, 2024-11-01, 2025-11-01, 2025-11-15-preview

release

(default)

6.0.0b1

2022-05-01, 2023-04-01, 2024-11-01, 2025-05-15-preview (default)

5.3.X - Latest stable release

3.0, 3.1, 2022-05-01, 2023-04-01 (default)

5.2.X

3.0, 3.1, 2022-05-01 (default)

5.1.0

3.0, 3.1 (default)

5.0.0

3.0

API version can be selected by passing the api_version

keyword argument into the client. For

the latest Language service features, consider selecting the most recent beta API version. For
production scenarios, the latest stable version is recommended. Setting to an older version
may result in reduced feature compatibility.

Authenticate the client
Get the endpoint
You can find the endpoint for your Language service resource using the Azure Portal or Azure
CLI:
Bash
# Get the endpoint for the Language service resource
az cognitiveservices account show --name "resource-name" --resource-group "resourcegroup-name" --query "properties.endpoint"

Get the API Key
You can get the API key from the Cognitive Services or Language service resource in the Azure
Portal. Alternatively, you can use Azure CLI snippet below to get the API key of your resource.
az cognitiveservices account keys list --name "resource-name" --resource-group "resourcegroup-name"

Create a TextAnalyticsClient with an API Key Credential

Once you have the value for the API key, you can pass it as a string into an instance of
AzureKeyCredential

. Use the key as the credential parameter to authenticate the client:

Python
import os
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalysisClient
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
key = os.environ["AZURE_TEXT_KEY"]
text_client = TextAnalysisClient(endpoint, AzureKeyCredential(key))

Create a TextAnalyticsClient with an Azure Active Directory Credential
To use an Azure Active Directory (AAD) token credential, provide an instance of the desired
credential type obtained from the azure-identity

library. Note that regional endpoints do not

support AAD authentication. Create a custom subdomain name for your resource in order to
use this type of authentication.
Authentication with AAD requires some initial setup:
Install azure-identity
Register a new AAD application
Grant access to the Language service by assigning the "Cognitive Services Language
Reader" role to your service principal.

After setup, you can choose which type of credential
example, DefaultAzureCredential

from azure.identity to use. As an

can be used to authenticate the client:

Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
Use the returned token credential to authenticate the client:
Python
import os
from azure.ai.textanalytics import TextAnalysisClient
from azure.identity import DefaultAzureCredential
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
text_client = TextAnalysisClient(endpoint, credential=credential)

Key concepts
TextAnalyticsClient
The Text Analytics client library provides a TextAnalyticsClient

to do analysis on batches of

documents. It provides both synchronous and asynchronous operations to access a specific use
of text analysis, such as language detection or key phrase extraction.

Input
A document is a single unit to be analyzed by the predictive models in the Language service.
The input for each operation is passed as a list of documents.
Each document can be passed as a string in the list, e.g.
Python
documents = ["I hated the movie. It was so slow!", "The movie made it into my top
ten favorites. What a great movie!"]

or, if you wish to pass in a per-item document id or language / country_hint , they can be
passed as a list of DetectLanguageInput

or TextDocumentInput

or a dict-like

representation of the object:
Python
documents = [
{"id": "1", "language": "en", "text": "I hated the movie. It was so slow!"},
{"id": "2", "language": "en", "text": "The movie made it into my top ten
favorites. What a great movie!"},
]

See service limitations

for the input, including document length limits, maximum batch size,

and supported text encoding.

Return Value
The return value for a single document can be a result or error object. A heterogeneous list
containing a collection of result and error objects is returned from each operation. These
results/errors are index-matched with the order of the provided documents.
A result, such as AnalyzeSentimentResult , is the result of a text analysis operation and
contains a prediction or predictions about a document input.

The error object, DocumentError

, indicates that the service had trouble processing the

document and contains the reason it was unsuccessful.

Document Error Handling
You can filter for a result or error object in the list by using the is_error attribute. For a result
object this is always False and for a DocumentError

this is True .

For example, to filter out all DocumentErrors you might use list comprehension:
Python
response = text_analytics_client.analyze_sentiment(documents)
successful_responses = [doc for doc in response if not doc.is_error]

You can also use the kind attribute to filter between result types:
Python
poller = text_analytics_client.begin_analyze_actions(documents, actions)
response = poller.result()
for result in response:
if result.kind == "SentimentAnalysis":
print(f"Sentiment is {result.sentiment}")
elif result.kind == "KeyPhraseExtraction":
print(f"Key phrases: {result.key_phrases}")
elif result.is_error is True:
print(f"Document error: {result.code}, {result.message}")

Long-Running Operations
Long-running operations are operations which consist of an initial request sent to the service
to start an operation, followed by polling the service at intervals to determine whether the
operation has completed or failed, and if it has succeeded, to get the result.
Methods that support healthcare analysis, custom text analysis, or multiple analyses are
modeled as long-running operations. The client exposes a begin_<method-name> method that
returns a poller object. Callers should wait for the operation to complete by calling result()
on the poller object returned from the begin_<method-name> method. Sample code snippets are
provided to illustrate using long-running operations below.

Examples

The following section provides several code snippets covering some of the most common
Language service tasks, including:
Analyze Sentiment
Recognize Entities
Recognize Linked Entities
Recognize PII Entities
Recognize PII Entities with multiple redaction policies
Recognize PII Entities with confidence score
Extract Key Phrases
Detect Language
Healthcare Entities Analysis
Multiple Analysis
Custom Entity Recognition
Custom Single Label Classification
Custom Multi Label Classification
Extractive Summarization
Abstractive Summarization

Analyze Sentiment
analyze_sentiment

looks at its input text and determines whether its sentiment is positive,

negative, neutral or mixed. It's response includes per-sentence sentiment analysis and
confidence scores.
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
MultiLanguageTextInput,
MultiLanguageInput,
TextSentimentAnalysisInput,
AnalyzeTextSentimentResult,
)

def sample_analyze_sentiment():
# settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)

# input
text_a = (
"The food and service were unacceptable, but the concierge were nice. "
"After talking to them about the quality of the food and the process to get
room service "
"they refunded the money we spent at the restaurant and gave us a voucher
for nearby restaurants."
)
body = TextSentimentAnalysisInput(
text_input=MultiLanguageTextInput(
multi_language_inputs=[MultiLanguageInput(id="A", text=text_a,
language="en")]
)
)
# Sync (non-LRO) call
result = client.analyze_text(body=body)
# Print results
if isinstance(result, AnalyzeTextSentimentResult) and result.results and
result.results.documents:
for doc in result.results.documents:
print(f"\nDocument ID: {doc.id}")
print(f"Overall sentiment: {doc.sentiment}")
if doc.confidence_scores:
print("Confidence scores:")
print(f" positive={doc.confidence_scores.positive}")
print(f" neutral={doc.confidence_scores.neutral}")
print(f" negative={doc.confidence_scores.negative}")
if doc.sentences:
print("\nSentence sentiments:")
for s in doc.sentences:
print(f" Text: {s.text}")
print(f" Sentiment: {s.sentiment}")
if s.confidence_scores:
print(
" Scores: "
f"pos={s.confidence_scores.positive}, "
f"neu={s.confidence_scores.neutral}, "
f"neg={s.confidence_scores.negative}"
)
print(f" Offset: {s.offset}, Length: {s.length}\n")
else:
print("No sentence-level results returned.")
else:
print("No documents in the response or unexpected result type.")

The returned response is a heterogeneous list of result and error objects:
list[AnalyzeSentimentResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of sentiment analysis. To
see how to conduct more granular analysis into the opinions related to individual aspects (such
as attributes of a product or service) in a text, see [here][opinion_mining_sample].

Recognize Entities
recognize_entities

recognizes and categories entities in its input text as people, places,

organizations, date/time, quantities, percentages, currencies, and more.
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
MultiLanguageTextInput,
MultiLanguageInput,
TextEntityRecognitionInput,
EntitiesActionContent,
AnalyzeTextEntitiesResult,
)

def sample_recognize_entities():
# settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)
# input
text_a = (
"We love this trail and make the trip every year. The views are breathtaking
and well worth the hike! "
"Yesterday was foggy though, so we missed the spectacular views. We tried
again today and it was "
"amazing. Everyone in my family liked the trail although it was too
challenging for the less "
"athletic among us. Not necessarily recommended for small children. A hotel
close to the trail "
"offers services for childcare in case you want that."
)
body = TextEntityRecognitionInput(
text_input=MultiLanguageTextInput(
multi_language_inputs=[MultiLanguageInput(id="A", text=text_a,
language="en")]
),
action_content=EntitiesActionContent(model_version="latest"),
)

result = client.analyze_text(body=body)
# Print results
if isinstance(result, AnalyzeTextEntitiesResult) and result.results and
result.results.documents:
for doc in result.results.documents:
print(f"\nDocument ID: {doc.id}")
if doc.entities:
print("Entities:")
for entity in doc.entities:
print(f" Text: {entity.text}")
print(f" Category: {entity.category}")
if entity.subcategory:
print(f" Subcategory: {entity.subcategory}")
print(f" Offset: {entity.offset}")
print(f" Length: {entity.length}")
print(f" Confidence score: {entity.confidence_score}\n")
else:
print("No entities found for this document.")
else:
print("No documents in the response or unexpected result type.")

The returned response is a heterogeneous list of result and error objects:
list[RecognizeEntitiesResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of named entity
recognition and supported types

.

Recognize Linked Entities
recognize_linked_entities

recognizes and disambiguates the identity of each entity found in

its input text (for example, determining whether an occurrence of the word Mars refers to the
planet, or to the Roman god of war). Recognized entities are associated with URLs to a wellknown knowledge base, like Wikipedia.
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
MultiLanguageTextInput,
MultiLanguageInput,
TextEntityLinkingInput,
EntityLinkingActionContent,
AnalyzeTextEntityLinkingResult,
)

def sample_recognize_linked_entities():
# settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)
# input
text_a = (
"Microsoft was founded by Bill Gates with some friends he met at Harvard.
One of his friends, Steve "
"Ballmer, eventually became CEO after Bill Gates as well. Steve Ballmer
eventually stepped down as "
"CEO of Microsoft, and was succeeded by Satya Nadella. Microsoft originally
moved its headquarters "
"to Bellevue, Washington in January 1979, but is now headquartered in
Redmond"
)
body = TextEntityLinkingInput(
text_input=MultiLanguageTextInput(
multi_language_inputs=[MultiLanguageInput(id="A", text=text_a,
language="en")]
),
action_content=EntityLinkingActionContent(model_version="latest"),
)
# Sync (non-LRO) call
result = client.analyze_text(body=body)
# Print results
if isinstance(result, AnalyzeTextEntityLinkingResult) and result.results and
result.results.documents:
for doc in result.results.documents:
print(f"\nDocument ID: {doc.id}")
if not doc.entities:
print("No linked entities found for this document.")
continue
print("Linked Entities:")
for linked in doc.entities:
print(f" Name: {linked.name}")
print(f" Language: {linked.language}")
print(f" Data source: {linked.data_source}")
print(f" URL: {linked.url}")
print(f" ID: {linked.id}")
if linked.matches:
print(" Matches:")
for match in linked.matches:
print(f"
Text: {match.text}")
print(f"
Confidence score: {match.confidence_score}")
print(f"
Offset: {match.offset}")
print(f"
Length: {match.length}")

print()
else:
print("No documents in the response or unexpected result type.")

The returned response is a heterogeneous list of result and error objects:
list[RecognizeLinkedEntitiesResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of entity linking and
supported types

.

Recognize PII Entities
recognize_pii_entities

recognizes and categorizes Personally Identifiable Information (PII)

entities in its input text, such as Social Security Numbers, bank account information, credit card
numbers, and more.
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
MultiLanguageTextInput,
MultiLanguageInput,
TextPiiEntitiesRecognitionInput,
AnalyzeTextPiiResult,
)

def sample_recognize_pii_entities():
# settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)
# input
text_a = (
"Parker Doe has repaid all of their loans as of 2020-04-25. Their SSN is
859-98-0987. "
"To contact them, use their phone number 800-102-1100. They are originally
from Brazil and "
"have document ID number 998.214.865-68."
)
body = TextPiiEntitiesRecognitionInput(
text_input=MultiLanguageTextInput(
multi_language_inputs=[MultiLanguageInput(id="A", text=text_a,
language="en")]
)

)
# Sync (non-LRO) call
result = client.analyze_text(body=body)
# Print results
if isinstance(result, AnalyzeTextPiiResult) and result.results and
result.results.documents:
for doc in result.results.documents:
print(f"\nDocument ID: {doc.id}")
if doc.entities:
print("PII Entities:")
for entity in doc.entities:
print(f" Text: {entity.text}")
print(f" Category: {entity.category}")
# subcategory may be optional
if entity.subcategory:
print(f" Subcategory: {entity.subcategory}")
print(f" Offset: {entity.offset}")
print(f" Length: {entity.length}")
print(f" Confidence score: {entity.confidence_score}\n")
else:
print("No PII entities found for this document.")
else:
print("No documents in the response or unexpected result type.")

The returned response is a heterogeneous list of result and error objects:
list[RecognizePiiEntitiesResult

, DocumentError

]

Please refer to the service documentation for supported PII entity types

.

Note: The Recognize PII Entities service is available in API version v3.1 and newer.

Extract Key Phrases
extract_key_phrases

determines the main talking points in its input text. For example, for the

input text "The food was delicious and there were wonderful staff", the API returns: "food" and
"wonderful staff".
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
MultiLanguageTextInput,
MultiLanguageInput,
TextKeyPhraseExtractionInput,
KeyPhraseActionContent,
AnalyzeTextKeyPhraseResult,

)

def sample_extract_key_phrases():
# get settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)
# Build input
text_a = (
"We love this trail and make the trip every year. The views are breathtaking
and well worth the hike! "
"Yesterday was foggy though, so we missed the spectacular views. We tried
again today and it was "
"amazing. Everyone in my family liked the trail although it was too
challenging for the less "
"athletic among us. Not necessarily recommended for small children. A hotel
close to the trail "
"offers services for childcare in case you want that."
)
body = TextKeyPhraseExtractionInput(
text_input=MultiLanguageTextInput(
multi_language_inputs=[MultiLanguageInput(id="A", text=text_a,
language="en")]
),
action_content=KeyPhraseActionContent(model_version="latest"),
)
result = client.analyze_text(body=body)
# Validate and print results
if not isinstance(result, AnalyzeTextKeyPhraseResult):
print("Unexpected result type.")
return
if result.results is None:
print("No results returned.")
return
if result.results.documents is None or len(result.results.documents) == 0:
print("No documents in the response.")
return
for doc in result.results.documents:
print(f"\nDocument ID: {doc.id}")
if doc.key_phrases:
print("Key Phrases:")
for phrase in doc.key_phrases:
print(f" - {phrase}")
else:
print("No key phrases found for this document.")

The returned response is a heterogeneous list of result and error objects:
list[ExtractKeyPhrasesResult , DocumentError

]

Please refer to the service documentation for a conceptual discussion of key phrase extraction.

Detect Language
detect_language

determines the language of its input text, including the confidence score of

the predicted language.
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
TextLanguageDetectionInput,
LanguageDetectionTextInput,
LanguageInput,
AnalyzeTextLanguageDetectionResult,
)

def sample_detect_language():
# get settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)
# Build input
text_a = "Sentences in different languages."
body = TextLanguageDetectionInput(
text_input=LanguageDetectionTextInput(language_inputs=[LanguageInput(id="A",
text=text_a)])
)
# Sync (non-LRO) call
result = client.analyze_text(body=body)
# Validate and print results
if not isinstance(result, AnalyzeTextLanguageDetectionResult):
print("Unexpected result type.")
return
if not result.results or not result.results.documents:
print("No documents in the response.")
return
for doc in result.results.documents:

print(f"\nDocument ID: {doc.id}")
if doc.detected_language:
dl = doc.detected_language
print(f"Detected language: {dl.name} ({dl.iso6391_name})")
print(f"Confidence score: {dl.confidence_score}")
else:
print("No detected language returned for this document.")

The returned response is a heterogeneous list of result and error objects:
list[DetectLanguageResult

, DocumentError

]

Please refer to the service documentation for a conceptual discussion of language detection
and language and regional support.

Healthcare Entities Analysis
Long-running operation begin_analyze_healthcare_entities

extracts entities recognized within

the healthcare domain, and identifies relationships between entities within the input document
and links to known sources of information in various well known databases, such as UMLS,
CHV, MSH, etc.
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
MultiLanguageTextInput,
MultiLanguageInput,
AnalyzeTextOperationAction,
HealthcareLROTask,
HealthcareLROResult,
)

def sample_analyze_healthcare_entities():
# get settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)
# Build input
text_a = "Prescribed 100mg ibuprofen, taken twice daily."
text_input = MultiLanguageTextInput(
multi_language_inputs=[
MultiLanguageInput(id="A", text=text_a, language="en"),

]
)
actions: list[AnalyzeTextOperationAction] = [
HealthcareLROTask(
name="Healthcare Operation",
),
]
# Start long-running operation (sync) â€“ poller returns ItemPaged[TextActions]
poller = client.begin_analyze_text_job(
text_input=text_input,
actions=actions,
)
# Operation metadata (pre-final)
print(f"Operation ID: {poller.details.get('operation_id')}")
# Wait for completion and get pageable of TextActions
paged_actions = poller.result()
# Final-state metadata
d = poller.details
print(f"Job ID: {d.get('job_id')}")
print(f"Status: {d.get('status')}")
print(f"Created: {d.get('created_date_time')}")
print(f"Last Updated: {d.get('last_updated_date_time')}")
if d.get("expiration_date_time"):
print(f"Expires: {d.get('expiration_date_time')}")
if d.get("display_name"):
print(f"Display Name: {d.get('display_name')}")
# Iterate results (sync pageable)
for actions_page in paged_actions:
print(
f"Completed: {actions_page.completed}, "
f"In Progress: {actions_page.in_progress}, "
f"Failed: {actions_page.failed}, "
f"Total: {actions_page.total}"
)
for op_result in actions_page.items_property or []:
if isinstance(op_result, HealthcareLROResult):
print(f"\nAction Name: {op_result.task_name}")
print(f"Action Status: {op_result.status}")
print(f"Kind: {op_result.kind}")
hc_result = op_result.results
for doc in hc_result.documents or []:
print(f"\nDocument ID: {doc.id}")
# Entities
print("Entities:")
for entity in doc.entities or []:
print(f" Text: {entity.text}")

print(f" Category: {entity.category}")
print(f" Offset: {entity.offset}")
print(f" Length: {entity.length}")
print(f" Confidence score: {entity.confidence_score}")
if entity.links:
for link in entity.links:
print(f"
Link ID: {link.id}")
print(f"
Data source: {link.data_source}")
print()
# Relations
print("Relations:")
for relation in doc.relations or []:
print(f" Relation type: {relation.relation_type}")
for rel_entity in relation.entities or []:
print(f"
Role: {rel_entity.role}")
print(f"
Ref: {rel_entity.ref}")
print()
else:
# Other action kinds, if present
try:
print(
f"\n[Non-healthcare action] name={op_result.task_name}, "
f"status={op_result.status}, kind={op_result.kind}"
)
except Exception:
print("\n[Non-healthcare action present]")

Note: Healthcare Entities Analysis is only available with API version v3.1 and newer.

Multiple Analysis
Long-running operation begin_analyze_actions

performs multiple analyses over one set of

documents in a single request. Currently it is supported using any combination of the following
Language APIs in a single request:
Entities Recognition
PII Entities Recognition
Linked Entity Recognition
Key Phrase Extraction
Sentiment Analysis
Custom Entity Recognition (API version 2022-05-01 and newer)
Custom Single Label Classification (API version 2022-05-01 and newer)
Custom Multi Label Classification (API version 2022-05-01 and newer)
Healthcare Entities Analysis (API version 2022-05-01 and newer)
Extractive Summarization (API version 2023-04-01 and newer)
Abstractive Summarization (API version 2023-04-01 and newer)

Python
import os
from azure.identity import DefaultAzureCredential
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalysisClient
from azure.ai.textanalytics.models import (
MultiLanguageTextInput,
MultiLanguageInput,
EntitiesLROTask,
KeyPhraseLROTask,
EntityRecognitionOperationResult,
KeyPhraseExtractionOperationResult,
EntityTag,
)

def sample_analyze():
# get settings
endpoint = os.environ["AZURE_TEXT_ENDPOINT"]
credential = DefaultAzureCredential()
client = TextAnalysisClient(endpoint, credential=credential)
text_a = (
"We love this trail and make the trip every year. The views are breathtaking
and well worth the hike!"
" Yesterday was foggy though, so we missed the spectacular views. We tried
again today and it was"
" amazing. Everyone in my family liked the trail although it was too
challenging for the less"
" athletic among us. Not necessarily recommended for small children. A hotel
close to the trail"
" offers services for childcare in case you want that."
)
text_b = "Sentences in different languages."
text_c = (
"That was the best day of my life! We went on a 4 day trip where we stayed
at Hotel Foo. They had"
" great amenities that included an indoor pool, a spa, and a bar. The spa
offered couples massages"
" which were really good. The spa was clean and felt very peaceful. Overall
the whole experience was"
" great. We will definitely come back."
)
text_d = ""
# Prepare documents (you can batch multiple docs)
text_input = MultiLanguageTextInput(
multi_language_inputs=[
MultiLanguageInput(id="A", text=text_a, language="en"),

MultiLanguageInput(id="B", text=text_b, language="es"),
MultiLanguageInput(id="C", text=text_c, language="en"),
MultiLanguageInput(id="D", text=text_d),
]
)
actions = [
EntitiesLROTask(name="EntitiesOperationActionSample"),
KeyPhraseLROTask(name="KeyPhraseOperationActionSample"),
]
# Submit a multi-action analysis job (LRO)
poller = client.begin_analyze_text_job(text_input=text_input, actions=actions)
paged_actions = poller.result()
# Iterate through each action's results
for action_result in paged_actions:
print() # spacing between action blocks
# --- Entities --if isinstance(action_result, EntityRecognitionOperationResult):
print("=== Entity Recognition Results ===")
for ent_doc in action_result.results.documents:
print(f'Result for document with Id = "{ent_doc.id}":')
print(f" Recognized {len(ent_doc.entities)} entities:")
for entity in ent_doc.entities:
print(f"
Text: {entity.text}")
print(f"
Offset: {entity.offset}")
print(f"
Length: {entity.length}")
print(f"
Category: {entity.category}")
if hasattr(entity, "type") and entity.type is not None:
print(f"
Type: {entity.type}")
if hasattr(entity, "subcategory") and entity.subcategory:
print(f"
Subcategory: {entity.subcategory}")
if hasattr(entity, "tags") and entity.tags:
print("
Tags:")
for tag in entity.tags:
if isinstance(tag, EntityTag):
print(f"
TagName: {tag.name}")
print(f"
TagConfidenceScore:
{tag.confidence_score}")
print(f"
Confidence score: {entity.confidence_score}")
print()
for err in action_result.results.errors:
print(f" Error in document: {err.id}!")
print(f" Document error: {err.error}")
# --- Key Phrases --elif isinstance(action_result, KeyPhraseExtractionOperationResult):
print("=== Key Phrase Extraction Results ===")
for kp_doc in action_result.results.documents:
print(f'Result for document with Id = "{kp_doc.id}":')
for kp in kp_doc.key_phrases:
print(f"
{kp}")
print()

for err in action_result.results.errors:
print(f" Error in document: {err.id}!")
print(f" Document error: {err.error}")

The returned response is an object encapsulating multiple iterables, each representing results
of individual analyses.
Note: Multiple analysis is available in API version v3.1 and newer.

Optional Configuration
Optional keyword arguments can be passed in at the client and per-operation level. The azurecore reference documentation

describes available configurations for retries, logging,

transport protocols, and more.

Troubleshooting
General
The Text Analytics client will raise exceptions defined in Azure Core

.

Logging
This library uses the standard logging

library for logging. Basic information about HTTP

sessions (URLs, headers, etc.) is logged at INFO level.
Detailed DEBUG level logging, including request/response bodies and unredacted headers, can
be enabled on a client with the logging_enable keyword argument:
Python
import sys
import logging
from azure.identity import DefaultAzureCredential
from azure.ai.textanalytics import TextAnalyticsClient
# Create a logger for the 'azure' SDK
logger = logging.getLogger('azure')
logger.setLevel(logging.DEBUG)
# Configure a console output
handler = logging.StreamHandler(stream=sys.stdout)
logger.addHandler(handler)
endpoint = "https://<resource-name>.cognitiveservices.azure.com/"

credential = DefaultAzureCredential()
# This client will log detailed information about its HTTP sessions, at DEBUG level
text_analytics_client = TextAnalyticsClient(endpoint, credential,
logging_enable=True)
result = text_analytics_client.analyze_sentiment(["I did not like the restaurant.
The food was too spicy."])

Similarly, logging_enable can enable detailed logging for a single operation, even when it isn't
enabled for the client:
Python
result = text_analytics_client.analyze_sentiment(documents, logging_enable=True)

Next steps
More sample code
These code samples show common scenario operations with the Azure Text Analytics client
library.
Authenticate the client with a Cognitive Services/Language service API key or a token
credential from azure-identity
sample_authentication.py

:
(async version )

Common scenarios
Analyze sentiment: sample_analyze_sentiment.py

(async version )

Recognize entities: sample_recognize_entities.py

(async version )

Recognize personally identifiable information: sample_recognize_pii_entities.py
version

(async

)

Recognize personally identifiable information(Multiple redaction policies):
sample_recognize_pii_entities_with_redaction_policies.py

(async version )

Recognize personally identifiable information(Confidence score):
sample_recognize_pii_entities_with_confidence_score.py

(async version )

Recognize linked entities: sample_recognize_linked_entities.py
Extract key phrases: sample_extract_key_phrases.py
Detect language: sample_detect_language.py

(async version )

(async version )

(async version )

Healthcare Entities Analysis: sample_analyze_healthcare_entities.py
Multiple Analysis: sample_analyze_actions.py

(async version )

(async version )

Custom Entity Recognition: sample_recognize_custom_entities.py

(async_version

)

Custom Single Label Classification: sample_single_label_classify.py
Custom Multi Label Classification: sample_multi_label_classify.py
Extractive text summarization: sample_extract_summary.py
Abstractive text summarization: sample_abstract_summary.py

(async_version
(async_version

)
)

(async version )
(async version )

Additional documentation
For more extensive documentation on Azure Cognitive Service for Language, see the Language
Service documentation on learn.microsoft.com.

Contributing
This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 12/04/2025

or contact opencode@microsoft.com with any additional

Azure AI Language Question Answering
client library for Python - version 2.0.0b1
Question Answering is an Azure AI Language capability that lets you build a conversational,
questionâ€‘andâ€‘answer layer over your existing data. It extracts question/answer pairs from
semiâ€‘structured content (FAQ pages, manuals, documents) and uses them to answer user
questions with the most relevant answer automatically.
Source code

| Package (PyPI)

documentation | Samples

| Package (Conda)

| API reference

| Product

| Question Answering REST API

Python 2.7 is not supported. For details see the Azure SDK for Python end-of-support notice.

Getting started
Prerequisites
Python 3.9 or later.
An Azure subscription

.

An Azure Language resource

(with a custom domain endpoint if you plan to use Azure

Active Directory authentication).

Install the package
Install the Azure Question Answering client library for Python with pip

:

Bash
python -m pip install azure-ai-language-questionanswering

This version of the client library targets the service REST API version 2025-05-15-preview .

Authenticate the client
In order to interact with the Question Answering service, you'll create an instance of the
QuestionAnsweringClient

(or the AuthoringClient

in the separate authoring package). The

recommended approach is to use Azure Active Directory via DefaultAzureCredential from the
azure-identity

library. This avoids embedding keys, enables managed identity in production,

and unifies authentication across Azure SDKs.

Important: To use Azure AD (AAD) you must use your resource's custom subdomain
endpoint (for example: https://<my-subdomain>.cognitiveservices.azure.com/ ); legacy
regional generic endpoints (e.g., https://eastus.api.cognitive.microsoft.com ) do not
support AAD token authentication.

Recommended: DefaultAzureCredential
Prerequisites for AAD authentication:
Install azure-identity
Register an AAD application
Grant access to the Language resource (e.g., assign the "Cognitive Services Language
Reader" role, plus writer roles if needed for authoring)
Set these environment variables only if youâ€™re using a service principal with a client secret
(otherwise, if you rely on Azure CLI / VS Code login or Managed Identity, you can skip this
step): AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
Then create the client:
Python
from azure.identity import DefaultAzureCredential
from azure.ai.language.questionanswering import QuestionAnsweringClient
endpoint = "https://<my-subdomain>.cognitiveservices.azure.com/"
endpoint
credential = DefaultAzureCredential()

# custom subdomain

client = QuestionAnsweringClient(endpoint, credential)

Authoring (if using the separate authoring package):
Python
from azure.identity import DefaultAzureCredential
from azure.ai.language.questionanswering.authoring import AuthoringClient
endpoint = "https://<my-subdomain>.cognitiveservices.azure.com/"
credential = DefaultAzureCredential()
authoring_client = AuthoringClient(endpoint, credential)

Alternative: API key credential

For quick starts or scripts where you have not yet configured AAD, you can use an API key with
AzureKeyCredential . You can obtain the key from the Azure Portal, or via the CLI:

PowerShell
az cognitiveservices account keys list --resource-group <resource-group-name> --name
<resource-name>

Then:
Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.questionanswering import QuestionAnsweringClient
endpoint = "https://<my-account>.cognitiveservices.azure.com"
subdomain
credential = AzureKeyCredential("<api-key>")

# regional or custom

client = QuestionAnsweringClient(endpoint, credential)

Note: You can seamlessly switch between key and AAD auth â€” no code changes beyond
the credential object.
Why DefaultAzureCredential?
Eliminates hardâ€‘coded secrets
Works locally (developer tools), in CI (service principal / federated), and in production
(Managed Identity)
Centralizes token acquisition & caching
Supports future auth enhancements without code changes

Key concepts
QuestionAnsweringClient
The QuestionAnsweringClient

is the primary interface for asking questions using a

knowledge base with your own information, or text input using pre-trained models. For
asynchronous operations, an async QuestionAnsweringClient is in the
azure.ai.language.questionanswering.aio namespace.

Authoring (project creation, knowledge source management, deployment) has moved to a
separate package and is intentionally not covered in this runtime client README.

Examples
QuestionAnsweringClient usage examples
The azure-ai-language-questionanswering client library provides both synchronous and
asynchronous APIs.
Ask a question
Ask a follow-up question
Asynchronous operations

Ask a question (options object)
The only input required to ask a question using a knowledge base is just the question itself:
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.language.questionanswering import QuestionAnsweringClient
from azure.ai.language.questionanswering.models import AnswersOptions
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"] # must be a custom
subdomain for AAD
client = QuestionAnsweringClient(endpoint, DefaultAzureCredential())
options = AnswersOptions(
question="How long should my Surface battery last?",
# Optional extra parameters:
# confidence_threshold=0.2,
# top=5,
# short_answer_options=qna.ShortAnswerOptions(top=1)
)
response = client.get_answers(options, project_name="FAQ",
deployment_name="production")
for answer in response.answers:
print(f"({answer.confidence:.2f}) {answer.answer}")
print(f"Source: {answer.source}")

You can also pass optional parameters like confidence_threshold , top , or
short_answer_options inside the AnswersOptions object.

Ask a question (flattened)
For convenience, you can also call get_answers directly with keyword parameters:

Python
# Equivalent flattened form - same result as above
response = client.get_answers(
question="How long should my Surface battery last?",
project_name="FAQ",
deployment_name="production",
# Optional parameters can be passed directly:
# confidence_threshold=0.2,
# top=5
)
for answer in response.answers:
print(f"({answer.confidence:.2f}) {answer.answer}")
print(f"Source: {answer.source}")

Follow-up question (options object)
If your knowledge base is configured for chit-chat, the answers from the knowledge base may
include suggested prompts for follow-up questions

to initiate a conversation. You can ask a

follow-up question by providing the ID of your chosen answer as the context for the continued
conversation:
Python
from azure.ai.language.questionanswering.models import AnswersOptions,
KnowledgeBaseAnswerContext
follow_up_options = AnswersOptions(
question="How long should charging take?",
answer_context=KnowledgeBaseAnswerContext(previous_qna_id=previous_answer.qna_id),
)
follow_up = client.get_answers(follow_up_options, project_name="FAQ",
deployment_name="production")
for answer in follow_up.answers:
print(f"({answer.confidence:.2f}) {answer.answer}")

Follow-up question (flattened)
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.language.questionanswering import QuestionAnsweringClient
from azure.ai.language.questionanswering.models import KnowledgeBaseAnswerContext
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]

client = QuestionAnsweringClient(endpoint, DefaultAzureCredential())
output = client.get_answers(
question="How long should charging take?",
answer_context=KnowledgeBaseAnswerContext(previous_qna_id=previous_answer.qna_id),
project_name="FAQ",
deployment_name="production"
)
for candidate in output.answers:
print(f"({candidate.confidence}) {candidate.answer}")
print(f"Source: {candidate.source}")

Async usage (options object)
The above examples can also be run asynchronously using the clients in the aio namespace:
Python
import os
import asyncio
from azure.identity import DefaultAzureCredential
from azure.ai.language.questionanswering.aio import QuestionAnsweringClient
from azure.ai.language.questionanswering.models import AnswersOptions
async def main():
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
client = QuestionAnsweringClient(endpoint, DefaultAzureCredential())
options = AnswersOptions(question="How long should my Surface battery last?")
response = await client.get_answers(options, project_name="FAQ",
deployment_name="production")
for answer in response.answers:
print(f"({answer.confidence:.2f}) {answer.answer}")
asyncio.run(main())

Async usage (flattened)
Python
import os
import asyncio
from azure.identity import DefaultAzureCredential
from azure.ai.language.questionanswering.aio import QuestionAnsweringClient
async def main():
endpoint = os.environ["AZURE_QUESTIONANSWERING_ENDPOINT"]
client = QuestionAnsweringClient(endpoint, DefaultAzureCredential())
output = await client.get_answers(
question="How long should my Surface battery last?",

project_name="FAQ",
deployment_name="production"
)
for candidate in output.answers:
print(f"({candidate.confidence:.2f}) {candidate.answer}")
asyncio.run(main())

Filtering with metadata (QueryFilters)
You can narrow answers using metadata stored in your knowledge base:
Python
from azure.ai.language.questionanswering.models import (
AnswersOptions,
QueryFilters,
MetadataFilter,
MetadataRecord
)
# Tuple form (supported)
metadata_filter_tuple = MetadataFilter(metadata=[("product", "surface"), ("locale",
"en-US")])
# MetadataRecord form (recommended for static typing)
metadata_filter_records = MetadataFilter(metadata=[
MetadataRecord(key="product", value="surface"),
MetadataRecord(key="locale", value="en-US")
])
options = AnswersOptions(
question="How long should my Surface battery last?",
filters=QueryFilters(metadata_filter=metadata_filter_tuple),
confidence_threshold=0.2,
top=3
)
resp = client.get_answers(options, project_name="FAQ", deployment_name="production")
for ans in resp.answers:
print(f"{ans.answer} ({ans.confidence:.2f})")
# Note: Passing metadata as a dict (e.g. {'product': 'surface'}) is no longer
supported.

Optional Configuration
Optional keyword arguments can be passed in at the client and per-operation level. The azurecore reference documentation

describes available configurations for retries, logging,

transport protocols, and more.

Troubleshooting
General
Azure Question Answering clients raise exceptions defined in Azure Core

. When you interact

with the Cognitive Language Service Question Answering client library using the Python SDK,
errors returned by the service correspond to the same HTTP status codes returned for REST API
requests.
For example, if you submit a question to a non-existent knowledge base, a 400 error is
returned indicating "Bad Request".
Python
from azure.core.exceptions import HttpResponseError
try:
client.get_answers(
question="Why?",
project_name="invalid-knowledge-base",
deployment_name="production"
)
except HttpResponseError as error:
print("Query failed: {}".format(error.message))

Logging
This library uses the standard logging

library for logging. Basic information about HTTP

sessions (URLs, headers, etc.) is logged at INFO level.
Detailed DEBUG level logging, including request/response bodies and unredacted headers, can
be enabled on a client with the logging_enable argument.
See the full SDK logging documentation with examples in the logging guidance.

API Usage Notes
This library supports both explicit options objects (like AnswersOptions for get_answers and
AnswersFromTextOptions for get_answers_from_text ) and flattened keyword parameters for

convenience. Both approaches are fully supported and equivalent (and work regardless of
whether you use DefaultAzureCredential or an API key):

Options object approach: client.get_answers(AnswersOptions(question="...", top=5),
project_name="...", deployment_name="...")

Flattened parameters: client.get_answers(question="...", top=5, project_name="...",
deployment_name="...")

Choose whichever style best fits your coding preferences - both produce identical results.

Next steps
View our samples

.

Read about the different features of the Question Answering service.
Try our service demos.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 11/13/2025

or contact opencode@microsoft.com with any additional

Azure Pipelines set up now

Azure Conversational Language
Understanding client library for Python version 2.0.0b2
Conversational Language Understanding - aka CLU for short - is a cloud-based conversational
AI service which provides many language understanding capabilities like:
Conversation App: It's used in extracting intents and entities in conversations
Workflow app: Acts like an orchestrator to select the best candidate to analyze
conversations to get best response from apps like Qna and Conversation App
Conversational Summarization: Used to analyze conversations in the form of
issues/resolution, chapter title, and narrative summarizations
Source code
Samples

| Package (PyPI)

| Package (Conda)

| API reference documentation

|

| Product documentation | REST API documentation

Getting started
Prerequisites
Python 3.7 or later is required to use this package.
An Azure subscription
A Language service resource

Install the package
Install the Azure Conversations client library for Python with pip

:

Bash
pip install azure-ai-language-conversations

Note: This version of the client library defaults to the 2025-11-15-preview version of the
service

Authenticate the client

In order to interact with the CLU service, you'll need to create an instance of the
ConversationAnalysisClient

class. You will need an endpoint, and an API key to instantiate a

client object. For more information regarding authenticating with Cognitive Services, see
Authenticate requests to Azure Cognitive Services.

Get an API key
You can get the endpoint and an API key from the Cognitive Services resource in the Azure
Portal

.

Alternatively, use the Azure CLI command shown below to get the API key from the Cognitive
Service resource.
PowerShell
az cognitiveservices account keys list --resource-group <resource-group-name> --name
<resource-name>

Create ConversationAnalysisClient
Once you've determined your endpoint and API key you can instantiate a
ConversationAnalysisClient :

Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient
endpoint = "https://<my-custom-subdomain>.cognitiveservices.azure.com/"
credential = AzureKeyCredential("<api-key>")
client = ConversationAnalysisClient(endpoint, credential)

Create a client with an Azure Active Directory Credential
To use an Azure Active Directory (AAD) token credential, provide an instance of the desired
credential type obtained from the azure-identity

library. Note that regional endpoints do not

support AAD authentication. Create a custom subdomain name for your resource in order to
use this type of authentication.
Authentication with AAD requires some initial setup:
Install azure-identity
Register a new AAD application

Grant access to the Language service by assigning the "Cognitive Services Language
Reader" role to your service principal.
After setup, you can choose which type of credential
example, DefaultAzureCredential

from azure.identity to use. As an

can be used to authenticate the client:

Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID , AZURE_TENANT_ID , AZURE_CLIENT_SECRET
Use the returned token credential to authenticate the client:
Python
from azure.ai.language.conversations import ConversationAnalysisClient
from azure.identity import DefaultAzureCredential
credential = DefaultAzureCredential()
client = ConversationAnalysisClient(endpoint="https://<my-customsubdomain>.cognitiveservices.azure.com/", credential=credential)

Key concepts
ConversationAnalysisClient
The ConversationAnalysisClient

is the primary interface for making predictions using your

deployed Conversations models. For asynchronous operations, an async
ConversationAnalysisClient is in the azure.ai.language.conversation.aio namespace.

Examples
The azure-ai-language-conversation client library provides both synchronous and
asynchronous APIs.
The following examples show common scenarios using the client created above.

Analyze Text with a Conversation App
If you would like to extract custom intents and entities from a user utterance, you can call the
client.analyze_conversation() method with your conversation's project name as follows:

Python

import os
from azure.identity import DefaultAzureCredential
from azure.ai.language.conversations import ConversationAnalysisClient
from azure.ai.language.conversations.models import (
ConversationLanguageUnderstandingInput,
ConversationAnalysisInput,
TextConversationItem,
ConversationActionContent,
StringIndexType,
ConversationActionResult,
ConversationPrediction,
DateTimeResolution,
)

def sample_conversation_prediction():
# settings
endpoint = os.environ["AZURE_CONVERSATIONS_ENDPOINT"]
project_name = os.environ["AZURE_CONVERSATIONS_PROJECT_NAME"]
deployment_name = os.environ["AZURE_CONVERSATIONS_DEPLOYMENT_NAME"]
credential = DefaultAzureCredential()
client = ConversationAnalysisClient(endpoint, credential=credential)
# build request
data = ConversationLanguageUnderstandingInput(
conversation_input=ConversationAnalysisInput(
conversation_item=TextConversationItem(
id="1",
participant_id="participant1",
text="Send an email to Carol about tomorrow's demo",
)
),
action_content=ConversationActionContent(
project_name=project_name,
deployment_name=deployment_name,
string_index_type=StringIndexType.UTF16_CODE_UNIT,
),
)
# call sync API
response = client.analyze_conversation(data)
if isinstance(response, ConversationActionResult):
pred = response.result.prediction
if isinstance(pred, ConversationPrediction):
# top intent
print(f"Top intent: {pred.top_intent}\n")
# intents
print("Intents:")
for intent in pred.intents or []:
print(f" Category: {intent.category}")

print(f"
print()

Confidence: {intent.confidence}")

# entities
print("Entities:")
for entity in pred.entities or []:
print(f" Category: {entity.category}")
print(f" Text: {entity.text}")
print(f" Offset: {entity.offset}")
print(f" Length: {entity.length}")
print(f" Confidence: {entity.confidence}")
for res in entity.resolutions or []:
if isinstance(res, DateTimeResolution):
print(" DateTime Resolution:")
print(f"
Sub Kind: {res.date_time_sub_kind}")
print(f"
Timex: {res.timex}")
print(f"
Value: {res.value}")
print()
else:
print("Unexpected result type from analyze_conversation.")

Analyze Text with an Orchestration App
If you would like to pass the user utterance to your orchestrator (worflow) app, you can call the
client.analyze_conversation() method with your orchestration's project name. The

orchestrator project simply orchestrates the submitted user utterance between your language
apps (Luis, Conversation, and Question Answering) to get the best response according to the
user intent. See the next example:
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.language.conversations import ConversationAnalysisClient
from azure.ai.language.conversations.models import (
ConversationActionContent,
ConversationAnalysisInput,
TextConversationItem,
StringIndexType,
ConversationLanguageUnderstandingInput,
OrchestrationPrediction,
QuestionAnsweringTargetIntentResult,
ConversationActionResult,
)

def sample_orchestration_prediction():
# settings
endpoint = os.environ["AZURE_CONVERSATIONS_ENDPOINT"]

project_name = os.environ["AZURE_CONVERSATIONS_PROJECT_NAME"]
deployment_name = os.environ["AZURE_CONVERSATIONS_DEPLOYMENT_NAME"]
credential = DefaultAzureCredential()
client = ConversationAnalysisClient(endpoint, credential=credential)
# Build request using strongly-typed models
data = ConversationLanguageUnderstandingInput(
conversation_input=ConversationAnalysisInput(
conversation_item=TextConversationItem(
id="1",
participant_id="participant1",
text="How are you?",
)
),
action_content=ConversationActionContent(
project_name=project_name,
deployment_name=deployment_name,
string_index_type=StringIndexType.UTF16_CODE_UNIT,
),
)
# Call sync API
response = client.analyze_conversation(data)
# Narrow to expected result types
if isinstance(response, ConversationActionResult):
pred = response.result.prediction
if isinstance(pred, OrchestrationPrediction):
# Top intent name is the routed project name
top_intent = pred.top_intent
if not top_intent:
print("No top intent was returned by orchestration.")
return
print(f"Top intent (responding project): {top_intent}")
# Look up the routed target result
target_intent_result = pred.intents.get(top_intent)
if not isinstance(target_intent_result,
QuestionAnsweringTargetIntentResult):
print("Top intent did not route to a Question Answering result.")
return
qa = target_intent_result.result
if qa is not None and qa.answers is not None:
for ans in qa.answers:
print(ans.answer or "")
else:
print("Prediction was not an OrchestrationPrediction.")
else:
print("Unexpected result type from analyze_conversation.")

Conversational Summarization
You can use this sample if you need to summarize a conversation in the form of an issue, and
final resolution. For example, a dialog from tech support:
Python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.language.conversations import ConversationAnalysisClient
from azure.ai.language.conversations.models import (
TextConversationItem,
TextConversation,
ParticipantRole,
MultiLanguageConversationInput,
SummarizationOperationAction,
ConversationSummarizationActionContent,
SummaryAspect,
AnalyzeConversationOperationInput,
SummarizationOperationResult,
ConversationError,
)

def sample_conversation_summarization():
# settings
endpoint = os.environ["AZURE_CONVERSATIONS_ENDPOINT"]
credential = DefaultAzureCredential()
# Build conversation input
conversation_items = [
TextConversationItem(
id="1", participant_id="Agent_1", text="Hello, how can I help you?",
role=ParticipantRole.AGENT
),
TextConversationItem(
id="2",
participant_id="Customer_1",
text="How to upgrade Office? I am getting error messages the whole
day.",
role=ParticipantRole.CUSTOMER,
),
TextConversationItem(
id="3",
participant_id="Agent_1",
text="Press the upgrade button please. Then sign in and follow the
instructions.",
role=ParticipantRole.AGENT,
),
]
conversation_input = MultiLanguageConversationInput(

conversations=[TextConversation(id="1", language="en",
conversation_items=conversation_items)]
)
# Build the operation input and inline actions
operation_input = AnalyzeConversationOperationInput(
conversation_input=conversation_input,
actions=[
SummarizationOperationAction(
name="Issue task",
action_content=ConversationSummarizationActionContent(summary_aspects=
[SummaryAspect.ISSUE]),
),
SummarizationOperationAction(
name="Resolution task",
action_content=ConversationSummarizationActionContent(summary_aspects=
[SummaryAspect.RESOLUTION]),
),
],
)
client = ConversationAnalysisClient(endpoint, credential=credential)
poller = client.begin_analyze_conversation_job(body=operation_input)
# Operation ID
op_id = poller.details.get("operation_id")
if op_id:
print(f"Operation ID: {op_id}")
# Wait for result
paged_actions = poller.result()
# Final-state metadata
d = poller.details
print(f"Job ID: {d.get('job_id')}")
print(f"Status: {d.get('status')}")
print(f"Created: {d.get('created_date_time')}")
print(f"Last Updated: {d.get('last_updated_date_time')}")
if d.get("expiration_date_time"):
print(f"Expires: {d.get('expiration_date_time')}")
if d.get("display_name"):
print(f"Display Name: {d.get('display_name')}")
# Iterate results
for actions_page in paged_actions:
print(
f"Completed: {actions_page.completed}, "
f"In Progress: {actions_page.in_progress}, "
f"Failed: {actions_page.failed}, "
f"Total: {actions_page.total}"
)

for action_result in actions_page.task_results or []:
if isinstance(action_result, SummarizationOperationResult):
for conversation in action_result.results.conversations or []:
print(f" Conversation ID: {conversation.id}")
print(" Summaries:")
for summary in conversation.summaries or []:
print(f"
Aspect: {summary.aspect}")
print(f"
Text: {summary.text}")
if conversation.warnings:
print(" Warnings:")
for warning in conversation.warnings:
print(f"
Code: {warning.code}, Message:
{warning.message}")
else:
print(" [No supported results to display for this action type]")
# Errors
if d.get("errors"):
print("\nErrors:")
for error in d["errors"]:
if isinstance(error, ConversationError):
print(f" Code: {error.code} - {error.message}")

Optional Configuration
Optional keyword arguments can be passed in at the client and per-operation level. The azurecore reference documentation

describes available configurations for retries, logging,

transport protocols, and more.

Troubleshooting
General
The Conversations client will raise exceptions defined in Azure Core

.

Logging
This library uses the standard logging

library for logging. Basic information about HTTP

sessions (URLs, headers, etc.) is logged at INFO level.
Detailed DEBUG level logging, including request/response bodies and unredacted headers, can
be enabled on a client with the logging_enable argument.
See full SDK logging documentation with examples here.

Python
import sys
import logging
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient
# Create a logger for the 'azure' SDK
logger = logging.getLogger('azure')
logger.setLevel(logging.DEBUG)
# Configure a console output
handler = logging.StreamHandler(stream=sys.stdout)
logger.addHandler(handler)
endpoint = "https://<my-custom-subdomain>.cognitiveservices.azure.com/"
credential = AzureKeyCredential("<my-api-key>")
# This client will log detailed information about its HTTP sessions, at DEBUG level
client = ConversationAnalysisClient(endpoint, credential, logging_enable=True)
result = client.analyze_conversation(...)

Similarly, logging_enable can enable detailed logging for a single operation, even when it isn't
enabled for the client:
Python
result = client.analyze_conversation(..., logging_enable=True)

Next steps
More sample code
See the Sample README

for several code snippets illustrating common patterns used in the

CLU Python API.

Contributing
See the CONTRIBUTING.md

for details on building, testing, and contributing to this library.

This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do,
grant us the rights to use your contribution. For details, visit cla.microsoft.com .
When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the

instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.

Last updated on 11/12/2025

or contact opencode@microsoft.com with any additional

Azure Text Analytics client library for Java version 5.5.11
10/27/2025

The Azure Cognitive Service for Language is a cloud-based service that provides Natural
Language Processing (NLP) features for understanding and analyzing text, and includes the
following main features:
Sentiment Analysis
Entity Recognition (Named, Linked, and Personally Identifiable Information (PII) entities)
Language Detection
Key Phrase Extraction
Multiple Actions Analysis Per Document
Healthcare Entities Analysis
Abstractive Text Summarization
Extractive Text Summarization
Custom Named Entity Recognition
Custom Text Classification
Source code

| Package (Maven)

| API reference documentation

| Product Documentation

| Samples

Getting started
Prerequisites
A Java Development Kit (JDK), version 8 or later.
Here are details about Java 8 client compatibility with Azure Certificate Authority.
Azure Subscription
Cognitive Services or Language service account to use this package.

Include the Package
Include the BOM file
Please include the azure-sdk-bom to your project to take dependency on GA version of the
library. In the following snippet, replace the {bom_version_to_target} placeholder with the
version number. To learn more about the BOM, see the AZURE SDK BOM README .

XML

<dependencyManagement>
<dependencies>
<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-sdk-bom</artifactId>
<version>{bom_version_to_target}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
</dependencies>
</dependencyManagement>

and then include the direct dependency in the dependencies section without the version tag.
XML

<dependencies>
<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-ai-textanalytics</artifactId>
</dependency>
</dependencies>

Include direct dependency
If you want to take dependency on a particular version of the library that is not present in the
BOM, add the direct dependency to your project as follows.
XML

<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-ai-textanalytics</artifactId>
<version>5.5.11</version>
</dependency>

Note: This version of the client library defaults to the 2023-04-01 version of the service. It is a
newer version than 3_0 , 3_1 and 2022-05-01 .
This table shows the relationship between SDK services and supported API versions of the
service:
ï¾‰

Expand table

SDK version

Supported API version of service

5.3.x

3.0, 3.1, 2022-05-01, 2023-04-01 (default)

5.2.x

3.0, 3.1, 2022-05-01

5.1.x

3.0, 3.1

5.0.x

3.0

Create a Cognitive Services or Language Service resource
The Language service supports both multi-service and single-service access. Create a Cognitive
Services resource if you plan to access multiple cognitive services under a single endpoint/key.
For Language service access only, create a Language service resource.
You can create the resource using the Azure Portal or Azure CLI following the steps in this
document.

Authenticate the client
In order to interact with the Language service, you will need to create an instance of the Text
Analytics client, both the asynchronous and synchronous clients can be created by using
TextAnalyticsClientBuilder invoking buildClient() creates a synchronous client while
buildAsyncClient() creates its asynchronous counterpart.

You will need an endpoint and either a key or AAD TokenCredential to instantiate a client
object.

Looking up the endpoint
You can find the endpoint for your Language service resource in the Azure Portal
"Keys and Endpoint", or Azure CLI.
Bash

# Get the endpoint for the Language service resource
az cognitiveservices account show --name "resource-name" --resource-group
"resource-group-name" --query "endpoint"

Create a Text Analytics client with key credential

under the

Once you have the value for the key, provide it as a string to the AzureKeyCredential
can be found in the Azure Portal

. This

under the "Keys and Endpoint" section in your created

Language service resource or by running the following Azure CLI command:
Bash

az cognitiveservices account keys list --resource-group <your-resource-group-name>
--name <your-resource-name>

Use the key as the credential parameter to authenticate the client:
Java

TextAnalyticsClient textAnalyticsClient = new TextAnalyticsClientBuilder()
.credential(new AzureKeyCredential("{key}"))
.endpoint("{endpoint}")
.buildClient();

The Azure Text Analytics client library provides a way to rotate the existing key.
Java

AzureKeyCredential credential = new AzureKeyCredential("{key}");
TextAnalyticsClient textAnalyticsClient = new TextAnalyticsClientBuilder()
.credential(credential)
.endpoint("{endpoint}")
.buildClient();
credential.update("{new_key}");

Create a Text Analytics client with Azure Active Directory credential
Azure SDK for Java supports an Azure Identity package, making it easy to get credentials from
Microsoft identity platform.
Authentication with AAD requires some initial setup:
Add the Azure Identity package
XML

<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-identity</artifactId>
<version>1.13.1</version>
</dependency>

Register a new Azure Active Directory application
Grant access to Language service by assigning the "Cognitive Services User" role to
your service principal.
After setup, you can choose which type of credential

from azure.identity to use. As an

example, DefaultAzureCredential can be used to authenticate the client: Set the values of the
client ID, tenant ID, and client secret of the AAD application as environment variables:
AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET.
Authorization is easiest using DefaultAzureCredential. It finds the best credential to use in its
running environment. For more information about using Azure Active Directory authorization
with Language service, please refer to the associated documentation.
Java

TokenCredential defaultCredential = new DefaultAzureCredentialBuilder().build();
TextAnalyticsAsyncClient textAnalyticsAsyncClient = new
TextAnalyticsClientBuilder()
.endpoint("{endpoint}")
.credential(defaultCredential)
.buildAsyncClient();

Key concepts
Text Analytics client
The Text Analytics client library provides a TextAnalyticsClient

and TextAnalyticsAsyncClient

to do analysis on batches of documents. It provides both synchronous and asynchronous
operations to access a specific use of Language service, such as language detection or key
phrase extraction.

Input
A text input, also called a document, is a single unit of document to be analyzed by the
predictive models in the Language service. Operations on a Text Analytics client may take a
single document or a collection of documents to be analyzed as a batch. See service limitations
for the document, including document length limits, maximum batch size, and supported text
encoding.

Operation on multiple documents

For each supported operation, the Text Analytics client provides method overloads to take a
single document, a batch of documents as strings, or a batch of either TextDocumentInput or
DetectLanguageInput objects. The overload taking the TextDocumentInput or
DetectLanguageInput batch allows callers to give each document a unique ID, indicate that the

documents in the batch are written in different languages, or provide a country hint about the
language of the document.

Return value
An operation result, such as AnalyzeSentimentResult , is the result of a Language service
operation, containing a prediction or predictions about a single document and a list of
warnings inside of it. An operation's result type also may optionally include information about
the input document and how it was processed. An operation result contains a isError
property that allows to identify if an operation executed was successful or unsuccessful for the
given document. When the operation results an error, you can simply call getError() to get
TextAnalyticsError which contains the reason why it is unsuccessful. If you are interested in

how many characters are in your document, or the number of operation transactions that have
gone through, simply call getStatistics() to get the TextDocumentStatistics which contains
both information.

Return value collection
An operation result collection, such as AnalyzeSentimentResultCollection , which is the
collection of the result of analyzing sentiment operation. It also includes the model version of
the operation and statistics of the batch documents.
Note: It is recommended to use the batch methods when working on production environments
as they allow you to send one request with multiple documents. This is more performant than
sending a request per each document.

Examples
The following sections provide several code snippets covering some of the most common
Language service tasks, including:
Analyze Sentiment
Detect Language
Extract Key Phrases
Recognize Named Entities
Recognize Personally Identifiable Information Entities

Recognize Linked Entities
Analyze Healthcare Entities
Analyze Multiple Actions
Custom Entities Recognition
Custom Text Classification
Abstractive Text Summarization
Extractive Text Summarization

Text Analytics Client
Language service supports both synchronous and asynchronous client creation by using
TextAnalyticsClientBuilder ,
Java

TextAnalyticsClient textAnalyticsClient = new TextAnalyticsClientBuilder()
.credential(new AzureKeyCredential("{key}"))
.endpoint("{endpoint}")
.buildClient();

or
Java

TextAnalyticsAsyncClient textAnalyticsAsyncClient = new
TextAnalyticsClientBuilder()
.credential(new AzureKeyCredential("{key}"))
.endpoint("{endpoint}")
.buildAsyncClient();

Analyze sentiment
Run a predictive model to identify the positive, negative, neutral or mixed sentiment contained
in the provided document or batch of documents.
Java

String document = "The hotel was dark and unclean. I like microsoft.";
DocumentSentiment documentSentiment =
textAnalyticsClient.analyzeSentiment(document);
System.out.printf("Analyzed document sentiment: %s.%n",
documentSentiment.getSentiment());
documentSentiment.getSentences().forEach(sentenceSentiment ->

System.out.printf("Analyzed sentence sentiment: %s.%n",
sentenceSentiment.getSentiment()));

For samples on using the production recommended option AnalyzeSentimentBatch see here

.

To get more granular information about the opinions related to aspects of a product/service,
also knows as Aspect-based Sentiment Analysis in Natural Language Processing (NLP), see
sample on sentiment analysis with opinion mining see here

.

Please refer to the service documentation for a conceptual discussion of sentiment analysis.

Detect language
Run a predictive model to determine the language that the provided document or batch of
documents are written in.
Java

String document = "Bonjour tout le monde";
DetectedLanguage detectedLanguage = textAnalyticsClient.detectLanguage(document);
System.out.printf("Detected language name: %s, ISO 6391 name: %s, confidence
score: %f.%n",
detectedLanguage.getName(), detectedLanguage.getIso6391Name(),
detectedLanguage.getConfidenceScore());

For samples on using the production recommended option DetectLanguageBatch see here

.

Please refer to the service documentation for a conceptual discussion of language detection.

Extract key phrases
Run a model to identify a collection of significant phrases found in the provided document or
batch of documents.
Java

String document = "My cat might need to see a veterinarian.";
System.out.println("Extracted phrases:");
textAnalyticsClient.extractKeyPhrases(document).forEach(keyPhrase ->
System.out.printf("%s.%n", keyPhrase));

For samples on using the production recommended option ExtractKeyPhrasesBatch see
here

. Please refer to the service documentation for a conceptual discussion of key phrase

extraction.

Recognize named entities
Run a predictive model to identify a collection of named entities in the provided document or
batch of documents and categorize those entities into categories such as person, location, or
organization. For more information on available categories, see Named Entity Categories.
Java

String document = "Satya Nadella is the CEO of Microsoft";
textAnalyticsClient.recognizeEntities(document).forEach(entity ->
System.out.printf("Recognized entity: %s, category: %s, subcategory: %s,
confidence score: %f.%n",
entity.getText(), entity.getCategory(), entity.getSubcategory(),
entity.getConfidenceScore()));

For samples on using the production recommended option RecognizeEntitiesBatch see
here

. Please refer to the service documentation for a conceptual discussion of named entity

recognition.

Recognize Personally Identifiable Information entities
Run a predictive model to identify a collection of Personally Identifiable Information(PII)
entities in the provided document. It recognizes and categorizes PII entities in its input text,
such as Social Security Numbers, bank account information, credit card numbers, and more.
This endpoint is only supported for API versions v3.1-preview.1 and above.
Java

String document = "My SSN is 859-98-0987";
PiiEntityCollection piiEntityCollection =
textAnalyticsClient.recognizePiiEntities(document);
System.out.printf("Redacted Text: %s%n", piiEntityCollection.getRedactedText());
piiEntityCollection.forEach(entity -> System.out.printf(
"Recognized Personally Identifiable Information entity: %s, entity category:
%s, entity subcategory: %s,"
+ " confidence score: %f.%n",
entity.getText(), entity.getCategory(), entity.getSubcategory(),
entity.getConfidenceScore()));

For samples on using the production recommended option RecognizePiiEntitiesBatch see
here

. Please refer to the service documentation for supported PII entity types.

Recognize linked entities

Run a predictive model to identify a collection of entities found in the provided document or
batch of documents, and include information linking the entities to their corresponding entries
in a well-known knowledge base.
Java

String document = "Old Faithful is a geyser at Yellowstone Park.";
textAnalyticsClient.recognizeLinkedEntities(document).forEach(linkedEntity -> {
System.out.println("Linked Entities:");
System.out.printf("Name: %s, entity ID in data source: %s, URL: %s, data
source: %s.%n",
linkedEntity.getName(), linkedEntity.getDataSourceEntityId(),
linkedEntity.getUrl(), linkedEntity.getDataSource());
linkedEntity.getMatches().forEach(match ->
System.out.printf("Text: %s, confidence score: %f.%n", match.getText(),
match.getConfidenceScore()));
});

For samples on using the production recommended option RecognizeLinkedEntitiesBatch see
here

. Please refer to the service documentation for a conceptual discussion of entity linking.

Analyze healthcare entities
Text Analytics for health is a containerized service that extracts and labels relevant medical
information from unstructured texts such as doctor's notes, discharge summaries, clinical
documents, and electronic health records.
Healthcare entities recognition
For more information see How to: Use Text Analytics for health.

Custom entities recognition
Custom NER is one of the custom features offered by Azure Cognitive Service for Language. It
is a cloud-based API service that applies machine-learning intelligence to enable you to build
custom models for custom named entity recognition tasks.
Custom entities recognition
For more information see How to use: Custom Entities Recognition.

Custom text classification
Custom text classification is one of the custom features offered by Azure Cognitive Service for
Language. It is a cloud-based API service that applies machine-learning intelligence to enable

you to build custom models for text classification tasks.
Single label classification
Multi label classification
For more information see How to use: Custom Text Classification.

Analyze multiple actions
The Analyze functionality allows choosing which of the supported Language service features to
execute in the same set of documents. Currently, the supported features are:
Named Entities Recognition
PII Entities Recognition
Linked Entity Recognition
Key Phrase Extraction
Sentiment Analysis
Healthcare Analysis
Custom Entity Recognition (API version 2022-05-01 and newer)
Custom Single-Label Classification (API version 2022-05-01 and newer)
Custom Multi-Label Classification (API version 2022-05-01 and newer)
Abstractive Text Summarization (API version 2023-04-01 and newer)
Extractive Text Summarization (API version 2023-04-01 and newer)
Sample: Multiple action analysis
For more examples, such as asynchronous samples, refer to here

.

Troubleshooting
General
Text Analytics clients raise exceptions. For example, if you try to detect the languages of a
batch of text with same document IDs, 400 error is return that indicating bad request. In the
following code snippet, the error is handled gracefully by catching the exception and display
the additional information about the error.
Java

List<DetectLanguageInput> documents = Arrays.asList(
new DetectLanguageInput("1", "This is written in English.", "us"),
new DetectLanguageInput("1", "Este es un documento escrito en EspaÃ±ol.",

"es")
);
try {
textAnalyticsClient.detectLanguageBatchWithResponse(documents, null,
Context.NONE);
} catch (HttpResponseException e) {
System.out.println(e.getMessage());
}

Enable client logging
You can set the AZURE_LOG_LEVEL environment variable to view logging statements made in the
client library. For example, setting AZURE_LOG_LEVEL=2 would show all informational, warning,
and error log messages. The log levels can be found here: log levels

.

Default HTTP Client
All client libraries by default use the Netty HTTP client. Adding the above dependency will
automatically configure the client library to use the Netty HTTP client. Configuring or changing
the HTTP client is detailed in the HTTP clients wiki.

Default SSL library
All client libraries, by default, use the Tomcat-native Boring SSL library to enable native-level
performance for SSL operations. The Boring SSL library is an uber jar containing native libraries
for Linux / macOS / Windows, and provides better performance compared to the default SSL
implementation within the JDK. For more information, including how to reduce the
dependency size, refer to the performance tuning

section of the wiki.

Next steps
Samples are explained in detail here

.

Contributing
This project welcomes contributions and suggestions. Most contributions require you to agree
to a Contributor License Agreement (CLA)

declaring that you have the right to, and actually

do, grant us the rights to use your contribution.

When you submit a pull request, a CLA-bot will automatically determine whether you need to
provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repos using our
CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more information
see the Code of Conduct FAQ
questions or comments.
Impressions

or contact opencode@microsoft.com with any additional

Azure Text Analysis client library for
JavaScript - version 1.1.0-beta.2
Article â€¢ 03/07/2023

Azure Cognitive Service for Language

is a cloud-based service that provides advanced

natural language processing over raw text, and includes the following main features:
Note: This SDK targets Azure Cognitive Service for Language API version 2022-10-01preview.
Language Detection
Sentiment Analysis
Key Phrase Extraction
Named Entity Recognition
Recognition of Personally Identifiable Information
Entity Linking
Healthcare Analysis
Extractive Summarization
Abstractive Summarization
Custom Entity Recognition
Custom Document Classification
Dynamic Classification
Support Multiple Actions Per Document
Use the client library to:
Detect what language input text is written in.
Determine what customers think of your brand or topic by analyzing raw text for
clues about positive or negative sentiment.
Automatically extract key phrases to quickly identify the main points.
Identify and categorize entities in your text as people, places, organizations,
date/time, quantities, percentages, currencies, healthcare specific, and more.
Perform multiple of the above tasks at once.
Key links:
Source code
Package (NPM)
API reference documentation
Product documentation
Samples

Migrating from @azure/ai-text-analytics advisory âš ï¸
Please see the Migration Guide

for detailed instructions on how to update application

code from version 5.x of the AI Text Analytics client library to the new AI Language Text
client library.

What's New
Abstractive Summarization
Dynamic Classification
Script Detection
Automatic Language Detection
Entity Resolutions
Specifying healthcare document type for better FHIR results

Getting started
Currently supported environments
LTS versions of Node.js
Latest versions of Safari, Chrome, Edge, and Firefox.
See our support policy

for more details.

Prerequisites
An Azure subscription

.

An existing Cognitive Services or Language resource. If you need to create the
resource, you can use the Azure Portal

or Azure CLI following the steps in this

document.
If you use the Azure CLI, replace <your-resource-group-name> and <your-resource-name>
with your own unique names:
PowerShell

az cognitiveservices account create --kind TextAnalytics --resource-group
<your-resource-group-name> --name <your-resource-name> --sku <your-sku-name>
--location <your-location>

Install the @azure/ai-language-text package
Install the Azure Text Analysis client library for JavaScript with npm :
Bash

npm install @azure/ai-language-text

Create and authenticate a TextAnalysisClient
To create a client object to access the Language API, you will need the endpoint of your
Language resource and a credential . The Text Analysis client can use either Azure
Active Directory credentials or an API key credential to authenticate.
You can find the endpoint for your Language resource either in the Azure Portal

or by

using the Azure CLI snippet below:
Bash

az cognitiveservices account show --name <your-resource-name> --resourcegroup <your-resource-group-name> --query "properties.endpoint"

Using an API Key
Use the Azure Portal

to browse to your Language resource and retrieve an API key, or

use the Azure CLI snippet below:
Note: Sometimes the API key is referred to as a "subscription key" or "subscription API
key."
PowerShell

az cognitiveservices account keys list --resource-group <your-resourcegroup-name> --name <your-resource-name>

Once you have an API key and endpoint, you can use the AzureKeyCredential class to
authenticate the client as follows:
JavaScript

const { TextAnalysisClient, AzureKeyCredential } = require("@azure/ailanguage-text");

const client = new TextAnalysisClient("<endpoint>", new AzureKeyCredential("
<API key>"));

Using an Azure Active Directory Credential
Client API key authentication is used in most of the examples, but you can also
authenticate with Azure Active Directory using the Azure Identity library
DefaultAzureCredential

. To use the

provider shown below, or other credential providers provided

with the Azure SDK, please install the @azure/identity package:
Bash

npm install @azure/identity

You will also need to register a new AAD application and grant access to Language by
assigning the "Cognitive Services User" role to your service principal (note: other roles
such as "Owner" will not grant the necessary permissions, only "Cognitive Services
User" will suffice to run the examples and the sample code).

Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID , AZURE_TENANT_ID , AZURE_CLIENT_SECRET .
JavaScript

const { TextAnalysisClient } = require("@azure/ai-language-text");
const { DefaultAzureCredential } = require("@azure/identity");
const client = new TextAnalysisClient("<endpoint>", new
DefaultAzureCredential());

Key concepts
TextAnalysisClient
TextAnalysisClient is the primary interface for developers using the Text Analysis client

library. Explore the methods on this client object to understand the different features of
the Language service that you can access.

Input

A document represents a single unit of input to be analyzed by the predictive models in
the Language service. Operations on TextAnalysisClient take a collection of inputs to
be analyzed as a batch. The operation methods have overloads that allow the inputs to
be represented as strings, or as objects with attached metadata.
For example, each document can be passed as a string in an array, e.g.
TypeScript

const documents = [
"I hated the movie. It was so slow!",
"The movie made it into my top ten favorites.",
"What a great movie!",
];

or, if you wish to pass in a per-item document id or language / countryHint , they can be
given as a list of TextDocumentInput or DetectLanguageInput depending on the
operation;
JavaScript

const textDocumentInputs = [
{ id: "1", language: "en", text: "I hated the movie. It was so slow!" },
{ id: "2", language: "en", text: "The movie made it into my top ten
favorites." },
{ id: "3", language: "en", text: "What a great movie!" },
];

See service limitations for the input, including document length limits, maximum batch
size, and supported text encodings.

Return Value
The return value corresponding to a single document is either a successful result or an
error object. Each TextAnalysisClient method returns a heterogeneous array of results
and errors that correspond to the inputs by index. A text input and its result will have
the same index in the input and result collections.
An result, such as SentimentAnalysisResult , is the result of a Language operation,
containing a prediction or predictions about a single text input. An operation's result
type also may optionally include information about the input document and how it was
processed.

The error object, TextAnalysisErrorResult , indicates that the service encountered an
error while processing the document and contains information about the error.

Document Error Handling
In the collection returned by an operation, errors are distinguished from successful
responses by the presence of the error property, which contains the inner
TextAnalysisError object if an error was encountered. For successful result objects, this

property is always undefined .
For example, to filter out all errors, you could use the following filter :
JavaScript

const results = await client.analyze("SentimentAnalysis", documents);
const onlySuccessful = results.filter((result) => result.error ===
undefined);

Note: TypeScript users can benefit from better type-checking of result and error objects
if compilerOptions.strictNullChecks is set to true in the tsconfig.json configuration.
For example:
TypeScript

const [result] = await client.analyze("SentimentAnalysis", ["Hello
world!"]);
if (result.error !== undefined) {
// In this if block, TypeScript will be sure that the type of `result` is
// `TextAnalysisError` if compilerOptions.strictNullChecks is enabled in
// the tsconfig.json
console.log(result.error);
}

Samples
Client Usage
Actions Batching
Choose Model Version
Paging
Rehydrate Polling

Get Statistics

Prebuilt Tasks
Abstractive Summarization
Dynamic Classification
Language Detection
Entity Linking
Entity Regconition
Extractive Summarization
Healthcare Analysis
Key Phrase Extraction
Language Detection
Opinion Mining
PII Entity Recognition
Sentiment Analysis

Custom Tasks
Custom Entity Recognition
Custom Single-lable Classfication
Custom Multi-lable Classfication

Troubleshooting
Logging
Enabling logging may help uncover useful information about failures. In order to see a
log of HTTP requests and responses, set the AZURE_LOG_LEVEL environment variable to
info . Alternatively, logging can be enabled at runtime by calling setLogLevel in the
@azure/logger :
JavaScript

const { setLogLevel } = require("@azure/logger");
setLogLevel("info");

For more detailed instructions on how to enable logs, you can look at the
@azure/logger package docs .

Next steps
Please take a look at the samples

directory for detailed examples on how to use this

library.

Contributing
If you'd like to contribute to this library, please read the contributing guide
more about how to build and test the code.

Related projects
Microsoft Azure SDK for JavaScript

to learn

Foundry Tools support and help options
Here are the options for getting support, staying up to date, giving feedback, and reporting
bugs for Foundry Tools.

Get solutions to common issues
In the Azure portal, you can find answers to common Foundry Tool issues.
1. Go to your Foundry Tools resource in the Azure portal. You can find it on the list on this
page: Foundry Tools

. If you're a United States government customer, use the Azure

portal for the United States government .
2. In the left pane, under Help, select Support + Troubleshooting.
3. Describe your issue in the text box, and answer the remaining questions in the form.
4. You'll find Learn articles and other resources that might help you resolve your issue.

Create an Azure support request
Explore the range of Azure support options and choose the plan

that best fits, whether

you're a developer just starting your cloud journey or a large organization deploying businesscritical, strategic applications. Azure customers can create and manage support requests in the
Azure portal.
To submit a support request for Foundry Tools, follow the instructions on the New support
request

page in the Azure portal. After choosing your Issue type, select Cognitive Services in

the Service type dropdown field.

Post a question on Microsoft Q&A
For quick and reliable answers on your technical product questions from Microsoft Engineers,
Azure Most Valuable Professionals (MVPs), or our expert community, engage with us on
Microsoft Q&A, Azure's preferred destination for community support.
If you can't find an answer to your problem using search, submit a new question to Microsoft
Q&A. Use one of the following tags when you ask your question:
Foundry Tools
Azure OpenAI

Azure OpenAI
Vision
Vision
Azure Custom Vision
Azure Face
Azure Document Intelligence
Language
Azure Immersive Reader
Language
Azure Translator
Speech
Azure Speech in Foundry Tools

Post a question to Stack Overflow
For answers to your developer questions from the largest community developer ecosystem, ask
your question on Stack Overflow.
If you submit a new question to Stack Overflow, use one or more of the following tags when
you create the question:
Foundry Tools
Azure OpenAI
Azure OpenAI
Vision
Vision
Azure Custom Vision
Azure Face
Azure Document Intelligence
Language
Azure Immersive Reader
Azure Language

Azure Translator
Speech
Azure Speech service

Other support options
You can also use the following forums to ask questions and get help:
Discord - https://aka.ms/foundry/discord
GitHub - https://aka.ms/foundry/forum

Submit feedback
To request new features, post them on https://feedback.azure.com . Share your ideas for
making Foundry Tools and its APIs work better for the applications you develop.
Foundry Tools
Vision
Vision
Azure Custom Vision
Azure Face
Azure Document Intelligence
Video Indexer
Language
Azure Immersive Reader
Language Understanding (LUIS)
Azure QnA Maker
Language
Azure Translator
Speech
Azure Speech service
Decision
Azure Anomaly Detector
Content Moderator

Azure Metrics Advisor
Azure Personalizer

Stay informed
You can learn about the features in a new release or get the latest news on the Azure blog.
Staying informed can help you find the difference between a programming error, a service bug,
or a feature not yet available in Foundry Tools.
Learn more about product updates, roadmap, and announcements in Azure Updates
News about Foundry Tools is shared in the Azure blog
Join the conversation on Reddit

Next step
What are Foundry Tools?

Last updated on 11/18/2025

about Foundry Tools.

.

.

Previous updates for Azure Language in
Foundry Tools
This article contains a list of previously recorded updates for Azure Language in Foundry Tools.
For more current service updates, see What's new.

October 2021
Quality improvements for the extractive summarization feature in model-version 2021-0801 .

September 2021
Starting with version 3.0.017010001-onprem-amd64 The text analytics for health container
can now be called using the client library.

July 2021
General availability for text analytics for health containers and API.
General availability for opinion mining.
General availability for PII extraction and redaction.
General availability for asynchronous operation.

June 2021
General API updates
New model-version 2021-06-01 for key phrase extraction based on transformers. It
provides:
Support for 10 languages (Latin and CJK ).
Improved key phrase extraction.
The 2021-06-01 model version for Named Entity Recognition (NER) which provides
Improved AI quality and expanded language support for the Skill entity category.
Added Spanish, French, German, Italian, and Portuguese language support for the Skill
entity category

Text Analytics for health updates

A new model version 2021-05-15 for the /health endpoint and on-premises container
that provides
Five new entity types: ALLERGEN , CONDITION_SCALE , COURSE , EXPRESSION , and
MUTATION_TYPE ,

14 new relation types,
Assertion detection expanded for new entity types and
Linking support for ALLERGEN entity type
A new image for the Text Analytics for health container with tag 3.0.016230002-onpremamd64 and model version 2021-05-15 . This container is available for download from

Microsoft Container Registry.

May 2021
Custom question answering (previously QnA maker) can now be accessed using a Text
Analytics resource.
Preview API release, including:
Asynchronous API now supports sentiment analysis and opinion mining.
A new query parameter, LoggingOptOut , is now available for customers who wish to opt
out of logging input text for incident reports.
Text analytics for health and asynchronous operations are now available in all regions.

March 2021
Changes in the opinion mining JSON response body:
aspects is now targets and opinions is now assessments .

Changes in the JSON response body of the hosted web API of text analytics for health:
The isNegated boolean name of a detected entity object for negation deprecated and
replaced with assertion detection.
A new property called role is now part of the extracted relation between an attribute
and an entity and the relation between entities. This property adds specificity to the
detected relation type.
Entity linking is now available as an asynchronous task.
A new pii-categories parameter for the PII feature.
This parameter lets you specify select PII entities, and those entities not supported by
default for the input language.

Updated client libraries, which include asynchronous and text analytics for health
operations.
A new model version 2021-03-01 for text analytics for health API and on-premises
container that provides:
A rename of the Gene entity type to GeneOrProtein .
A new Date entity type.
Assertion detection that replaces negation detection.
A new preferred name property for linked entities that is normalized from various
ontologies and coding systems.
A new text analytics for health container image with tag 3.0.015490002-onprem-amd64 and
the new model-version 2021-03-01 is released to the container preview repository.
This container image will no longer be available for download from
containerpreview.azurecr.io after April 26, 2021.

Processed Text Records is now available as a metric in the Monitoring section for your
text analytics resource in the Azure portal.

February 2021
The 2021-01-15 model version for the PII feature, which provides:
Expanded support for nine new languages
Improved AI quality
The S0 through S4 pricing tiers are being retired on March 8, 2021.
The language detection container is now generally available.

January 2021
The 2021-01-15 model version for Named Entity Recognition (NER), which provides
Expanded language support.
Improved AI quality of general entity categories for all supported languages.
The 2021-01-05 model version for language detection, which provides added language
support.

November 2020
Portuguese (Brazil) pt-BR is now supported in sentiment analysis, starting with model
version 2020-04-01 . It adds to the existing pt-PT support for Portuguese.

Updated client libraries, which include asynchronous and text analytics for health
operations.

October 2020
Hindi support for sentiment analysis, starting with model version 2020-04-01 .
Model version 2020-09-01 for language detection, which adds added language support
and accuracy improvements.

September 2020
PII now includes the new redactedText property in the response JSON where detected
PII entities in the input text are replaced with an * for each character of those entities.

Entity linking endpoint now includes the bingID property in the response JSON for linked
entities.
The following updates are specific to the September release of the text analytics for
health container only.
A new container image with tag 1.1.013530001-amd64-preview with the new modelversion 2020-09-03 is released to the container preview repository.
This model version provides improvements in entity recognition, abbreviation
detection, and latency enhancements.

August 2020
Model version 2020-07-01 for key phrase extraction, PII detection, and language
detection. This update adds:
Added government and country/region specific entity categories for Named Entity
Recognition.
Norwegian and Turkish support in Sentiment Analysis.
An HTTP 400 error is now returned for API requests that exceed the published data limits.
Endpoints that return an offset now support the optional stringIndexType parameter,
which adjusts the returned offset and length values to match a supported string index
scheme.
The following updates are specific to the August release of the Text Analytics for health
container only.
New model-version for Text Analytics for health: 2020-07-24
The following properties in the JSON response are changed:

type is renamed to category
score is renamed to confidenceScore

Entities in the category field of the JSON output are now in pascal case. The following
entities are renamed:
EXAMINATION_RELATION is renamed to RelationalOperator .
EXAMINATION_UNIT is renamed to MeasurementUnit .
EXAMINATION_VALUE is renamed to MeasurementValue .
ROUTE_OR_MODE is renamed MedicationRoute .

The relational entity ROUTE_OR_MODE_OF_MEDICATION is renamed to RouteOfMedication .
The following entities are added:
Named Entity Recognition
AdministrativeEvent
CareEnvironment
HealthcareProfession
MedicationForm

Relation extraction
DirectionOfCondition
DirectionOfExamination
DirectionOfTreatment

May 2020
Model version 2020-04-01 :
Updated language support for sentiment analysis
New "Address" entity category in Named Entity Recognition (NER)
New subcategories in NER:
Location - Geographical
Location - Structural
Organization - Stock Exchange
Organization - Medical
Organization - Sports
Event - Cultural
Event - Natural
Event - Sports
The following properties in the JSON response are added:
SentenceText in sentiment analysis

Warnings for each document

The names of the following properties in the JSON response are changed, where
applicable:
score is renamed to confidenceScore
confidenceScore has two decimal points of precision.
type is renamed to category
subtype is renamed to subcategory

New sentiment analysis feature - opinion mining
New personal ( PII ) domain filter for protected health information ( PHI ).

February 2020
Added entity types are now available in the Named Entity Recognition (NER). This update
introduces model version 2020-02-01 , which includes:
Recognition of the following general entity types (English only):
PersonType
Product
Event
Geopolitical Entity (GPE) as a subtype under Location
Skill
Recognition of the following personal information entity types (English only):
Person
Organization
Age as a subtype under Quantity
Date as a subtype under DateTime
Email
Phone Number (US only)
URL
IP Address

October 2019
Introduction of PII feature
Model version 2019-10-01 , which includes:

Named entity recognition:
Expanded detection and categorization of entities found in text.
Recognition of the following new entity types:
Phone number
IP address
Sentiment analysis:
Significant improvements in the accuracy and detail of the API's text categorization
and scoring.
Automatic labeling for different sentiments in text.
Sentiment analysis and output on a document and sentence level.
This model version supports: English ( en ), Japanese ( ja ), Chinese Simplified ( zh-Hans ),
Chinese Traditional ( zh-Hant ), French ( fr ), Italian ( it ), Spanish ( es ), Dutch ( nl ),
Portuguese ( pt ), and German ( de ).

Next steps
See What's new for current service updates.

Last updated on 11/18/2025

Configure Foundry Tools virtual networks
Foundry Tools provide a layered security model. This model enables you to secure your
Foundry Tools accounts to a specific subset of networksâ€‹. When network rules are configured,
only applications that request data over the specified set of networks can access the account.
You can limit access to your resources with request filtering, which allows requests that
originate only from specified IP addresses, IP ranges, or from a list of subnets in Azure Virtual
Networks.
An application that accesses a Foundry resource when network rules are in effect requires
authorization. Authorization is supported with Microsoft Entra ID credentials or with a valid API
key.
ï¼‰ Important
Turning on firewall rules for your Foundry Tools account blocks incoming requests for data
by default. To allow requests through, one of the following conditions needs to be met:
The request originates from a service that operates within an Azure Virtual Network
on the allowed subnet list of the target Foundry Tools account. The endpoint request
that originated from the virtual network needs to be set as the custom subdomain
of your Foundry Tools account.
The request originates from an allowed list of IP addresses.
Requests that are blocked include those from other Azure services, from the Azure portal,
and from logging and metrics services.

ï¼— Note
We recommend that you use the Azure Az PowerShell module to interact with Azure. To
get started, see Install Azure PowerShell. To learn how to migrate to the Az PowerShell
module, see Migrate Azure PowerShell from AzureRM to Az.

Scenarios
To secure your Foundry Tools resource, you should first configure a rule to deny access to
traffic from all networks, including internet traffic, by default. Then, configure rules that grant
access to traffic from specific virtual networks. This configuration enables you to build a secure
network boundary for your applications. You can also configure rules to grant access to traffic

from select public internet IP address ranges and enable connections from specific internet or
on-premises clients.
Network rules are enforced on all network protocols to Foundry Tools, including REST and
WebSocket. To access data by using tools such as the Azure test consoles, explicit network
rules must be configured. You can apply network rules to existing Foundry Tools resources, or
when you create new Foundry Tools resources. After network rules are applied, they're
enforced for all requests.

Supported regions and service offerings
Virtual networks are supported in regions where Foundry Tools are available

. Foundry Tools

support service tags for network rules configuration. The services listed here are included in the
CognitiveServicesManagement service tag.

ï¼‚ Anomaly Detector
ï¼‚ Azure OpenAI
ï¼‚ Content Moderator
ï¼‚ Custom Vision
ï¼‚ Face
ï¼‚ Language Understanding (LUIS)
ï¼‚ Personalizer
ï¼‚ Speech service
ï¼‚ Language
ï¼‚ QnA Maker
ï¼‚ Translator
ï¼— Note
If you use Azure OpenAI, LUIS, Speechs, or Languages, the CognitiveServicesManagement
tag only enables you to use the service by using the SDK or REST API. To access and use
the Microsoft Foundry portal

, LUIS portal, Speech Studio, or Language Studio from a

virtual network, you need to use the following tags:
AzureActiveDirectory
AzureFrontDoor.Frontend
AzureResourceManager
CognitiveServicesManagement
CognitiveServicesFrontEnd
Storage (Speech Studio only)

For information on Foundry portal

configurations, see the Foundry documentation.

Change the default network access rule
By default, Foundry Tools resources accept connections from clients on any network. To limit
access to selected networks, you must first change the default action.
ï¼’ Warning
Making changes to network rules can impact your applications' ability to connect to
Foundry Tools. Setting the default network rule to deny blocks all access to the data unless
specific network rules that grant access are also applied.
Before you change the default rule to deny access, be sure to grant access to any allowed
networks by using network rules. If you allow listing for the IP addresses for your onpremises network, be sure to add all possible outgoing public IP addresses from your onpremises network.

Manage default network access rules
You can manage default network access rules for Foundry Tools resources through the Azure
portal, PowerShell, or the Azure CLI.
Azure portal

1. Go to the Foundry Tools resource you want to secure.
2. Select Resource Management to expand it, then select Networking. To deny access
by default, under Firewalls and virtual networks, select Selected Networks and
Private Endpoints.

ï Š

With this setting alone, unaccompanied by configured virtual networks or address
ranges, all access is effectively denied. When all access is denied, requests that
attempt to consume the Foundry Tools resource aren't permitted. The Azure portal,
Azure PowerShell, or the Azure CLI can still be used to configure the Foundry Tools
resource.
3. To allow traffic from all networks, select All networks.

ï Š

4. Select Save to apply your changes.

Grant access from a virtual network

You can configure Foundry Tools resources to allow access from specific subnets only. The
allowed subnets might belong to a virtual network in the same subscription or in a different
subscription. The other subscription can belong to a different Microsoft Entra tenant. When the
subnet belongs to a different subscription, the Microsoft.CognitiveServices resource provider
needs to be also registered for that subscription.
Enable a service endpoint for Foundry Tools within the virtual network. The service endpoint
routes traffic from the virtual network through an optimal path to Foundry Tools. For more
information, see Virtual Network service endpoints.
The identities of the subnet and the virtual network are also transmitted with each request.
Administrators can then configure network rules for the Foundry Tools resource to allow
requests from specific subnets in a virtual network. Clients granted access by these network
rules must continue to meet the authorization requirements of the Foundry Tools resource to
access the data.
Each Foundry Tools resource supports up to 100 virtual network rules, which can be combined
with IP network rules. For more information, see Grant access from an internet IP range later in
this article.

Set required permissions
To apply a virtual network rule to a Foundry resource, you need the appropriate permissions for
the subnets to add. The required permission is the default Contributor role or the Cognitive
Services Contributor role. Required permissions can also be added to custom role definitions.
The Foundry Tools resource and the virtual networks that are granted access might be in
different subscriptions, including subscriptions that are part of a different Microsoft Entra
tenant.
ï¼— Note
Configuration of rules that grant access to subnets in virtual networks that are a part of a
different Microsoft Entra tenant are currently supported only through PowerShell, the
Azure CLI, and the REST APIs. You can view these rules in the Azure portal, but you can't
configure them.

Configure virtual network rules
You can manage virtual network rules for Foundry Tools resources through the Azure portal,
PowerShell, or the Azure CLI.

Azure portal

To grant access to a virtual network with an existing network rule:
1. Go to the Foundry Tools resource you want to secure.
2. Select Resource Management to expand it, then select Networking.
3. Confirm that you selected Selected Networks and Private Endpoints.
4. Under Allow access from, select Add existing virtual network.

ï Š

5. Select the Virtual networks and Subnets options, and then select Enable.

ï¼— Note
If a service endpoint for Foundry Tools wasn't previously configured for the
selected virtual network and subnets, you can configure it as part of this
operation.
Currently, only virtual networks that belong to the same Microsoft Entra tenant
are available for selection during rule creation. To grant access to a subnet in a
virtual network that belongs to another tenant, use PowerShell, the Azure CLI, or
the REST APIs.

6. Select Save to apply your changes.
To create a new virtual network and grant it access:
1. On the same page as the previous procedure, select Add new virtual network.

ï Š

2. Provide the information necessary to create the new virtual network, and then select
Create.

3. Select Save to apply your changes.
To remove a virtual network or subnet rule:
1. On the same page as the previous procedures, select ...(More options) to open the
context menu for the virtual network or subnet, and select Remove.

ï Š

2. Select Save to apply your changes.

ï¼‰ Important
Be sure to set the default rule to deny, or network rules have no effect.

Grant access from an internet IP range
You can configure Foundry Tools resources to allow access from specific public internet IP
address ranges. This configuration grants access to specific services and on-premises networks,
which effectively block general internet traffic.
You can specify the allowed internet address ranges by using CIDR format (RFC 4632)

in the

form 192.168.0.0/16 or as individual IP addresses like 192.168.0.1 .
îª€ Tip
Small address ranges that use /31 or /32 prefix sizes aren't supported. Configure these
ranges by using individual IP address rules.
IP network rules are only allowed for public internet IP addresses. IP address ranges reserved
for private networks aren't allowed in IP rules. Private networks include addresses that start
with 10.* , 172.16.* - 172.31.* , and 192.168.* . For more information, see Private Address
Space (RFC 1918)

.

Currently, only IPv4 addresses are supported. Each Foundry Tools resource supports up to 100
IP network rules, which can be combined with virtual network rules.

Configure access from on-premises networks
To grant access from your on-premises networks to your Foundry Tools resource with an IP
network rule, identify the internet-facing IP addresses used by your network. Contact your
network administrator for help.
If you use Azure ExpressRoute on-premises for Microsoft peering, you need to identify the NAT
IP addresses. For more information, see What is Azure ExpressRoute.
For Microsoft peering, the NAT IP addresses that are used are either customer provided or
supplied by the service provider. To allow access to your service resources, you must allow
these public IP addresses in the resource IP firewall setting.

Managing IP network rules
You can manage IP network rules for Foundry Tools resources through the Azure portal,
PowerShell, or the Azure CLI.
Azure portal

1. Go to the Foundry Tools resource you want to secure.
2. Select Resource Management to expand it, then select Networking.
3. Confirm that you selected Selected Networks and Private Endpoints.
4. Under Firewalls and virtual networks, locate the Address range option. To grant
access to an internet IP range, enter the IP address or address range (in CIDR format).
Only valid public IP (nonreserved) addresses are accepted.

ï Š

To remove an IP network rule, select the trash can î icon next to the address range.
5. Select Save to apply your changes.

ï¼‰ Important
Be sure to set the default rule to deny, or network rules have no effect.

Use private endpoints
You can use private endpoints for your Foundry Tools resources to allow clients on a virtual
network to securely access data over Azure Private Link. The private endpoint uses an IP
address from the virtual network address space for your Foundry Tools resource. Network
traffic between the clients on the virtual network and the resource traverses the virtual network
and a private link on the Microsoft Azure backbone network, which eliminates exposure from
the public internet.
Private endpoints for Foundry Tools resources let you:
Secure your Foundry Tools resource by configuring the firewall to block all connections
on the public endpoint for Foundry Tools.
Increase security for the virtual network, by enabling you to block exfiltration of data from
the virtual network.

Securely connect to Foundry Tools resources from on-premises networks that connect to
the virtual network by using Azure VPN Gateway or ExpressRoutes with private-peering.

Understand private endpoints
A private endpoint is a special network interface for an Azure resource in your virtual network.
Creating a private endpoint for your Foundry Tools resource provides secure connectivity
between clients in your virtual network and your resource. The private endpoint is assigned an
IP address from the IP address range of your virtual network. The connection between the
private endpoint and Foundry Tools uses a secure private link.
Applications in the virtual network can connect to the service over the private endpoint
seamlessly. Connections use the same connection strings and authorization mechanisms that
they would use otherwise. The exception is Speechs, which require a separate endpoint. For
more information, see Private endpoints with the Speechs in this article. Private endpoints can
be used with all protocols supported by the Foundry Tools resource, including REST.
Private endpoints can be created in subnets that use service endpoints. Clients in a subnet can
connect to one Foundry Tools resource using private endpoint, while using service endpoints to
access others. For more information, see Virtual Network service endpoints.
When you create a private endpoint for a Foundry resource in your virtual network, Azure
sends a consent request for approval to the Foundry Tools resource owner. If the user who
requests the creation of the private endpoint is also an owner of the resource, this consent
request is automatically approved.
Foundry Tools resource owners can manage consent requests and the private endpoints
through the Private endpoint connection tab for the Foundry Tools resource in the Azure
portal

.

Specify private endpoints
When you create a private endpoint, specify the Foundry Tools resource that it connects to. For
more information on creating a private endpoint, see:
Create a private endpoint by using the Azure portal
Create a private endpoint by using Azure PowerShell
Create a private endpoint by using the Azure CLI

Connect to private endpoints

ï¼— Note
Azure OpenAI in Foundry Models uses a different private DNS zone and public DNS zone
forwarder than other Foundry Tools. For the correct zone and forwarder names, see Azure
services DNS zone configuration.

ï¼’ Warning
Requests from clients to the private endpoint MUST specify the custom subdomain of
your Foundry Tools resource as the endpoint base URL. Do NOT call the internal URL
*.privatelink.openai.azure.com which is part of the intermediary CNAME resolution

internal to Azure.
Clients on a virtual network that use the private endpoint use the same connection string for
the Foundry Tools resource as clients connecting to the public endpoint. The exception is the
Speech service, which requires a separate endpoint. For more information, see Use private
endpoints with the Speech service in this article. DNS resolution automatically routes the
connections from the virtual network to the Foundry Tools resource over a private link.
By default, Azure creates a private DNS zone attached to the virtual network with the necessary
updates for the private endpoints. If you use your own DNS server, you might need to make
more changes to your DNS configuration. For updates that might be required for private
endpoints, see Apply DNS changes for private endpoints in this article.

Use private endpoints with the Speech service
See Use Speech service through a private endpoint.

Apply DNS changes for private endpoints
When you create a private endpoint, the DNS CNAME resource record for the Foundry Tools
resource is updated to an alias in a subdomain with the prefix privatelink . By default, Azure
also creates a private DNS zone that corresponds to the privatelink subdomain, with the DNS
A resource records for the private endpoints. For more information, see What is Azure Private
DNS.
When you resolve the endpoint URL from outside the virtual network with the private
endpoint, it resolves to the public endpoint of the Foundry Tools resource. When it's resolved
from the virtual network hosting the private endpoint, the endpoint URL resolves to the private
endpoint's IP address.

This approach enables access to the Foundry Tools resource using the same connection string
for clients in the virtual network that hosts the private endpoints and clients outside the virtual
network.
If you use a custom DNS server on your network, clients must be able to resolve the fully
qualified domain name (FQDN) for the Foundry Tools resource endpoint to the private
endpoint IP address. Configure your DNS server to delegate your private link subdomain to the
private DNS zone for the virtual network.
îª€ Tip
When you use a custom or on-premises DNS server, you should configure your DNS
server to resolve the Foundry Tools resource name in the privatelink subdomain to the
private endpoint IP address. Delegate the privatelink subdomain to the private DNS
zone of the virtual network. Alternatively, configure the DNS zone on your DNS server and
add the DNS A records.
For more information on configuring your own DNS server to support private endpoints, see
the following resources:
Name resolution that uses your own DNS server
DNS configuration

Grant access to trusted Azure services for Azure
OpenAI
You can grant a subset of trusted Azure services access to Azure OpenAI, while maintaining
network rules for other apps. These trusted services will then use managed identity to
authenticate your Azure OpenAI service. The following table lists the services that can access
Azure OpenAI if the managed identity of those services have the appropriate role assignment.
ï¾‰

Service

Resource provider name

Foundry Tools

Microsoft.CognitiveServices

Azure Machine Learning

Microsoft.MachineLearningServices

Azure Search

Microsoft.Search

Expand table

You can grant networking access to trusted Azure services by creating a network rule exception
using the REST API or Azure portal:

Using the Azure CLI
Bash
accessToken=$(az account get-access-token --resource https://management.azure.com -query "accessToken" --output tsv)
rid="/subscriptions/<your subscription id>/resourceGroups/<your resource
group>/providers/Microsoft.CognitiveServices/accounts/<your Foundry Tools resource
name>"
curl -i -X PATCH https://management.azure.com$rid?api-version=2023-10-01-preview \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $accessToken" \
-d \
'
{
"properties":
{
"networkAcls": {
"bypass": "AzureServices"
}
}
}
'

To revoke the exception, set networkAcls.bypass to None .
To verify if the trusted service has been enabled from the Azure portal,
1. Use the JSON View from the Azure OpenAI resource overview page

ï Š

2. Choose your latest API version under API versions. Only the latest API version is
supported, 2023-10-01-preview .

ï Š

Using the Azure portal
1. Navigate to your Azure OpenAI resource, and select Networking from the navigation
menu.
2. Under Exceptions, select Allow Azure services on the trusted services list to access this
cognitive services account.
îª€ Tip
You can view the Exceptions option by selecting either Selected networks and
private endpoints or Disabled under Allow access from.

ï Š

Pricing
For pricing details, see Azure Private Link pricing

.

Next steps
Explore the various Foundry Tools
Learn more about Virtual Network service endpoints

Last updated on 01/06/2026

Security for Foundry Tools
Security should be considered a top priority in the development of all applications, and with
the growth of artificial-intelligence-enabled applications, security is even more important. This
article outlines various security features available for Foundry Tools. Each feature addresses a
specific liability, so multiple features can be used in the same workflow.
For a comprehensive list of Azure service security recommendations, see the Foundry Tools
security baseline article.

Security features
ï¾‰

Expand table

Feature

Description

Transport Layer
Security (TLS)

All of the Foundry Tools endpoints exposed over HTTP enforce the TLS 1.2 protocol, or
optionally TLS 1.3. With an enforced security protocol, consumers attempting to call a
Foundry Tools endpoint should follow these guidelines:
The client operating system (OS) needs to support TLS 1.2 (or 1.3).
The language (and platform) used to make the HTTP call need to specify TLS 1.2
(or 1.3) as part of the request. Depending on the language and platform,
specifying TLS is done either implicitly or explicitly.
For .NET users, consider the Transport Layer Security best practices

Authentication
options

Authentication is the act of verifying a user's identity. Authorization, by contrast, is the
specification of access rights and privileges to resources for a given identity. An
identity is a collection of information about a principal , and a principal can be either
an individual user or a service.
By default, you authenticate your own calls to Foundry Tools using the subscription
keys provided; this is the simplest method but not the most secure. The most secure
authentication method is to use managed roles in Microsoft Entra ID. To learn about
this and other authentication options, see Authenticate requests to Foundry Tools.

Key rotation

Each Foundry resource has two API keys to enable secret rotation. This is a security
precaution that lets you regularly change the keys that can access your service,
protecting the privacy of your service if a key gets leaked. To learn about this and
other authentication options, see Rotate keys.

Environment
variables

Environment variables are name-value pairs that are stored within a specific
development environment. Environment variables are more secure than using
hardcoded values in your code. For instructions on how to use environment variables
in your code, see the Environment variables guide.
However, if your environment is compromised, the environment variables are

Feature

Description
compromised as well, so this isn't the most secure approach. The most secure
authentication method is to use managed roles in Microsoft Entra ID. To learn about
this and other authentication options, see Authenticate requests to Foundry Tools.

Customer-

This feature is for services that store customer data at rest (longer than 48 hours).

managed keys
(CMK)

While this data is already double-encrypted on Azure servers, users can get extra
security by adding another layer of encryption, with keys they manage themselves. You
can link your service to Azure Key Vault and manage your data encryption keys there.
Check to see if CMK is supported by the service that you want to use in the Customermanaged keys documentation.

Virtual networks

Virtual networks allow you to specify which endpoints can make API calls to your
resource. The Azure service rejects API calls from devices outside of your network. You
can set a formula-based definition of the allowed network, or you can define an
exhaustive list of endpoints to allow. This is another layer of security that can be used
in combination with others.

Data loss
prevention

The data loss prevention feature lets an administrator decide what types of URIs their
Azure resource can take as inputs (for those API calls that take URIs as input). This can
be done to prevent the possible exfiltration of sensitive company data: If a company
stores sensitive information (such as a customer's private data) in URL parameters, a
bad actor inside that company could submit the sensitive URLs to an Azure service,
which surfaces that data outside the company. Data loss prevention lets you configure
the service to reject certain URI forms on arrival.

Customer

The Customer Lockbox feature provides an interface for customers to review and

Lockbox

approve or reject data access requests. It's used in cases where a Microsoft engineer
needs to access customer data during a support request. For information on how
Customer Lockbox requests are initiated, tracked, and stored for later reviews and
audits, see the Customer Lockbox guide.
Customer Lockbox is available for the following services:
Azure OpenAI
Translator
Conversational language understanding
Custom text classification
Custom named entity recognition
Orchestration workflow

Bring your own

The Speech service doesn't currently support Customer Lockbox. However, you can

storage (BYOS)

arrange for your service-specific data to be stored in your own storage resource using
bring-your-own-storage (BYOS). BYOS allows you to achieve similar data controls to
Customer Lockbox. Keep in mind that Speech service data stays and is processed in
the Azure region where the Speech resource was created. This applies to any data at
rest and data in transit. For customization features like Custom Speech and Custom
Voice, all customer data is transferred, stored, and processed in the same region where

Feature

Description
the Speech service resource and BYOS resource (if used) reside.
To use BYOS with Speech, follow the Speech encryption of data at rest guide.
Microsoft doesn't use customer data to improve its Speech models. Additionally, if
endpoint logging is disabled and no customizations are used, then no customer data
is stored by Speech.

Next step
Explore Foundry Tools and choose a service to get started.

Last updated on 11/18/2025

Language encryption of data at rest
The Language automatically encrypts your data when persisted to the cloud. The Language
encryption protects your data and helps you meet your organizational security and compliance
commitments.

Foundry Tools encryption
Data is encrypted and decrypted using FIPS 140-2

compliant 256-bit AES

encryption.

Encryption and decryption are transparent, meaning encryption and access are managed for
you. Your data is secure by default and you don't need to modify your code or applications to
take advantage of encryption.

About encryption key management
By default, your subscription uses Microsoft-managed encryption keys. There's also the option
to manage your subscription with your own keys called customer-managed keys (CMK). CMK
offers greater flexibility to create, rotate, disable, and revoke access controls. You can also audit
the encryption keys used to protect your data.

Customer-managed keys with Azure Key Vault
There's also an option to manage your subscription with your own keys. Customer-managed
keys (CMK), also known as Bring your own key (BYOK), offer greater flexibility to create, rotate,
disable, and revoke access controls. You can also audit the encryption keys used to protect
your data.
You must use Azure Key Vault to store your customer-managed keys. You can either create
your own keys and store them in a key vault, or you can use the Azure Key Vault APIs to
generate keys. The Microsoft Foundry resource and the key vault must be in the same region
and in the same Microsoft Entra tenant, but they can be in different subscriptions. For more
information about Azure Key Vault, see What is Azure Key Vault?.

Enable customer-managed keys
A new Foundry resource is always encrypted using Microsoft-managed keys. It's not possible to
enable customer-managed keys at the time that the resource is created. Customer-managed
keys are stored in Azure Key Vault. The key vault must be provisioned with access policies that
grant key permissions to the managed identity that is associated with the Foundry resource.

The managed identity is available only after the resource is created using the Pricing Tier for
CMK.
To learn how to use customer-managed keys with Azure Key Vault for Foundry Tools
encryption, see:
Configure customer-managed keys with Key Vault for Foundry Tools encryption from the
Azure portal
Enabling customer managed keys also enable a system assigned managed identity, a feature of
Microsoft Entra ID. Once the system assigned managed identity is enabled, this resource is
registered with Microsoft Entra ID. After being registered, the managed identity is given access
to the Key Vault selected during customer managed key setup. You can learn more about
Managed Identities.
ï¼‰ Important
If you disable system assigned managed identities, access to the key vault is removed and
any data encrypted with the customer keys are no longer accessible. Any features
dependent on this data stop working.

ï¼‰ Important
Managed identities don't currently support cross-directory scenarios. When you configure
customer-managed keys in the Azure portal, a managed identity is automatically assigned
under the covers. If you move a subscription, resource group, or resource to another
Microsoft Entra directory, the managed identity doesn't transfer to the new tenant. Thus,
the customer-managed keys may no longer work. For more information, see Transferring
a subscription between Microsoft Entra directories in FAQs and known issues with
managed identities for Azure resources.

Store customer-managed keys in Azure Key Vault
To enable customer-managed keys, you must use an Azure Key Vault to store your keys. You
must enable both the Soft Delete and Do Not Purge properties on the key vault.
Only RSA keys of size 2048 are supported with Foundry Tools encryption. For more information
about keys, see Key Vault keys in About Azure Key Vault keys, secrets, and certificates.

Rotate customer-managed keys

You can rotate a customer-managed key in Azure Key Vault according to your compliance
policies. When the key is rotated, you must update the Foundry resource to use the new key
URI. To learn how to update the resource to use a new version of the key in the Azure portal,
see the section titled Update the key version in Configure customer-managed keys for
Foundry Tools by using the Azure portal.
Rotating the key doesn't trigger re-encryption of data in the resource. There's no further action
required from the user.

Revoke access to customer-managed keys
To revoke access to customer-managed keys, use PowerShell or Azure CLI. For more
information, see Azure Key Vault PowerShell

or Azure Key Vault CLI. Revoking access

effectively blocks access to all data in the Foundry resource, as the encryption key is
inaccessible by Foundry Tools.

Next steps
Learn more about Azure Key Vault

Last updated on 11/18/2025

Migrating to Azure Language in Foundry
Tools
On November 2, 2021, Azure Language in Foundry Tools was released into public preview.
Azure Language unifies the Text Analytics, QnA Maker, and Language Understanding (LUIS)
service offerings, and provides several new features as well.

Do I need to migrate to Azure Language if I'm
using Text Analytics?
Text Analytics is incorporated into Azure Language, and its features are still available. If you're
using Text Analytics features, your applications should continue to work without breaking
changes. If you're using Text Analytics API (v2.x or v3), see the Text Analytics migration guide to
migrate your applications to the unified Language endpoint and the latest client library.
Consider using one of the available quickstart articles to see the latest information on service
endpoints, and API calls.

How do I migrate to Azure Language if I'm using
Language Understanding (LUIS)?
If you're using Language Understanding (Language Understanding (LUIS)), you can [import
your Language Understanding (LUIS) JSON file](../conversational-languageunderstanding/how-to/migrate-from-Language Understanding (LUIS).md) to the new
Conversational language understanding feature.

How do I migrate to Azure Language if I'm using
QnA Maker?
If you're using QnA Maker, see the migration guide for information on migrating knowledge
bases from QnA Maker to question answering.

See also
Azure Language in Foundry Tools overview
Conversational language understanding overview
Question answering overview

Last updated on 01/23/2026

Migrate from Text Analytics API to Azure
Language Service API
îª€ Tip
Just getting started with Azure Language in Foundry Tools? See the overview article for
details on the service, available features, and links to quickstarts for information on the
current version of the API.
If your applications are still using the Text Analytics API, or client library (before stable v5.1.0),
this article helps you upgrade your applications to use the latest version of the Azure Language
in Foundry Tools features.

Unified Language endpoint (REST API)
This section applies to applications that use the older /text/analytics/... endpoint format for
REST API calls. For example:
HTTP
https://<your-customsubdomain>.cognitiveservices.azure.com/text/analytics/<version>/<feature>

If your application used the endpoint format, the REST API endpoint for the following
Language features is changed:
Entity linking
Key phrase extraction
Language detection
Named entity recognition (NER)
Personally Identifying Information (PII) detection
Sentiment analysis and opinion mining
Text analytics for health
The Language now provides a unified endpoint for sending REST API requests to these
features. If your application uses the REST API, update its request endpoint to use the current
endpoint:
HTTP

https://<your-language-resource-endpoint>/language/:analyze-text?api-version=202205-01

Additionally, the format of the JSON request body is changed. You need to update the request
structure that your application sends to the API, for example the following entity recognition
JSON body:
JSON
{
"kind": "EntityRecognition",
"parameters": {
"modelVersion": "latest"
},
"analysisInput":{
"documents":[
{
"id":"1",
"language": "en",
"text": "I had a wonderful trip to Seattle last week."
}
]
}
}

Use the quickstarts links to see current example REST API calls for the feature you're using, and
the associated API output.

Client libraries
To use the latest version of the client library, you need to download the latest software package
in the Azure.AI.TextAnalytics namespace. See the quickstart articles, for example, code and
instructions for using the client library in your preferred language.

Version 2.1 functionality changes
If you're migrating an application from v2.1 of the API, there are several changes to feature
functionality you should be aware of.

Sentiment analysis v2.1
Sentiment Analysis in version 2.1 returns sentiment scores between 0 and 1 for each document
sent to the API, with scores closer to one indicating more positive sentiment. The current

version of this feature returns sentiment labels (such as "positive" or "negative") for both the
sentences and the document as a whole, and their associated confidence scores.

NER, PII, and entity linking v2.1
In version 2.1, the Text Analytics API used one endpoint for Named Entity Recognition (NER)
and entity linking. The current version of this feature provides expanded named entity
detection, and has separate endpoints for NER and entity linking requests. Additionally, you
can use another feature offered in Azure Language that lets you detect detect personal (PII)
and health (PHI) information.
You also need to update your application to use the entity categories returned in the API's
response.

Version 2.1 entity categories
The following table lists the entity categories returned for NER v2.1.
ï¾‰

Expand table

Category

Description

Person

Names of people.

Location

Natural and human-made landmarks, structures, geographical features, and geopolitical
entities

Organization

Companies, political groups, musical bands, sport clubs, government bodies, and public
organizations. Nationalities and religions aren't included in this entity type.

PhoneNumber

Phone numbers (US and EU phone numbers only).

Email

Email addresses.

URL

URLs to websites.

IP

Network IP addresses.

DateTime

Dates and times of day.

Date

Calender dates.

Time

Times of day

DateRange

Date ranges.

TimeRange

Time ranges.

Category

Description

Duration

Durations.

Set

Set, repeated times.

Quantity

Numbers and numeric quantities.

Number

Numbers.

Percentage

Percentages.

Ordinal

Ordinal numbers.

Age

Ages.

Currency

Currencies.

Dimension

Dimensions and measurements.

Temperature

Temperatures.

Language detection v2.1
The language detection feature output is changed in the current version. The JSON response
contains ConfidenceScore instead of score . The current version also only returns one language
for each document.

Key phrase extraction v2.1
The key phrase extraction feature functionality currently isn't changed outside of the endpoint
and request format.

See also
What is Azure Language in Foundry Tools?
Language developer guide
See the following reference documentation for information on previous API versions.
Version 2.1
Version 3.0
Version 3.1
Use the following quickstart guides to see examples for the current version of these
features.
Entity linking
Key phrase extraction

Named entity recognition (NER)
Language detection
Personally Identifying Information (PII) detection
Sentiment analysis and opinion mining
Text analytics for health

Last updated on 01/23/2026

Azure Language in Foundry Tools release
history
Azure Language is a robust cloud-based service that offers advanced natural language
processing (NLP) capabilities for text analysis and understanding. Azure Language utilizes AI
and machine learning to enable developers to build intelligent applications. Key features
include sentiment analysis, key phrase extraction, named entity recognition, and language
detection. Here, you can explore key milestones and enhancements in the evolution of Azure
Language. For more information on recent advances, see What's new?.

December 2023
Text Analytics for health new model 2023-12-01 is now available.
New Relation Type: BodySiteOfExamination
Quality enhancements to support radiology documents
Significant latency improvements
Various bug fixes: Improvements across NER, Entity Linking, Relations, and Assertion
Detection

November 2023
Named Entity Recognition Container is now Generally Available (GA).

July 2023
Custom sentiment analysis is now available in preview.

May 2023
Custom Named Entity Recognition (NER) Docker containers are now available for onpremises deployment.

April 2023
Custom Text analytics for health is available in public preview, which enables you to build
custom AI models to extract healthcare specific entities from unstructured text
You can now use Azure OpenAI to automatically label or generate data during authoring.
Learn more with the following links:

Autolabel your documents in Custom text classification or Custom named entity
recognition.
Generate suggested utterances in Conversational language understanding.
The latest model version ( 2022-10-01 ) for Language Detection now supports 6 more
International languages and 12 Romanized Indic languages.

March 2023
New model version ('2023-01-01-preview') for Personally Identifiable Information (PII)
detection with quality updates and new language support
New versions of the text analysis client library are available in preview:
C#

Package (NuGet)
Changelog/Release History
ReadMe
Samples

February 2023
Conversational language understanding and orchestration workflow now available in the
following regions in the sovereign cloud for China:
China East 2 (Authoring and Prediction)
China North 2 (Prediction)
New model evaluation updates for Conversational language understanding and
Orchestration workflow.
New model version ('2023-01-01-preview') for Text Analytics for health featuring new
entity categories for social determinants of health.
New model version ('2023-02-01-preview') for named entity recognition features
improved accuracy and more language support with up to 79 languages.

December 2022

New version (v5.2.0-beta.1) of the text analysis client library is available in preview for
C#/.NET:
Package (NuGet)
Changelog/Release History
ReadMe
Samples
New model version ( 2022-10-01 ) released for Language Detection. The new model
version comes with improvements in language detection quality on short texts.

November 2022
Expanded language support for:
Opinion mining
Conversational PII now supports up to 40,000 characters as document size.
New versions of the text analysis client library are available in preview:
Java
Package (Maven)
Changelog/Release History
ReadMe
Samples
JavaScript
Package (npm)
Changelog/Release History
ReadMe
Samples
Python
Package (PyPi)
Changelog/Release History
ReadMe
Samples

October 2022
The summarization feature now has the following capabilities:
Document summarization:
Abstractive summarization, which generates a summary of a document that can't
use the same words as presented in the document, but captures the main idea.

Conversation summarization
Chapter title summarization, which returns suggested chapter titles of input
conversations.
Narrative summarization, which returns call notes, meeting notes or chat summaries
of input conversations.
Expanded language support for:
Sentiment analysis
Key phrase extraction
Named entity recognition
Text Analytics for health
Multi-region deployment and project asset versioning for:
Conversational language understanding
Orchestration workflow
Custom text classification
Custom named entity recognition
Regular expressions in conversational language understanding and required components,
offering an added ability to influence entity predictions.
Entity resolution in named entity recognition
New region support for:
Conversational language understanding
Orchestration workflow
Custom text classification
Custom named entity recognition
Document type as an input supported for Text Analytics for health FHIR requests

September 2022
Conversational language understanding is available in the following regions:
Central India
Switzerland North
West US 2
Text Analytics for Health now supports more languages in preview: Spanish, French,
German Italian, Portuguese and Hebrew. These languages are available when using a
docker container to deploy the API service.
The Azure.AI.TextAnalytics client library v5.2.0 are generally available and ready for use in
production applications. For more information on Language client libraries, see the
Developer overview.
Java
Package (Maven)
Changelog/Release History

ReadMe
Samples
Python
Package (PyPi)
Changelog/Release History
ReadMe
Samples
C#/.NET
Package (NuGet)
Changelog/Release History
ReadMe
Samples

August 2022
Role-based access control for Azure Language.

July 2022
New AI models for sentiment analysis and key phrase extraction based on z-code
models

, providing:

Performance and quality improvements for the following 11 languages supported by
sentiment analysis: ar , da , el , fi , hi , nl , no , pl , ru , sv , tr
Performance and quality improvements for the following 20 languages supported by
key phrase extraction: af , bg , ca , hr , da , nl , et , fi , el , hu , id , lv , no , pl , ro , ru ,
sk , sl , sv , tr

Conversational PII is now available in all Azure regions supported by Azure Language.
A new version of Azure Language API ( 2022-07-01-preview ) is available. It provides:
Automatic language detection for asynchronous tasks.
Text Analytics for health confidence scores are now returned in relations.
To use this version in your REST API calls, use the following URL:
HTTP
<your-language-resource-endpoint>/language/:analyze-text?api-version=2022-0701-preview

June 2022
v1.0 client libraries for conversational language understanding and orchestration
workflow are Generally Available for the following languages:
C#
Python
v1.1.0b1 client library for conversation summarization is available as a preview for:
Python
There's a new endpoint URL and request format for making REST API calls to prebuilt
Language features. See the following quickstart guides and reference documentation for
information on structuring your API calls. All text analytics 3.2-preview.2 API users can
begin migrating their workloads to this new endpoint.
Entity linking
Language detection
Key phrase extraction
Named entity recognition
PII detection
Sentiment analysis and opinion mining
Text analytics for health

May 2022
PII detection for conversations.
Rebranded Text Summarization to Document summarization.
Conversation summarization is now available in public preview.
The following features are now Generally Available (GA):
Custom text classification
Custom Named Entity Recognition (NER)
Conversational language understanding
Orchestration workflow
The following updates for custom text classification, custom Named Entity Recognition
(NER), conversational language understanding, and orchestration workflow:
Data splitting controls.
Ability to cancel training jobs.
Custom deployments can be named. You can have up to 10 deployments.
Ability to swap deployments.
Auto labeling (preview) for custom named entity recognition
Enterprise readiness support

Training modes for conversational language understanding
Updated service limits
Ability to use free (F0) tier for Language resources
Expanded regional availability
Updated model life cycle to add training configuration versions

April 2022
Fast Healthcare Interoperability Resources (FHIR) support is available in the Language
REST API preview for Text Analytics for health.

March 2022
Expanded language support for:
Custom text classification
Custom Named Entity Recognition (NER)
Conversational language understanding

February 2022
Model improvements for latest model-version for text summarization
Model 2021-10-01 is Generally Available (GA) for Sentiment Analysis and Opinion Mining,
featuring enhanced modeling for emojis and better accuracy across all supported
languages.
Question Answering: Active learning v2 incorporates a better clustering logic providing
improved accuracy of suggestions. It considers user actions when suggestions are
accepted or rejected to avoid duplicate suggestions, and improve query suggestions.

December 2021
The version 3.1-preview.x REST endpoints and 5.1.0-beta.x client library are retired.
Upgrade to the General Available version of the API(v3.1). If you're using the client
libraries, use package version 5.1.0 or higher. See the migration guide for details.

November 2021

Based on ongoing customer feedback, we increased the character limit per document for
Text Analytics for health from 5,120 to 30,720.
Azure Language release, with support for:
Question Answering (now Generally Available)
Sentiment Analysis and opinion mining
Key Phrase Extraction
Named Entity Recognition (NER), Personally Identifying Information (PII)
Language Detection
Text Analytics for health
Text summarization preview
Custom Named Entity Recognition (Custom NER) preview
Custom Text Classification preview
Conversational Language Understanding preview
Preview model version 2021-10-01-preview for Sentiment Analysis and Opinion mining,
which provides:
Improved prediction quality.
Added language support for the opinion mining feature.
For more information, see the project z-code site

.

To use this model version, you must specify it in your API calls, using the model version
parameter.
SDK support for sending requests to custom models:
Custom Named Entity Recognition
Custom text classification
Custom language understanding

Last updated on 01/23/2026

